Sequential Pattern Mining with Multiple Minimum 
Supports: a Tree Based Approach 
Ya-Han Hu
Department of Information 
Management 
National Chung Cheng University 
Chia-Yi, Taiwan, R.O.C. 
yahan.hu@mis.ccu.edu.tw 
Fan Wu
Department of Information 
Management 
National Chung Cheng University 
Chia-Yi, Taiwan, R.O.C. 
kfwu@mis.ccu.edu.tw 
Yi-Chun Liao 
Department of Information 
Management 
National Chung Cheng University 
Chia-Yi, Taiwan, R.O.C. 
wagajim@gmail.com 
Abstract—Frequent pattern mining is an important data-mining 
method for determining correlations among items/itemsets. Since 
the frequencies for various items are always varied, specifying a 
single minimum support cannot exactly discover interesting 
patterns. To solve this problem, Liu et al. propose an apriori-
based method to include the concept of multiple minimum 
supports (MMS in short) on association rule mining. It allows 
user to specify MMS to reflect the different natures of items. 
Since the mining of sequential pattern may face the same 
problem, we extend the traditional definition of sequential 
patterns to include the concept of MMS in this study. For 
efficiently discovering sequential patterns with MMS, we develop 
a data structure, named PLMS-tree, to store all necessary 
information from database. After that, a pattern growth method,
named MSCP-growth, is developed to discover all sequential 
patterns with MMS from PLMS-tree. 
Keywords: Sequential pattern, multiple minimum supports,
pattern growth 
I. INTRODUCTION
Sequential pattern mining is one of the most important 
research issues in data mining, which was first introduced by 
Agrawal and Srikant[2] and can be described as follows: we 
are given a set of data-sequences, as the input data. Each data-
sequence contains an ordered list of transactions (itemsets). 
Given a user-specified minimum support threshold (minsup),
sequential pattern mining finds all subsequences with 
frequencies larger than minsup in a sequence database. 
Some previous studies on association analysis [1, 5-7]
indicate that using only minsup as single frequency threshold 
does not deliberate on the nature of items in real-life 
applications. That is, single minsup implicitly assumes that all 
items in the database have similar frequency. However, some 
items may appear very frequently in the database while others 
rarely appear. Under such circumstance, if we set the value of 
minsup too high, we will not find those rules involving rare 
items in the database. On the other hand, if we set that value 
too low, it will generate a huge amount of meaningless patterns. 
Therefore, Liu et al. [5] first address this dilemma (also known 
as the rare item problem) and propose the concept of multiple 
minimum supports (MMS in short) to redefine the problem of 
association rule mining. The method allows users to specify 
different minsups for different items (MIS values) to reflect 
their unique natures and generate different association rules, 
depending on which items are in the patterns. 
Recently there have been some extended studies on MMS
[1, 5, 7-9]. These researches show that MMS can both enhance 
the efficiency of the mining process and generate more 
interesting patterns. However, most of previous studies deal 
with the rare item problem on association rule mining but 
rarely on sequential pattern mining. In this study, we first 
extend the definitions of sequential pattern with consideration 
to MMS, which allows users to specify multiple item supports 
to reflect different frequencies of items. Items with low 
frequencies will be specified with lower minimum supports and 
patterns involving these items can be easily retrieved for 
further decision support.  
The major problem on frequent pattern mining with MMS 
is that the downward closure property no longer holds in the 
mining process, which means that a super-pattern of an 
infrequent pattern might be a frequent pattern. To effectively 
reduce the search space in level-wise methods, Liu et al. [5]
proposes the sorted closure property, where all items in the 
dataset are sorted in ascending order by their MIS values. Keep 
this ordering in subsequent operations and let k-patterns 
generated from (k-1)-pattern share the same prefix, i.e. the 
smallest minsup among all items in k-patterns, the complete set 
of frequent patterns with MMS can be easily discovered. The 
sorted closure property, however, is invalid in sequential 
pattern mining since the order of events in a data-sequence 
cannot be altered. Therefore, to discover complete set of 
sequential patterns with MMS is not straightforward. Base on 
the new definitions of sequential pattern with MMS, we first 
propose an extended version of the PLWAP-tree structure [4],
named Preorder Linked Multiple Supports tree (PLMS-tree), to 
compress and store all necessary information from sequence 
databases. After that, we extend PLWAP-Mine to develop an 
efficient algorithm, named Multiple Supports Candidate 
Pattern growth (MSCP-growth), for discovering complete set 
of sequential patterns with MMS. We extend PLWAP-tree and 
PLWAP-Mine because they have been proven to be a compact 
data structure and a highly efficient algorithm on sequential 
pattern mining. 
The structure of this paper is as follows. In Section 2, we 
first review the concept of PLWAP-tree, which is considered as 
the basis of our approach. Section 3 introduces the definition of 
sequential pattern mining with MMS. Our data structure 
428
PLMS-tree and the mining algorithm MSCP-growth will be 
presented in Section 4, and we have a conclusion in Section 5. 
II. RELATED WORK
Preorder Linked WAP tree (PLWAP-tree) algorithm is 
proposed by Ezeife et al. [4]. Extended from WAP-tree [3], 
PLWAP-tree enhances its performance by avoiding 
reconstruction the intermediate tree. The PLWAP-tree stores 
the sequence databases into a preorder linked tree, and each 
node in the PLWAP-tree has a unique binary code to indicate 
its position. The position code makes this tree be able to 
directly recognize the ancestors and descendants of a node by 
comparing the assigned position codes between nodes and 
avoid re-constructing the intermediate trees during the mining 
process. In order to makes it easier to explain how to 
implement the position codes, the PLWAP-tree uses an 
equivalent binary tree to represent the original tree. Given a 
node n, n’s first child node in the original tree is n’s left child 
node in equivalent binary tree and n’s first sibling node in the 
original tree is n’s right child node in equivalent tree. Fig. 1 is 
an example on describing how to represent WAP-tree as an 
equivalent binary tree and how to assign the position code on 
each node.  
The rule of assigning the position code is as follows, given 
an equivalent binary tree, and nodes in the tree including a
node n1, a left child of n1 named n2 and a right child node of n1
named n3. Given n1’s position code is “a”. n2’s position code 
will be assigned with an additional 1 in the bottom of “a” and 
becomes “a1”. n3’s position code will be assigned with an 
additional 1 in the bottom of “a” and becomes “a0”. For 
example, the leftmost child of root node in tree is B, node B 
will be the root node of equivalent binary tree. The leftmost 
child node of B is D, and the first sibling node of B is C, so the 
equivalent binary tree is represented as Fig. 1 and the position 
codes of B, D and C are “1”, “11” and “10”, respectively. The 
other nodes are assigned in the same way. 
Figure 1. Identify the decendants by position code 
Fig. 1 also explains how to use position codes on identifing 
relationships between ancestors and descendants of a given 
node. Given a node n with positon code “a”, suppose we 
assighs an additional 1 in the bottom of “a” and becomes “a1”,
all nodes with the same prefix “a1” are descendants of n. For 
example, the position code of node B is “1”. We assigns an 
additional 1 in the end of B’s postion code, i.e. “11”, and each 
node with prefix “11” is the descendant of node B. 
III. PROBLEM DEFINITION
In this section, we formally give definitions used in the 
discovery of sequential pattern with MMS. Let I denote the set 
of items in the database, and a subset of I is called an itemset. 
A customer’s data-sequence is an ordered list of items with 
time stamps. Therefore, a sequence, say ?, can be represented 
as <(a1: t1), (a2: t2), (a3: t3), …, (an: tn)>, where aj is an item, 
and tj stands for the time when aj occurs, 1 ? j?n, and tj-1?
tj for 2 ? j? n. If several items occur at the same time in the 
sequence, they are ordered alphabetically. 
Definition 1. Given an itemset Iq = (i1, i2...im), we say 
itemset Iq occurs in ? if integers 1 ? k1 < k2 <…< km? n exist 
such that, i1 = ak1, i2 = ak2, ..., im = akm and tk1= tk2=...= tkm. We 
refer to k1 and tk1 jointly as the position and the time that Iq
occurs in ?, respectively.  
Definition 2. Let ? = <I1, I2...Is> (Iq  I for 1 ? q? s) be 
a sequence of itemset. Assume that each Iq in ? occurs in ?.
Then we say sequence ? occurs in ?, or is a subsequence of ? if 
tI1 < tI2<...< tIs, where tIq (1 ? q ? s) is the time, at which Iq
occurs in ?. 
Definition 3. A sequence database S is formed by a set of 
records < sid, s >, where sid is the identifier of this data-
sequence and s is a data-sequence. For a given sequence ?, the 
support count of sequence ? in S are defined as follows:
supp(?) = |{(sid, s)| (sid, s)  S  ? is a subsequence in s}|
The following definitions are related to the concept of 
MMS. In this model, the definition of the minimum support is 
changed. Each item in the database can have its minsup, which 
is expressed in terms of minimum item support (MIS). In other 
words, users can specify different MIS values for different 
items.  
Definition 4. Let MIS(i) denote the MIS value of item i (i
 I). Given an itemset Iq = (i1,i2,...,im), the MIS value of 
itemset Iq = (i1,i2,...,im) (1?k?m), denoted as MIS(Iq), is equal 
to: 
min[ MIS(i1), MIS(i2),…, MIS(im)]
Definition 5. Given a sequence ? = <I1.I2...Is> (Iq  I for 1 
? q ? s), the minimum support threshold of ?, denoted as 
MIS(?), is equal to: 
min[ MIS(I1), MIS(I2),…, MIS(Is)]
Definition 6. Given a sequence database S and a sequence ?,
we call ? is frequent in S or ? is a sequential pattern in S if 
supp(?)? MIS(?).
By providing different minimum item support thresholds 
for different items, the user can effectively express different 
support requirements for different data-sequences. MMS allow 
us to have higher minimum supports for sequences involving 
highly frequent items, and also allow us to have lower 
minimum supports for sequences involving rare items. In 
summary, the problem we addressed can be stated as follows. 
Given a sequence database S and a set of MIS values for all 
items in S, we discover all sequential patterns that satisfy 
MIS(?).  
429
IV. THE PROPOSED ALGORITHM
In this section, we first proposed a PLWAP-tree-like 
algorithm, called PLMS-tree, to store all necessary information 
from sequence database. After that, we than propose a new 
pattern growth algorithm, named MSCP-growth, to generate 
the complete set of sequential patterns with MMS based on the 
PLMS-tree.  
A. PLMS-tree construction 
The PLMS-tree is an extension of PLWAP-tree. But there 
are still some differences between these two tree structures.
The first is that PLWAP-tree contains only items with support 
no less than minsup while PLMS-tree retain items with support 
no less than MIS(I), i.e. the smallest MIS value among all items. 
It is because items with support no less than MIS(I) have
possibilities to be part of sequential patterns. The second is that 
in PLMS-tree we need to add some additional information into 
the tree structure for discovering complete set of sequential 
patterns. This part will be discussed later. 
Similar to PLWAP-tree, PLMS-tree is composed of a 
header table and tree nodes. The header table stored related 
information of items in the sequence database, where iname
records item’s name, mis records the MIS value of this item 
and nlink records the link to the first node in the tree node 
shares the same item-name with this item in prefix order. Items 
in the header table are sorted in increasing order based on its 
MIS value.  
The tree node stores the information including iname, iflag,
icount, mps, pcode and the related node links (plink, clink, slink
and nlink). The iflag and mps are two new fields. The iflag is 
used to identify the last item of a transaction. Since we consider 
multiple items in an event, the mining process has to 
distinguish whether the nodes with item-name=i and their 
suffix are in the same itemset. The mps is the minimum 
possible support, which is equal to the minimum MIS value 
among this node and all nodes in its suffix tree. This value is 
used to be part of threshold in our candidate pattern growth 
algorithm to reduce the number of candidate patterns. The 
iname records which item this node represents. The icount
records the number of sequences represented by the portion of 
the path reaching this node. The pcode is position code used to 
identify the ancestor and descendent of this node. The related 
node links, including plink, clink, slink and nlink, are linked to 
its parent node, first child node, first sibling node and the next 
node sharing the same item name, respectively. 
We use an simple tree with three sequences, (ab)(a), (abc)
and (ac), to explain our tree in Fig. 2 first. Node (a:3) is not the 
last item of an itemset, it’s iflag is false. Node (_c:1) is the last 
item of an itemset, it’s iflag is true. The mps value of node (a:3)
is 30% because the minimum MIS value in it’s suffix tree is 
30%. The represent tree is used to describe the sequence 
database and the data structure tree shows the related node 
links between nodes, in the following content, we will use the 
represent to describe our example.  
Based on the definitions of PLMS-tree, we proposed a 
PLMS-tree construction algorithm. As shown in Fig. 3, and we
will use an example to explain the PLMS-tree constructing 
steps as follows. 
Figure 2. Example tree 
Input : sequence database S and the MIS vale of each item in I
Output : PLMS-tree T, header table HT
Method : tree_construction(S, MIS value of each item i in I)
1. Initial PLMS-tree T // create root
2. Initial header table HT based on MIS value for each item i in I
3. For each sequence s in S
4. Call insert_tree(s, T) //insert s into the tree
5. End loop
6. For each node n in T
7. If with supp(n.iname) < MIS(I) then
8.         Call prune(i , T) //prune items that cant reach the threshold
9. End if
10. End loop
11. For each node n in T
12.      Call merge(T) //merge nodes and give position code to each node
13.      If n is a leaf node of T
14.          Call thres_set(T) //set the threshold to each node
15.      End if
16. End loop
17. Call bulidLinkage(root of T) //build linkage with the same iname
Figure 3. Algorithm 2 – Tree construction 
In the following example, node n1 (iname1:icount1:pcode1)
denotes n1’s item name, item count and position code,
respectively. The path from n1 to its descendent nr is denoted as 
(iname1:icount1:pcode1)… (inamer:icountr:pcoder). Given 
a sequence database and assume the MIS value of each item is 
defined as follows: a:70%, b: 80%, c:40%, d:65%, e:30%, 
f:40%. Based on the MIS values above, we can sort each 
sequence shown in table I. 
TABLE I. SORTED SEQUENCE DATABASE
SID Unsorted sequence Sorted sequence
10 <(ac)(ab)(bc)> <(ca)(ab)(cb)>
20 <(ac)(ab)(c)> <(ca)(ab)(c)>
30 <(a)(e)(c)(a)(b)> <(a)(e)(c)(a)(b)>
40 <(ac)(f)(ab)(d)> <(ca)(f)(ab)(d)>
50 <(a)(e)(c)(d)(b)> <(a)(e)(c)(d)(b)>
At the first step, we create a root node of tree T labeled as 
“NULL”. A header table HT records all items’ names and their 
MIS values. The first scan of database S inserts the sequences s
into T by turns. We use the first two sequences to explain the 
insertion process as follows. 
1. The first sequence s10 = <(ca)(ab)(cb)>. Since the child 
node of root is NULL, we create new nodes as Fig. 4(a). Noted 
that each sequences in the S is a combination of itemsets, we 
mark the last item i of each itemset as _i to distinguish among 
430
itemsets, and the items in the same itemset are sorted by their 
MIS value in increasing order.
2. For the second sequence s20, it shares the same prefix 
(ca)(ab)(c) with the s10. But our study wants to discuss about 
the situation of itemset, the last item “(c)” in s20 is the last item 
of one itemset, and the same item in s10 is not, the last item 
“(c)” in s20 will be considered as a different item with item 
“(c)” in s10. The icount in branch (c:1)  (_a:1)  (a:1) 
(_b:1) are increased by 1 because they shares the same prefix,
but the next item of s20, i.e. (c), is not the same node with s10.
Item (c) in s20 cannot share node with the item c in s10, it will be 
a new child node (_c:1) of (_b:2) as Fig. 4(b).
The remaining sequences will be inserted in the same way, 
the insert algorithm is shown at Fig. 5 and the result after step 1 
is shown at Fig. 6(a). 
Figure 4(a)    Figure 4 (b) 
Figure 4. Tree construction steps 
Input : sequence s , PLMS-tree T
Method : Insert_tree(s , T)
1. point to root
2. For each itemset t in s
3. Sort all items in t based on their MIS values in increasing order
4. If item i is the last item of t then
5.         lastitem = TRUE //distinguish between itemset
6. End if
7.  For each item i in t
8.         If there exists a child node e which e.iname = i.iname and the 
same lastitem value then
9.             e.icount ++
10.             Point to e
11.         Else 
12.             Create a new node e with e.iname = i.iname
13.             Set the related information about e
14.             Point to e
15.         End if
16. End loop
17. If i appears in s the first time
18.         supp(i) ++  //count item i’s support
19. End if
20. End loop
Figure 5. Method: insert tree – insert S into tree 
At the second step, since all items in S are inserted into T, it 
possibly has items with support less than MIS(I), i.e. these 
items are impossible to be part of sequential patterns. Follow 
the example above, the supports of each item is as follows:
a:100%, b:80%, c:100%, d:40%, e:40%, f:20%. Since the 
MIS(I) is 30%, all nodes with iname=f will be removed 
because supp(f) is less than MIS(I). The pruning step is shown 
in Fig. 6. In this case, node (_f:1) is going to be removed and 
the node links related to (_f:1) will be modified to keep the data 
structure right at the same time. Several links related to this 
node needed to be modified, including node (a:2)’s slink is 
linked to (_f:1) and node (a:1)’s plink is linked to (_f:1), Node 
(a:2)’s slink will be linked to (_f:1)’s child node (a:1) and node 
(a:1)’s parent link will linked to (_f:1)’s parent (_a:3). The 
complete pruning algorithm is shown in Fig. 7. 
Figure 6(a).                               Figure 6(b) 
Figure 6. PLMS-tree – Step 2 – pruning 
Figure 7. Method: prune – remove infrequent items 
Since we prune some nodes from the tree, there might exist 
nodes sharing the same item name with their sibling node. The 
third step merges nodes with their sibling node if they share the 
same item name and assign position code to the nodes which 
do not need to be merged. Follow the example in Fig. 6, we 
find that two child nodes of (_a:3), i.e. (a:2) and (a:1), share 
the same iname=a. Moreover, both of them are not the last item 
of an itemset, so they will be merged together and the merging 
step is shown in Fig 8(b). At the same time, we modify the 
related node link to keep the data structure right. The node 
(_b:1)’s plink will be changed to the merged node (a:3) and 
node (_b:2)’s slink is changed to (_b:1) because they have the 
same parent node. The following two nodes (_b:2) and (_b:1) 
will do so in the same way as Fig. 8(c). 
Figure 8(a)                    Figure 8 (b)             Figure 8 (c) 
Figure 8. PLMS-tree – Step 3 – merging steps 
The position code is assigned at the same time when we 
merge nodes by traversing T from root, if a given node n does 
Input : node n with supp(n.iname) < MIS(I)
Method : prune(e)
1. Modify the link linked to e’s child node and sibling node
2. Remove e
431
not need to be merged, it will be assigned with a position code.
Since the leftmost node of root in T is (c:3), we use (c:3) to be 
the root of equivalent binary tree, which means it is assigned 
with position code 1. As we motioned in section 2, its leftmost 
child (_a:3) is assigned with an additional 1 in the bottom and 
becomes 11, the first sibling node (_c:1) is assigned with an 
addition 0 in the bottom and becomes 10. The other nodes in T
are assigned with unique position code in the same way. The 
result of PLMS-tree with position code is shown in Fig. 12, too 
and the merging and assigning algorithm shows in Fig. 9. 
Input : Tree T
Method : merge(T)
1. Form the root of T, trace each node e in T
2. If e’s slink != NULL then
3. For each sibling nodes of e
4.         If this node is appeared the first time then
5.          assign position code
6.         Else
7.             merge to the node with same node name
8.         End if
9. End loop
10. Else 
11. assign position code
12. End if
Figure 9. Method: merge – merge tree and assign position code 
The fourth step is to set the mps value of each node e in T. 
This value is used to record the minimal possible support in 
this node’s suffix tree and it will play an important rule in our 
mining process. The mps is equal to the minimum MIS value 
between e and all nodes in e’s suffix trees. We trace form the 
leaf nodes in T to assign their mps value. In this case, the mps
of node (_b:1:1111111) is 80% and its parent node (c:1:11111) 
is 40%, so the mps value of (c:1:11111) does not to be 
modified, but (c:1:11111)’s parent node, (_b:3:1111)’s mps
value is 80%, so the mps value of (_b:3:1111) needs to be 
modified to 40% which is equal to it’s child’s mps value. The 
other nodes in T are assigned with mps vales in the same way. 
The result of PLMS-tree with mps value is shown in Fig. 12
and the mps setting algorithm is shown in Fig. 10.  
Input : leaf node of T
Method : thres_reset(T)
1. If e.plink.mps > e.mps then
2. e.plink.mps = e.mps
3. Else
4. break
5. End if
Figure 10. Method thres_reset(T) 
The final step is building nlink between header table and 
the nodes, so we can trace all nodes with the same iname
through header table. In this example, the nlink of item e in the 
header table is linked to (_e:1:11111001) first, and the nlink of 
node (_e:1:11111001) in linked to next node with the same 
item name (_e:2:101) in prefix order. The other nodes are also 
linked together by the same way. The algorithm of building 
nlink shows in Fig 11 and the complete PLMS-tree with part of 
node link value shows in Fig. 12. 
B. MSCP-growth 
After the PLMS-tree is constructed, we develop the MSCP-
growth algorithm to generate a complete set of sequential 
patterns with MMS. The complete pseudo code of MSCP-
growth is shown in Fig. 13. Due to the limit of space, we 
briefly explain the main procedure of MSCP-growth below.  
Input : node n
Method : buildLinkage (n)
1. If n != NULL then
2.         If HT[n.iname].nlink == NULL then
3.             HT[n.iname].nlink = n;
4.         Else
5.             link the last node linked by HT[n.iname].nlink’s nlink to n
6.         End if
7.         buildLinkage(n.clink);
8.         buildLinkage(n.slink); 
9. End if
Figure 11. Method: thres_set – set mps to each node 
Figure 12. Complete PLMS-tree with part of node link 
Input : Root set RS, root
Output : the complete set of sequential patterns
Method : MSCP-growth(RS, ?, MIS(?))
1. If RS is empty then return
2. For each item i with supp(i) > MIS(I) in HT
3. For each node n belongs to i in T
4.         If n is a descendent node of RS and n is not recounted then
5.             icount += n.icount
6.             Join n’s child into new root set nRS
7.           If n.mps < mps then
8.             mps = n.mps
9.             End if
10.         End if
11. End loop
12. If icount > MIS(?) || icount > mps then
13.         Append i to ? as ?’
14.         If icount > MIS(?’) then
15.             Join ?’ to sequential patterns set
16.     End if
17.        MSCP-growth(nRS, ?’, MIS(?’))
18. End if
19. End loop
Figure 13. Algorithm 3: MSCP-growth – generate sequential patterns set 
In line 2 through line 11, firstly, the algorithm counts each 
item i’s support in ?’s conditional PLMS-tree (the suffix tree of 
root set RS). In PLMS-tree, the nodes with the same iname in 
the same path cannot be counted more than once since each 
item in the same sequence can be counted once only. Here, the 
binary code can be used to quickly identify the relationships 
between any two nodes in PLMS-tree. While cumulating icount
for node n, in line 4 through line 6, the algorithm records n’s 
child node in a new root set nRS. The nRS will be used to 
432
locate (?’)’s conditional PLMS-tree in the mining of sequence 
(?’)’s super-patterns. In line 7 through 9, we examine the mps
of each node n and record their minimum mps. This value will 
be used in next step. 
In line 12 through line 18, secondly, we generate candidate 
patterns and output sequential patterns according to each item’s
icount. Three conditions may occur here. The first is that the 
icount satisfies MIS(?’). This sequence is directly appended to 
the sequential patterns set. The second is that the icount doesn’t
satisfy MIS(?’) but satisfies mps. That is, the sequence is 
currently not a sequential pattern, but its super-pattern (i.e. 
appending suffix in the sequence) has possibility to be. 
Therefore, both two conditions have to keep growing and 
recursively call procedure MSCP-growth for further examining 
their super-patterns. The third is that the count of sequence 
doesn’t satisfy both MIS(?’) and mps. It means that any super-
pattern of the sequence ?’ will never be a sequential pattern. 
The procedure will terminate while the root set becomes empty.  
We use the constructed PLMS-tree in Fig.12 to explain the 
mining process in detail. At first, the RS is root and we trace all 
nodes belong to the first item e in HT. According to the nlink of 
item e from HT, we find (_e:1:11111001) n1 at first, the 
position code shows that n1 is a descent node of RS, and n1 is 
not recounted in the same branch. n1.icount will be add into 
icount, since n1 does not have any child node, no node will be 
joined into nRS. According to the nlink in (_e:1:11111001), the 
next node (_e:2:101) n2 will be found, n2 is a descent node of 
root and n2 is not recounted, n2.icount will be added into icount.
n2 also has a child node (_c:2:1011), that can be joined into the 
nRS to be a new root set if we need to growth this pattern. And 
the next node (_e:1:10111011) is recounted so the node count 
of this node will not be added into icount. Since the icount of 
item e is 60% which reaches the candidate growth condition we 
mentioned before, we use the nRS and e as a candidate pattern 
to generate new patterns. Pattern e also reaches the threshold of 
sequential pattern, i.e. MIS(e), and can be joined into the 
sequential patterns set. The second step uses the node in the 
nRS, (_c:2:1011), as the new root set of conditional tree to 
growth new pattern(s). According to HT, the icount of first item 
e rooted by (_c:2:1011) is 20%, which does not reach the 
candidate growth condition. The icount of second item c in HT
rooted by (_c:2:1011) is 40%, which reaches the condition to 
growth new candidate patterns, so we use the child nodes of 
(_c:2:1011), (_a:1:10111) and (_b:1:10111), as nRS to generate 
new candidate patterns. This pattern also larger than MIS(ec) 
which means it can be joined into the sequential patterns set. 
The complete sequential patterns set with beginning item e can 
be generated by repeating these steps.  
The reason we use the mps value and candidate growth will 
show in following example. We trace the item b now, the 
icount of item b is 100% which reaches the condition to 
generate new candidate and the threshold to be joined into the 
sequential patterns set. As the tracing step goes, we have the 
icount of item d=60% in the suffix tree of pattern b, it reaches 
the mps value=30%, which is the second condition to candidate 
growth we mentioned before, and it means that this pattern still 
has possibility to be part of sequential pattern. Noted that it 
doesn’t reach MIS(bd)=65%, which means it is not a sequential 
pattern. In the next candidate growth step, we have item e with 
icount=40% which reaches the MIS(bde) = 30%, pattern(bde) 
can be joined into the sequential patterns set. We can see that 
an infrequent pattern’s super set become frequent pattern in this 
case, that is why we need to consider about the mps value. The 
other sequential patterns which are started from other items are 
generated in the same way. Finally, we can have the complete 
sequential patterns with MMS by MSCP-growth algorithm. 
V. CONCLUTION
In this study, we first address the rare item problem in the 
mining of sequential patterns. To generate more interesting 
patterns, we extend the concept of MMS on sequential pattern 
mining and propose an algorithm, named MSCP growth, to 
discover complete set of sequential pattern with MMS. In 
experimental study, we will compare three well-known 
traditional methods, including GSP, PrefixSpan and PLWAP-
tree, with MSCP-growth using several synthetic datasets as 
well as real-life datasets. Some research directions are briefly 
discussed. First, users may have to tune items’ supports many 
times while discovering useful patterns. Instead of restarting 
the whole mining process, it is valuable if a support tuning 
mechanism can be developed. Second, the concept of MMS 
can be further applied to constraint-based sequential pattern 
mining, such as time constraints or duration constraints. 
ACKNOWLEDGMENT
This research was supported by the National Science 
Council of the Republic of China under the Grant NSC 98-
2410-H-194-054.  
REFERENCES
[1] Y.-H. Hu and Y.-L. Chen, "Mining association rules with multiple 
minimum supports: a new mining algorithm and a support tuning 
mechanism," Decision Support Systems, vol. 42, is. 1, pp.1-24, 2006. 
[2] R. Agrawal and R. Srikant, "Mining sequential patterns," Data 
Engineering (ICDE’95), Taipei, Taiwan, Mar, 1995, pp.3-14.  
[3] J. Pei, J. Han, B. Mortazavi-Asl, and H. Zhu, "Mining access patterns 
efficiently from web logs," Lecture notes in computer science, pp.396-
407, 2000. 
[4] C. Ezeife and Y. Lu, "Mining web log sequential patterns with position 
coded pre-order linked wap-tree," Data Mining and Knowledge 
Discovery, vol. 10, no. 1, pp.5-38, 2005.  
[5] B. Liu, W. Hsu, and Y. Ma, "Mining association rules with multiple 
minimum supports,", Proceedings of the fifth ACM SIGKDD 
international conference, San Diego, CA, USA August 15-18, 1999, 
p.341. 
[6] K. Wang, Y. He, and J. Han, "Mining frequent itemsets using support 
constraints,"?Proceedings of the 26th international conference on VLDB, 
2000, Cairo, Egypt, pp.43-52. 
[7] Y. Lee, T. Hong, and W. Lin, "Mining fuzzy association rules with 
multiple minimum supports using maximum constraints," Lecture notes 
in computer science, vol. 3014, pp.1283-1290, 2004. 
[8] M. Tseng and W. Lin, "Mining generalized association rules with 
multiple minimum supports," International Conference on Data 
Warehousing and Knowledge Discovery (DaWaK’01), Munich, 
Germany, 2001, pp.11-20. 
[9] Y. Lee, T. Hong, and W. Lin, "Mining association rules with multiple 
minimum supports using maximum constraints," International Journal 
of Approximate Reasoning, vol. 40, pp.44-54, 2005. 
433
