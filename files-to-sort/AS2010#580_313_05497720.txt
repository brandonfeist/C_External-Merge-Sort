Mining Frequent Closed Itemsets Using Antecedent-Consequent Constraint and 
Length-Decreasing Support Constraint 
Zhi Liu, Qiuying Li, Mingyu Lu 
 Information Science and Technology 
Dalian Maritime University 
Dalian, China, 
e-mail: lzsgmsc@126.com 
Hao Xu 
National Integrated Center of Cardiovascular Disease 
Chin-Japan Friendship Hospital 
Beijing, China
Abstract—At present, many frequent itemsets mining 
algorithms adopt a constant support threshold value strategy, 
which is not convenient to potential valuable long itemsets 
discovery. Length-decreasing support constraints can address 
this problem probably. Existing algorithms make 
improvements on classical algorithm, which result to low 
efficiency. Tailored to the medical data, this paper proposes a 
frequent closed itemsets mining algorithm called ACLCMiner, 
which uses the antecedent-consequent constraint and the 
length-decreasing support constraint. With the antecedent- 
consequent constraint, the algorithm greatly reduces the 
number of generated frequent itemsets. The experimental 
results show that ACLCMiner is efficient and can find more 
long itemsets with potential values. 
Keywords- frequent closed itemsets; antecedent-consequent 
constraint; length-decreasing support constraint; FP-tree 
I. INTRODUCTION
Association rule mining is an important research subject 
in data mining. The study of association rules mainly 
contains two research directions: frequent itemsets
discovering and rules correlation analysis. The contributing 
factor of the low algorithm efficiency is frequent itemsets 
discovering, so many frequent itemsets discovering 
algorithms have been raised to address the problem, 
including constraint-based frequent pattern mining[1-2], 
frequent closed itemsets mining algorithm[3], maximal 
frequent itemsets discovering algorithm[4] and so on. These 
algorithms make support threshold a constant, so they can 
not find long itemsets which have a low support with 
potential values. In practical applications, the users are often 
actually interested in the rules generated from long itemsets. 
Thus, the length-decreasing support constraint[5] is 
presented, which can help to find the long itemsets. Then 
many algorithms based on the length-decreasing support 
constraint are proposed, such as, LPMiner[5], SLPMiner[6], 
WLPMiner[7-8], BAMBOO[9] and SCCMETree [10]. 
Association rules can be used in the medical field to 
improve the accuracy of diagnosis and treatment. Each rule 
represents a simple prediction model, and the decision 
attribute (diagnosis type) is determined by non-decision 
attribute (dialectical factors). In another word, the position 
which the attributes appear in the rule is limited. Therefore, 
according to the characteristics of medical data, this paper 
presents a frequent closed itemsets mining algorithm based 
on the antecedent- consequent constraint and the 
length-decreasing support constraint(ACLCMiner). The 
antecedent-consequent constraint[11] is used to restrict the 
number of generated frequent itemsets, and the 
length-decreasing support constraint[5] is used to get more 
valuable long itemsets. ACLCMiner adopts the strategy of 
CLOSET+[3] to get frequent closed patterns and uses some 
pruning strategies to reduce the search space. The compound 
frequent pattern tree and Rtree are used to store transaction 
data and generated frequent closed itemsets, respectively. 
Experimental results show that the efficiency of ACLCMiner 
is significantly higher than BAMBOO. 
II. PROBLEM STATEMENT AND RELATED WORK
A. Problem Statement 
Here we give the definition of association rules. Assume 
I={i1, i2,…, in}is an item set, which parameters i1, i2,…, in 
are called items. Transaction T is a set of item, T I. The set 
of transactions is denoted by TDB, as in Table . An itemset 
X is a non-empty subset of I and it is called k-itemset if it 
contains k items (i.e., |X|=k). A transaction T is said to 
contain itemset X if X?T. Association rules are those 
implications depicted as X Y, where X I, Y I, and X
Y=?.  
TABLE I. A transaction database TDB
tid Set of items Ordered item list 
001 a, c, f, m, p f, c, a, m, p 
002 a, c, f, i, m, p f, c, a, m, p, i 
003 a, b, c, f, m f, c, a, b, m 
004 b, f f, b 
005 b,c,p c, b, p 
Definition 1 The support of the rule X Y is the ratio 
between the number of transactions containing X and Y in 
TDB and all the transaction number (i.e., P(XY)), which 
can be written as support(X Y). The confidence of the rule 
X Y is the ratio between the transaction number including 
X and Y and those including X (i.e., P(Y|X)), which can be 
written as confidence (X Y).   
Definition 2 Find out all X’s non-empty subset t to each 
frequent itemset X. Association rule t?X-t can be obtained 
when sup(X)/sup(t)>=minconf and minconf is defined as the 
minimum confidence of the rule. Sup(X)/sup (t) is defined as 
the confidence of the rule t?X-t where t is the antecedent of 
the rule and X-t is the consequent of the rule. 
V1-580978-1-4244-5824-0/$26.00 c©2010 IEEE
Definition 3(length-decreasing support constraint) A 
function ( )f l with respect to a transaction database TDB is 
called a length-decreasing support constraint w.r.t. TDB, if it 
satisfies ( )0 1 ( ) 1f l f l? + ? ?  for any positive integer l . 
An itemset X is frequent w.r.t. length-decreasing support 
constraint of ( )f l , if sup( ) (| |)X f X? . 
Property 1 Smallest Valid Extension[5] (SVE) Given 
itemsets X and X ? , where X X? ?  and sup( ) (| |)X f X< . 
Then the necessary condition which X ?  is a frequent set 
w.r.t. ( )f l  is that the length of X ?  is greater or equal to 
( )( ) ( ) ( ){ }1 sup min | supf X l f l X? = ? . 
Definition 4 (closed itemset) An itemset X is a closed 
itemset if there exists no proper superset X X? ? such 
that ( ) ( )sup supX X? = . 
Definition 5 (frequent closed itemset) If an itemset X is a 
closed itemset and frequent w.r.t. ( )f l , X is called a frequent 
closed itemset which satisfies the length-decreasing support 
constraint of ( )f l , in short, X is a frequent closed itemset 
w.r.t. ( )f l .  
B. Related Work 
Mining frequent itemsets using the length-decreasing 
support constraint was first proposed in [5], which also 
proposed the algorithms with the length-decreasing support 
constraint (LPMiner and SLPMiner). These algorithms could 
effectively find all itemsets that satisfy a length-decreasing 
support constraint. LPMiner used the SVE property to add 
several pruning methods to compress the search space on 
the basis of FP-Growth algorithm. However, LPMiner has a 
number of limitations. Firstly, the efficiency of the algorithm 
is not high; secondly, when the data is large, the constructed 
FP-tree is too large to be placed in memory; thirdly, its result 
set may contain a large number of redundant (i.e., 
non-closed) itemsets. WLPMiner[7-8] weighted each item in 
the itemsets differently on the basis of LPMiner algorithm. 
To tackle the shortage of LPMiner, BAMBOO[9] was 
proposed, which is a frequent closed itemsets mining 
algorithm with length- decreasing support constraint. Taking 
advantages of the property of length-decreasing support 
constraint, Invalid Item Pruning method and Unpromising 
Prefix Itemset Pruning method were proposed to make 
BAMBOO more efficient than LPMiner. But the overall 
efficiency was affected due to the functionally overlapping 
in pruning of aforementioned methods in BAMBOO. 
III. ACLCMINER ALGORITHM
A. Length-Decreasing Support Constraint 
According to definition 2, the length-decreasing support 
constraint ( )f l is defined as: 
( ) ( ) ( )
max 1 min
max min min max / max min( )
min max
min max
s l l
s l l s s l lf l
l l l
s l l
? ? ??
+ ? × ? ??
= ?
< <?? ??
when 
when 
when 
 (1) 
Where l is called the length of an itemset, 
[ ]max, min 0,1 max mins s s s? ?and .lmax is the maximum 
length while lmin is the minimum length. When the length of 
frequent itemsets is smaller or equal to lmin, the threshold of 
support gets the maximum value smax. When the length of 
frequent itemsets is greater or equal to lmax, the threshold of 
support will be the minimum value smin and the support will 
no longer reduce. Figure 1 gives the function curve diagram 
for length-decreasing support constraint.
Figure 1. Function curve of the minimum support. 
B. Antecedent-Consequent Constraint 
Based on the characteristics of the data, we will divide 
the attributes of the database into decision attributes and 
non-decision attributes in accordance to these attributes as 
the different aspects in the rules. Non-decision attribute 
appears in the antecedent of rule, while the decision attribute 
appears in the consequent. For the rule X Y, we need to 
calculate support (Sup(X Y)=P(XY))and confidence 
(conf(X Y =P(Y|X) =P(XY)/P(X)), without the need to 
calculate P(Y) value. So we do not need to generate the 
frequent itemsets which only contain decision attributes. 
Antecedent-consequent constraint restricts each itemset 
generate only one rule, which greatly reduces the number of 
generated rules. 
Our algorithm adds the constraint of the antecedent and 
consequence, so we need to encode the attribute values block 
by block in TDB. For an itemset I={i1, i2,…, in}, we encode 
the decision attributes those act as the antecedent of rules 
from zero to m and the non-decision attributes those act as 
the consequents of rules from m+1 to n. 
C. Storage Structure     
1) Compound Frequent Pattern Tree: Since the use of 
length-decreasing support constraint, if the itemset X is 
frequent, the support of X must satisfy that support threshold 
corresponding to the length of X, which is (| |)f X . 
Therefore, we re-define the structure of f_list to make the 
determinaton of frequent 1-itemset and the length of 
transitions containing this itemset correlated. According to 
the data in Table , we give the structure of f_list, as 
1 lmax 
minsup
smax=f(lmin) 
smin=f(lmax)
lmin Length of itemsets 
[Volume 1] 2010 2nd International Conference on Future Computer and Communication V1-581
shown in Figure 2. Itemset is counted according to the 
length of transactions. Assume the parameters 
of ( )f l are: max 0.8s = , min 0.4s = , min 3l = , max 5l = . So
the function ( )f l is: ( )( ) 0.8 3f l l= ? , ( )( ) 0.6 4f l l= = , 
( )( ) 0.4 5f l l= ? , it is easy to figure out that {f, e, a, g, h} is 
the complete set of frequent closed 1-itemsets w.r.t ( )f l . 
For {b}, the support of any length does not satisfy ( )f l , so 
any itemset containing b is not frequent. The reason is given 
in the invalid item pruning strategy. Therefore, {b} is 
non-frequent itemsets. 
f 4 
e 4 
a 3 
b 3 
g 3 
h 3 
i 1  
Figure 2.  Compound f_list structure. 
Figure 3.  FP_tree.               Figure 4.   CFP_Tree. 
The compound nodes in the compound frequent pattern 
tree (CFP_Tree) are obtained by merging.  
Theorem 1. Given CFP_Tree. For nodes P and C, if each 
path containing P has C and there is ( ) ( )sup supP C=  in 
each branch, C can be merged with P to form a new node 
and C can be removed from the CFP_Tree. 
Proof: If each path containing P in the CFP_Tree has C and 
there is ( ) ( )sup supP C=  in each branch. Then P and C 
always appear together, or both of them do not appear. So all 
of the closed itemsets generated must include both P and C 
or neither. Therefore, P and C can be merged. 
Figure 3 shows the FP_tree constructed with the data in 
Table , while Figure 4 is the CFP_Tree corresponding to 
the FP_tree. CFP_Tree has fewer nodes than FP_tree. Also, 
in the f_list P and C can be merged into an item. Therefore, 
CFPTree need smaller space than FP_tree. 
2) RTree Structure: To speed up the closure checking, 
we should choose the tree-like structure to store the 
generated frequent closed itemsets. This structure has good 
performance in the search speed and storage space. 
Therefore, we propose a structure called RTree to store the 
generated frequent closed itemsets. Each node stored the 
item and its support. Figure 5 shows an example which 
stores totally four closed itemsets: {f,e,a,g,h:2}, {f,e,a,g:3}, 
{e:4}, {f:4}. 
Figure 5.  RTree structure. 
In RTree, if the supports of parent node and the supports 
of child nodes are different, a frequent closed itemsets from 
the root node to the parent node is then formulated. For 
example, in Figure 5, the support of {f} node is 0.8 and the 
support of {e} node is 0.6, then we have a frequent itemset 
{f: 0.8}. Also, if the current node is a leaf node, a frequent 
closed itemsets from the root node to the current node is 
formulated, such as {f, e, a, g, h: 0.4}. 
D. Pruning Strategy 
In ACLCMiner, an efficient pruning method, Invalid 
item pruning, is adopted, which is based on the SVE 
property. Meanwhile, according to the antecedent- 
consequent constraint and the requirements of closed 
itemsets, ACLCMiner uses decision attribute pruning, item 
merger and sub-itemset pruning to reduce the search space. 
1) Invalid Item Pruning: 
Definition 7(Invalid item) Given a conditional database 
TDB|P w.r.t. a particular prefix itemset P. For any item x 
which appears in TDB|P, We maintain a total number of 
max_ xl  supports, denoted as [ ]s 1 max_x xl… , where max_ xl is 
the maximal length of the transactions which contain item x 
and [ ] ( )s 1 max_x xk k l? ? records the support of item x in 
transactions which the length is k. Item x is called an invalid 
item w.r.t. TDB|P if [ ] [ ] ( )
max_
, sup s | |x
l
x x
j k
k k j f k P
=
? = < +? . 
Here is Invalid item pruning. For a particular prefix 
itemset P and its conditional database TDB|P, there is no 
hope to grow P with any invalid item w.r.t. TDB|P to get 
closed itemsets satisfying the length-decreasing support 
constraint. Invalid Item Pruning is used in the original 
transaction database and the conditional transaction database. 
When building a frequent pattern tree, other than LPMiner 
algorithm based on smin, the support and the length of 
transactions are taken into account at the same time. This 
constructs a smaller CFP-tree. From Figure 2, we know that 
f 
e 
a,g 
h
root
f: 0.8
e: 0.6
h: 0.4
a,g: 0.6
e: 0.8
f: 4 
e: 3 
a: 3 
h: 2 
g: 3 
e: 1 
h: 1 
f: 4 
e: 3 
{a,g}: 3 
h: 2 
e: 1 
h: 1 
rootroot
Total support count (sup) 
6 1  
Length of transaction Support count ( [ ]s k )
2  1 6  1   5  2 
3  1 6  1   5  2 
6  1   5  2  
6  1   5  2  
3  1 6  1   5  1 
2  1 6  1   3 1
V1-582 2010 2nd International Conference on Future Computer and Communication [Volume 1]
[ ]s 5 0.2b = , [ ]s 3 0.2b = and [ ]s 2 0.2b = , then [ ]sup 5 0.2b = , 
[ ]sup 3 0.4b = and [ ]sup 2 0.6b = .We can get 
[ ] ( ),supbk k f k? < , so {b} is a invalid items. 
2) Decision Attribute Pruning: Thanks to the 
antecedent-consequent constraint, we do not need calculate 
the support of itemsets including only the decision 
attributes. Therefore, we use a pruning strategy for the 
decision attributes. First, the algorithm scans the transaction 
database to generate the frequent 1-itemsets and the support 
counts. Then the frequent 1-itemsets are respectively sorted 
in descending order of support according to decision 
attributes and non-decision attributes, and all decision 
attributes are ranked in front of non-decision attributes. 
Hence, the non-decision attributes are the children of the 
decision attributes in the constructed CFP-tree. In the process 
of mining frequent itemsets, we only generate conditional 
CFP-tree for the decision attributes, while the decision 
attributes are not considered. This makes that there are not 
itemsets which contain only the decision attributes in the 
generated frequent itemsets. This will reduce the number of 
the conditional pattern tree generated and improve the 
efficiency of algorithm. 
3) Unpromising Pre?x Itemsets Pruning: Since we are 
only interested in closed itemsets, we choose two existing 
pruning methods to stop mining patterns with unpromising 
pre?x itemsets as soon as possible. They have been 
popurlarly used in several closed itemset mining algorithms, 
such as CLOSET+. 
The ?rst pruning technique is item merging. Given a 
conditional database TDB|P w.r.t. a particular pre?x itemset 
P. If the itemsets X can be contained in each of the 
transactions in the TDB|P, X can be merged with P to form a 
new pre?x, and we do not need to mine closed itemsets 
containing P but no X. 
The second pruning technique is sub-itemset pruning. Let 
X be the frequent itemset currently under consideration. If X 
is a proper subset of an already found frequent closed itemset 
Y and sup(X)=sup(Y), then X and all of X’s descendants in 
the TDB|X cannot be frequent closed itemsets and thus can 
be pruned. 
E. The Algorithm 
The ACLCMiner algorithm is shown as follow. 
ACLCMiner TDB, smin, smax, lmin, lmax, m
INPUT TDB: a transaction database smin, smax, lmin, lmax are 
the parameters of ( )f l and m: the coding boundary for the 
decision attributes and the non-decision attributes. 
OUTPUT  the complete set of closed itemsets that satisfy the 
length-decreasing support constraint 
BEGIN 
01. ITree NULL; f(l) F(smin, smax, lmin, lmax) 
02. ACLCMiner( , TDB); 
03. Out(ITree); 
ALGPRITHM aclcminer P, PTDB
INPUT P: a prefix itemset PTDB: the conditional database w.r.t. 
prefix P. 
BEGIN 
04. f_list Create_acconstraint_flist(PTDB, m); 
05. f_list invalid_item_pruning(f_list, f(l), |P|); 
06. S item_merging(f_list); P P S; f_list f_list-S; 
07. if ( P ) 
08.   if (sup(P) f(|P|) ) 
09.     if(sub_itemset_pruning(P, ITree)) 
10.       return; 
11.     else  
12.       insert(ITree, P); 
13.   end if 
14. end if 
15. if (f_list ) 
16.   CFPTree Create_cfptree(f_list, PTDB); 
17.   for all (x f_list and x m) do 
18.      P? P {x}; 
19.      PTDB? build_cond_database( P? , CFPTree); 
20.       ACLCMiner( P? , PTDB? ); 
21.   end for 
22. end if 
END 
At first, ACLCMiner uses the input values of smin, 
smax, lmin, lmax to get the function ( )f l , and then calls the 
subroutine aclcminer(P, PTDB). The subroutine first builds 
f_list according to the decision attributes pruning strategy 
(line 03). Then it applies the invalid item pruning method, 
and the set of valid items is denoted as f_list (line 04). Next 
aclcminer() uses the item merging technique to identify the 
set of items that has the same support as P, and denoted as S, 
S will be merged with P and removed from f_list (line 05). 
While getting a new itemset, the algorithm will first check if 
it can satisfy the support constraint, if not, the algorithm do 
not need to check if it is closed and it will not be inserted 
into the frequent closed itemsets(lines 08-13). If an itemset is 
a closed itemset that satisfies the support constraint, it will be 
inserted into the RTree (line 12). Next the conditional CFP- 
tree is built (line 16), and aclcminer() recursively calls itself 
to mine the closed itemsets with the length-decreasing 
support constraint by growing prefix P under the bottom-up 
divide-and-conquer paradigm (lines 17-25). Finally, 
according to the RTree, we will output the frequent closed 
itemsets satisfied requirements. ACLCMiner removed the 
unpromising prefix pruning strategy in BAMBOO, because 
it repeats with the invalid item pruning strategy. 
IV. EXPERIMENTAL RESULTS
This experiment is carried out on an Intel Pentium PC, 
2GB memory, 2.4GHz. All procedures were implemented 
using JAVA language. Data is the real data from the HIS 
system database of China-Japan Friendship Hospital, and we 
selected two groups of data , which are the effectiveness of 
Traditional Chinese Medicine(TCM) and the herbal nature in 
the coronary heart disease database. The characteristics of 
[Volume 1] 2010 2nd International Conference on Future Computer and Communication V1-583
the data table are shown in Table (the last column shows 
the average and maximal transaction length). Table 
shows the two parameters of ( )f l , i.e. sim and smax. For 
the data of TCM effectiveness lmin is 8 and lmax is 20, and 
for the data of the herbal nature lmin is 3 and lmax is 8. We 
ran respectively BAMBOO and ACLCMiner on the same 
environment, and the experimental results are shown in 
Table . The comparison results for the two algorithms are 
shown in Figure 6 and Figure 7.  
TABLE II. DATASET CHARACTERISTICS 
TABLE III. EXPERIMENTAL RESULTS  
0
50
100
150
200
250
300
support
N
u
m
be
r 
o
f i
te
m
se
ts
(th
ou
sa
n
d)
0
30
60
90
120
150
180
R
un
tim
e
 
(s)
Figure 6.  Comparison between ACLCMiner and BAMBOO 
TCM effectiveness . 
0
5
10
15
20
25
30
35
support
N
u
m
be
r o
f i
te
m
se
ts
(th
ou
sa
n
d)
0
3
6
9
12
15
18
R
u
n
tim
e
 (s
)
Figure 7.  Comparison between ACLCMiner and BAMBOO 
herbal nature . 
From Figure 6 and Figure 7 we could see that 
ACLCMiner generates a small number of frequent closed 
itemsets, and the algorithm runtime is faster than 
BAMBOO. And as decreasing the value of smin and smax, 
the effect is increasingly apparent. ACLCMiner’s higher 
performance stems from two aspects: on the one hands, it 
adopts the antecedent-consequent constraint to mine much 
small number of valid itemsets than BAMBOO, on the other 
hand, it adopts effective pruning methods and storage 
structure, which greatly improves the running efficiency. 
From the comparison of Figure 6 and Figure 7, we can see 
when the decision attributes are more and the number of 
items contained in the transaction is large, the effect is 
particularly evident. 
V. CONCLUSIONS
In this paper, according to the characteristics of the 
medical data, a freqnet closed itemsets mining algorithm 
based on the antecedent-consequent constraint and the 
length-decreasing support constraint (ACLCMiner) is 
proposed. The proposed algorithm greatly saves memory 
space and improves the efficiency. Meanwhile, the algorithm 
generates interesting rules and the number of generated rule 
is small. The rules mined from ACLCMiner can help to 
improve the accuracy of clinical syndromes, so it has very 
important theoretical and practical value. At the same time, 
ACLCMiner can also be extended to other areas. 
ACKNOWLEDGMENT
 This work was supported by a grant from the National 
Science Foundation of China (No. J0724003 60773084
60603023) and a grant from the Ph.D. Programs Foundation 
of Ministry of Education of China (No. 20070151009). 
REFERENCES
[1] F. Bonchi, C. Lunnhese, “Pushing tougher constraints in frequent 
pattern mining,” PAKDD’05, 2005, pp. 114-124. 
[2] Jianyong Wang, Jianwei Han and Jian Pei,“CLOSET+: Searching for 
the Best Strategies for Mining Frequent Closed 
Itemsets,”SIGKDD’03, Washington, DC, USA,2003. 
[3] Grahne G, Zhu J, “Efficiently using prefix-trees in mining frequent 
itemsets,” Proc. ICDM’03 international workshop on frequent itemset 
mining implementations (FIMI’03), Melbourne, FL, 2003, pp. 
123–132. 
[4] Burdick D, Calimlim M, Gehrke J, “MAFIA: a maximal frequent 
itemset algorithm for transactional databases,” Proc. 2001 
international conference on data engineering (ICDE’01), Heidelberg, 
Germany, 2001, pp. 443–452. 
[5] M.Seno,G.Karypis, “LPMiner: An Algorithm for Finding Frequent 
Itemsets Using Length- Decreasing Support Constraint,” ICDM’01, 
Nov.2001. 
[6] M.Seno,G.Karypis, “Finding Frequent Patterns Using Length- 
Decreasing Support Constraints ,” Data Mining and Knowledge 
Discovery,2005, pp. 197-228. 
[7] Unil Yun, John J. Leggett, “WLPMiner: Weighted Frequent Pattern 
Mining with Length-decreasing support constraints,” PAKDD`05, 
2005, pp. 555-567. 
[8] Unil Yun, “An efficient mining of weighted frequent pattern with 
Length-decreasing support constraints,” Knowledge-Based Systems, 
2008,21, pp. 741-752. 
[9] Jianyong Wang, George Karypis, “BAMBOO: Accelerating Closed 
Itemset Mining by Deeply Pushing the Length-Decreasing Support 
Attributes Transactions 
Dataset Total 
Non-decisi
on 
attributes 
Decision 
attributes Total A.(M.) 
TCM 
effectiveness 212 25 187 2547 
58 
(105) 
herbal 
nature 59 25 34 2547 
15 
(24) 
Number of frequent 
closed itemsets Runtime(ms) Dataset smin (smax) ACLC- 
Miner 
BAM- 
BOO 
ACLC- 
Miner 
BAM- 
BOO 
0.2(0.45) 40712 261055 17698 157604 
0.2(0.5) 10755 98575 14875 127802 
0.25(0.5) 9294 87747 7880 43609 
TCM 
effective- 
ness 
0.25(0.55) 2479 32518 7302 38458 
0.02(0.05) 20757 30453 8385 15932 
0.03(0.06) 18632 24937 4906 6532 
0.04(0.08) 15511 21312 3172 4031 
herbal 
nature 
0.05(0.1) 12842 18344 2406 2828 
V1-584 2010 2nd International Conference on Future Computer and Communication [Volume 1]
Constraint,” In: Proceedings of the Fourth SIAM International 
Conference on Data Mining, Lake Buena Vista, Florida, USA, 2004. 
[10] Genlin Ji, Yingwen Zhu, “Mining closed and maximal frequent 
embedded subtrees using length-decreasing support constraint,” In: 
Proceedings of the Seventh International Conference on Machine 
Learning and Cybernetics, Kunming, 2008, pp. 268-273. 
[11] C.Ordonez, “Association Rule Discovery With the Train and Test 
Approach for Heart Disease Prediction,” IEEE Transactions on 
Information Technology in Biomedicine, 2006,10(2), pp. 334-343.  
[Volume 1] 2010 2nd International Conference on Future Computer and Communication V1-585
