Fig.1 Time series 
Subsequence Partition of Time Series based on Different Width Time Window 
Hongwei Guo, Jianliang Zhang, Yanchi Liu, Xuedong Gao 
University of Science and Technology Beijing, Beijing 100083 
ghwvip@gmail.com 
 
Abstract—Focusing on the limitation of traditional 
methods that the subsequences of time series are divided 
by the same width time window, an improved event series 
generation algorithm is proposed. Based on the improved 
event series generation algorithm, a new approach that 
divides the time series with different width time window is 
put forward. An example of time series division with 
different width time window is also presented in the 
paper. 
Keywords-Data Mining; Time Series; Time Window; Event 
Series 
I. INTRODUCTION 
Subsequences partition based on time window is a 
basic step of time series data mining (e.g. forecasting, 
decision tree mining, association rules mining), and the 
partition results directly affect the mining result[1]. Up to 
now, methods for subsequences partition are mainly 
focusing on dividing subsequences from the time series by 
the same width time window.  
In our paper, a new approach that divides the time 
series with different width time window is put forward. 
Firstly, the limitation of subsequence partition with the 
same width time window is analyzed. And several partition 
algorithms of time series are introduced. Then, an 
improved event series generation algorithm is proposed 
and a subsequence partition algorithm by different width 
time window is put forward base on the event series 
generated. Also, the proposed algorithm is tested by the 
data from Shanghai Stock Market. 
II. LIMITATION OF SUBSEQUENCES PARTITION 
BY THE SAME WIDTH TIME WINDOW 
Suppose ),,,( 21 Nxxxs "=  is a time series and 
subsequence ),,,( 11 ?++= wiiii xxxs "  is acquired by 
dividing s  by the same width time window with length 
w . Do the single-step slippage on s from the start point 
to end point, so we can get a series of subsequences 
121 ,,, +?wNsss "  all with the width w , which are noted as:  
}1,2,1|{),( +?== wNiswsw i "       (1) 
Fig.1 shows that the subsequence series A and series 
B have a high similarity. However, it is impossible to get A 
and B at the same time according to traditional methods of 
subsequences partition by the same width time window, 
because the width of the time widow of A is 6 while that of 
B is 5. 
After the above analysis, we can see the limitations of 
subsequences partition by the same width time window are: 
(1) There is a great deal of duplicated information which 
will result in redundancy, or if the subsequences are not 
duplicated, they may miss crucial information points; (2) it 
is not allowed to contract the shape of subsequences series 
on the time axis. 
III. SEGMENTATION ALGORITHMS OF TIME 
SERIES 
To describe the character model of the time series is 
the first thing to be solved when executing time series data 
mining and the event points selection is an important 
method for such description. The result of event points 
selection will decide the shape of characters mode of the 
time series, and finally affect the data mining results[1, 2].  
All partition algorithms need a measure method to 
identify how to do the partition calculation. Up to now, 
three measure methods base on fitting errors are mainly 
used: accumulative residual AE , mean residual ME and 
maximum residual DE , respectively[2]. 
There are mainly three kinds of methods for partition 
of time series base on errors: (1) Sliding Window method: 
to increase the segments till the accumulative residual 
reaches the preset threshold[3-5]; (2) Top-Down method: to 
repeat the dichotomy of the time series till the fitting error 
reaches the preset threshold[6-7]; (3) Bottom-Up method:  
Beginning at the maximum segment (it is also the highest 
fitting one), to combine the neighbored segments one by 
one till the fitting error reaches the preset threshold[8-9]. 
2010 International Conference on e-Education, e-Business, e-Management and e-Learning
978-0-7695-3948-5/10 $26.00 © 2010 IEEE
DOI 10.1109/IC4E.2010.66
57
Among them, the Sliding Window method is the 
simplest, most direct, and its computation complexity is 
least as O(Ln) , where n is the length of the series, and L is 
the average distance of the adjacent event points. However, 
the correctness of this method is low, because it only cares 
about the minimum error of local parts of the time series, 
but neglects the changing characters of the global ones. 
The Top-Down method pays attention to the global 
partition errors of the series, so it has a better partition 
result than the Sliding Window method. However, its 
computation complexity is higher as )( 2KnO , where k is 
the partition number, LnK /= . Besides, it needs more 
parameters including the global partition error threshold, 
the expected partition number and the error threshold of 
each part. The Bottom-Up method focuses not only on the 
fitting error after subsequences combination, but also on 
the partition characters by selecting the minimum 
combination cost, so it has a better partition performance 
than the pervious method. What more, it needs a less 
computation complexity as )(LnO . However, this method 
also bases on the least squares algorithm which is time 
consuming, and it needs to preset the error threshold of 
combination, expected event point number after 
combination and so on. 
The algorithms mentioned above all use the error 
threshold to deal with the time series. However, as the time 
series have different linearity, the partition result may not 
be satisfied according to the time trend. Here are two main 
limitations: (1) partition algorithms based on the 
accumulative residual can not effectively identify the large 
trend change of a short time; (2) partition algorithms based 
on the mean errors may be weak to identify the inflection 
point of a time series which have been fitted well for a long 
time.  
IV. SUBSEQUENCES PARTITION BY DIFFERENT 
WIDTH TIME WINDOW 
A. Improved Event Series Generation Algorithm 
Definition: event series is a series, which can 
approximate represents the original time series, by 
selecting the crucial position points of the time series. 
Njgggs Nj ??= 1),,,,,( 1 "" represents a time 
series, where N is the length of s , ),( jj yjg = , jy  is the 
observation value of the series at time j . 's  represents a 
event series, and MiGGGss Mi ??=? 1),,,,,( 1' "" , 
where iG  is the i th crucial point of the event series 
's  
and the j th point of the time series s , and it is described 
as ),( ji yjG = . Define sGi .  and vGi .  as the time of the 
event series corresponding to the time series and the 
observation value of the time series, meaning 
jsGi =. , ji yvG =. . 
Till now, all of the partition algorithms have their own 
shortages in presenting the trend of the time series. Among 
them, the Bottom-Up method performs best according to 
the literatures, so in our paper, the improved Bottom-Up 
partition method is proposed. The procedure is as follows: 
Step 1: Time series are firstly reduced by triangle 
reduction method. The points after reduction are the 
candidates of the event series. ),( '' ii yx  represents the i th 
point of the time series , where Ni ??1 . iy is the 
observation value of v. The adjacent three points can form 
a triangle as shown in Fig.2. If they satisfy formula (2), 
where ),( jiDist  is used to measure the Euclidean 
distance between i and j , and 1?  is the reduction 
threshold, then i  is not the crucial point and should be 
reduced.  
 
1)1,1()1,(),1( ?<+??++? iiDistiiDistiiDist      (2) 
2''2'' )()(),( jiji yyxxjiDist ?+?=                (3) 
Step 2: Use DE instead of AE  base on Bottom_Up 
partition method to generate event series. Firstly, select the 
event points from the set of candidates, and link them on 
by on, so to get segments. Secondly, combine the segments, 
calculate each maximum error of segments after 
combination, and select the minimum error among them. If 
such value is less than 2? , then reduce the middle point of 
the segments, repeat the above process till the minimum 
error is larger than 2? . Finally, the left points are the event 
points.   
Figures below show the process of event series 
generation. Fig.3 is the original time series. Firstly, the 
candidate set of event series are acquired by the triangle 
reduction method as shown in Fig.4. Secondly, the 
improved Bottom-Up method is adopted and then the 
event series are generated, which is shown as Fig.5. 
B. Subsequences generation 
To partition the event series 's  by subsequences 
generation method, we can get the subsequence of time 
series by the different width time window, represented by 
collection ''s . ViSSSs Vi ??= 1),,,,,( 1
'' "" , where iS  
represents the i th subsequence, V  is the number of 
subsequence of the time series. 
teitsGGGS teitsi ??= ),,,,,( "" , and 
),( '' ii yx
),( ' 1
'
1 ++ ii yx  ),( ' 1
'
1 ?? ii yx  
Fig.2 Triangle Relation between the adjacent three points 
58
 0
10
20
30
1 4 7 10 13 16 19 22 25 28
Ti me
Ob
se
rv
at
io
n 
Va
lu
e
Fig.3 Original time series 
 
0
5
10
15
20
25
30
0 3 6 9 12 15 18 21 24 27 30
Ti me
Ob
se
rv
at
io
n 
Va
lu
e
Fig.4 Candidate set of event series by the triangle reduction method 
 
0
5
10
15
20
25
30
0 3 6 9 12 15 18 21 24 27 30
Ti me
Ob
se
rv
at
io
n
Va
lu
e
Fig.5 Event series 
- 0. 5
1. 5
3. 5
0 50 100 150 200 250
Ser i al  Number
Ch
an
ge
 R
at
e Event  Ser i es Shanghai  St ock I
 
Fig.6 Shanghai stock index and event series 
)..,..min(.. 11 wsGsGwsGsGwsGsG tstetstetste ??????? +? , 
meaning the subsequence is combined by crucial points 
whose width in the time series are most nearly to the 
different time window w . Point tsG  in iS  is the 
1+tsG in  1?iS , nu  is the number of iS , 1+?= tstenu . 
The subsequences partition method is described as 
follows: 
Input: the time series s , the event series 's  
Output: the subsequences ''s  
(1) Set 0=v ; 
(2) If Mv =+1 then go to end; else 1+= vv ?
vi = ? vi SG ? ? 1. =nuSv ? 1+= ii ; 
(3) If Mi =  then vi SG ? , 1.. += nuSnuS vv ,go to 
end; else if 1?? Mi , then go to (3);else if 1?? Mi , then 
go to (3); 
(4) If wsGsGwsGsG iiii ????? +? .... 11 then 
vi SG ? , 1.. += nuSnuS vv , go to (2); 
(5) If wsGsGwsGsG iiii ??>?? +? .... 11  then 
vi SG ? , 1.. += nuSnuS vv , 1+= ii , go to (2). 
V. Example 
Take 241 data points of Shanghai Stock Market in 
May 19th, 2006 as the original time series. Firstly, the 
improved event series generation algorithm was used to 
acquire event series from the time series. Secondly, the 
subsequences generation algorithm was adopted based on 
the partition method with different width time window. 
Finally, the 24 crucial points of event series were acquired, 
as shown in Fig.6. 
Set the width of the different time window w  to 40, 
and use the single-step slipping method to divide the time 
series. 21 subsequences can be generated, which is far less 
than the 211 subsequences acquired by traditional partition 
method. Parts of the subsequences are shown in Table 1. 
Table 1 Part result of subsequence partition 
Subsequence 
No. Points of Event Series Width 
1 (0,0.31) (3,0.79) (12,0.21) (34,-0.38) (41,0.39) 42 
… … … 
10 (95,1.13) (120,2.26) (122,2.72) (130,2.50) 36 
… … … 
21 (204,2.67) (219,2.88) (228,2.55) (240,2.61) 37 
 
VI. CONCLUSIONS 
In this paper, an improved event series generation 
algorithm is proposed and a subsequence partition 
algorithm with different width time window base on the 
proposed method is put forward. The advantages of the 
above method are: 
(1) It can avoid information duplication between 
subsequences generated by the same time series partition; 
(2) The subsequences generated by single-step 
slipping method won’t miss key information points; 
(3) It is allowed to contract the shape of subsequences 
since the lengths of the series are different. 
However, there are still some shortages with the 
proposed method: 
(1) The compute complexity of the proposed method 
is higher than traditional method; 
(2) The method only fits for high-dimensional time 
series, while not for low-dimensional ones; 
59
(3) The subsequences generated by the proposed 
method are with different length. Till now, there are no 
mature algorithms for distance measurement of 
subsequences with different length.  
 
REFERENCE 
 
[1] D.T. Pham, A.B. Chan, “Control chart pattern recognition using a new 
type of self organizing neural network”, Proc. Instn, Mech, Engrs. 1998, 
Vol.212, pp.115-127. 
[2] T. Liang, “Research on the technology of time series mining and 
similarity search”, Master Paper in Shanghai Normal University. 2004. 
[3] Shatkay H, “Approximate queries and representations for large data    
sequences”. Technical Report cs-95-03, Department of Computer 
Science, Brown University. 1995. 
[4] K. Par, S. Kim, W. Chu, “Segment-based approach for subsequence 
searches in sequence databases”, In proceedings of the 16th ACM 
Symposium on Applied Computing. Las Vegas, NV. 2001, 
pp.248-252. 
[5] C. Wang, S. Wang, “Supporting content based searches on time series 
via approximation”, Proceedings of the 12th International Conference 
on Scientific Database Management. 2000 
[6] C. Li, P. Yu, Castelli V. “MALM: a framework for mining sequence 
database at multiple abstraction levels”. Proceedings of the 9th 
International Conference on Information and Knowledge Management. 
1998, pp.267-272. 
[7] S. Park, D. Lee, W. W. Chu, “Fast retrieval of similar subsequences in 
long sequence databases”, Proceedings of the 3rd IEEE Knowledge 
and Data Engineering Exchange Workshop.1999. 
[8] Hunter, J. McIntosh, “Knowledge-based event detection in complex 
time series data”. Artificial Intelligence in Medicine. Springer.1999, 
pp.271-280. 
[9] Eamonn J. Keogh, Michael J. Pazzani, “An enhanced representation of 
time series which allows fast  and accurate classification”, Clustering 
and Relevance Feedback, KDD. 1998, pp.239-243. 
 
60
