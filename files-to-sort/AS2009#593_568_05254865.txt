133
Market-Basket Problem Solved With 
Depth First Multi-Level Apriori Mining 
Algorithm 
Mirela Pater and Daniela E. Popescu 
Department of Computer Science, 
 Faculty of Electric Engineering and Information Technology, University of Oradea, 
University Str. no.1, Oradea, Romania 
Phone: +40 (0)259 408-250, 
 E-Mail: mirelap@uoradea.ro, depoepscu@uoradea.ro 
 
Abstract-The problem of deriving association rules from data 
was first formulated in [9] and is called the “market-basket 
problem”. This paper presents an efficient version of APRIORI 
algorithm for mining multi-level association rules in large 
databases to solve market-basket problem. Our algorithm, 
named DEPTH FIRST MULTI-LEVEL APRIORI (DFMLA), 
uses the benefits of multi-leveled databases, by using the 
information gained by studying items from one concept level for 
the study of the items from the following concept levels. 
 
Keywords: data mining, support constrains, knowledge discovery in 
databases, multi-level databases, multi-level association rules 
mining 
 
I. INTRODUCTION 
Data Mining refers to extracting or “mining” knowledge 
from large amount of data. Data mining is the nontrivial 
extraction of implicit, previously unknown, and potentially 
useful information from data as shows Dunham in [5] and 
Frawley et al. in [6]. Through the extraction of knowledge in 
databases, large databases serve as rich, reliable sources for 
knowledge retrieval and verification, and the discovered 
knowledge can be applied to information management, 
decision making, process control and many other 
applications.  
The aim of data mining is the discovery of patterns within 
data stored in databases. Mining for association rules is a data 
mining method that lends itself to formulating conditional 
statements such as “if customers buy product x  then they 
also buy product y “. 
Therefore, data mining has been considered as one of the 
most important and challenge research areas. Some 
researchers like Strikant and Agrawal, in [9] and Rajkumar et 
al. in [10] found that for many applications, it is difficult to 
find strong and interesting associations among data items at 
the primitive levels of abstraction due to the sparsity of data. 
However, many strong associations discovered at rather high 
concept levels are common sense knowledge. Therefore, a 
mining system with the capabilities to mine association rules 
at multiple levels of abstraction and traverse easily among 
different abstraction spaces is more desirable like Han et al. 
in [8] and Rajkumar et al. in [10] indicate. 
II. ASSOCIATION RULES MINING 
 
Association rule discovery has emerged as an important 
problem in knowledge discovery and data mining. The 
association mining task consists of identifying the frequent 
itemsets, and then forming conditional implication rules 
among them. 
Finding frequent itemsets is one of the most investigated 
fields of data mining.  
The problem was first resented in [1]. The subsequent 
paper of Agrawal, [2] is considered as one of the most 
important contributions to the subject. Its main algorithm, 
APRIORI, not only influenced the association rule mining 
community, but it affected other data mining fields as well. 
Association rule and frequent itemset mining became an 
ideally researched area, and hence faster and faster 
algorithms have been presented. Numerous of them are 
APRIORI based algorithms or APRIORI modifications. 
We theoretically and experimentally analyze APRIORI 
which is the most established algorithm for frequent itemset 
mining. 
One of the most well-studied problems in data mining is 
mining for association rules in market basket data. 
Association rules, whose significance is measured via support 
and confidence, are intended to identify rules of the type, "A 
customer purchasing item A often also purchases item B." 
Since the introduction of association rules, many 
algorithms have been developed to perform the 
computationally very intensive task of association rule 
mining. During recent years there has been the tendency in 
research to concentrate on developing algorithms for 
specialized tasks, e.g. for mining optimized rules or 
incrementally updating rule sets. Here we return to the 
"classic" problem, namely the efficient generation of all 
association rules that exist in a given set of transactions with 
respect to minimum support and minimum confidence. 
We try to solve the market basket problem using 
association rules mining algorithms. In this paper we discuss 
the depth first [8] implementation of APRIORI [1], one of the 
fastest known data mining algorithms to find all frequent 
itemsets in a large database, i.e., all sets that are contained in 
at least minsup transactions from the original database. Here 
978-1-4244-5056-5/09/$26.00 ©2009 IEEE
134
SOFA 2009 • 3rd International Workshop on Soft Computing Applications • 29 July – 1 August • Szeged (Hungary) – Arad (Romania)
minsup is a threshold value given in advance. There exist 
many implementations of APRIORI [6, 11]. 
 
III. PROBLEM STATEMENT 
 
Frequent itemset mining came from efforts to discover 
useful patterns in customers’ transaction databases. A 
customers’ transaction database is a sequence of transactions 
(T = _t1,..., tn_), where each transaction is an itemset (ti ? 
I). An itemset with k elements is called a k-itemset. In the rest 
of the paper we make the (realistic) assumption that the items 
are from an ordered set, and transactions are stored as sorted 
itemsets. The support of an itemset X in T, denoted as 
suppT(X), is the number of those transactions that contain X, 
i.e. suppT(X) = |{tj : X  tj}|. An itemset is frequent if its 
support is greater than a support threshold, originally denoted 
by min supp. The frequent itemset mining problem is to find 
all frequent itemset in a given transaction database. 
The first, and maybe the most important solution for 
finding frequent itemsets, is the APRIORI algorithm [2]. 
Later faster and more sophisticated algorithms have been 
suggested, most of them being modifications of APRIORI. 
Therefore if we improve the APRIORI algorithm then we 
improve a whole family of algorithms. We assume that the 
reader is familiar with APRIORI [2] and we turn our 
attention to its central data structure. 
A typical example of association rule mining is market 
based analysis presented by Dunham [5]. This process 
analyzes customer buying habits by finding association 
between the different items that customers place in their 
“market basket”. The discovery of such association can help 
retailers develop marketing strategies by gaining insight into 
witch items of frequently purchased together by customers.  
We assume every 1-itemset to be frequent; this can be 
effected by the first step of the algorithms we are looking at, 
which might be considered as preprocessing. 
A “database query” is defined as a question of the form 
“Does customer _ buy product _?” (or “Does transaction  has 
item _?”), posed to the original database. Note that we have 
__ database queries in the “preprocessing” phase in which the 
supports of the 1-itemsets are computed and ordered: every 
field of the database is inspected once.  
The number of database queries for DFMLA equals: 
 
For instance, if customers buying milk, how likely are they 
to also buy bread on the same trip to super market? Such 
information can lead to increased sale by helping retailers do 
selective marketing and plan there self space. For example, 
placing milk and bread with in close proximity may further 
encourage the sale of these items together within visit to the 
store. 
Let I= {I1, ..., Im }   be a set of attributes called items. A 
subset X ?  I is called an itemset. A k-itemset is an itemset 
that contains k items.  
Let the database D= {T1 ,… ,Tn } be a multi-set of 
transactions, where each transaction Ti , I € { 1, … , n } , is 
an itemset [3]. A transaction T contains an itemset X if X ?  
T. Each itemset has a certain statistical significance called 
support or frequency. The support of an itemset X is the 
fraction of transactions in the database D containing itemset 
X, i.e. 
 
||
|}|{|)( ...
D
TXDTXS ??=    (2) 
 
An association rule is an implication X --> Y, where X ?  
I, Y ?  I, and X   Y =? .  X is called the antecedent and Y 
is called the consequent of the rule. The rule X --> Y holds 
with confidence C  if  C is the fraction of transactions 
containing X that also contain Y , i.e.: 
 
)(
)(),(
XS
YXSYXC =    (3) 
 
The confidence denotes a rule’s strength. A confidence 
threshold Cmin is used to exclude rules that are not strong 
enough to be interesting. Accordingly, there is a support 
threshold Smin that excludes all rules whose number of 
transactions containing the union of the antecedent and the 
consequent is below a certain amount. Itemsets with 
minimum support are called frequent or large itemsets.  
The two kinds of thresholds must not be confused. The 
support threshold is defined over itemsets. When applied to 
association rules, it describes the minimum percentage of 
transactions containing all items that appear in the rule. The 
confidence threshold specifies the minimum probability for a 
consequent to be true if the antecedent is true. A confidence 
of nearly 100% characterizes only very stringent rules. 
 
IV. MULTI-LEVEL ASSOCIATION RULES 
 
Based on the concept hierarchy and some existing 
algorithms for mining single-level association rules, a group 
of efficient algorithms for mining multilevel association rules 
are proposed by Han et al. [7], [8] and Rajkumar et al. [10] 
We assume that the database contains: 
1) an item data set which contains the description of 
each item in I in the form of Ai; description, where Ai is the 
product’s code. 
2) a transaction data set, T = { T1, …, Tn}, which 
consists of a set of transactions Ti= { Ap ,.., Aq}  in the format 
of  (TID, Ai); Where  TID is  a  transaction identifier and Ai 
is an item from the item data set. 
Rajkumar et al. [10] show that in multi-level databases use 
hierarchy-information encoded transaction table instead of 
the original transaction table. This is useful when we are 
interested in only a portion of the transaction database such 
as food, instead of all the items. This way we can first collect 
the relevant set of data and then work repeatedly on the task-
135
 Mirela Pater, Daniela E. Popescu • Market-Basket Problem Solved With Depth First Multi-Level Apriori Mining Algorithm
relevant set. Thus in the transaction table each item is 
encoded as a sequence of digits. 
Example: The item Pepsi Cola is encoded as “0201” 
according to the table 1 below. The first digit ‘0’ represents 
“Food” at the first level, the 2nd digit represents “Drinks” in 
the 2nd level, 3rd digit is “Cola” in the 3rd level and, the 4th 
digit “1” represents product “1” in the “Cola” category. The 
information required to create such a database is either 
implicit  
“Pepsi Cola” is obviously part of the “cola” category 
which is part of the “drinks” category, etc., or it is provided 
by the user. Some information, like price or brand can be 
used to transform items into categories. For example White 
Bread can become a category which contains “1$ White 
Bread” or “Dorbob” White Bread 
 
Table 1.Item codes and descriptions 
 
 
 
 
 
 
 
 
 
 
 
 
 
The problem of finding association rules can be stated as 
follows. Given a set I of items, a database D of itemsets, a 
support threshold Smin, and a confidence threshold Cmin, find 
all association rules  X --> Y in  D  that have support  X   
Y ? Smin  and confidence  C( X,Y ) ? Cmin. 
 
 
Fig.1. Hierarchy-based multi-level database 
 
We generally are not interested in all implications but only 
those that are important. Here importance usually is measured 
by two features called support and confidence. 
The support of an item (or set of items) is the percentage 
of transactions in which that item (or items) occurs. The 
confidence or strength for an association rule X -> Y is the 
ratio of the number of transactions that contains X U Y to the 
number of transactions that contain X. 
In the multilevel database the item data set will be entered 
like in the table 1. This table also contains category codes and 
a description for each code (category or item) which is only 
needed for the final display. Item’s quantity, price, brand or 
other information should be present but are not considered in 
this paper.  
 
Table 2.Transactions components 
 
Transaction Products 
T1 Bread White, Milk 1.5%, Coca Cola, Prigat 
T2 Bread White, Milk 1,5%, Milk 2%, Prigat 
T3 Milk 2%, Coca Cola 
T4 Milk 2%, Pepsi Cola 
T5 Bread Wheat, Coca Cola, Milk 2%, Prigat 
 
Table 3.Transactions data set 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
From the tables 1 and 2 we can deduce that the 
transactions are represented in table 3: 
Multi-level rules are rules in which the concepts or 
constants may be at multiple conceptual levels in conceptual 
hierarchies. 
The problem is to discover the rules at multiple conceptual 
levels, i.e. multiple-level rules. Most rule discovery methods 
find primitive levels which only involve concepts at the 
primitive level. In multi-level databases this methods are 
extended by finding rules at different levels as opposed to 
rules at one specific level. 
Thus, we have the ability to focus our attention on 
discovering informative rules between categories, or even 
between an item and a category, not just rules between items. 
The support, the confidence and other mining rules can be 
used in new ways in a multilevel database.  
For example the support of a category is the percentage of 
transactions in which that category occurs. Support for the 
Milk category: 
%100100*
5
5100*
..
)..(.S(Milk) ===
transnrTotal
productsMilkwithtranzNr
 
Code Description 
0 Food 
00 Bread 
01 Milk 
02 Drinks 
000 White (Bread) 
001 Wheat (Bread) 
010 1,5% (Milk) 
011 2% (Milk) 
020 Cola 
021 Prigat 
0200 Coca (Cola) 
0201 Pepsi (Cola) 
Transaction Item Code 
T1 000 
T1 010 
T1 0200 
T1 021 
T2 000 
T2 010 
T2 011 
T2 021 
T3 011 
T3 0200 
T4 011 
T4 0201 
T5 001 
T5 0200 
T5 011 
T5 021 
136
SOFA 2009 • 3rd International Workshop on Soft Computing Applications • 29 July – 1 August • Szeged (Hungary) – Arad (Romania)
One has to consider that the support of a category may not 
be the sum of the supports for the descendants. However, the 
support for a category can’t be lower than the support for one 
of its descendants. 
For example, because a customer may have bought more 
than one item from the same category (see transaction T2: 
milk 1.5% and milk 2%) in the same transaction, the support 
for the “milk category is counted only once, and it’s smaller 
(by 1) than the sum of the supports of these items. 
If a category doesn’t have minimum support then its 
descendants won’t meet minimum support either. This is 
because the support for a category is lower or equal to the 
sum of the supports for its descendants and it’s obvious that if 
the sum of sup1 and s2 is lower than min_sup than neither s1 
nor s2 can be bigger than min_sup. 
 
Fig.2. Rule Inheritance 
 
IV. DEPTH FIRST MULTI-LEVEL APRIORI 
ALGORITHM (DFMLA) 
 
Description. This is an improvement of the Apriori 
algorithm presented by Agrawal and Strikant [2] in witch 
there are generally not interested in all implications but only 
those that are important like Han and Fu demonstrated [8]. 
Here importance is measured by support.  
The algorithm is for multi-level databases which gives us 
the advantage of using a progressive deepening method that 
is developed by extending the Apriori algorithm for mining 
single-level association rules in [8]. The method first finds 
frequent data items at the topmost level and then 
progressively deepens the mining process into their frequent 
descendants at lower concept levels. 
At first, this algorithm views the database as single-level 
which contains only itemsets from the first concept level. It 
uses the original Apriori algorithm to find association rules 
between these itemsets (which, in our example, are 
categories). 
Each time we find a new rule between 2 itemsets (f ?g) 
we test it for importance (by support and confidence). If it 
doesn’t pass the importance threshold then there won’t be 
association rules between their descendents. 
However if it does pass the importance tests, it means that 
there may be rules between their descendants. We take all the 
descendants of these itemsets from the next concept level and 
try to find a rule f2?g2 (where f2 and g2 are subsets of f, g 
from the previous concept level). If we find a rule we 
immediately continue at next concept level in the same way 
until we don’t find interesting rules. 
Implementation In implementation, this algorithm is like 
the Apriori algorithm for single-level databases presented in 
[2]. The only difference is that after the importance tests, if 
the rule between the 2 itemsets passes, it does the steps 
described in the following recursive function: 
 
function CheckDescendants(k, lvl, f, g )  
{       
    for (f2=1; f2<nr.of.itemsets.descented.from.f.on.lvl+1;  
f2++) 
       for (g2=1;  
g2<nr.of.itemsets.descented.from.g.on.lvl+1; g2++)  
          if (f2?g2 rule is interesting) 
    { 
               Add.Itemset(f2,g2) to L[k, lvl+1];     
               CheckDescented( k, lvl+1, f2, g2);   //if f2 and g2 
exist…} 
} 
 
Where: 
- k – nr of items from a k-itemset 
- f,g are the itemsets from the f?g rule 
- lvl – level where f,g are found 
 
This function must be called before trying to find another 
rule at the same level (lvl) because we need to know exactly 
which subsets (f and g) were used in this rule. 
Because f2 and g2 are derivations of the f and g subsets, 
the transactions (T(k,lvl+1)) in which they are found are 
actually some (or all) of the transactions in which f and g are 
found (T(k,lvl)).  
Thus, when the “CheckDescendants()” function is called, 
we can send it a selection (or just the IDs) of transactions 
from T(k,lvl). The function won’t have to scan the entire 
database to check for importance, it will just scan the 
selection of transactions. In fact, as we progress deeper, from 
on concept level (lvl) to the one following it (lvl+1), the 
number of transactions should be smaller and smaller and 
scanning will be faster and faster 
In implementation, to obtain T(k,lvl+1) we filter, from the 
previous transaction selection, only the transactions which 
contain items common to the f and g itemsets. 
 
V. PERFORMANCE STUDY & COMPARISONS 
 
These algorithms don’t try to find rules between all 
subsets from the k-itemsets (k>1) that remained after the 
prune step, [8] find rules between derivations of the subsets 
from the rules that were previously found to be interesting. 
For example, if they find a rule at the first concept level, a 
rule between categories, they go deeper and try to find which 
subcategories that rule apply to. 
If a rule doesn’t pass the importance threshold, then there 
won’t be important rules between subcategories at the 
following concept levels. 
To make this advantage more noticeable, we will compare 
the first algorithm to a similar multi-level Apriori algorithm 
described in [7]. This algorithm uses the candidate generation 
algorithm in Apriori on each concept level. When it finishes 
with one level it moves on to the next level. We can say that 
this is a width-first algorithm and that the algorithms 
presented in this paper are depth-first.  
137
 Mirela Pater, Daniela E. Popescu • Market-Basket Problem Solved With Depth First Multi-Level Apriori Mining Algorithm
On the database presented in the following example, 
DFMLA algorithm will test the importance of the rules 1?2, 
1.1?2.1, 1.1.1?2.1.1, 1?3 and 2?3. Rule 1?3 and 2?3 
won’t pass the importance test and thus, the DFMLA 
algorithm won’t test rules 1.1?3.1, 1.1?3.2, 2.1?3.1 etc, 
contrary to the other algorithm which will eventually test the 
rules when it gets to the respective level. 
 
 
 
Fig.3.Multi-level Apriori vs. DFMLA. Performance comparison 
 
An important disadvantage that DFMLA algorithm has is 
its memory usage. In a database with many concept levels, 
when we get to the last level, TIDs for each previous level 
will be retained. 
The studies were made on a multi-level database, created 
as presented at section 4. The database had 45000 database 
entries, 7500 transactions, each one having no more than 15 
items; 750 distinct items which were encoded on 10 concept 
levels. 
The experiments were conducted at a Pentium-IV machine 
with 512 MB memory at 2.8 GHz, running Red Hat Linux 
7.3. The program was implemented in Java. 
 
VI. CONCLUSIONS 
 
The scope of data-mining has been broadened by the study 
on the mining on multiple-level rules. The mining of multiple 
level rules can provide more information for the users and 
enhance the flexibility and power of data-mining systems. 
This algorithm finds new rules by using the knowledge 
gained from previously found rules. If a rule fails at the first 
concept level many rules from the following concept levels 
won’t be studied and thus,  the more concept levels a 
database has, the faster it will be to get results compared to 
other algorithms. 
Storing the database in the primary memory is no longer a 
problem. On the other hand, storing the candidates causes 
trouble in situations, where a dense database is considered 
with a small support threshold. This is the case for any 
algorithm using candidates. Therefore, it would be desirable 
to look for a method which stores candidates in secondary 
memory. This is an obvious topic for future research. 
Our conclusion is that DFMLA is a simple, 
practical, straight forward and fast algorithm for finding all 
frequent itemsets. 
 
REFERENCES 
 
[1] Agrawal R., Imielinski T. and Swami A., “Mining 
association rules between sets of items in large databases”. In 
Proc. of the ACM SIGMOD Conference on Management of 
Data, pages 207–216, 1993 
[2] Agrawal R. and Strikant R., “Fast algorithms for mining 
association rules”, In Proc. 1994 Int. Conf. Very Large Data 
Bases (VLDB’95), Santiago, Chile, 487-499, 1994 
[3] Agrawal R. and Strikant R., “Mining sequential patterns”, 
In Proc. 1995 Int. Conf. Data Engineering, 3-14, Taipei, 
Taiwan, 1995 
[4] Agrawal R.,Mannila H., Srikant R., Toivonen H., and 
Verkamo A.I., “Fast discovery of association rules”, In 
Advances in Knowledge Discovery and Data Mining, pages 
307–328, 1996 
[5] Dunham M.H. “Data Mining Introductory and Advanced 
Topics”, Prentice Hall, Pearson Education, Inc, Upper 
Saddle River, New Jersey, 2003 
[6] Frawley W.J., Piateetsky-Shapiro G. and Matheus C.J. 
“Knowledge discovery in databases: An overview”, In G. 
Piateetsky-Shapiro and W.J.Frawley, eds. Knowledge 
Discovery in Databases, 1-27, AAAI/MIT Press, 1991 
[7] Han J. and Fu Y., “Discovery of multiple-level 
association rules from large databases”, In Proc. 1995 Int. 
Conf. Very Large Data Bases (VLDB’95), Zurich, 
Switzerland, 420-431, 1995 
[8] Han J. and Fu Y., “Mining Multiple-level Association 
Rules in Large Databases”, IEEE Transactions on Knowledge 
and Data Engineering, Vol.11, No.5, 1999 
[9] Han Y. F. Jiawei, “Discovery of multiple-level 
association rules from large databases.”, In Proc. of the 21st 
International Conference on Very Large Databases (VLDB), 
Zurich, Switzerland, 1995.  
[10] Huhtala Y., Kinen J., Porkka P., and. Toivonen H, 
“Efficient discovery of functional and approximate 
dependencies using partitions.”. In ICDE, pages 392–401, 
1998. 
[11] Huhtala Y., Karkkainen J., Porkka P., and H. Toivonen, 
TANE: “An efficient algorithm for discovering functional 
and approximate dependencies.” The Computer Journal, 
42(2):100–111, 1999. 
[12] Knuth D. E.. “The Art of Computer Programming Vol. 
3.”. Addison-Wesley, 1968. 
[13] Mannila H., Toivonen H., and Verkam A. I., 
“Discovering frequent episodes in sequences.”. In 
Proceedings of the First International Conference on 
Knowledge Discovery and Data Mining, pages 210–215. 
AAAI Press, 1995. 
[14] Ozden B., Ramaswamy S., and Silberschatz A.. “Cyclic 
association rules.”.  In ICDE, pages 412–421, 1998 [11] Park 
J. S., Chen M.-S., Yu and P. S., ”An effective hash based 
algorithm for mining association rules”. In M. J. Carey and 
D. A. Schneider, editors, Proceedings of the 1995 
138
SOFA 2009 • 3rd International Workshop on Soft Computing Applications • 29 July – 1 August • Szeged (Hungary) – Arad (Romania)
ACMSIGMOD International Conference on Management of 
Data, pages 175–186, San Jose, California, 22–25, 1995 
[15] Park J. S., Chen M.-S., and Yu P. S.. “An effective hash 
based algorithm for mining association rules.” In M. J. Carey 
and D. A. Schneider, editors, Proceedings of the 1995  
ACMSIGMOD International Conference on Management of 
Data, pages 175–186, San Jose, California, 22–25 1995. 
 [16] Rajkumar N., Karthik M.R and Sivanandam S.N., “Fast 
Algorithm for Mining Multilevel Association Rules”, IEEE 
Web Technology and Data Mining. 687-692,   TENCON, 
2003 
 [17] Sarasere A., Omiecinsky E., and Navathe S., “An 
efficient algorithm for mining association rules in large 
databases.”,  In Proc. 21st International Conference on Very 
Large Databases (VLDB), Zurich, Switzerland, Also Gatech 
Technical Report No. GIT-CC-95-04, 1995. 
 [18] Strikant R. and Agrawal R., “Mining generalized 
association rules”, In Proc. 1995 Int. Conf. Very Large Data 
Bases (VLDB’95), Zurich, Switzerland, 1995 
[19] R. Srikant and R. Agrawal, “Mining sequential patterns: 
Generalizations and performance improvements.” Technical 
report, IBM Almaden Research Center, San Jose, California, 
1995. 
[20] Toivonen H., “Sampling large databases for association 
rules”. In The VLDB Journal, pages 134–145, 1996 
[21] Thomas S., Bodagala S., Alsabti K., and Ranka S.. An 
efficient algorithm for the incremental updation of 
association rules in large databases. In Knowledge Discovery 
and Data Mining, pages 263–266, 1997. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
