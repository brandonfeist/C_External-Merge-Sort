Improving the Performance of a Parallel Data Mining 
Algorithm using Communication Message Scheduling 
 
Nunnapus Benjamas and Putchong Uthayopas 
High Performance Computing and Network Center (HPCNC),  
Department of Computer Engineering, Kasetsart University, Bangkok 10900, Thailand 
{g4885043, pu}@ku.ac.th 
 
 
Abstract-In this paper, we propose several strategies for 
communication scheduling that can help increase the speed of 
parallel data mining algorithm. The proposed method based       
on the smart scheduling that enable parallel data mining   
application to fully utilize the interconnection infrastructure for 
communication. We compare the proposed strategies with a non 
communication scheduling algorithm on 2 to 32 cores multicore 
cluster systems using simulation. The experimental results show 
that a significantly performance improvement can be gained when 
a smart communication scheduling is included in the data mining 
algorithm. 
I. INTRODUCTION AND RELATED WORKS 
One of the key challenges that human society are facing is 
how to extract the meaning out of the massive amount of data 
being produced and collected every day. Data mining 
technique is frequently used to extract novel significant or 
interesting knowledge that is implicit in these large volumes of 
data. Association rule mining [1], [2] is one of the most 
important techniques in data mining. This technique is used to 
extract significant patterns from transaction databases and 
generates rules used in many decision support systems. 
Anyway, the mining of large volumes of data is very compute 
intensive. Therefore, parallel computing is used to improve the 
performance of association rule mining applications.  
There are three broad strategies for parallelizing association 
rule mining algorithms [3]. Firstly, task parallelism (or control 
parallelism) is exploited by having each process executes 
different operations on the database. Secondly, SPMD 
parallelism can be employed by having a set of processes 
executes the same algorithm in parallel on difference partitions 
of the database [4], [5], [6]. Thirdly, independent parallelism is 
exploited when processes are executed in parallel in an 
independent way; generally each process has access to the 
whole database [10].   
All of these strategies must be implemented on the parallel 
computing systems. Among the most popular is shared 
memory multiprocessor system [6] and distributed memory 
multicomputer system [5], [7]. Most of the works address how 
an association rule mining algorithm will perform on an ideal 
distributed system. Nevertheless, parallel data mining 
applications usually generates a lot of communication 
messages. Thus, a proper optimization of these 
communications on an infrastructure has a potential to 
substantially increase the performance of a parallel data mining 
application. 
In this paper, we propose that the proper scheduling of 
communication message that consider the right sequence of 
message transmission, message size, and interconnection 
network utilization will yield a great performance enhancement 
to parallel data mining application. The parallel FI-growth data 
mining is analyzed and used to demonstrate the proposed 
concept. Using simulation, it can be shown that by properly 
schedule the communication message and interconnection 
switch usage in data mining applications, substantial 
improvement in performance can be attained.  
The rest of the paper is organized as follows: Section 2 
provides the system model. Section 3 presents the designing 
the scheduling strategies. Section 4 describes the experiments 
and discusses the results. Finally, Section 5 concludes the 
paper and discusses some future development. 
II. SYSTEM MODEL 
In this paper, the parallel FI-growth program is modeled 
using a directed acyclic graph (DAG). Given a set of tasks T = 
{T0, T1, ... ,Tn} to be executed, a directed acyclic graph can be 
defined as G= (V, E), where V is a set of nodes {v1,v2, …,vn} 
and E is the set of directed edges {eij}. The edge eij in the DAG 
connecting nodes vi and vj represents the communication and 
precedence constraints among the nodes. The weight of an 
edge denoted by W(eij) is the communication cost of the edge. 
(See the task graph in Fig. 1). A node vi in the DAG 
corresponds to task Ti. Each node of the task graph has a 
weight W(vi) that represents the computing cost. 
In this application model, we assume that a task is an 
indivisible unit of computation, and that a processor executes 
one task at a time. When all input data to a task have arrived to 
it, the task can be started to execution.   
To express resource contentions, we assume that only a 
limited number of communications can pass from the processor 
into the network and from the network into the processor at 
one communication in time. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. The parallel FI-growth task graph (left) and Machine model (right) 
 
In this work, we use a multicore cluster system as the target 
architecture. A multicore cluster system is a set machine M = 
{m1, m2, …, m|M|} of |M| machines and each machine mi  
include a set processor Pi = {p1, p2, …, p|pi|} of |Pi|  processor. 
Each processor pj of machine mi has multiple core Ci = {c1, 
c2, …, c|Cij|} of  |Cij| cores.  
In Fig. 1, multicore cluster system is a tree-structure with 
cores (c) as leaves, processors (p) as intermediate nodes being 
a parent for cores, machines (m) as intermediate nodes 
combining processors and the entire machine or system (S) as 
root node. Let S = (V, E, WV, WE), where V = {v1, v2, …, vn} is 
a set of nodes, and E is a set of undirected edges. V represents 
the set of cores in the system, and an edge eij ? E represent 
communication link between core vi and vj. WV(vi) and WE(eij) 
is the computational cost of core vi and the communication cost 
between core vi and vj, respectively. 
In this paper, a multi-core cluster system will have a 
hierarchy of communication networks. First, an inter-core 
communication using the shared-cache between cores on the 
same processor. Second, an inter-processor communication 
using shared-memory between cores on the different processor. 
Finally, an inter-machine communication based on message 
passing between machines. In this work, we assume that the 
bandwidth between cores is 20 GBps, bandwidth between 
processors is 10 GBps, and bandwidth between machines is 1 
GBps. It is assumed that the number of processors and the 
number of tasks are static and known beforehand.  In this paper, 
we use an all-to-all personalized communication to illustrate 
that a proper communication scheduling of FI-growth data 
mining can substantially increase the speed of the application. 
 
III. PROPOSED COMMUNICATION SCHEDULING CONCEPT  
In the following subsection, various scheduling strategies are 
proposed for parallel FI-growth data mining. Then, their 
impacts on application performance are studied using 
simulation.  
A. Non-Bandwidth Scheduling 
In this work, we assume that each task will have all its 
communication messages queued prior to the real transmission. 
Thus, a schedule is created for communication task by 
assigning them a priority. We proposed three ways of 
determining the priority of a node as follows. 
L-Max: The task with maximum message size or maximum 
communication weight, max(W(eij)), will be scheduled first. 
L-Min: The task with minimum message size or minimum 
communication weight, min(W(eij)), will be scheduled first. 
L-MaxMin: The task with maximum message size will be 
scheduled first followed by a task with minimum message size. 
Maximum message size and minimum message size will be 
scheduled alternately. 
B. Bandwidth Scheduling 
In order to further enhance the scheduling strategy for 
parallel FI-growth application, the communication bandwidth 
utilization of the interconnection network is considered. In 
general, interconnection network in a cluster usually consists 
of a switch fabric and a network links to computation node 
with a certain bandwidth such as 1Gbps. When more than one 
communication sessions are initiated to a single target node, 
the switch to node link bandwidth will be shared. The 
bandwidth is then reduced and more latency is inserted into the 
computation.   
To avoid this problem, we proposed that the communication 
should be schedule. In our scheme, the execution consists of 
two step; nodes partitioning and communication scheduling. 
These two steps execute alternately. For each step, the 
execution will proceed as follows: 
Partitioning step: A set of n node is divided into two 
balanced partitions and then recursively divide the two 
partitions until the number of node of each partition is one. 
Communication step: In each partitioning step, all nodes in 
each partition will be scheduled to send/receive message 
to/from all nodes within another partition. The number of 
communication phases will be equal to number of nodes in the 
divided partition. Total of communication phases is n-1.  An 
example of how the partitioning works is given in Fig. 2. 
            
Partition#1 (0 1 2 3)  (4  5 6 7) Comm.
Sender  0 1 2 3  4  5 6 7 Phase 
Receiver 4 5 6 7  0  1 2 3 I 
 5 6 7 4  1  2 3 0 II 
 6 7 4 5  2  3 0 1 III 
 7 4 5 6  3  0 1 2 IV 
Partition#2 (0 1) (2 3)  (4  5) (6 7)  
Sender  0 1 2 3  4  5 6 7  
Receiver 2 3 0 1  6  7 4 5 V 
 3 2 1 0  7  6 5 4 VI 
Partition#3 (0) (1) (2) (3)  (4)  (5) (6) (7)  
Sender  0 1 2 3  4  5 6 7  
Receiver 1 0 3 2  5  4 7 6 VII 
 
Figure 2. Example of results of bandwidth scheduling strategy of 8 nodes  
 
 
1 
5   
0 
9  
13  
17  
21  
25  
2 
6   
10  
14  
18  
22  
3 
7    
11  
15  
19  
23  
4   
8   
12  
16  
20  
24  
p
m
p
c c c c
p 
m 
p 
c c c c 
S 
From the example in Fig. 2, it is a result of bandwidth 
scheduling strategy of 8 nodes. In Patition#1, there are two 
partitions, (0, 1, 2, 3) and (4, 5, 6, 7). There are 4 nodes per 
partition. The number of communication phases is also equal to 
the number of nodes per partition. Let an order pair (a, b) 
denotes the communication from node a to node b. Then, the 8 
scheduled communication in the first communication phase 
(Comm. Phase I) is given by a set: {(0,4), (1,5), (2,6), (3,7), 
(4,0), (5,1), (6,2), (7,3)} and the total number of  
communication in this phase is 7.  
The benefit of our proposed strategy is that the 
communication between a pair of node is contention free. 
Hence, the full utilization of available interconnection 
bandwidth is ensured. The result is an improved in attainable 
performance and execution speed of the parallel data mining 
algorithm. 
 
IV. EXPERIMENTAL EVALUATION 
In order to generate the test task graph, we mined a 60 
million transaction database using the parallel FI-growth 
program [8]. We utilized the standard “IBM synthetic data 
generator” [9] to synthesize a transaction database. We used 
1000 unique items to create 60 million records; each has 
average transaction length of 10. The communication volume 
associated with each edge varies from 96 bytes to 768.8 Kbytes.  
After the test graph is generated, we can run a scheduling 
simulator on this task graph using different scheduling 
strategies. To evaluate these scheduling strategies, we ran all of 
the proposed strategies on various kind of cluster architecture 
as listed in Table I. 
TABLE I 
MULTI-CORE VARIATION ARCHITECTURE 
Total number 
 of cores 
# of machines - # of processors per machine - 
# of cores per processor 
32 1-1-32 , 1-2-16 , 1-16-2 , 1-32-1 , 2-1-16 , 
2-2-8 , 2-4-4 , 2-8-2 , 2-16-1 , 4-2-4 , 
4-4-2 , 8-2-2 , 16-1-2 , 16-2-1, 32-1-1 
16 1-1-16 , 1-2-8 , 1-4-4 , 1-8-2 , 1-16-1 , 
2-1-8 , 2-2-4 , 2-4-2 , 2-8-1 , 4-1-4 , 4-2-2 , 
4-4-1 , 8-1-2 , 8-2-1 , 16-1-1 
8 1-1-8 , 1-2-4 , 1-4-2 , 1-8-1 , 2-1-4 , 
2-2-2 , 2-4-1 , 4-1-2 , 4-2-1 , 8-1-1 
4 1-1-4 , 1-2-2 , 1-4-1 , 2-1-2 , 2-2-1 , 4-1-1 
2 1-1-2 , 1-2-1 , 2-1-1 
 
All the scheduling strategies in our simulator have been 
written in Java programming language. All the experiments are 
conducted on PC with Core 2 Duo 1.66 GHz CPU, 2.00 GB 
memory and hard disk 140 GB. The operating system used is 
Windows Vista. 
 
0
10
20
30
40
50
60
70
80
90
Max Min MaxMin L-Max L-Min L-MaxMin
Ru
n 
Ti
m
e (
se
c.)
Scheduling Strategies
2 cores
1-1-2
1-2-1
2-1-1
 
(a) 
 
  
0
10
20
30
40
50
60
70
80
90
Max Min MaxMin L-Max L-Min L-MaxMin
Ru
n 
Ti
m
e (
se
c.)
Scheduling Strategies
4 cores
1-1-4
1-2-2
1-4-1
2-1-2
2-2-1
4-1-1
 
(b) 
 
 
0
10
20
30
40
50
60
70
80
90
Max Min MaxMin L-Max L-Min L-MaxMin
Ru
n 
Ti
m
e (
se
c.)
Scheduling Strategies
8 cores 1-1-8
1-2-4
1-4-2
1-8-1
2-1-4
2-2-2
2-4-1
4-1-2
4-2-1
8-1-1
 
(c) 
 
010
20
30
40
50
60
70
80
90
Max Min MaxMin L-Max L-Min L-MaxMin
Ru
n 
Ti
m
e (
se
c.)
Scheduling Strategies
16 cores 1-1-16
1-2-8
1-4-4
1-8-2
1-16-1
2-1-8
2-2-4
2-4-2
2-8-1
4-1-4
4-2-2
4-4-1
8-1-2
8-2-1
16-1-1
 
(d) 
 
0
10
20
30
40
50
60
70
80
90
Max Min MaxMin L-Max L-Min L-MaxMin
Ru
n 
Ti
m
e (
se
c.)
Scheduling Strategies
32 cores 1-1-32
1-2-16
1-16-2
1-32-1
2-1-16
2-2-8
2-4-4
2-8-2
2-16-1
4-2-4
4-4-2
8-2-2
16-1-2
16-2-1
32-1-1
 
(e) 
 
Figure 3. Run time of parallel FI-growth of the scheduling variation strategies 
using (a) 2 cores, (b) 4 cores, (c) 8 cores, (d) 16 cores, and (e) 32 cores          
on variation architectures 
 
0.0
0.5
1.0
1.5
2.0
2.5
3.0
1-1-2 1-1-4 1-1-8 1-1-16 1-1-32
Ru
n 
Ti
m
e (
se
c.)
Multi-core variation architecture
 
Figure 4. Run time of parallel FI-growth of the bandwidth scheduling strategy 
on variation architectures 
 
 
Fig. 3a, 3b, 3c, 3d, and 3e illustrate the run times for six 
scheduling strategies using 2, 4, 8, 16, and 32 cores, 
respectively. The results clearly show the use of 
communication bandwidth scheduling (Max, Min, and 
MaxMin) result in a substantially lower execution time than 
the non-bandwidth scheduling (L-Max, L-Min, and L-
MaxMin).  
Fig. 4 shows the run times for bandwidth scheduling 
strategies using 2, 4, 8, 16, and 32 cores, respectively. The 
results show that run-time drops rapidly as the number of 
processors increase from 2 to 8, and decreases more slowly as 
additional processors are used. This is caused by the additional 
communication overhead incurred. 
V. CONCLUSION 
In this paper, we propose a set of communication scheduling 
strategies that helps reduce the execution time of parallel FI-
growth data mining algorithm. The results show that adding 
bandwidth scheduling in data mining applications can 
substantially help improve the performance of the application. 
Currently, we are working on integrating the computation and 
communication scheduling into a unified framework that is 
applicable to a broader class of knowledge discovery algorithm. 
This will result in the potential for the development of fast, 
scalable data mining application that can handle a massive 
amount of data.  
REFERENCES 
[1] R. Agrawal, T. Imielinski, and A. Swami, “Mining Association Rules 
between Sets of Items in Large Databases”, In Proceedings of the ACM 
SIGMOD International Conference on Management of Data, 1993, pp. 
207-216. 
[2] R. Agrawal and R. Srikant, “Fast Algorithms for Mining Association 
Rules”, In Proceedings of the 20th International Conference on Very 
Large DataBases, 1994, pp. 487-499. 
[3] D.B. Skillicorn, “Strategies for parallel data mining”, In IEEE 
Concurrency, vol. 7, pp. 26-35, 1999. 
[4] R. Agrawal and J.C. Shafer, “Parallel Mining of Association Rules”, In 
IEEE Transactions on Knowledge and Data Engineering, vol. 8, no. 6, 
pp. 962-969, 1996. 
[5] J.S. Park, M. Chen, and P.S. Yu, “An Effective Hash-Based Algorithm 
for Mining Association Rules”, In Proceedings of the ACM SIGMOD 
International Conference on Management of Data, 1995, pp. 175-186. 
[6] O.R. Zaïane, M. El-Hajj, and P. Lu, “Fast Parallel Association Rule 
Mining without Candidacy Generation”, In Proceedings of the IEEE 
International Conference on Data Mining, 2001, pp. 665-668. 
[7] A. Javed and A. Khokhar, “Frequent Pattern Mining on Message Passing 
Multiprocessor Systems”, In Distributed and Parallel Databases, vol. 16, 
no. 3, pp. 321-334, 2004. 
[8]  B. Manaskasemsak, N. Benjamas, A. Rungsawang, A. Surarerks, and P. 
Uthayopas, “Parallel association rule mining based on FI-growth 
algorithm”, In Proceedings of the International Conference Parallel and 
Distributed System, vol. 2, pp. 1-8, 2007. 
[9] R. Srikant, “Synthetic Data Generation Code for Association and 
Sequential patterns”, Available from the IBM Quest web site at 
http://www.almaden.ibm.com/cs/projects/iis/hdb/Projects/data_mining/da
tasets/syndata.html 
[10] K.M. Yu, J. Zhou, and W. Hsiao, “Load Balancing Approach Parallel 
Algorithm for Frequent Pattern Mining”, In Parallel Computing 
Technologies, pp. 623-631, 2007. 
