2010 International Conference on Computer Application and System Modeling (ICCASM 2010) 
A New Improvement of Apriori Algorithm for Mining Association Rules 
Ou Ping 
East China Institute of Technology 
Nanchang, China 
e-mail: hys8418@163.com 
Abstract-among the many mining algorithms of association 
rules, Apriori Algorithm is a classical algorithm that has 
caused the most discussion; it can effectively carry out the 
mining association rules. However, based on Apriori 
Algorithm, most of the traditional algorithms existed "item 
sets generation bottleneck" problem, and are very time­
consuming. An enhance algorithm associating which is based 
on the user interest and the importance of itemsets is put 
forward by the paper, incorporate item that user is interested 
in into the itemsets as a seed item, then scan the database, 
incorporate all other items which are in the same transaction 
into item sets, Construct user interest itemsets, reduce 
unnecessary itemsets; through the design of the support 
functions algorithm not only considered the frequency of 
itemsets, but also consider different importance between 
different itemsets. The new algorithm reduces the storage 
space, improves the efficiency and accuracy of the algorithm. 
Keywords- Association Rules; Apriori Algorithm;Importance 
of Frequent Itemsets; Improved Algorithm 
I. INTRODUCTION 
Association rules were presented by R. Agrawal and 
others in 1993, it is an important research issue in data 
mining. It is the most typical application is to discovery new 
useful information in the sales transaction database. For 
example, 85% of customers buy goods A and B, while C and 
o to buy goods, to find all the similar rules for determining 
sales strategy is useful. As the database is usually very large, 
therefore it requires an efficient algorithm to mining 
association rules. Apriori algorithm is the most classical 
algorithm for mining association rules, its basic idea is to use 
the known (k -1) dimensional k frequent itemsets to 
generate the frequent itemsets, that is, using the known 
(k -1) dimensional k frequent itemsets to generate 
candidate itemsets, and then scan a database to determine 
whether candidate itemsets frequent itemsets. In this process, 
there are insufficient: 
If the generation before the candidate itemsets can be 
judged by some of the non-candidate itemsets frequent 
itemsets, Then, it can not generate these can determine in 
advance the non-frequent item sets, this can prevent the 
connection of these candidate itemsets generation time 
overhead, and avoid the time when the cost of scanning the 
database. 
In the linker to repeat the same item more too, if we can 
avoid duplication of comparison, can improve the efficiency 
of the algorithm. 
Gao Y ongping 
East China Institute of Technology 
Nanchang, China 
e-mail: ypgao@ecit.edu.cn 
Each candidate itemsets generation later, they have to go 
back to scan the database to determine whether these 
candidate item sets are frequent itemsets, In the process of 
scanning the database, and some can be judged not go 
scanning the items or services was repeatedly scanned, If you 
can avoid these unnecessary scans, you can improve the 
efficiency of Apriori algorithm. 
II. MINING ASSOCIA nON RULES 
Set 1 =  {i1' iz" . " im} is a collection of items, set 
D = {I;, Tz, .. " z;,} is a collection of database transactions 
related to task, where each transaction is a collection of items 
made 1; ? I. 
Set A is an itemsets, itemsets containing k items is called 
k - item sets. Transaction 1; contains A if and only if 
A ? Ii' The support degree of item sets A in transaction 
sets D that is to say, sup (A) is the percentage of the 
transaction with Au B in D, that is, sup( A) = P( A u B) . 
If the sup(A) ? minsup , where minsup is the given 
minimum support is said to A for the frequent item sets. 
Association rules is a containing type like A => B , in 
which A c I and A (\ B = ¢ . The confidence 
conf(A => B) of Association rules A=> B in the 
transaction sets D is the percentage of the transaction, 
which includes A and B in D at the same time, that is 
conf(A => B) = PCB I A). 
Therefore, the problem of mining association rules can be 
converted into two sub-issues: 
• To identifY all the frequent item sets, that are set to 
meet the pre-defined minimum support ( minsup ). 
• The frequent item sets generated by the strong 
association rules, these rules must also meet the 
minimum support and minimum confidence. Issues 
(2) implementation is simple and easy. The 
performance of association rule mining algorithm 
focused on issues (1), in which the most algorithms 
are focused on how to efficiently discover frequent 
item sets. 
At present, there have been a number of association rules 
mining algorithm, which first proposed by R. Agrawal, etc. 
The Apriori Algorithm is a more efficient algorithm for 
mining frequent item sets. The basic idea of the algorithm is 
to use a hierarchical order to complete the search of the 
978-1-4244-7237-6/10/$26.00 ©20l0 IEEE V2-529 
2010 International Conference on Computer Application and System Modeling (ICCASM 2010) 
iterative methods of mining frequent item sets, namely the 
use of k item sets to generate (k + 1) item sets using 
candidate set Ck to find frequent item sets Lk . First of all, to 
find a collection of frequent 1- item sets, denoted L1, LI is 
used to find a collection of frequent 2- item sets of L2, and 
L2 is used to find L3, so go on until the frequent k - item 
sets can not be found. To find each Lk requires one database 
scan. Once from the database D, the frequent item sets are 
found, the strong association rules can be directly produced 
from the minimum degree of confidence. The defect about 
the algorithm: It may need to generate a large number of 
candidate sets, Apriori may need to repeatedly scan the 
database, through the pattern matching checks a large 
collection of candidates. 
III. ApRIORI ALGORITHM 
A basic algorithm Apriori, designed by Agrawal and 
others in 1993, generated all frequent item sets, Apriori uses 
the recursive method, the core algorithm is: 
LI =find_frequent_I-itemsets( D); 
for (k=2;LH :;t¢;k++){ 
Ck = apriori _gen(Lk-l,minsup). , 
for each transaction tED {//scan L = Uk Lk for counts 
C1 = subset( Ck, t)  ;//get the subsets of t that are 
candidates 
for each candidate C E C1 
c.count+ +; 
Lk = {CE Ck I c.count ? minsup} 
} 
returnL = ukLk; Iiall of Lk; 
First, scan database once, resulting in frequent I item sets 
of LI ; and then loop, in the first k cycles, the first the 
frequent k -1 item sets through self-connection and pruning, 
to generate the candidate frequent k item sets Ck, and then 
use the Hash function to store Ck in a tree, scanning the 
database for each transaction T to use the same Hash 
functions to calculate the candidate frequent k item sets the 
transaction T contains, and make the support number of the 
candidate frequent k item sets plus 1, If the candidate 
frequent k item sets support number is greater than or equal 
to the number of minimum support, then the candidate 
frequent k item sets are frequent k item sets; the loop ends 
until the candidate frequent k item sets are not generated any 
longer. 
1) Apriori-gen Function 
Lk -1 is the parameter of function Apriori-gen, which is 
the production function of candidate set. The function returns 
the superset of set which include all large k-item sets. 
• when connecting to the database, connect Lk - 1 and 
Lk -1 to obtain a superset Ck which is the final set 
of candidate items: 
Insert into Ck 
select p[I],p[2], ... ,p[k -1],q[k -1] 
from Lk - lp,Lk - ,q 
where 
p[l] = q[I], ... ,p[k -2] = q[k -2],p[k -1] < q[k -1] 
• It will delete all the item sets C E Ck , when pruning 
the candidate item set, if some (k -1) subset of C 
are not in Lk -1 , for any items which have minimum 
support in Lk - 1 , each k -1 subset must have 
minimum support, in this way, it is easy to maintain 
the integrity of the process of generating candidate 
items. So, expand each item sets in Lk -1 using all 
possible item, then delete all (k -1) subset which is 
not in Lk -I, can obtain a superset of item sets in 
Lk . Merger operation is equivalent to using all items 
of the database to expand Lk -I , if the k -1 item set 
which is obtained after removed the first k -1 items 
in expansion item set is not in Lk -1 , then delete this 
expansions item set. p[ k -1] 
< q[ k -1] Condition 
ensures the expansion of entries will not be the same. 
After merger operation, Lk ? Ck ; similarly, in the 
deletion operation, remove item set in Ck ,if its k -1 subset 
is not in Lk -1 , also haven't deleted item sets which are 
included in. 
For example: set L3 as {{I, 2, 3}, {I, 2, 4}, {l, 3, 4}, {2, 
3, 4} }. After connected to the database, C 4 is {{ 1, 2, 3, 4}, 
{I, 3, 4, 5}}. It will delete item set {I, 2, 4, 5} (item set {I, 4, 
5} is not in L3 ) when pruning, then there only {I, 2, 3, 4} 
remains in C 4 • 
At the time of pruning, need to check a new (k -1) 
subset whether in Lk -1 • In order to make the test of 
membership faster, large items are stored in a Hash table. 
2) Subset/unction 
Candidate item set Ck is stored in a Hash tree, one node 
of Hash tree includes a linked list of item set (one leaf node) 
or a Hash table (one internal node), on internal node, each 
Bucket of Hash table point to another node. The depth of a 
Hash tree root is defined as I, the node in depth of d points 
to the node which is in depth of d + 1 , item set is stored in 
leaf. When leading an item set C , start from the root to a leaf. 
On a node which is in depth of d, need to determine which 
branch to choose, could use a Hash function on the d1h item 
of this item set, then follow the corresponding pointer in the 
Bucket; all nodes initially created as leaf nodes, When the 
number of items in a leaf node is more than a defined value, 
this leaf node is converted to an internal node. 
V2-530 
2010 International Conference on Computer Application and System Modeling (ICCASM 2010) 
Begin with the root node, Subset function search all 
candidate items which is included in a transaction t , the 
procedure includes: 
If at root node, search this leaf, find out these items 
which are included in t, and add references that point to the 
answer set to them. Else if at a internal node, and arrived to 
this node through the Hash item i , then do Hash on each 
item which is behind i in t ,  and recursively apply this 
process to the corresponding node in Buckset. For root node, 
do Hash on each item which is in t . 
The disadvantages of Apriori Algorithm: 
• The number of the candidate frequent k item sets 
generated from frequent k - 1 item sets through 
self-connection is huge. 
• To verify the candidate frequent k item sets is very 
time-consuming when scanning the entire database. 
IV. IMPROVEMENT OF ApRIORI ALGORITHM 
A. The generation of interesting itemsets 
The basic principle of resulting in a item set is: first, 
items of interest to the user entry into the item set as a seed 
set, and then scan the database for the first time, all other 
items that appears with the seed item in the same transaction 
are incorporated into the item set, when the 1 st scan is 
complete, the project focused on all the items in tum as a 
seed key to the 2nd scan database, using the same approach 
to new items entry into item set, so the cycle continues until 
there are no new items can be incorporated into the item set. 
Specific to algorithm design, can use a count structure to 
record the state of the items which is accessed in database, 
record numbers of all items in database constitute this kind 
of count structure. All record numbers of seed items are 
initialized to 1, rather than to O. In each database scan, if the 
retrieved record to a item record number in a transaction is 
not 0, then the record numbers of all items in transaction add 
1, while the state of access to this transaction is marked as 
visited, in order to prevent for repeat visits at the next time 
that it scans the database. Check whether the count structure 
has been updated after one time of database scanning. If so, 
then continue to scan the database; If not, stop scanning, all 
the recorded value is not 0 items constitute a set of user 
interest item set. 
B. Design of support functions 
In the process of generating frequent itemsets, need to 
calculate the degree of support for each item set, and then 
compare them with the support threshold value given by the 
user, it is frequent item sets if it greater than the threshold of 
support threshold value. In most existing algorithms, the 
calculation of support only considered the frequency of 
itemsets appearances, without taking into account the 
different item sets have different importance. For example, 
shopping malls sell 10,000 towels one month, but only 300 
TV sets, in the traditional association rule mining algorithm, 
the TV is likely to be ignored. However, from the view of the 
total sales amount, TV is far more than a towel, derived from 
the traditional association rules algorithm is obviously wrong. 
Therefore, the same consideration of a towel and a TV is 
obviously unreasonable, need to take into account the 
frequency of item sets and importance of item sets when 
calculate the support degree of item set, in this way, 
association rules will be more scientific and reliability. 
If only considering the frequency of item sets, then 
calculating item set support functions can be defined as: 
f(X) = numTids(tids(X))/ numTids (tids (rfJ)) (1) 
C. Set the weight of the elements in the item set 
In order to consider the importance of item sets in 
calculating support, need introduce a function which 
compute support degree after quantify the importance of 
item set, weight is a key quantitative indicators of measuring 
the importance of item set. 
Because the weight of items is a quantitative indicators 
that is used to measure the importance of item, when 
establish a database, can set a individual column to store 
weight, also could make use of a existing column to measure 
the weight. For example, in a database that records the sales 
records of shopping mall, each item will have a price, 
suppose that the Px means the price of goods I , Pmax 
means the price of the most expensive goods, then the weight 
of goods X can be calculated using the following formula: 
Wx = Px / Pmax (2) 
This calculation may be not very appropriate, but it does 
reflect to some extent the importance of each item. For this 
algorithm, the most important consideration is that existing 
algorithms often ignore these items which have great 
importance but frequency less than other items. Therefore, 
such an idea can be used to assign weight to the item in the 
item set: increase these support degree of items that have 
great importance but less frequency. In order to do this, the 
following formula can be used to assign weight to the item: 
Wx = (1- /3) x numTids (tids (x)) I max ,ci (numTids (tids ((=))) (3) 
I stands for item set, fJ is a parameter in the range of 
[0,1] , which control the degree of correlation between the 
frequency and weight of item. If fJ = 0 , then Wx = 1, that 
means there is no correlation between the weight and the 
frequency of items ,it is always constant to 1, only 
considered the frequency when calculate item set and the 
support of item set. 
D. The description and process of algorithm 
LI = init(1); 
for (k = 2; Lk > k 
-
1; k + +) 
Lk = GenltemSet(LH,minsupp); 
L=UkLk; 
R=GenAssociationRule (Lk' mincorif) ; 
In which, obtain frequent item set through GenltemSet sub 
function. Pseudo code as follows: 
procedure LargeItemGen (Lk_l, minsupp) { 
for X and Y is in LH { 
V2-53J 
2010 International Conference on Computer Application and System Modeling (ICCASM 2010) 
if first k - 2 items of X and Yare same and any subset of 
XuY is in LH { 
tids(X u Y) = tids(X) n tids(Y) 
If 
(X u Y) = min(Wx' Wy)! numTids(tids(X n Y»;) and 
f( X U Y » minsupp 
insert Xu y, tids( X n Y ) and f1: Xu Y ) into 
Lk; } 
return Lk; 
} 
And obtain association rule from frequent item set through 
GenAssociationRule sub function, Pseudo code as follows: 
procedure GenAssociationRule (Lk' mincorif) { 
for any X ELand any nonempty subset Y of X 
if f(X)! feY) ? minconf 
insert Y ? (X - Y) into R; 
return R;} 
Code shown in the above process, first, generate a set of 
items of user interest as mining object from the database, and 
then scan the database one time to represent item set with 
transaction identification number. After generating itemsets, 
assign weight to element in item set, then use the 
introduction function of the support of the weight to 
calculate the item set degree of support to generate frequent 
item set, finally, obtain association rule from these frequent 
item set. 
V. CONCLUSIONS 
Among the many mining algorithm of association rules, 
Apriori algorithm is a classical algorithm that has caused the 
most discussion. It can effectively carry out the mining 
association rules. However, most of the traditional 
algorithms based on Apriori algorithm have the problem of 
"item sets generation bottleneck" problem, and are time­
consuming. This paper presents several ideas to improve the 
algorithm, which are basically consistent with the Apriori 
algorithm. What they have in common is that the support 
given by date scanning can't be less than the frequent item 
sets of the minimum support given by the user. Meanwhile 
the algorithm put forward in this passage has its own 
advantages, That is the pruning is done prior to the database 
scan, after the pruning it is re-connected to scan the database, 
thus reducing the number of scans. 
In the algorithm efficiency, through data compression the 
data that is mined can be scanned into memory once, 
avoiding duplication of disk I/O operations. While the date 
that isn't compressed can not be read into memory one-off, 
thereby increasing the computational efficiency; besides, 
data compression can reduce the character length of each one, 
especially when we compare whether the two are the same, 
the number of characters need comparing is much less, 
which can improve the operating speed. In a word, data 
compression can save memory and reduce the time of 
comparing the candidate sets. So the speed of generating 
frequent item sets will be faster, which can solve the 
problems of Apriori algorithm better. 
REFERENCES 
[I] H.Toivonen. Sampling large databases for association rules. In Proc. 
2006 Int. ConfVery Large Data Bases(VLDB'06),pages 134-145, 
Bombay, India, Sep.2006. 
[2] Han J,Kamber M.Data Mining:Concepts and Techniques.Higher 
Education Press,200 I. 
[3] Jong S P,Ming S C,Philip S Y.An effective hash based algorithm for 
mining association rules.In Proceedings of the 2005 ACM SIGMOD 
International Conference On Management of Data.2005,24(2): 175-
186. 
[4] Han,] Pei,Y Yin and R Mao.Mining Frequent Patterns without 
Candidate Generation:A Frequent-Pattern Tree Approach. Data 
Mining and Knowledge Discovery,2004,8:53-87. 
[5] Kong Fang , Qian Xue-zhong,Research of improved apriori algorithm 
in mining association rules,Computer Engineering and Design, 
2008,v29, n16, p4220-4223. 
[6] Li Qingzhong , Wang Haiyang, Yan Zhongmin, Efficient mining of 
association rules by reducing the number of passes over the database, 
Computer Science and Technology,2008,p 182-188. 
V2-532 
