Elimination Algorithm of Redundant Association 
Rules Based on Domain Knowledge
Jing Zhang 
School of Computer and Information  
Hefei University of Technology 
Hefei, Anhui, China 
jzhang_zj@163.com 
Bin Zhang 
School of Computer and Information  
Hefei University of Technology 
Hefei, Anhui, China 
365439820@163.com
Zihua Wang 
USTC E-Business Technology Co., Ltd  
Hefei, Anhui, China 
zhwang@ustcsoft.com 
Lijun Shi 
School of Computer and Information  
Hefei University of Technology 
Hefei, Anhui, China 
Abstract— Many association rule mining algorithms have been 
developed to extract interesting patterns from large databases. 
However, a large amount of knowledge explicitly represented in 
domain knowledge has not been used to reduce the number of 
association rules. A significant number of known associations are 
unnecessarily extracted by association rule mining algorithms. 
The result is the generation of hundreds or thousands of non-
interesting association rules. This paper presents an algorithm 
named DKARM, which takes into account not only database 
itself, but also related domain knowledge, so as to eliminate 
extraction of known associations in domain knowledge. 
Experiments show this algorithm can reasonably eliminate 
redundant rules, and effectively reduce the number of rules. 
Keywords-Data Mining; Association Rules; Domain Knowledge; 
Redundant Rules
I. INTRODUCTION 
The purpose of association rule mining technique is that 
discovers the association rules which are novel, useful, 
interesting and hiding in the item set[1]. The algorithm of 
association rule mining should find the relation from the new 
and hiding item. However, most classic algorithms [2, 3] only 
consider the data itself. Whereas the abundant data related 
domain knowledge is not regarded as the priori knowledge to 
eliminate the known patterns. The result is that a lot of 
redundant rules are created within the mined knowledge. The 
association rule mining algorithms also extract a lot of known 
associations, which are regarded common knowledge in 
domain, so as to create a lot of redundancy. For eliminating the 
redundancy, most researches in the association discovery use 
the method that brings down the threshold to reduce the 
number of rules.  
In the knowledge discovery system, the Domain 
Knowledge (DK) is defined as the knowledge that guide and 
restrict the interesting knowledge on searching, or is called 
background knowledge. Usually the two definitions are not 
differentiated. DK refers to the important factors or concepts in 
a specific domain and the relations between these factors and 
concepts. The knowledge discovery algorithm shall reasonably 
make use of the DK, so as to cut down the searching time. In 
the paper, the DK is understood as the information that is 
useful for discovering knowledge. It is not only the 
professional knowledge in some application domains, but also 
includes some related knowledge on knowledge discovery and 
other related background knowledge and so on. The knowledge 
discovery shall be based on the DK, and make use of the DK to 
do the targeted knowledge discovery. On the one hand, it may 
reduce the range of the searching object and improve the 
efficiency. On the other hand, it may improve the discovery 
pattern or the interest and reliability of the result. In the 
meantime, the interest and the reliability also relate to DK itself. 
The paper presents an association rule algorithm based on 
the domain knowledge named DKARM. It not only take into 
account the DK factor on the data itself, but also improves the 
classical association rule. 
II. THE ASSOCIATION RULES MINING RESTRICTED BY 
DOMAIN KNOWLEDGE
Because some items owns the mandatory dependence as is 
known by everyone among them under guided or restricted by 
the DK, the rules that the association mining get always include 
plenty of the redundant rules. For examples, in the market 
selling database, {nail, sinker} owns the mandatory association, 
and it is a very credible rule. But the user may not be interested 
to the kind of the rules. What they are really interested in is the 
rules similarly as {diaper, beer}. The rules are really interesting 
and can’t be got according to the restriction of the DK. In the 
traditional association mining process, most mined rules may 
exist in the intense relation with the known DK. However, the 
discovery of the useful and new knowledge can’t make use of 
the rules. That leads to a result which is the interesting and 
non-interesting association rules mixed together. Therefore, the 
users have to distinguish and explain them one by one for 
discovering the interesting pattern. 
Obviously, under the DK guiding, those association rules 
obviously token on shall be put out the association mining. 
2010 Seventh Web Information Systems and Applications Conference
978-0-7695-4193-8/10 $26.00 © 2010 IEEE
DOI 10.1109/WISA.2010.23
13
This will avoid these rules being seen and extracted by users. In 
order to diminish the number of the known pattern in the 
association mining, the paper presents the DKARM algorithm, 
which use the DK as the priori knowledge to eliminate the 
redundant association rules and the rules derived from 
themselves. 
A. Instance Analysis 
Agrawal et al. in 1993[2] firstly proposes the mining 
customer transaction database association rules question 
among itemset. Its main idea is the recursion way based on the 
frequent item theory. This is a valid association rule mining 
algorithm. But it may generate lots of candidate itemsets, and 
need plenty of times to scan the database. In the process, 
plenty of time is consumed on the exchanging data in the 
database in the memory. 
Agrawal et al. has proved that the association rules own 
the following properties: 
Property 1: The subset of the frequent items is also 
frequent items. 
Property 2: The superset of the non-frequent items is also 
non-frequent. 
To the firmly flaw of the Apriori algorithm, J.Han et al. 
proposes a method which doesn’t generate the candidate 
frequent items-FP tree algorithm [3]. Its main policy is divide 
and rule. The algorithm main idea is that sets up the structure 
with no candidate sets based on the FP-tree frequent pattern. It 
compresses every transaction in the original database to a FP-
tree, reduces the mining time of the FP-Growth too much, and 
improves the mining efficiency. 
However, there is also restricting on FP algorithm. When it 
creates the conditional- pattern base to mine the frequent item, 
if the branch of the tree is too long, the recursion will still take 
time. 
Discuss the problems of mining the mandatory association 
rule without removing the explicit DK guiding through 
analyzing the instance. Considering a set of elements 
={A,B,C,D,E}, all possible combinations of these elements 
produce the sets: {AB}, {AC}, {AD}, {AE}, {BC}, {BD}, 
{BE}, {CD}, {CE}, {DE}, {ABC}, {ABD}, {ABE}, {ACD}, 
{ACE}, {ADE}, {BCD}, {BCE}, {BDE}, {CDE}, {ABCD}, 
{ABCE}, {ABDE},{ACDE},{BCDE}, {ABCDE}?Without 
considering any threshold, the number of possible subsets is 
26, and the maximum number of rules produced with these 
subsets is 180.  

If consider that the element C and D have a mandatory 
association. Notice that there are eight sets where C and D 
appear together ({CD}, {ACD}, {BCD}, {CDE}, {ABCD}, 
{ACDE}, {BCDE}, {ABCDE}). The eight sets will produce 
92 rules, and in every rule, C and D will appear. The result is 
that 51.11% of the whole amount of rules is created with the 
dependence between C and D. 
It is important that we can’t directionally remove C and D 
from in the preprocessing, because C or D may have an 
association with A?B or E. Nevertheless, we can avoid the 
combination of C and D in the same set. This eliminates the 
possibility of generating rules including both C and D. 

B. Association Rules Mining Algorithm Based on DK 
(DKARM) 
In this section, we will use an instance to describe DKARM 
on how to eliminate the redundant rules under the DK guiding. 
For studying conveniently, the redundant rules are restricted to 
including two items, which is called as the pairs of the 
redundancy item. Table 1 is the market database transaction 
table. 
 Describe the DKARM Algorithm 
Input: D: the transaction database; min_sup: minimum 
support;  : redundant rules under the DK guiding. 
Output: P: the frequent pattern set after removing the pairs 
of the redundancy items  
TABLE I. SUPERMARKET TRANSACTION DATABASE
TID ITEMSET
T100 I1,I2,I5 
T200 I2,I4 
T300 I2,I3 
T400 I1,I2,I4 
T500 I1,I3 
T600 I2,I3 
T700 I1,I3 
T800 I1,I2,I3,I5 
T900 I1,I2,I3 
TABLE II. 1-ITEMSET COUNT SORT BY DESCENDING AFTER SCAN THE 
DATABASE A TIME
ITEM Support count 
I2 7 
I1 6 
I3 6 
I4 2 
I5 2 
TABLE III. SUPERMARKET TRANSACTION DATABASE (ITEMSET SORTS 
SUPPORT )
TID ITEMSET
T100 I2,I1,,I5 
T200 I2,I4 
T300 I2,I3 
T400 I2,I1,I4 
T500 I1,I3 
T600 I2,I3 
T700 I1,I3 
T800 I2,I1,I3,I5 
T900 I1,I1,I3 
The step 1: Scan the database a time to gain 1-candidate 
sets, and remove the item whose support is less than 
min_sup to gain all frequent item that is 1-frequent itemsets. 
The item of 1-frequent itemsets sorts support by descending. 
The result is the table ?after the table? sort support by 
descending. And every transaction in the database rearrange 
14
according to the 1-frequent itemsets order to gain L, as the 
table? shows. This is ready for combine the repeat path. 
The step 2: Scan the database for the second time. Create 
the FP-tree with the branch number on the base of step 1.  
Firstly, create the root node of the tree, which is signed by 
“NULL”. And then, scan the database on the second time, 
and insert the transaction of the database to the root node 
one by one, according to the order of the L. The FP-tree 
with the branch number is created through the table 3, as 
the Figure1 shows. For example, node I3:2(12) indicates 
the node whose name is I3 and whose support is 2. The 
branch number is expressed by the string in the brackets. So 
the “(12)” is the path from the root node to the first branch 
and then from the first branch to its second sub-branch. 
TID 
Support 
Node chain 
NULL 
I2:7(1) I1:2(2)
I3:2(12) I1:4(11) 
I4:1(13) 
I3:2(21)
I3:2(112) I5:1(111) I4:1(113) 
I5:1(1221) 
I2   7 
I1   6 
I3   6 
I4   2 
I5   2 
Figure 1. FP-tree with branch number
The step 3: From the root node beginning, mine the each 
branch in order to get the 2-itemsets which is the 
combination with the first item of the branch. And then 
count the support of the item from each branch. Compare 
the result of counting to the min_sup to get the 2-frequent 
itemsets. For example in the figure 1, from the “NULL” 
node beginning, the node I2 generates the 2-itemset 
{I2I1:4(11),I2I3:2(12)+2(112),I2I4:1(13)+1(113),I2I5:1(11
1)+1(1221)}, and the node I1 generates the 2-
itemset{I1I3:2(112)+2(21),I1I5:1(111)+1(1221)}. 
According to the node order of the table ?, compute the 
node I3?I4?I5 like that to generate the each 2-itemset of 
themselves. Suppose the min_sup is 2, and we can get the 
2-frequent itemsets{I2I1:4(11),I2I3:2(12)+2(112),I2I4:1(13) 
+1(113),I2I5:1(111)+1(1221),I1I3:2(112)+2(21),I1I5:1(111
)+1(1221)}
The step 4: Remove the redundant pairs of the items with 
the DK guiding to get the set   from all 2-frequent 
itemsets. 
The step 5: According to the property of the Apriori, 
generate the K-order candidate item through the K-1-order 
item. The algorithm is end when there is an order item that 
is an empty set, and gets the all frequent pattern set P 
removed the redundant pairs.  
As the 2-itemset reserve the support of each transaction in 
the each branch, we can get the support of the 3-candidate 
set to avoid to scanning the database again or searching 
recursively the FP-tree. For example, owing to the set 
{I2I1:4(11), I2I3:2(12) +2(112), I1I3:2(112) +2(21)} is the 
frequent itemsets, the 3-itemset {I2I1I3} is also frequent. 
And according to the prefix of sharing the branch number, 
we can get its support that is 2, and that is {I2I1I3:2(112)}. 
 Algorithm illustration 
The step 1 and step 2 almost keep the consistence with the 
FP-tree algorithm. However, we increase the branch 
number in create the FP-tree for getting the information of 
the more order itemset in the follow-up steps. 
The step 3 is different from the FP-tree in the searching 
progress. The FP-tree uses the bottom-up method, but we 
do the combination of the item up-bottom.  
In the step 4, after generating the 2-candidate sets, all the 
redundant element pairs are removed from 2-frequent 
itemsets in . Due to eliminate the redundant pairs of the 
item from DK in  (as {C,D}), this does not only avoid to 
generating the rule C->D, but also avoid to generating all 
the derived rules(as {D->C,C->AD}). These rules include 
the known mandatory association. According to the 
property 1 and property 2, the step makes sure that the 
redundant pairs of the item can’t appear together in the 
follow-up frequent itemsets, and don’t appear in the 
association rule. This makes the algorithm be very valid, 
and be independent any the threshold restricting of the 
minimum support minimum confidence and so on. The 
algorithm eliminates all candidate set that includes the 
redundant pairs of items, and is independent any concept 
levels. This is the character of the DKRAM algorithm that 
is different from the other removing the redundant rule 
algorithm. 
In the first two steps, we make use of the FP algorithm to 
compress every transaction in the original database to a FP-
tree, and avoid to scanning the database too many times. In 
the step 5, we also make use of the core idea of the Apriori 
algorithm. According to the property of the Apriori, we get 
the more order itemsets through join and pruning on the 
base of the 2-itemset. Through increasing the branch 
number of the FP-tree, we reserve the support of each 
transaction in the each branch as getting the 2-itemsets. 
Therefore, we can get the support of the 3-candidate 
itemsets, and don’t recursive scan the database again. So 
the DKARM algorithm avoids the defect of scanning the 
database too many times, as well as avoids the recursive 
mining way in the FP-Growth algorithm. 
III. EVALUATION
Don’t consider the threshold of the minimum support and 
the minimum confidence. The figure 2 shows the theoretic 
computational result of the number of rules after eliminating a 
different number of the redundant pairs of the item. We use the 
theory to analyze the transaction database which includes the 
number of the transaction items from 2 to 12, and then 
respectively discuss the zero, one and two redundancy pairs of 
the dependences from the set. When the number of the 
15
redundancy pairs is zero, it is the result without dependence 
elimination. Notice that in the three cases, as the number of 
dependences is zero, the number of rules grows exponentially 
with the number of the element of the set growing, but the 
elimination of dependences does considerably reduce the 
number of the rules. Obviously, the higher the number of well 
known dependences is, the more significant will the rule 
reduction be. 
The percentage reduction of rules produced when zero, one 
and two pairs of dependences are eliminated is shown in Figure 
3. For example, the elimination of one pair generates only 55% 
of the total number of rules, and eliminates well know rules in 
45%. When two pairs are removed, only 30% of the rules are 
generated, and the reduction increases to 70%. Notice that even 
if the number of elements increases, these values represent 
saturation points for these curves. 
Meantime, the operation system is windows XP, CPU Interl 
T2050 1.60GHz and Memory is 1.00GB. We use java to 
perform the experiment of the DKARM algorithm. The 
experiment data are from the transaction of the AllElectronics 
in [1], which includes 9 transactions and 5 items, the RDG1 
dataset which includes 100 transactions and 10 items generated 
by weka, and the contact-lenses dataset which includes 24 
transactions and 12 items from weka. We set the support 0.2 
and the confidence 0.6 in the experiment. The table? shows 
the result of the experiment. 
Notice the result of the experiment from the actual data, and 
we can find that the number of rules and time is obviously cut 
down. To the user who do the data mining, eliminate the 
number of the known rules under being restricted by the DK, 
which is more important than removing the time as generating 
the association rule. The main purpose of the paper is that fully 
consider the point of the DK guiding in the mining progress. 
However, with eliminating the association pairs of dependence 
set, it will generate less candidate items. Therefore, the time of 
the algorithm certainly eliminate. 
TABLE IV. THE EXPERIMENT RESULT OF THE DKARM 
IV. CONCLUSIONS AND FUTURE WORK
The paper presents a method of mining under the DK 
guiding, which can be used to eliminate the redundancy 
association, and make sure the generated rules that is 
interesting to the user. The threshold restricting of the 
minimum support etc. doesn’t impact on the elimination. Under 
the relational domain guiding, how to gain the more 
complicated items dependences as the priori knowledge to 
guide our follow-up mining work, and how to evaluate the 
interest of the rules after mining, both them need us to continue 
to study. 
No elimination 
Eliminate one pair 
of the redundant 
items 
Eliminate two 
pairs of the 
redundant items Dataset
The
number 
of rules
Time(ms) 
The
number 
of rules 
Time(ms) 
The
number 
of rules
Time(ms)
RDG1 13 99093.7 8 83302.3 7 71713.3
contact-lenses 23 440.6 13 393.8 8 349.8
AllElectronics 10 78.0 6 62.0 4 62.0







          
1XPEHURI,WHPVQ

*H
QH
UD
WL
RQ
U
XO
HV
SDLU
SDLUV
Figure 2. Percentage reduction of rules considering one and two pairs 
of dependences 
ACKNOWLEDGMENT
This work is supported in part by Teaching and Research 
Project in Anhui Province (No.2008jyxm240) and Science 
Development Project of Hefei University of Technology 
(No.2009HGXJ0035). 
REFERENCE
[1] Jiawei Han, Micheline, Kamber. Data Mining-concepts and techniques, 
High Education Press. Morgan Kaufman Publishers, 2001. 
[2] R. Agrawal, T. Imielinski, and A. Swami. Mining association rules 
between sets of items in large databases. Proceedings of the ACM 
SIGMOD Conference on Management of data, pp. 207-216, 1993. 







          
number of items(n)M
ax
im
um
 n
um
be
r o
f a
ss
oc
at
io
n 
ru
le
s
0 pair
1 pair
2 pairs
Figure 3. Maximum number of rules eliminating zero (reference), 
one, and two pairs of dependences 
[3] J.Han,J.Pei, and Y.Yin.Mining. Frequent patterns without candidate 
generation. In Proc. 2000 ACM-SIGMOD Int. Conf. Management of 
Data (SIGMOD00), p.p. 1-12, May 2000. 
[4] Usama Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth, From 
Data Mining to Knowledge Discovery: An Overview. In: U. Fryyad, G. 
Piatetsky-Shapiro, P.Smyth and R.Uthurusamy, eds. Advances in 
Knowledge Discovery and Data Mining. Menlo Park, California: AAAI 
Press/The MIT Press, 1996, pp. 1-35. 
[5] Zaki, M. Generating non-redundant association rules, In Proceedings of 
the 6th ACM (SIGKDD'2000) International Conference on Knowledge 
Discovery and Data Mining, Boston, Massachussets, USA, pp.34-43,        
2000.  
[6] B. Liu, W. Hsu, and Y. Ma, Pruning and summarizing the discovered 
associations. Proceedings of the fifth ACM SIGKDD international 
conference on Knowledge discovery and data mining. San Diego, 
California, pp. 125-134,1999. 
16
