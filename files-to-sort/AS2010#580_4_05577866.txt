Classification of Closed Frequent Patterns Improved by Feature Space 
Transformation 
 
Cheng Hao Jin1, Gouchol Pok2, Hi-Seok Kim3, Eun-Jong Cha4, Keun Ho Ryu1 
1Database/Bioinformatics Laboratory, Chungbuk National University, South Korea 
2Department of Computer Science, Yanbian University of Science and Technology, China 
3School of Electronics & Information Engineering, Cheongju University, South Korea 
4Department of Mechanical Engineering, Chungbuk National University, South Korea 
kimsunghoyust@gmail.com, gcpok2000@gmail.com, khs8391@cju.ac.kr,  
ejcha@chungbuk.ac.kr, khryu@dblab.chungbuk.ac.kr ,  
 
 
Abstract—In some real-world applications, the predefined 
features are not discriminative enough to represent well the 
distinctiveness of different classes. Therefore, building a more 
well-defined feature space becomes an urgent task. The main 
goal of feature space transformation is to map a set of features 
defined in a space into a new more powerful feature space so 
that the classification based on the transformed data can 
achieve performance gain compared to the performance in the 
original space. In this paper, we introduce a feature 
transformation method in which the feature transformation is 
conducted using the closed frequent patterns. Experiments on 
real-world datasets show that the transformed features 
obtained from combining the closed frequent patterns and the 
original features are superior in terms of classification 
accuracy than the approach based solely on closed frequent 
patterns.  
Keywords- Closed Frequent Pattern, Feature Space 
Transformation, Classification Accuracy 
I.  INTRODUCTION 
Classification such as decision trees [1, 17], Support 
Vector Machine [2], neural network [3], Bayesian 
networks[30], k-nearest neighbor[31], case-based 
reasoning[32], etc. has been received considerable attention 
in various communities. Classification is a supervised 
learning task that can predict the class labels of unseen 
instances. If the predefined features represent well the 
distinctiveness of different classes, the classification could 
achieve high classification performance. However, in some 
cases, original single features could not satisfactorily capture 
the intrinsic structure of each class and thus cannot provide 
useful information to the classifier. Thus, in order to improve 
the classification performance, there is an emerging need for 
constructing a good feature space for the complex structural 
data. Feature space transformation, which is used to 
transform the original single feature space to another new 
more powerful feature space, can solve this problem. 
One of attempts to improve classification accuracy is to 
apply frequent patterns in the classification task. In this 
research area, frequent patterns are used as features which 
                                                          
 Corresponding author 
make classification models more accurate and easier to 
understand. Frequent pattern mining (also association 
mining) was first proposed by Agrawal, Imielinski and 
Swami [4] and widely studied in the past decades. A frequent 
pattern is generated from the given database with its 
frequency no less than user-specified minimum support 
count. Frequent patterns contain much more underlying 
semantics than single features since a frequent pattern is the 
combination of single features. However, usually a large 
number of frequent patterns can be generated at a given 
minimum threshold so that it is impracticable to use the 
whole frequent patterns as features and becomes difficult to 
interpret. This led the researchers to study closed patterns, 
which can extract the whole frequent patterns even their 
exact frequencies without any information loss. Also the 
number of closed frequent patterns is much smaller than that 
of frequent patterns. Thus, it is a good choice to use closed 
frequent patterns as features instead of the whole frequent 
patterns.  
The contributions of this paper can be summarized as 
follows: 1) We introduce closed frequent pattern-based 
feature space transformation, 2) the extensive experimental 
results on real-world datasets show that the transformed 
feature space obtained from combing the closed frequent 
patterns and single features could achieve better 
classification accuracy than that solely on closed frequent 
patterns.  
The rest of the paper is organized as follows. In Section 
II, we review related work. The basic terminology and the 
problem definition are presented in Section III. The 
extensive experimental evaluations on real-world datasets are 
reported in Section IV and our conclusion and future work 
are given in Section V. 
II. RELATED WORK 
In this paper, we use closed frequent patterns as features 
to represent the data and then classification method is 
applied on this transformed feature space. Thus, our research 
belongs to pattern-based classification task. Associative 
classification [5, 6, 7, 13, 14, 15, 16, 20, 21, 22, 23, 24, 25, 
26, 27, 28], which integrates classification and association 
mining, is related to our research topic. During last decades, 
978-0-7695-4108-2/10 $26.00 © 2010 IEEE
DOI 10.1109/CIT.2010.235
1306
2010 10th IEEE International Conference on Computer and Information Technology (CIT 2010)
there are a large number of publications about associative 
classification, however, here we list a few of them.  
CBA(Classification based on Associations) proposed by 
Liu, Hsu and Ma is the first frequent pattern-based 
classification method [5]. At first it generates all the class 
association rules and ranks them in the descending order of 
confidence and support. Rule selection is performed on the 
ranked rule list with a sequential coverage paradigm and 
prediction is based on whether the rule condition satisfies the 
test instance. If there is no rule to match the test instance, the 
takes the default class value like C4.5.  
CMAR(Classification based on Multiple Association 
Rules) another pattern-based classification method proposed 
by Li, Han and Pei [6]. To avoid classification bias and 
overfitting caused by the single rule in classification, CMAR 
predicts the class label with multiple rules and the 
classification task uses a weighted F2 on multiple high 
confidence rules. To predict a test instance, all rules 
matching the test instance are collected. If all of them have 
the same class label, assigns this class label to test instance. 
However, if they have different class labels, CMAR divides 
the rules into groups according to class labels and compare 
the effects of the groups and then assign the class label of the 
strongest group. 
CPAR(Classification based on Predictive Association 
Rules) proposed by Yin and Han [7] uses the basic idea of 
FOIL[18] in rule generation. CPAR uses a greedy algorithm 
to generate rules directly from the training data and selects 
multiple literals and builds multiple rules simultaneously.  
To avoid overfitting, it uses expected accuracy to evaluate 
rules and choose the best k rules in prediction.  
Top-k rule mining[13] discovers top-k covering rule 
groups for each row of gene expression profiles for pattern-
based classification. It uses several pruning steps, such as 
top-k pruning based on support and confidence upper bounds 
and backward pruning, to filter the uninteresting rules. Then 
RCBT classifier is build from these top-k covering rule 
groups and prediction is performed based on the 
classification score which combines the support and 
confidence measures of the rules.  
Almost all of associative classification methods have a 
drawback that should manipulate a large number of 
association rules. Thus, it is impracticable to use frequent 
patterns as features in classification tasks. To overcome this 
problem, we can use closed frequent patterns in pattern-
based classification instead of the whole frequent patterns, 
because the number of closed frequent patterns is much 
smaller than that of frequent patterns.  
The more recent works that most relate to our research 
are [8, 9, 19]. All of them use closed frequent patterns as 
features for choosing the most discriminative patterns, 
however, any of them do not mention how the transformed 
feature space affects the classification accuracy with respect 
to different minimum support thresholds. 
III. BASIC TERMINOLOGY AND PROBLEM DEFINITION 
Given a database D and a user-specified minimum 
support threshold T (0 ?T ? 1), F = {f1, f2, …, fm} is the set 
of all categorical features and C = {c1, c2, ..., ck} is the set of 
class labels. Database D contains n transactions D = {xi, 
yi} ni 1  , where xi ? F is a set of features and yi ? C is a class 
label. A subset of F is called an itemset. The support of an 
itemset X is defined as the number of instances that contain 
the itemset X and denoted as support(X). For numeric 
features, the continuous values are discretized first.  
Definition 1  Frequent Pattern 
For a database D, an itemset X is frequent if support(X) t 
n * T, where n is the number of database transactions.  
 
Example: Given a transaction database TDB in Fig. 1 (a) 
and T = 0.4, then n = 5 and minimum support count = 5 * 0.4 
= 2. The generated frequent patterns are shown in Fig 1 (b).  
 
 
Figure 1. (a) Transaction database and (b) frequent patterns 
Definition 2  Closed Frequent Pattern 
A frequent itemset X is closed if there does not exist an 
itemset Y such that X ? Y and support(X) = support(Y).  
 
Example: Still take Fig. 1 (a) as an example, where n = 5 
and minimum support count = 2. According to the property 
of closed frequent pattern, the generated closed frequent 
patterns are shown in Fig. 2 (b). From Fig. 2 (b), we can see 
that the number of closed frequent patterns is smaller than 
that of frequent patterns in Fig. 1 (b).  
 
 
Figure 2. (a) Transaction database and (b) closed frequent patterns 
1307
Definition 3  Feature Space Transformation 
Assume P = {p1, p2, …, pp} be the set of closed frequent 
patterns generated from the given database D at a given T. 
Therefore, the feature space transformation can be expressed 
as follows: D = {xi, yi} ni 1 o  D’ = {zi, yi} ni 1   where zi  ? P is 
a set of generated closed frequent patterns.   
 
Example: Still take Fig. 1 (a) as an example and the 
generated closed frequent patterns are illustrated in Fig. 2 (b). 
Then the transformed feature space is shown in Fig. 3 (b).  
 
 
Figure 3. (a) original transaction database and (b) transformed feature 
space  
IV. EXPERIMENTAL RESULTS 
We use several datasets from UCI Machine Learning 
Repository [10] to test the effect of closed frequent pattern-
based feature space transformation for the classification 
accuracy. In these experiments, we compare classification 
results of two transformed feature space. One feature space is 
from solely on closed frequent patterns and the other is from 
combining closed frequent patterns and single features. We 
show the results at a wide range of Ts. A summary of these 
datasets is shown in Table I.  
TABLE I.  DATASET USED IN EXPERIMENTS 
Dataset Instances Features Classes 
Diabetes 768 8 2 
Glass 214 9 7 
Heart-statlog 270 13 2 
Iris 150 4 3 
Waveform 5000 40 3 
Wine 178 13 3 
Zoo 101 16 7 
 
First, all the continuous features in these datasets are 
discretized into binary ones with entropy-based method [11] 
and discretizations are performed with Weka version 3.6.0 
[12]. The discretized datasets are shown in Table II.  
TABLE II.  DISCRETIZED DATASET  
Dataset Instances(n) Features(F) Classes(C) 
Diabetes 768 15 2 
Glass 214 20 7 
Heart-statlog 270 18 2 
Iris 150 12 3 
Waveform 5000 109 3 
Wine 178 37 3 
Zoo 101 34 7 
 
 The state-of-the-art C4.5 [1] and SMO in Weka and 
C5.0 in Clementine are chosen as classification models. 
Notice that, classification accuracy is the primary evaluation 
criterion for the experiments and 10-fold cross validation is 
used. The results of classification accuracy from the 
transformed feature space are shown in Table III, IV, V, VI 
VII, VIII and IX. In these tables, P is the set of closed 
frequent patterns and F ? P is the set of features union of 
original single features and closed frequent features. From 
different maximum value of T  listed in each table, we can 
see that the stop point of generating closed frequent is 
different in each dataset respectively. These tables also give 
average classification accuracy. From these tables, we 
conclude that at most cases the transformed feature space 
from F ? P achieves much higher classification accuracy 
than features only from P.    
TABLE III.  CLASSIFICATION RESULTS ON DIABETS 
T(%) C4.5 SMO C5.0 
P F?P P F?P P F?P 
5 77.99 77.99 76.30 76.30 79.95 79.95 
10 77.86 77.86 77.08 77.08 79.30 79.30 
15 76.56 76.56 74.09 74.09 78.65 78.65 
20 76.17 75.78 75.39 75.00 79.30 79.56 
25 75.91 75.91 75.65 74.09 77.73 79.56 
30 71.22 76.04 70.70 75.13 70.96 78.65 
35 71.22 75.91 67.84 76.69 70.96 78.39 
40 69.40 76.04 68.49 75.78 70.31 78.39 
45 69.40 76.17 68.10 75.91 70.44 78.52 
50 69.40 75.91 68.62 74.74 70.31 78.39 
55 69.14 75.78 68.23 77.08 70.05 78.65 
60 69.14 75.78 68.23 77.08 70.05 78.65 
65 69.14 75.78 68.23 77.08 70.05 78.65 
70 70.05 75.78 69.14 77.08 70.05 78.65 
75 67.84 75.78 67.84 76.95 67.84 78.65 
Average 72.03 76.21 70.93 76.01 73.06 78.84 
Single 
Feature 75.78 76.82 78.65 
TABLE IV.  CLASSIFICATION RESULTS ON GLASS 
T(%) C4.5 SMO C5.0 
P F?P P F?P P F?P 
5 77.10 77.10 80.37 80.37 79.91 79.91 
10 76.17 76.17 78.97 78.97 82.24 82.24 
1308
15 76.64 76.64 77.10 77.10 80.37 80.37 
20 72.90 76.64 77.10 78.04 77.57 80.37 
25 72.43 76.17 77.57 78.50 78.04 80.37 
30 72.90 76.64 77.57 78.97 78.50 80.37 
35 71.03 74.77 76.17 78.50 77.57 79.44 
40 69.63 74.77 71.03 79.44 75.23 79.44 
45 68.69 74.77 69.63 74.30 71.96 79.44 
50 68.69 72.90 70.09 75.70 71.96 79.44 
55 65.42 73.83 62.62 76.17 66.82 79.44 
60 64.49 74.77 63.08 73.83 66.82 79.44 
65 50.47 74.77 48.60 73.83 50.00 79.44 
70 50.47 74.77 48.60 75.23 50.00 79.44 
75 49.07 74.77 49.07 76.17 47.20 79.44 
80 49.07 74.77 45.79 74.77 47.20 79.44 
85 47.20 74.30 47.20 75.70 47.20 79.44 
Average 64.84 75.21 65.92 76.80 65.76 79.85 
Single 
Feature 74.77 73.36 79.44 
TABLE V.  CLASSIFICATION RESULTS ON HEART-STATLOG 
T(%) C4.5 SMO C5.0 
P F?P P F?P P F?P 
5 82.96 82.96 80.37 80.37 90.00 90.00 
10 85.93 85.93 80.37 80.37 90.37 90.37 
15 83.33 83.33 81.48 81.48 88.89 88.89 
20 81.11 81.11 81.11 81.11 88.89 88.89 
25 79.63 79.63 83.33 83.33 87.78 87.78 
30 83.33 83.33 78.89 78.89 88.89 88.89 
35 81.85 81.85 80.74 80.74 87.41 87.41 
40 80.74 80.74 83.33 82.96 87.04 87.04 
45 82.96 82.96 83.70 81.48 87.04 87.04 
50 82.59 82.59 83.70 82.59 86.67 86.67 
55 81.11 80.37 82.22 84.07 83.70 87.04 
60 72.96 81.48 72.96 83.33 74.07 87.41 
65 72.96 81.48 72.96 83.33 74.07 87.41 
70 70.00 81.48 70.00 84.44 70.00 87.41 
75 70.00 81.48 70.00 84.44 70.00 87.41 
Average 79.43 82.05 79.01 82.20 83.65 87.98 
Single 
Feature 81.85 84.07 87.41 
TABLE VI.  CLASSIFICATION RESULTS ON IRIS 
T(%) C4.5 SMO C5.0 
P F?P P F?P P F?P 
5 95.33 95.33 94.67 94.67 96.00 96.00 
10 95.33 95.33 94.67 94.67 96.00 96.00 
15 94.00 94.00 94.67 94.67 96.00 96.00 
20 94.00 94.00 94.67 94.67 96.00 96.00 
25 94.00 94.00 94.00 94.00 96.00 96.00 
30 94.00 94.00 94.67 94.67 96.00 96.00 
35 94.67 94.67 95.33 94.67 96.00 96.00 
Average 94.48 94.48 94.67 94.57 96.00 96.00 
Single 
Feature 94.00 94.00 96.00 
TABLE VII.  CLASSIFICATION RESULTS ON WAVEFORM 
T(%) C4.5 SMO C5.0 
P F?P P F?P P F?P 
5 73.06 73.04 78.68 78.66 84.38 84.38 
10 74.66 75.58 85.36 85.58 86.26 86.16 
15 74.24 76.16 82.86 85.42 82.06 85.80 
20 73.06 75.66 77.78 85.84 78.30 87.20 
25 62.94 76.06 64.30 85.92 66.44 87.06 
30 52.44 76.10 51.18 85.96 52.90 87.18 
35 40.50 76.24 41.10 85.92 41.10 87.18 
40 40.50 76.24 41.10 85.92 41.10 87.18 
Average 61.43 75.64 65.30 94.90 66.57 86.82 
Single 
Feature 76.44 85.96 87.32 
TABLE VIII.  CLASSIFICATION RESULTS ON WINE 
T(%) C4.5 SMO C5.0 
P F?P P F?P P F?P 
5 94.94 94.94 97.75 97.75 98.88 98.88 
10 94.38 94.38 98.88 98.88 98.88 98.88 
15 95.51 95.51 98.31 98.31 98.88 98.88 
20 96.07 96.07 97.75 97.19 99.44 99.44 
25 95.51 94.94 96.63 97.75 98.88 98.88 
30 95.51 94.38 94.38 98.31 99.44 98.31 
35 92.13 93.26 94.38 98.88 97.19 98.31 
40 94.94 96.63 94.38 98.88 98.88 97.75 
45 95.51 96.63 93.82 99.44 97.19 98.88 
50 95.51 96.63 95.51 98.88 97.19 98.88 
55 96.07 96.07 94.94 98.88 96.63 98.88 
60 82.02 95.51 81.46 98.88 86.52 100 
65 73.60 95.51 71.91 98.31 76.97 100 
70 74.72 95.51 72.47 98.31 76.40 98.88 
75 44.38 95.51 44.38 98.31 44.38 98.88 
80 44.38 95.51 44.38 98.31 44.38 98.88 
85 44.38 95.51 44.38 98.31 44.38 98.88 
Average 82.91 95.44 83.28 98.45 85.56 98.91 
Single 
Feature 95.51 98.31 98.88 
TABLE IX.  CLASSIFICATION RESULTS ON ZOO 
T(%) C4.5 SMO C5.0 
P F?P P F?P P F?P 
5 93.07 93.07 96.04 96.04 100 100 
10 88.12 88.12 96.04 96.04 100 100 
15 91.09 91.09 96.04 96.04 100 100 
20 90.10 90.10 96.04 96.04 100 100 
25 89.11 89.11 92.08 96.04 98.02 100 
30 90.10 90.10 92.08 96.04 98.02 100 
35 89.11 89.11 93.07 97.03 97.03 99.01 
40 91.09 91.09 93.07 96.04 97.03 99.01 
45 91.09 91.09 93.07 96.04 97.03 99.01 
50 90.10 90.10 93.07 96.04 97.03 99.01 
55 91.09 91.09 93.07 96.04 97.03 99.01 
1309
60 86.14 91.09 87.13 95.05 89.11 99.01 
65 87.13 93.07 85.15 96.04 89.11 99.01 
70 87.13 93.07 86.14 96.04 89.11 99.01 
75 89.11 93.07 87.13 97.03 89.11 99.01 
80 81.19 93.07 79.21 97.03 79.21 99.01 
85 40.59 93.07 40.59 96.04 40.59 99.01 
Average 86.20 91.21 88.18 96.16 91.61 99.36 
Single 
Feature 92.08 97.03 99.01 
 
V. CONCLUSION  AND FUTURE WORK 
In this paper, we introduce closed frequent pattern-based 
feature space transformation and the experimental results on 
real-world datasets show that the transformed feature space 
from union of closed frequent patterns and original features 
achieves much higher classification accuracy than that from 
only the closed frequent patterns with respect to various Ts.  
In the future, I plan to extend my research to mining the 
most discriminative closed frequent patterns efficiently from 
a large number of frequent patterns, to address the real-world 
application challenges and further improve the learning 
performance. 
 
ACKNOWLEDGMENT 
This research was financially supported by the Ministry 
of Education, Science Technology (MEST) and Korea 
Industrial Technology Foundation (KOTEF) through the 
Human Resource Training Project for Regional Innovation 
and also supported by Basic Science Research Program 
through the National Research Foundation of Korea (NRF) 
funded by the Ministry of Education, Science and 
Technology (NRF No. 2010-0001732). 
 
REFERENCES 
 
[1] J. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 
1993. 
[2] Corinna Cortes and V. Vapnik. Support-vector networks. Machine 
Learning, 20:273-297, 1995. 
[3] R. Duda, P. Hart, and D. Stork. Pattern Classification. Wiley 
Interscience, 2nd edition, 2000. 
[4] R. Agrawal, T. Imielinski, and A. Swami, “Mining association rules 
between sets of items in large databases,” Proc. SIGMOD, 1993, pp. 
207-216. 
[5] B. Liu, W. Hsu, and Y. Ma, “Integrating classification and association 
rule mining,” Proc. Fourth International conference on Knowledge 
Discovery and Data Mining(KDD), 1998, pp. 80-86. 
[6] W. Li, J. Han, and J. Pei, “CMAR: Accurate and efficient 
classification based on multiple class-association rules,”. Proc 
International Conference on Data Mining(ICDM), 2001, pp. 369-376. 
[7] X. Yin and J. Han, “CPAR: Classification based on predictive 
association rules,” Proc. SIAM International Conference on Data 
Mining (SDM'03), 2003, pp. 331-335 
[8] H. Cheng, X. Yan, J. Han, and C.-W. Hsu, “Discriminative frequent 
pattern analysis for effective classification,” Proc. International 
Conference on Data Engineering(ICDE 07), Turkey, 2007, pp. 716–
725. 
[9] W. Fan, K. Zhang, H. Cheng, J. Gao. X. Yan, J. Han, P. S. Yu O. 
Verscheure, “Direct Mining of Discriminative and Essential Frequent 
Patterns via Model-based Search Tree,” Proc 14th ACM SIGKDD 
International Conference on Knowledge Discovery and Data Mining, 
2008, pp. 230-238 
[10] D. Newman, S. Hettich, C. Blake,C. Merz, “UCI Repository of 
Machine Learning Databases,” 1998 
[11] U. Fayyad, K. Irani, “Multi-interval Discretization of Continous-
valued Attributes for Classification Learning,” Proc. IJCAI 1993, pp. 
1022–1027  
[12] I. Witten, E. Frank, “Data Mining: Practical Machine Learning Tools 
and Techniques,” Morgan Kaufmann, San Francisco, 2nd edition 
(2005) 
[13] G. Cong, K. Tan, A. Tung, and X. Xu, “Mining top-k covering rule 
groups for gene expression data,”. Proc. ACM International 
Conference on Management of Data (SIGMOD), 2005, pp. 670-681 
[14]  J. Wang and G. Karypis, “HARMONY: Efficiently mining the best 
rules for classification,” Proc. SIAM international conference on Data 
Mining Proceedings (SDM'05), 2005, pp. 205-216 
[15]  F. Thabtah., P. Cowling and Y. Peng, “MCAR: Multi-class  
Classification based on Association Rule”, Proc. IEEE International 
Conference on Computer Systems and Applications ,2005, pp. 127-
133. 
[16] A. Zimmermann and L. De Raedt, “Corclass : Correlated association 
rule mining for classification,”. In Discovery Science, volume 
3245/2004 of Lecture Notes in Computer Science, 2004, pp. 60–72.  
[17] L. Breiman, J. Friedman, R. Olshen and C. Stone, “Classification and 
regression trees,” Wadsworth Intl., 1984  
[18] J. R. Quinlan and R. M. Cameron-Jones, “FOIL: A midterm report,”  
Proc. European Conf. Machine Learning, Vienna, Austria, 1993, pp. 
3-20  
[19] D. Lo, H. Cheng, J. Han, S. Khoo, and C. Sun, “Classification of 
software behaviors for failure detection: a discriminative pattern 
mining approach,” Proc. 15th ACM SIGKDD International 
Conference on Knowledge Discovery and Data mining, 2009, pp. 
557–566 
[20] G. Dong, X. Zhang, L. Wong, and J. Li, “CAEP:classification by 
aggregating emerging patterns,” Proc. of The Second International 
Conference on Discovery Science (DS'99), 1999, pp. 43-55 
[21] J. Li, G. Dong, K. Ramamohanarao, and L. Wong, “DeEPs: a new 
instance-based lazy discovery and classification system,” Machine 
Learning, 2002, 54(2): 99-124 
[22] F. Thabtah, P. Cowling, and Y. Peng, “MCAR: Multi-class 
Classification based on Association Rule”, IEEE International 
Conference on Computer Systems and Applications ,2005, pp. 127-
133. 
[23] F. Thabtah, P. Cowling, and Y. Peng, “MMAC: A new multi-class, 
multi-label associative classification approach,” Proc. Fourth IEEE 
International Conference on Data Mining(ICDM), 2004, pp. 217-224 
[24] Z. Tang and Q. Liao, “A New Class Based Associative Classification 
Algorithm,” Proc. International Multiconference of Engineers and 
Computer Scientists, 2007, 685-689. 
[25] G. Chen, H. Liu et al, “A new approach to classification based on 
association rule mining”, Decision Support Systems 42, 2006, pp. 
674-689 
[26] R. Thonangi and V. Pudi, “ACME: An associative classifier based on 
maximum entropy principle,” Proc. 16th International Conference on 
Algorithmic Learning Theory (ALT), Singapore, Springer, 2005, 
122–134. 
[27] X  Li., D  Qin., C. Yu., “ACCF: Associative Classification Based on 
Closed Frequent Itemsets,” Proc. Fifth International Conference on 
Fuzzy Systems and Knowledge Discovery (FSKD), 2008, pp. 380-
384 
[28] A. Veloso, W. Meira Jr., and M. Zaki, “Lazy associative 
classification,”  Proc. Sixth International Conference on Data 
Mining(ICDM), 2006, pp. 645-654 
1310
[29] B. Arunasalam and S. Chawla, “Cccs: a top-down associative 
classifier for imbalanced class distribution,” Proc. the 12th ACM 
SIGKDD international conference on Knowledge Discovery and Data 
Mining(ICDM)), 2006, pp. 517–522 
[30] Pearl, J. & Russel, S, “Bayesian networks. Report (R-277),” In Proc. 
Handbook of Brain Theory and Neural Networks, . Arbib, ed, MIT 
Press, Cambridge, 2000, pp. 157–160. 
[31] D. Aha, D. Kibler and M. Albert”Instancebased learning algorithms,” 
Machine Learning., 1991, pp. 37–66. 
[32] I. Watson and F. Marir, “Case-based reasoning: A review,” The 
Knowledge Engineering Review, 1994 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
1311
