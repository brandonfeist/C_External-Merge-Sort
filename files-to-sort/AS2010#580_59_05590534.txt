Intelligent Data Prefetching for Hybrid Flash-Disk Storage Using Sequential 
Pattern Mining Technique 
Un-Keun Yoon       Han-joon Kim* 
School of Electrical and Computer Engineering 
The University of Seoul 
Seoul, Korea 
{khj, ds5eqe}@uos.ac.kr 
Jae-young Chang 
Department of Computer Engineering 
Hansung University 
Seoul, Korea 
jychang@hansung.ac.kr 
 
Abstract—This paper presents an intelligent prefetching 
technique that significantly improves hybrid flash-disk storage, 
a combination of hard disk and flash memory. As a prefetching 
strategy, we adopt the sequential pattern mining, a variant of 
association rule mining. Our goal is to minimize overall I/O 
processing time of hybrid storage systems with using the Fully 
Associated Sector Translation (FAST) technique that is known 
to be the best mapping method in managing flash memory. It is 
very significant to further enhance the system performance of 
the hybrid storage when applying FAST to it. In our work, the 
hybrid storage uses the flash memory as a cache space to 
improve system performance. With this memory architecture, 
the proposed method is to prefetch objects onto ‘prefetching’ 
blocks in the level of both file and block in hybrid storage 
systems. Through extensive experiments using real UCC data 
and synthetic data, we show that the proposed prefetching 
method outperforms conventional ones. 
Keywords- flash-disk storage; prefetching; sequential pattern 
mining; fully-associative sector translation 
I.  INTRODUCTION 
The hybrid flash-disk storage is a new type of storage 
that includes flash memory storage space in addition to the 
usual magnetic disk storage [1]. Normally, flash memory 
embedded in the hybrid storage acts as a storage ‘cache’ 
space, holding frequently accessed data. For example, a 
hybrid storage HM12HII/DOM made by SAMSUNG 
company has 256Mbytes of flash memory space with 
120Gbytes of HDD. Flash memory has prospective features 
useful to a storage system which enable it to provide much 
better performance than hard disks. Flash memory is non-
volatile solid-state memory with a number of powerful 
features; recently, its density and I/O performance have 
improved to a level at which it can be used as a mass storage 
in general computing systems. It is as fast as DRAM in 
reading operations and much faster than a hard disk in 
writing1. Also, flash memory is more scalable than DRAM 
due to its simple cell structure and insensitivity to cell 
failure. These features enable the creation of a large memory 
space. However, since the flash memory still costs much 
                                                           
* Corresponding author 
1 It has been reported that read operations in NAND-type flash memory 
takes about 15 ?s, and write operations take about 200 ?s per sector 
(typically, 512bytes). 
higher than HDD in terms of price2, it is now hard for flash 
memory to replace conventional HDD. Hence, the hybrid 
flash-disk storage device that combines relatively smaller 
space of flash memory with HDD has been suggested [1, 2]. 
Due to the numerous advantages of flash memory, the hybrid 
storage has a great possibility to replace conventional hard 
disks [3]. 
In order for the hybrid storage to be used as a mass 
storage, it is necessary to develop high-level management 
skills fit for the storage [4, 5]. For the efficient management 
of the hybrid storage, we assume that the hybrid storage 
system uses the ‘Fully Associative Sector Translation’ 
(FAST) technique [6] that is known to be the best mapping 
method in managing flash memory as a kind of Flash 
Translation Layer (FTL). Here, flash memory space consists 
of ‘data’ blocks and ‘log’ blocks. The FAST technique can 
effectively handle sequential accesses while highly utilizing 
‘log’ block space because sequential access is inherently 
frequent in flash memory space. Thus, to enhance the FAST-
based hybrid storage systems, we utilize the sequential 
pattern mining [7], a variant of association rule mining, 
which is to extract sequential patterns with time information 
from large-scale sequence data. The sequential patterns can 
contribute to greatly improving I/O speed of hybrid storage 
systems through prefetching operations.  
In our work, we assume that the hybrid storage considers 
the flash memory space as a cache space including 
prefetching area under the FAST scheme. We propose a new 
way of ‘two-level’ prefetching technique that is to prefetch 
some objects to be accessed in the future onto ‘prefetching’ 
area at both file- and block-levels according to sequential 
access patterns. We have found that there exist significant 
sequential patterns for file references as well as block 
references after analyzing real UCC (User Created Contents) 
trace data. This observation implies that we can predict 
future access patterns almost perfectly, and can achieve 
successful prefetching. 
                                                           
2 The flash memory costs $8~9 for 1Gbyte and in contrast the HDD costs 
$0.16 for 1Gbyte in 2008. 
9th IEEE/ACIS International Conference on Computer and Information Science
978-0-7695-4147-1/10 $26.00 © 2010 IEEE
DOI 10.1109/ICIS.2010.19
280
II. PRELIMINARIES 
A. Fully associative sector translation (FAST) 
In our work, to implement a file system with the hybrid 
flash-disk storage, we use the ‘fully associative sector 
translation’ (FAST) technique [6]. This technique can reduce 
the number of erase operations due to a lot of write 
operations, and it assumes that flash memory space is 
composed of ‘data’ blocks and ‘log’ blocks. The log blocks 
are divided into two areas: one is used for ‘sequential’ writes 
and the other for ‘random’ writes. This technique uses a 
combination of sector mapping and block mapping. That is, 
the FAST technique keeps a block-level mapping table and it 
also performs ‘sector’ mapping within a particular block3. 
This technique enables overwrite operations to be minimized 
compared to other methods. 
When an overwrite operation occurs, FAST determines 
whether it is a sequential or random overwrite. In case of 
sequential overwrite operations, FAST allocates one 
sequential log block and performs the write operation on the 
block. This block is managed by fully associative sector 
mapping; one log block contains a pre-defined number of 
sectors. Several write operations for the same logical sector 
number (LSN) can be done in one log block if it has enough 
free space. Conversely, when all sectors within sequential 
log block are already used, FAST needs to conduct a so-
called ‘switch’ operation in the sequential log block; FAST 
exchanges it with its data block, erases the data block, and 
returns the erased data block to a pool of free blocks. 
Secondly, in case of random overwrite operations, FAST 
allocates random log blocks and performs write operation on 
the blocks. The switch operation for random log blocks is the 
same as sequential overwrite operation. In our work, to 
enhance the FAST scheme in the hybrid flash-disk storage, 
the sequential log block is used to save the prefetched data, 
which is thus called ‘prefetch log block’. However, although 
we have modified the internal memory architecture of the 
FAST technique, the procedure of basic I/O operations is the 
same to that of the original FAST. 
B. Sequential pattern mining 
The sequential pattern mining (SPM) is a variant of 
association rule mining that finds co-occurrence 
relationships, called association, among data items 
purchased. The SPM finds frequently occurring events, 
called sequences, with considering the order of events from 
large-scale sequence data, in applications where orderings 
are significant [7, 8]. For example, in market basket analysis, 
it is significant to recognize that people buy particular items 
in sequence, e.g., buying a ‘CANON digital camera’ first and 
then buying a ‘SAMSUNG color printer’ some time later. 
A sequence is an ordered list of non-empty set of items 
during a given time interval. We denote a sequence s by <e1, 
e2, e3, ..., en> where ei is a set of items. For example, the 
sequence <{i1}, {i2,i3}, {i4,i5}> means that the events 
                                                           
3 Flash memory cannot be updated in situ; its contents must be erased 
before new data can be stored. The erase operations are executed in a 
‘block’ unit: usually, one block consists of 32 sectors. 
corresponding to the set of items {i1}, {i2,i3} and {i4,i5} 
happen sequentially in this order. Given a set of sequence 
data, the problem of sequential pattern mining is to find a set 
of significant sequential patterns with high support. Here, the 
‘support’ for a sequence means the number of records that 
contain the pattern in a given sequence database. By using 
such frequently occurring patterns, we can predict which 
event(s) will happen next after a particular event occurs. That 
is, the SPM technique is used to extract significant sequential 
patterns from a sequence database of file references for the 
purpose of prefetching files or blocks before they are 
actually requested. 
For example, suppose that the file access sequence of 
Table 1 is given, then from this database, we can discover a 
frequent sequence <F5, F4, F1>, where Fi represents that an 
event of accessing a file object Fi occurs. In our work, an 
event is assumed to contain a single access for files or 
blocks. The pattern <F5, F4, F1> means that files F5, F4, and 
F1 are referenced in the order with high probability (or 
support). Hence, when F5 is accessed, fetching F4 and F1 
beforehand onto flash memory space can contribute to 
speeding up I/O operations. Similarly, from the block access 
sequence of Table 1, we can discover a frequent sequence 
<B2, B1>, where Bi represents that an event of accessing a 
block Bi occurs within the same file. Suppose there is a 
pattern <B2, B1>, then when B2 is accessed, we can pre-fetch 
B1 onto flash memory space. 
Generally, a sequence database of file access trace is very 
large, particularly in multimedia service server such as UCC 
(User Created Contents) server. Thus, there may be an 
excessive number of sequential patterns from the large 
database, even if we try to properly adjust a threshold of 
minimum support to reduce the number of patterns. As we 
intend to prefetch file (or block) objects included in 
discovered patterns, too many patterns results in degrading 
I/O performance of the system due to excessive prefetching 
operations. Hence, in our work, to prefetch an appropriate 
number of objects onto flash memory, we isolate frequent 
event pairs (i.e., a sub-sequence of access pattern) from 
discovered patterns. For this, we represent sequential 
patterns as a graph structure whose edge has its weight 
information that means the degree occurrence of related sub-
sequence pattern. Eventually, we achieve to minimize overall 
file I/O processing time by prefetching files (or blocks)4 to 
be accessed in the future into a ‘cache’ of flash memory.  
TABLE I.  EXAMPLE OF SEQUENCE DATABASE5 
SID File Access Sequence SID Block Access Sequence 
S1 
S2 
S3 
S4 
S5 
<F5,F4, F3,F1 > 
<F2,F5,F10,F4,F15,F4,F15,F1>  
<F5,F10,F4,F1 > 
<F2,F5,F5,F4,F2,F1> 
<F2,F5,F6,F4,F9,F1> 
SF51 
SF52 
SF53 
SF54 
SF55 
<B2,B1, B3 > 
<B2,B1,B3,B4>  
<B2,B3 > 
<B1,B2,B1,B4> 
<B1,B2,B3,B1,B5,B1> 
                                                           
4 Throughout the paper, we shall call the files or blocks to be prefetched 
simply ‘objects’. 
5 The files belonging to a sequential pattern are denoted in boldface. SF51 
~SF55 denote the sequence ID of the blocks accessed within the file F5. 
281
III. MANAGEMENT OF HYBRID FLASH-DISK STORAGE 
A. System architecture 
 
Figure 1.  The proposed system architecture 
The hybrid storage system proposed in this paper is 
illustrated in Fig. 1. This figure shows the process in which 
the object requested by an application is loaded in main 
memory; firstly, the application submits a request signal to 
the hybrid storage manager, which determines whether the 
requested object exists in prefetch log block with the help of 
FTL (or FAST). If the requested file exists in sequential log 
block, then it will be loaded in the main memory without 
referencing hard disk. After the signal is forwarded to the file 
system, the hybrid storage manager locates files that will be 
accessed in the near future by looking up sequential patterns 
discovered. Lastly, promising files are fetched in advance 
onto prefetch log block in idle times by FTL. 
The hybrid storage manager allows external applications 
to regard the hybrid storage as the conventional disk storage, 
and it tries to improve I/O performance through prefetching 
operations. The module is composed of three major 
components: sequential pattern miner, prefetching controller 
and file map handler. The sequential pattern miner is 
responsible for generating the sequential pattern from the 
past sequences of file reference. The prefetching controller is 
responsible for fetching selected files onto flash memory 
space by using sequential patterns before the files are 
requested afterwards. Lastly, the file map handler is 
responsible for deciding and managing the number of sector 
to save the prefetched objects onto prefetch log blocks. Note 
that files to be prefetched should be stored in the prefetch log 
blocks against sequential access. 
B. Modification to the FAST scheme for intelligent two-
level prefetching 
This section describes the modified memory architecture 
of the FAST scheme, and how objects to be prefetched are 
actually uploaded onto prefetch log blocks. As shown in Fig. 
2, to prefetch file objects, the file map handler manages the 
used-LSN table and the file map table. The file map table 
contains meta-data information such as file name, size, and 
LSN information to the memory architecture of the FAST. 
 
Figure 2.  Modification to internal memory architecture of FAST 
As mentioned before, the prefetch log blocks correspond to 
the sequential log blocks in conventional FAST technique. 
We assume that the size of page is 10Kbytes. Thus, as 
seen in Fig. 2, ‘File5.avi’ of 10Kbytes and ‘File4.avi’ of 
28Kbytes have one LSN and three LSNs, respectively. When 
the application requests write operation for the file F5 
(referred to as ‘File5.avi’ in the figure), given the sequential 
pattern <F5, F4, F1> extracted from Table 1, then the hybrid 
storage manager interprets the sequential pattern to 
determine which file objects are prefetched from hard disk. 
According the sequential patterns, the file F4 (referred to as 
‘File4.avi’) will be prefetched onto prefetch log block. To 
prefetch it, FAST would obtain three LSNs such as ‘LSN 8’, 
‘LSN 7’ and ‘LSN 6’, which are saved in used-LSN table, as 
seen in Fig. 2. Next, before processing write operations for 
LSNs 8, 7 and 6, FAST needs to locate physical blocks by 
referring the block-level mapping table, and then FAST 
examines whether these LSNs can be stored in prefetch log 
block. In case of Fig. 2, since all sectors of prefetch log 
block are currently empty, FAST can write the data for all 
requested LSNs onto pre-fetch log block. Specifically, since 
LSN 6 corresponds to prefetched data, LBN is computed by 
dividing LSN by the number of prefetch log blocks; that is, 
?6/3?=2. And, a sector number is obtained by computing 
LSN % (number of prefetch log blocks), that is 6%3=0. As a 
result, the data for LSN 6 is saved in the 0th block of 
prefetch log block area. 
C. Generating sequential patterns 
In order to locate the files or blocks to be prefetched from 
hard disks, we use the information of sequential patterns 
extracted beforehand. The sequential pattern miner extracts 
time-sequential patterns from a given sequence data of file 
(or block) references. Given the input data of Table 1, the 
module can extract a set of sequential patterns such as 
<F5,F4,F1>. 
Here, we need to clarify which files (or blocks) should be 
prefetched onto flash memory with the extracted sequential 
282
patterns. It is because a sequential pattern can be utilized 
with different ways of prefetching. For example, if the 
sequential pattern <F5, F4, F1> is captured, then when F5 is 
accessed, both F4 and F1 can be prefetched, or when F5 and 
F4 are accessed, F1 is made to be prefetched. Here, we have 
two considerations. One is that it is necessary to avoid 
excessive prefetching operations due to many sequential 
patterns. The other is that checking whether the past 
sequence of files accessed is matched to all patterns is a big 
overhead to the prefetching controller. Therefore, our system 
assumes that accessing an object allows to prefetch only its 
next object; in other words, for a sequential pattern <…, Fi, 
Fi+1, …>, accessing the file Fi leads to prefetching only its 
subsequent file Fi+1. 
According to such a policy for pattern extraction, the 
final sequential patterns are generated which are submitted to 
the prefetching controller. Here, to isolate highly significant 
patterns, we need to carefully adjust a threshold of minimum 
support; in general, to obtain a best minimum support, we 
perform the mining activities repeatedly with various values 
of minimum support. 
D. Prefetching objects from disk using pattern weights 
The prefetching controller utilizes sequential patterns 
generated by the sequential pattern miner to determine which 
objects are prefetched from hard disk. This module keeps a 
simple data structure to internally process sequential 
patterns. A set of sequential patterns is represented by a 2-
dimension array corresponding to its directed graph; since a 
sequential pattern expresses a sequence of time-orderly 
events, it can be represented as a directed graph. Each edge 
of the graph contains the weight of frequency with which its 
related pair of objects occurs in the set of sequential patterns. 
The weight information is used to choose more significant 
patterns. 
Note that excessive amount of prefetching can cause too 
frequent page faults due to limited space of flash memory. 
Such frequent page faults (or replacements) can be avoided 
by adjusting the threshold value of pattern weight. We set the 
threshold of weight to an appropriate value so that frequently 
accessed objects can reside in the flash space before the 
request for them and cannot incur page faults. In our 
empirical results, the optimal threshold value of pattern 
weight is empirically found to be 2 or 3. 
TABLE II.  PROCEDURE OF TWO-LEVEL PREFETCHING 
Procedure two-level Prefetching 
Input: a file f and a set of its blocks B to be requested 
1. begin 
2. if (Size of the requested file < ?) then  
3.     Search the file-level sequential patterns related to the file f 
4.     Identify a set of file objects F to be prefetched 
5.     foreach  f ? F { 
6.     Search the block-level sequential patterns related to the 
blocks belonging to the file f 
7.        Identify a set of block objects B to be prefetched 
8.        if (B is not empty) then  
9.            Prefetch the blocks B onto the prefetch log blocks 
10.        else Prefetch the file f onto the prefetch log blocks 
11.  } 
12. End 
As mentioned before, we propose the ‘two-level’ 
prefetching technique that considers both file-level and 
block-level with sequential access patterns when prefetching. 
The proposed method proceeds as shown in Table I. When 
an application requests a particular file, the system first 
examines whether its size is more than ? (e.g., 1024Kbytes). 
Since the flash memory space is limited in size, too big files 
are not excluded in prefetching operation. Then, the system 
searches for a set of pairs of (file) objects related to the file f 
in the set of file-level sequential patterns, and it identifies a 
set of file objects to be prefetched. Next, we investigate 
whether there are block-level sequential patterns for each of 
the candidate files or not. Thus, the system searches for a set 
of pairs of (block) objects related to the blocks belonging to 
the file f in the set of block-level sequential patterns. If such 
patterns do exist, then the related blocks existing in the 
sequential patterns will be prefetched onto the prefetch log 
blocks. Otherwise, the requested file f will be prefetched 
wholly. 
IV. PERFORMANCE ANALYSIS 
A.  Simulator and experiment data 
As a main measurement in our work, we have used 
‘processing time’, which means the time during which the 
system provides file (or block) objects corresponding to I/O 
requests of applications. As a baseline for performance 
measurement, we have used the FAST technique combined 
with Top-N prefetching method, which is denoted as 
FAST+Top-N. The Top-N prefetching is a simple way of 
prefetching top-N popular file objects periodically [9]. 
Additionally, we have compared our method with the 
prefetching method considering only file-level, which is 
denoted as FAST+File-level. Our proposed method is 
denoted as FAST+2-level.  
To evaluate our proposed method, we have built a 
simulator of FAST-based hybrid storage manager with 
512Mbytes of flash memory space. The details pertaining to 
flash memory and FAST technique are described in Table 3. 
To build the sequential pattern miner, we have chosen 
PLWAP algorithm [10] that is evaluated to perform best 
among various sequential pattern mining algorithms.  
For reliable performance evaluation, we have used two 
types of data: UCC data and synthetic data. The UCC data 
have click stream information for one month, in which about 
two million users requests more than six million files. The 
size of UCC data is between 4Kbytes and 2Mbytes. The data 
contains 6 elements: the used date, duration, size, UCC file 
path name, host IP (server) address, and guest IP (user) 
address. Because many users access only a few files in the 
UCC data, we have used part of the UCC data in which some 
users access a large number of files. As another test data, we 
have synthetically generated a set of various workloads 
(denoted as SYN data) that are characterized by frequent 
request to diverse files. The size of files is between 1Kbyte 
and 10Mbytes. The synthetic workloads exhibit certain 
amounts of locality of file reference. In this paper, to denote 
the degree of locality, we use the notation similar to that of 
[11]; that is, ‘y : x’. This means that x % of access requests  
283
TABLE III.  SIMULATION PARAMETERS 
Parameters for Flash Memory 
Space 
Parameters for Modified FAST 
scheme 
NAND Flash 
Memory Size 
512 Mbytes Space for data 
blocks 
192 ~ 414 MB 
Page Size 512 bytes Space for random log blocks 
64 MB 
Writing speed 210.5?  Space for prefetch 
(sequential) log 
blocks 
64 ~ 128 MB Reading speed 200?  
Erasing speed 1.2 sec 
go to y % of the data, and the other (100-x) % go to (100-y) 
% of the data and thus ‘50:50’ means no locality. Moreover, 
we have conformed to the Zipf’s law to generate the block 
reference within a particular file. The Zipf's law states that 
the product of rank number and frequency make up a 
constant; that is, while only a few blocks within a file are 
used very often, many or most are used rarely. 
B. Experimental results 
In our experiment, we have evaluated our proposed 
method in three aspects. First, it is necessary to determine 
optimal amounts of training data to develop sequential 
patterns. Second, in case of using synthetic data, we will 
investigate how much the locality of reference affects the 
system performance. Lastly, we need to check the effect 
which results from adjusting the threshold of pattern weight.  
In principle, we need to check effect which results from 
changing the size of data block, where the data block means 
a unit of data transfer; a file consists of a set of data blocks. 
In case of MPEG data, a data block corresponds to GOP 
(Group of Pictures), whose size is not fixed. However, we 
have found that the processing time is not greatly dependent 
upon the size of data block, which implies that our proposed 
method does not need to have any regard for some special 
issues depending upon a particular file format. 
1) Effect of changes in the size of training data: We 
expect that extracted patterns can get more salient as the size 
of training data get larger. For example, given an extracted 
pattern F2?F1 for 2,000 training files, one of its related 
patterns may be F2?F3?F5?F1 for 3,000 training files. 
This section describes how much the size of training data can 
affect on performance of prefetching. In general, when 
developing a prediction model with machine learning 
algorithms, a larger training data always does not result in 
developing a better model [6].  
Fig. 3 shows changes in the processing time as the 
training data gets larger in size in case of using SYN and 
UCC data. In this figure, we can see that in order for 
prefetching to reduce the overall process time, the size of 
training data had better be greater than some threshold, but 
excessive amounts of training data causes an increase in 
process time; this is because excessive amounts of many 
training data cause pattern over-fitting which leads to 
prefetch too many files onto flash memory space. In our 
experiment, the optimal size of training data is 3,500 and 
40,000 I/O requests for SYN and UCC data, respectively.  
2) Effect of changes in locality with the synthetic data: 
This section describes how much the degree of reference of 
locality can influence on performance of prefetching for 
SYN data. In our experiment, we have generated the SYN 
data with three kinds of locality of reference; 2:8, 3:7, 4:6. 
Fig. 4 shows the processing time according to changes in 
reference of locality, it shows that our proposed method 
outperforms the conventional one for all types of locality, 
and 2:8 of locality shows the best performance. On the 
whole, the processing time decreases as the degree of locality 
increases. 
3) Effect of changes in threshold of pattern weight 
As stated before, lower (or higher) threshold value of 
pattern weight can incur excessive prefetching operations. It 
is thus important to find the optimal threshold for the best 
performance. 
 
 
(a) In case of SYN data 
 
(b) In case of SYN data 
Figure 3.Changes of processing time from varying the size of training data 
 
 
Figure 4. Changes of processing time from varying reference of locality
284
Fig. 5 shows the execution time according to changes in 
the threshold value of weight when using SYN and UCC 
data. From this figure, we can see that the execution time is 
greatly influenced by the threshold values. In contrast, in 
case of file-level prefetching, the change in threshold value 
of weight does not give a great effect on the execution time. 
For both SYN and UCC data, when the threshold value is set 
to 2, the proposed method shows the shortest execution time. 
In case of less than 3 (i.e., prefetching all (or most of) the 
objects belonging to the generated sequential patterns), 
excessive prefetching causes a big increase in the execution 
time. Conversely, as the threshold value becomes higher than 
the optimal value, the favorable effect from prefetching 
decreases gradually. 
V. SUMMARY 
In this paper, we have proposed an intelligent two-level 
prefetching method in which sequential pattern mining is 
incorporated into the FAST-aware hybrid flash-disk storage 
system. In our work, the hybrid storage uses the flash 
memory as a cache space, and we focus on prefetching file 
(or block) objects onto prefetch (or sequential) log blocks to 
be accessed in the near future through sequential pattern 
mining. To achieve the best performance, it is very important 
to determine optimal values in terms of threshold of pattern 
weight and the size of training data. To be noted is that the 
proposed method works at the level of file and block objects. 
 
 
(b) In case of SYN data 
 
 
(b) In case of SYN data 
Figure 5: Effect of changes in weight threshold 
In contrast, previous studies focused on prefetching 
block-level objects [12] with simple heuristics even though 
they cannot be applied to hybrid storage systems. On the 
whole, the proposed two-level prefetching is more effective 
than file-level prefetching in UCC and the synthetic data. 
Empirical results show that in terms of the execution time, 
the proposed method improves the I/O performance of 
hybrid storage by about 16% and 18% for UCC and SYN 
data, respectively, compared to the Top-N prefetching under 
the FAST scheme. We submit that two-level prefetching can 
help to significantly enhance the future commercial hybrid 
storage products. In the future, we plan to study a way of 
semi-automatically adjusting several parameters such as the 
weight threshold and the size of training data for sequential 
pattern mining.  
ACKNOWLEDGMENT 
This work was supported by the Basic Research Program through the 
Korea Science and Engineering Foundation funded by the Ministry of 
Education, Science and Technology (Grant Number: R01-2007-000-20649-
0). 
REFERENCES 
[1] Hybrid drive: Wikipedia, http://en.wikipedia.org/wiki/Hybrid_drive 
[2] S. L. Min and E. H Nam, “Current trends in flash memory 
technology: invited paper”, Proceedings of the 2006 conference on 
Asia South Pacific design automation, 2006, pp. 332-333. 
[3] Y.H. Bae, “Design Technique of High Performance Flash Memory 
SSD (Solid State Disk)”, Journal of Korean Institute of Information 
Scientists and Engineers, Vol. 25, No. 6, 2007, pp.18-28. 
[4] E. Gal and S. Toledo, “Algorithms and Data Structures for Flash 
Memories”, ACM Computing Surveys, Vol. 37, No. 2,  2005, pp.138 
– 163. 
[5] H.J. Kim, and S.G. Lee, “An Effective Flash Memory Manager for 
Reliable Flash Memory Management”, IEICE Transactions on 
Information and Systems, Vol. 85, No. 6, 2002, pp.950-964. 
[6] S. W. Lee, D. J. Park, T. S. Ching, D. H. Lee, S. W. Park, and H. J. 
Song, “A Log Buffer-Based Flash Translation Layer Using Fully-
Associative Sector Translation”, ACM Transactions on Embedded 
Computing Systems, Vol. 6, No. 3, 2007, pp.18-44. 
[7] R. Agrawal, R. Srikant, “Mining sequential patterns”, Proceedings of 
the 11th International Conference on Data Engineering (ICDE'95), 
1995, pp.3-14. 
[8] P.N. Tan, M. Steinbach, and V. Kumar, Introduction to Data Mining, 
Addison-Wesley, 2006. 
[9] E.P. Markatos and C. Chronaki, A Top-10 Approach to Prefetching 
on the Web, Proceedings of the INET 98 Conference, 1998.  
[10] C.I. Ezeife, Y. Lu, and Y. Liu, “PLWAP sequential mining: open 
source code”, Proceedings of the 1st international workshop on open 
source data mining (OSDM’05), 2005, pp.26-35. 
[11] M.L. Chiang, Paul C.H. Lee, and R.C. Chang, “Using data clustering 
to improve cleaning performance for flash memory”, Software 
Practice and Experience, Vol.29, No.3, 1999, pp.267-290. 
[12] Z. Li, Z. Chen, S. M. Srinivasan and Y. Zhou, “C-Miner: Mining 
Block Correlations in Storage Systems”, Proceedings of the 3rd 
USENIX Conference on File and Storage Technology (FAST’04), 
2004, pp.173-186. 
 
285
