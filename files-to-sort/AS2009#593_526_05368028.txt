 
 
 
Searching and Clustering on Social Tagging Sites 
Ying Zhou 
School of Information Technologies,  
The University of Sydney, Australia 
Email: zhouy@it.usyd.edu.au 
 
Abstract.  Social Tagging Site has increasingly become a main 
avenue for people to share resources online. Users can use simple 
tools to publish everything from bookmarks to video clips on those 
sites. However, effective querying of resources in such sites is still a 
challenging question in industry and academic research. This 
paper reports a novel algorithm of presenting the web resources 
query result on the social tagging sites. It adopts a two step 
clustering approach to organize and rank resources based on their 
relative similarities with each other. Initial term similarities are 
computed using user and tag information of the resource. The 
query results are then organized as a group of concepts represented 
by a few semantically related terms.  The resources that are related 
with each concept are rank with respect to the concept. In addition, 
concepts are also ranked by representing terms and the number of 
resources associated.  
Keywords  Social Tagging, Association Rule Mining, 
Clustering 
I. INTRODUCTION 
The Web 2.0 era brings along various tools that enable 
Internet users to publish all sorts of objects on the Web. It is 
very easy for any user with Internet connection to publish 
photos, videos and other non textual object on the web with 
the help of such tools. Once a photo or video is published, 
users can make it either private or public. Public resources 
rely on description text to make them retrievable by other 
users.  Instead of providing a lengthy description, many users 
choose to tag their resource with a few keywords. The 
keywords based tagging activity has gained popularity in 
websites like flickr, citeULike, youTube and many other sites. 
The key feature contributing to its popularity is the 
uncontrolled vocabulary.  Users may choose any number of 
terms from any language to describe the object that becomes 
public or private resource on the web. This feature, on the one 
hand, encourages people to upload and tag their objects; on 
the other hand, it presents lots of new challenges for the 
searching services. Such challenges cannot be addressed by 
traditional information retrieval techniques for the following 
reasons.  First, the query target is usually a non-textual web 
resource.  This is different from a textual webpage containing 
complete or partial information of a particular query term. 
Second, the textual information about the target is highly 
condensed and limited to, in many cases, only a few terms.  
Third, there is no explicit relationship such as hyperlink 
between resources that can be used for ranking purpose. The 
traditional relevance/popularity ranking algorithms do not 
work very well under these three constraints. Currently, many 
websites provide alternative ranking mechanisms such as rank 
by date or rank by subjective measures like interestedness 
used in flickr. This paper presents a novel way of organizing 
the object search result on social tagging sites. It adopts a two 
step clustering approach to group and rank resources based on 
their relative similarities with each other. The query results are 
associated with a group of concepts represented by a few 
semantically related terms.  Resources are ranked by their 
similarities with respect to those concepts.  In particular, the 
paper makes the following contributions:  
It uses the association rule mining algorithm to obtain an 
initial similarity measure between popular terms appearing in 
a result set. The association rule based measure can effectively 
remove noises generated during the tagging process. Typical 
noises include relationship generated by single user or non-
standard terms used by single user. The set of rules is used to 
establish a relation graph between terms.  
It proposes a two step clustering algorithm to associate 
resources with related concepts. The algorithm first clusters 
the terms into semantically related concepts based on the 
initial similarity measures and term relation graph. It then 
filters resources into groups associated with particular 
concepts. Resources are also ranked inside each group.  
It proposes new ways to measure resource-cluster 
similarity, to rank resources in a cluster and to rank clusters in 
a query result set.  
The rest of the paper is organized as follows. Section 2 
describes several important related works. Section 3 describes 
the algorithm of obtaining initial similarity and the design of 
cluster based measure and the clustering algorithms.  We 
show the experiments and results in section 4. Section 5 
concludes the paper. 
II. RELATED WORK 
Fonseca et al. [5] uses query log to build concept based on 
association rule mining technique. Each query is mapped to an 
item and a set of consecutive queries issued by a same user in 
a small predefined time frame is mapped to a transaction. 
After applying standard association rule mining algorithm, a 
directed graph is built with each node representing a query 
and each edge representing a rule between a pair of queries. 
Concepts are identified as the minimal cycles in the graph.  
2009 Fifth International Conference on Semantics, Knowledge and Grid
978-0-7695-3810-5/09 $26.00 © 2009 IEEE
DOI 10.1109/SKG.2009.72
99
 
 
 
 
Dong et al. [4] presents an association rule/clustering based 
approach to find concepts in a set of web services. The 
document is the parameter names of input/output functions of 
web service operations, such as 
“LocalTimeByZipCodeResult”. The terms correspond to 
words in the parameter names, such as “Time” and “Zip”. The 
problem can be seen as finding term relations in very short 
documents with no grammatical constraints, which is similar 
to folksonomy. It adopts the simple “occurrences as support” 
approach.  
Nie et. Al [6] proposes an object ranking model PopRank 
to extend the popular PageRank algorithm to object level. 
PopRank focuses on building relation between various types 
of objects and estimating the popularity propagation factor 
from a predefined partial ranking list. The algorithm is 
implemented in an academic paper search engine Libra. 
PopRank deals with a complex graph consisting of various 
objects and relationship. Such complexity adds valuable 
information for computing ranks. The need of a predefined 
partial ranking list makes PopRank less flexible and reduces 
its general applicability. On the contrary, our algorithm treats 
objects of various types as retrievable resources. The minimal 
requirement is that each resource should have some tags as 
description. Our algorithm tries to rank resources by 
examining shared tags among them. It does not require a 
predefined initial ranking. It can be applied to rank objects of 
different types as long as they are described by tags.  
III. DESIGN 
Our ranking algorithm uses a two step approach. We first 
derive concepts from tags appearing in the result space. Each 
concept is labeled by its highest ranking tags. All resources 
are then associated with the concepts and ranked in each 
cluster. Although tag clusters are mutually exclusive, one 
resource may appear in multiple clusters. Clusters are also 
ranked by its internal cohesion and the number of resources it 
represents. 
I. Tag Clustering  
In any social tagging system, a tag is defined as a word or a 
phrase used to describe a uniquely addressed resource on the 
Internet. The resource could be a webpage, an academic 
paper, a photo or a video clip. A resource may have more than 
one tag, for instance: Australian Open and Melbourne , to 
describe it. Such co-occurrence is an important indicator of 
possible semantic relationship between tags ([7,11]). Many 
social tagging sites provide tag based query engine to 
facilitate resource query. The simplest form of such engine 
would return all resources containing the query term.  
As co-occurrence may happen for various reasons, it is a 
necessary but not sufficient condition on the existence of 
semantic relation. Our observation made on several sites 
shows that a pair of co-occurring tags usually has one of the 
following five relations: 
Inclusive relationship. For example, Australia and Sydney  
Expressing one concept in conjunction. For example, 
Social and Network. 
Synonyms, hypernyms and hyponyms. For example, cat 
and gatto (the Italian word for cat);  
Expressing different aspects of the resource. For 
example, AustralianOpen and 2009.  
Non-relevant. There are tags assigned purely for personal 
use such as toread, or mystuff. These tags almost occur 
uniformly in the tag space; they may co-occur with any other 
tags.  
We consider the first three as valid and strong relations. 
The fourth one may be valid but weak relation in certain 
cases. This is especially true when all tags involved are 
popular ones. For instance, party and 2006 may happen to co-
occur a lot even though the relation is quite weak  
  A good similarity measure stronger than co-occurrence 
should be able to identify and filter out the weak and non-
relevant relation. We propose using association rule to 
compute the relation between a tag pair. Each resource is 
considered as a transaction and tags used to describe the 
resource are considered as items. Each association rule has 
two numbers: support and confidence, to describe the 
popularity and strength of a particular rule. Following the 
traditional asocial rule mining approach[1], we can count 
support as the number of co-occurrence in the whole 
transaction space. This equals the number of resources having 
all the tags in a rule. However, in systems like Flickr or 
Youtube, it is possible and quite often for a user to use a 
similar set of tags over and over again to describe a collection 
of pictures or videos. There is high possibility for any pair in 
that set to become a rule with co-occurrence support measure. 
The rule set may include the fourth and fifth relations we 
described above. We propose using user count instead to 
measure the support of tag pairs.  Any user may assign a pair 
of tags multiple times to different resources. The support of 
that pair is increased by 1 in that case. Confidence computed 
from user count support can offset the effect of general tags 
that may be associated with all sorts of other tags. It also 
provides a normalized measure on the strength of each rule.  
We use confidence to measure the strength of tag relation. 
Table 1 Comparing two approaches on support 
Candidate Rule Co-occurrence  User Count  
 Supp. Conf. Supp. Conf. 
WashingtonDC Æ 
panthera 216 0.6 1 0.02 
Bangkok Æ 2004 80 0.75 1 0.06 
Taronga Æ zoo 34 0.92 26 0.9 
Deutschland Æ 
Germany 28 0.97 11 0.92 
 
Table 1 shows the outcome of the two approaches on a 
flickr data set. The co-occurrence approach considers all tag 
pairs in the table as highly relevant. However, the first two 
pairs are weakly related. The high co-occurrence numbers are 
contributed by one user using the pair on hundreds of pictures. 
The user count approach with appropriate thresholds can 
100
 
 
 
 
easily filter these noises out. Spam and non-relevant tags 
(such as toread, mystuff) can also be filtered out. 
Tags and their relations is then expressed in a graph. It is 
straight forward to generate the tag relation graph from a set 
of association rules. The tag relation graph ),,( WENG consists 
of nodes N, directed edges E and a weight matrix W. Each 
node of the graph corresponds to a tag in an association rule. 
If we have two tags p and q, and an association rule qp o , 
there is an edge from node p to node q in the graph. The 
weight of the edge Wpq  equals the confidence of the 
association rule. 
We take the agglomerative hierarchical clustering approach 
to organize tags (nodes) into cohesive clusters. The algorithm 
starts with |N| clusters each containing an individual node. List 
1 shows an algorithm CutAvg for tag clustering. It runs 
iteratively by merging the two clusters that are closest to each 
other in each iteration until the stop condition is met. 
Hierarchical clustering algorithms may use different stop 
conditions. We adopt the simple and straightforward 
similarity threshold approach, that is, the algorithm will stop 
when the highest similarity computed between clusters in a 
current iteration is lower than a predefined threshold. To 
achieve this, we need to measure and compute the similarity 
of clusters.  
List 1 Tag Graph Clustering Algorithm 
method CutAvg 
  Collection clusters = each node in its own cluster; 
       Cluster cx,cy; //the two clusters to be merged at a stage 
maxSim = 0;  //highest similarity so far 
for each cluster pair {c1, c2} in clusters do 
             double similarity = calSimilarity(c1, c2); 
             if (similarity > maxSim) 
             maxSim = similarity; cx = c1; cy = c2;    
             end if 
end for 
if  maxSim >= simThreshold  
             merge (cx, cy);  //merge the two clusters 
end if 
end method 
 
The similarity between two nodes p and q equals Wpq in the 
tag graph. Similarity between clusters is computed based on 
node similarity. Our aim is to develop a non-local measure for 
cluster similarity. In graph theory, all edges across two 
subcomponents (clusters) are called “cut edges”, where “cut” 
refers to the partition of these two clusters. Previous work on 
spectral graph partition [3] suggests that the optimal result 
occur where the cut of two clusters is minimal, and the 
cohesion within each cluster is maximized. Intuitively, we 
may define the cut size of two possible clusters as the sum of 
weights of all cut edges between them.  At each iteration, the 
algorithm could always merge the two clusters with the 
maximum cut size. Cut size defined as such is a local 
measure. It only includes the edges across two clusters in the 
computation. It does not consider the size of each cluster or 
other edges in the clusters.  To make the measure non-local, 
we propose to use the size of clusters to normalize the cut 
between them. The similarity of cluster A and B is thus 
defined as: 
||
),(
||
),(),(
B
BAcut
A
BAcutBASim  
   
(1)
                      
 
where |A| represents the number of nodes in cluster A and 
cut(A,B) is the cut size of cluster A and B defined as: 
¦
??
 
BvAu
uvWBAcut
,
),(
    
(2) 
II. Parameter Setting 
In our algorithm, support and confidence thresholds are 
used to control the number of tags and relations in the tag 
graph. Some initial experiment results indicate that the major 
clusters and popular tags are always preserved for any 
threshold values in a reasonable range. We run a test on apple 
query data with support varying from 5 to 20 and confidence 
varying from 0.5 to 0.8. The result shows that high support 
threshold only misses a few less frequent tags and concepts; 
the clustering result is otherwise quite similar with low 
support threshold case. High confidence value has a slightly 
bigger impact as it would “cut” the tag relation graph into 
many small pieces. The values we choose for the following 
experiment are: 5 (users) as minimum support threshold, and 
0.5 as minimum confidence threshold.  
Similarity threshold as the guard for stop condition is 
another important parameter that may affect the final results. 
Similarity threshold is bounded by confidence threshold.  
Similarity value between any two nodes is in a range between 
[min_conf., 2]. The CutAvg similarity value obtained by 
formula (1) is in the range [0, 2].  
Formula (1) ensures that for any one node cluster, there 
will be another cluster with whom the similarity is larger than 
the confidence threshold. We use a simple example to 
illustrate this feature.  Suppose we are at an iteration with 
clusters {a}, {b,c,d} and {e,f}. The definition of the tag graph 
guarantees that each node in the graph would have at least an 
edge with another node. Suppose a has an edge connecting 
with b.  The similarity between {a} and {b,c,d} would be 
3/baba ConfConf !!  . Regardless of the actual values of the 
confidence of rule ba o , the calculated cluster similarity 
would always be greater than the confidence threshold. If the 
similarity threshold is set to equal the confidence threshold, 
single node cluster {a} would eventually be merged with 
{b,c,d} to become a big cluster. Error! Reference source not 
found. clearly illustrates such feature in some example 
datasets from queries tiger, apple, bridge and Australia. The 
number of clusters grows rapidly when similarity threshold is 
greater than 0.5 (the confidence threshold). The observed 
increase consists largely of single node clusters. On the other 
side of 0.5 the number of clusters grows much slower with the 
increase of similarity threshold. This “knee” or turning point 
101
 
 
 
 
of a curve is also considered as good choice of number of 
clusters 6. Hence we take the confidence threshold as 
similarity bound. 
 
Figure 1. Effects of Similarity Threshold 
III. Associating Resources with Concepts 
The tag clusters obtained can be viewed as semantically 
related concepts expressed as a set of tags. The next step is to 
associate the resources described by a set of tags in the result 
set with those concepts. There are two simple and intuitive 
observations we follow to determine the membership and rank 
of a resource with respect to concepts. They are:  
First, any resource whose tags overlap with a concept’s tag 
set should be considered as a member of that concept.  
Second, the strength of the relationship between a resource 
and a tag cluster concept depends on the portion of tags they 
share.  
Table 2 Simple Example Data, Association Rules and Clusters 
Resource Tags 
r1 t1,t2,t3 
r2 t3,t4,t5 
r3 t5,t6,t7 
r4 t1,t2,t4 
r5 t1,t2,t5 
r6 t6,t7 
r7 t1,t3 
r8 t5,t6 
2.a  Resources and Tags 
 
 
 
 
 
The first observation deals with the membership problem 
while the second one deals with the rank of resources. Both 
are quite intuitive and easy to understand. Table 2 shows a 
simple example of a set of resources and their tags. We 
assume each resource belongs to a different user and that user 
assigns all tags for a resource. Under this assumption, the user 
count is the same as co-occurrence count. Table 2.a shows the 
resources and their tags. Table 2.b shows rules generated with 
support threshold set to 2 and confidence threshold set to 0.5. 
Table 2.c shows concept obtained using the clustering 
techniques described in section 3.2. It also shows the 
important tags in each cluster. We first examine a few typical 
pairs of resources where the concept membership and the rank 
are obvious. One good exam is the pair r1 and r3. They are 
fully represented by concept C1 and C2 respectively. 
Moreover, r1’s tags do not overlap with tags of all other 
concepts at all. r3 has the same feature.  We can define a 
similarity function ),( CrSim between a resource and a 
concept to calculate a similarity value in the range [0,1]. 
Ideally we should have 
1),( 11  CrSim ; 
0),( 21  CrSim ; 
0),( 12  CrSim  and 
1),( 22  CrSim  
The second example contains the pair r1 and r7, both of 
which are clearly members of C1. However, r7 only contains a 
subset of the C1’s tags. Intuitively, we should have 
),(),( 1711 CrSimCrSim ! .  A less straight forward case is 
the pair  r4 and r7, each of which has two tags overlapping 
with the tags of concept C1.  While r7 has all its tags included 
in C1 tag set, r4 only has two third of them in it. The similarity 
measure should give ),(),( 7417 CrSimCrSim ! .  
Based on the above observation of pairs and orderings, we 
develop a naive object-cluster similarity metric as: 
||
||
||
||),(
C
Cr
r
CrCrSim ?u? 
  
(3)
  
 
Where || Cr ?  represents the number of overlapping tags 
between resource r and concept C; |r| represents the number 
of tags resource r has and |C| represents the number of tags a 
concept C has. For the above example, the simple measure 
will compute 
3/2),(;9/4),(;1),( 171411    CrSimCrSimCrSim  
which satisfies our observation. Several trial experiments with 
small data sets show that the simple metric performs well for 
concepts with only a few tags. However, for concepts 
containing a large number of tags, it is likely that most 
resources overlap with a small portion of the concept tags. In 
that case, the naïve similarity measure cannot avoid certain 
randomness in ordering. Several resources may end up with a 
same similarity measure even if they cover different portion of 
the concept tag set. For instance, in concept C2, resource r6 
and r8 would have the same similarity value as 2/3. Without 
other evidence, r6 and r8 can only be ranked randomly. Such 
randomness exists because the naive similarity metric treats 
all tags belonging to a concept as equally representing the 
concept. Again, it is not a big issue in concepts with a few 
tags. Yet it is ineffective for general concepts with large 
number of tags. A more refined measure will first calculate 
the concept-tag weight and adjust the overlapping portion with 
Rule Confidence 
t1Æ t2 0.75 
t2Æ t1 1 
t1Æ t3 0.5 
t3Æ t1 0.67 
t5Æ t6 0.5 
t6Æ t5 0.67 
t6Æ t7 0.67 
t7Æ t6 1 
2.b  Association rules 
Concept Tags 
C1 t1,t2,t3 
C2 t5,t6,t7 
2.c Clusters 
0
50
100
150
200
250
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Similarity Threshod
N
um
be
r o
f C
lu
st
er
s
apple tiger
australia bridge
102
 
 
 
 
the respective weightings of tags. This ensures that a resource 
overlapping with important tags of a concept be ranked higher 
than a resource overlapping with less important ones. The 
concept-tag weight denoted as w(t,C) is defined as: 
®¯­ ?
? 
CtifCoupInvcohesion
Ctif
Ctweight
.*
0
),(
 
      
(4) 
¦
?
 
Cv
vtWcohesion ,
    
 (5) 
¦
? Cu utWCoupInv ,2.     (6) 
The cohesion measures the strength of connectivity of a tag 
with other tags in the same concept, the Inv.Coup (inverse 
coupling) measures the strength of connectivity of a tag with 
other tags not in the same concept. This tag-concept similarity 
measure is developed based on the classic TFIDF measure in 
IR field[8]. A tag with lots of internal links should rank higher 
in this concept than another tag with only a few internal links 
but many external ones. With the concept-tag weight 
information, the similarity between a resource and a concept is 
adjusted to: 
¦¦
¦
??
??
 
rtCt
Crt
rtwCtw
Ctw
CrSim
),(),(
)),((
),(
2
)(
  
(7)
  
 
The resource-concept similarity value is also in the range 
[0,1].   w(t,r) denotes the weight of tags in each resource. It is 
determined by its weight in the concept it joins. If a tag does 
not belong to any cluster, its resource-tag weight will be zero.  
 
®¯­ ?
? 
Ct
CtCtw
rtw
|0
|),(
),(
   
(8) 
Once resources are associated and ranked in concepts, it is 
worthwhile to rank the concepts itself. Intuitively, concepts 
representing large number of resources in the result set should 
have higher rank. However, we should also evaluate the 
quality of such big concept. Concepts are ranked by its 
internal cohesion and the number of resources it represents. 
Average tag weight in a concept is a simple way of measuring 
the quality of internal cohesion of a concept. Formally the 
ConceptRank is defined as: 
 
N
N
C
Ctw
R cCtc  
¦
?
||
),(
   
(9) 
where N denotes the total number of resources in the result 
set and NC denotes the number of resources in concept C. The 
first part of formula (9) computes the average tag weight in 
the concept. The second part normalizes the number of 
resources in the concept by total result number. Table 3 shows 
the tag-concept weight, resource-concept similarity and 
ConceptRank values for the simple example in Table 2. The 
new metrics preserve most of the observed orders we 
described. However, there are a few exceptions. For instance, 
resource r4 is ranked higher in concept C1 than resource r7. 
Such discrepancy is a result of applying tag weights. In this 
particular example, t2  has higher weight in C1 than t3  has, 
indicating that  r4 is better represented by C1 than r7 is.  In 
addition, the new metrics is able to precisely rank resources 
such as r6 and r8 in concept C2. r6 is ranked higher because it 
overlaps with more important tags in C2. 
Table 3 Similarity measure 
C1 C2 
weight(t1,C1) = 2.92 
weight(t2,C1) = 1.75 
weight(t3,C1) = 1.17 
weight(t5,C2) = 1.17 
weight(t6,C2) = 2.84 
weight(t7,C2) = 1.67 
Sim(r1,C1)  = 1.00 
Sim(r2,C1)  = 0.10 
Sim(r4,C1)  = 0.80 
Sim(r5,C1)  = 0.64 
Sim(r7,C1)  = 0.70 
Sim(r2,C2)  = 0.10 
Sim(r3, C2)  = 1.00 
Sim(r5, C2)  = 0.04 
Sim(r6, C2)  = 0.79 
Sim(r8, C2)  = 0.71 
ConceptRank = 1.22 ConceptRank = 1.83 
IV. EXPERIMENT AND RESULTS 
The proposed algorithms are evaluated on several data sets 
obtained from CiteULike (www.citeulike.org). CiteULike is 
an online bibliography organizer. It allows users to save 
bibliographic information about published research papers 
online. Users can tag those papers with keywords. A paper can 
be retrieved through all tags collectively assigned by various 
users. We collected several data sets using query terms 
including “algorithm”, “software”, “web” and so on.  
Table 4 shows the clustering results on the data set 
obtained by query term “web”. We only show the top 
concepts whose ConceptRank values are greater than 0.1. For 
each concept, we list the representing tags in decreasing order 
of their weights in the concept. We also show the top 5 papers 
based on resource-concept similarity values. The result clearly 
distinguishes several research areas related with keyword 
“web”. The first and the third concepts are in the area of web 
information retrieval with different focuses. The first concept 
has a focus on query log analysis and user behavior study; 
while the third one represents more traditional approach based 
on hyperlink analysis. The second concept has a focus on 
semantic web and web services research. The fourth has a 
focus on more recent research on web 2.0 and social web. 
 
 
103
 
 
 
 
Table 4 “web” query result 
Concept Members 
R: 0.92 
#t: 20 
#r:96 
 
search (24.21), query_log_analysis (14.47), queries(14.47), 
web_search (13.15), logs (12.81), search_behaviour (12.47) 
search_engines (12.15), engines (9.64), study (9.43), forsigir2008(6) 
Searching the Web: the public and their queries (0.79) 
Vox Populi: the public searching of the Web (0.79) 
U.S. versus European web searching trends (0.79) 
Analysis of a very large web search engine query log(0.77) 
An analysis of web searching by European AlltheWeb.com users (0.72) 
R: 0.73 
#t: 9 
#r: 153 
 
service (8.21), semantic (6.01) ,composition (5.14), choreography(4) 
coordination (3), ontology (2.73), matching (2.22), discovery (1.71), rdf (1.6) 
Composition-oriented Service Discovery (0.69) 
Automated Composition of Semantic Web Services into Executable Processes 
(0.69) 
A software framework for matchmaking based on semantic web technology (0.69) 
Combining RDF and OWL with SOAP for Semantic Web Services (0.67) 
An ontology-driven framework for data transformation in scientific workflows 
(0.63) 
R: 0.28 
#t: 5 
#r: 92 
information-retrieval (3.0), prodei (1.67), link-analysis(1.67), algorithms (1.0) 
SALSA: the stochastic approach for link-structure analysis (1) 
What is this page known for? Computing Web page reputations (0.92) 
Enhanced hypertext categorization using hyperlinks (0.92) 
Measuring index quality using random walks on the Web (0.92) 
Efficient crawling through URL ordering (0.79) 
R: 0.17 
#t:7 
#r: 57 
 
Folksonomy (4.17), Social (3.83), Eni (3.33), Folksonomies (2) 
Networks (1.67), Collaboration(1), Tagging (1) 
Collaborative tagging as a tripartite network (0.82) 
Social bookmarking in the enterprise (0.71) 
Referral Web: Combining Social Networks and Collaborative Filtering (0.61) 
The Tipping Point: How Little Things Can Make a Big Difference (0.55) 
Collaborative thesaurus tagging the Wikipedia way (0.53 ) 
 
Table 5 shows the clustering result on dataset obtained 
using query term “algorithm”. This dataset overlaps partially 
with the “web” dataset. They both have papers in the web 
information retrieval research area. Interestingly, because 
our algorithm dynamically clusters query results, the papers 
in the shared part are put into slightly different concepts with 
relations to the query and to other results. The first concept 
in “algorithm” result has similar focus with the third concept 
in “web” data set.  Both have “information-retrieval”, “web” 
and “algorithm” as keywords. The complete sets of 
keywords are different which reflect slightly different 
focuses of the groups. The first concept in “algorithm” data 
set has “google” and “pagerank” as keywords. The original 
PageRank paper by S.Brin and L. Page is ranked fourth in 
that concept. This paper has a large number of tags assigned 
by different users which dilutes the focus and makes it 
ranked lower than a few other papers with less tags. The 
third concept in “web” data set does not include specific 
words like “google” or “pageRank” as its keywords and it 
focuses on general theme on link based web ranking. The 
Brin&Page paper belongs to the concept. It is not one of the 
top 5 results though. The rest of the concepts in “algorithm” 
datasets are about gene analysis, graphic algorithm and 
social network analysis.  
V. CONCLUSION 
This paper reported a novel algorithm for presenting the 
web resource search result on the social tagging sites. It 
adopts a two step clustering approach to organize and rank 
resources based on their relative similarities with each other. 
Initial similarities are computed using user and tag 
information. The query results are then organized as a group 
of concepts represented by a few semantically related terms.  
The resources that are related with each concept are rank 
within the concept. In addition, concepts are also ranked by 
its representing terms and the number of resources they 
represent. We run experiment on data sets obtained from an 
online bibliography organizing sites allowing people to share 
their favorite academic papers. The results show that our 
algorithm can effectively distinguish various research areas 
in query results and associate papers in those areas. 
 
 
104
 
 
 
 
Table 5 “algorithm” result 
Concept Members 
R: 0.44 
#t: 9 
#r: 64 
google (8.33), ir (6.11), ranking (5.41), search (5.04) 
information-retrieval (4.83), web (3.94), informationretrieval (3.89) 
pagerank (3.05), information (1) 
Inside PageRank (0.88) 
The anatomy of a large-scale hypertextual Web search engine (0.84) 
Authoritative sources in a hyperlinked environment (0.65) 
The PageRank Citation Ranking: Bringing Order to the Web (0.57) 
Understanding Search Engines: Mathematical Modeling and Text Retrieval (Software, 
Environments, Tools), Second Edition (0.48) 
R:0.34 
#t:9 
#r:55 
ica (11.2), brain (6.2), emg (4.63), neuroscience(4.43), eeg (3.69), fmri (2) 
eog (2), erp (2), statistics  (1.11) 
Recovering EEG brain signals: Artifact suppression with wavelet enhanced independent 
component analysis. (0.92) 
Temporally constrained ICA: an application to artifact rejection in electromagnetic brain 
signal analysis. (0.89) 
R:0.33 
#t:9 
#r:51 
Microarray (8.34), Expression (5.49), gene_set_analysis (4.74), comparison (2.50) 
functional_annotation (4.58), gene_list_analysis (1.43) 
Group testing for pathway analysis improves comparability of different microarray data 
sets. (0.90) 
GenePattern 2.0 (0.85) 
Significance analysis of functional categories in gene expression studies: a structured 
permutation approach (0.85) 
R:0.28 
#t:8 
#r: 74 
social_networks (4.77), social-networks (3.77), networks (3.70) 
community (2.56), network (2.55), sna (2), graphs (1.14), social (1) 
Finding community structure in very large networks (0.77) 
Finding and evaluating community structure in networks (0.71) 
Fast algorithm for detecting community structure in networks (0.62) 
The Small-World Phenomenon: An Algorithmic Perspective (0.62) 
A measure of betweenness centrality based on random walks (0.60) 
R: 0.22 
#t: 6 
#r: 44 
 
feedback (6), aggregation (4), vertex (3.5), set (3.5), rank (2), graph (1.88) 
Ordering by weighted number of wins gives a good ranking for weighted tournaments 
(1.0) 
Ranking Tournaments (1.0) 
Aggregating inconsistent information: ranking and clustering (0.75) 
A Min-Max Theorem on Feedback Vertex Sets (0.71) 
ACKNOWLEDGMENT 
This research is supported by MSRA ISAR Research Grant. 
REFERENCE 
1. Agrawal, R., Imieli?ski, T., and Swami, A., Mining association 
rules between sets of items in large databases. In Proceedings of 
the 1993 ACM SIGMOD (Washington, D.C., USA,). 207-216. 
2. Cai, D., et al. Hierarchical clustering of WWW image search 
results using visual, textual and link information. In 
Proceedings of the 12th annual ACM international conference 
on Multimedia (New York, NY, USA, 2004). 952-959. 
3. Ding, C., et al. A min-max cut algorithm for graph partitioning 
and data clustering. In Proceedings of IEEE Internal Conference 
on. Data Mining, (San Jose, CA, USA, 2001). 107-114. 
4. Dong, X., et al. Similarity search for web services. In 
Proceedings of the 30th VLDB Conference, (Toronto, Canada, 
2004). 372-383. 
5. Fonseca, B. M., et al. Concept-based interactive query 
expansion. In Proceedings of the 14th ACM international 
Conference on information and Knowledge Management 
(Bremen, Germany, 2005). CIKM '05. ACM Press, 696-703. 
6. Nie, Z., et al., Object-level ranking: bringing order to Web 
objects, in Proceedings of the 14th international conference on 
World Wide Web. 2005, ACM Press: Chiba, Japan. 
7. Qiu, Y. and Frei, H. 1993. Concept based query expansion. In 
Proceedings of the 16th Annual international ACM SIGIR 
Conference on Research and Development in information 
Retrifeval (Pittsburgh, PA, USA, 1993). SIGIR '93., 160-169. 
8. Salton, G., Wong, A. and Yang, C.S., A vector space model for 
automatic indexing. Communications of the ACM, Vol. 18, No. 
11 (November 1975), 613-620. 
9. Sanderson, M. and Croft, B. Deriving concept hierarchies from 
text. In Proceedings of the Annual international 22nd ACM 
SIGIR Conference in Information Retrieval, (Berkeley, CA, 
USA. 1999) SIGIR’99, ACM Press, 206-213. 
10.Tibshirani, R., Walther, G. and Hastie,T. Estimating the number 
of clusters in a dataset via the gap statistic. Journal of the Royal 
Statistical Society: Series B (Statistical methodology). (2001), 
63(2), 411-423. 
11.Xu, J. and Croft, W.B.  Query expansion using local and global 
document analysis. In Proceedings of the 19th annual 
international ACM SIGIR conference on Research and 
development in information retrieval. (Zurich, Switzerland. 
1996).SIGIR’96, 4-11.  
105
