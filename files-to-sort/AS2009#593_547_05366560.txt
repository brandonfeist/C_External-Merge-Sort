Frequent Itemsets Discovery Algorithm and its application based on Frequent Matrix 
Lilin FAN    
 College of Computer & Information technology 
 Henan Normal University 
Xinxiang , China,453007 
e-mail:swjtufll@163.com 
 
Abstract: There are varieties of linkages among the 
properties of the data in a large database,  These linkages are 
hidden in the data of the database, the purpose of association 
mining is to identify these hidden association rules. The 
discovery of association rules is based on frequent itemsets, so 
how could we make frequent itemsets fast and accurate is the 
main research of association data mining. In this paper, a 
matrix-based frequent discovery algorithm is raised according to 
the problems in the existing algorithms in finding frequent 
itemsets, and it has a practical application in the automobile 
fault parts association analysis system of automotive industry 
chain business intelligence platform.. 
Key words: frequent itemsets, association rules, data mining 
? INTRODUCTION 
The quality of automobile is the base of the a automotive 
manufacturers’ survival, there are many ways to improve the 
quality of products, but it is mainly by improving the quality 
of parts and vehicle design to achieve the objectives of 
demand of improving product quality for enterprises. One of 
the major premises is built on a scientific model fault 
analysis. Models’ fault analysis is mainly performed by 
seeking the relationship between the fault and the 
information of vehicle mileage, the use of time, the territory 
of use, accessories brands and related fault accessories when 
some accessories error occurred. 
The basic concepts and classical algorithm 
algorithm-Apriori of Association Rules Mining were 
proposed by R.Agrawal [1], algorithm-Apriori is comprised 
of looking for frequent itemsets and the discovery of 
association rules in two parts, the algorithm is simple, 
intuitive, easy to implement, but there are still two serious 
problems in its frequent itemsets search[2]: A great deal of 
I/O load is needed when scanning the database several 
times; It may produce a large candidate itemsets, it’s a 
challenge to time and living space. 
In this paper, Frequent Itemsets Mining Algorithms 
Based on Frequent Matrix is raised for the two problems. 
This algorithm avoid the frequent scanning the databases, 
minimize the number of candidate itemsets and improve the 
efficiency of Association Rules Mining.    
? FREQUENT ITEMSETS MINING ALGORITHMS BASED ON 
FREQUENT MATRIX 
Frequent discovery algorithm based on binomial itemsets 
matrix raised in this paper is a kind of algorithm that 
discovers all the most likely Frequent itemsets and then 
verify whether they are frequent or not one by one in the 
constraints of special matrix. 
A. The definition of frequent itemsets 
  Assume I = {i1?i2?… , im} is a collection of all the 
items. D is a collection of database transactions, transaction 
T is a subset of a project. Each transaction has a unique 
identifier TID. Assume A is a collection consisting of 
project, called itemset, transaction T contains A. If the 
project collection A contains k items, then called k-item, 
recorded as Length(A)=k, times that project A appears in 
transaction database D is called Itemset Support. If the 
Support is more than the minimum support threshold given 
by user, we call the itemset is a frequent itemset.     
B. Definition and nature of binomial itemsets matrix 
Definition 1: Set the total number of items in database to 
p, serious number of items followed by 1, 2, …, p, matrix 
Ap×p’s element a[i,j]=Support, 1?i?p, 1?j?p (a[i,i] regarded 
as a special binomial itemset), we regard A as binomial 
itemsets phalanx of D, It is obviously that the matrix is 
symmetric on the main diagonal. 
Definition 2: Assume X is an itemset of database, 
Length(X)=k (k?3),  Bk×k is a phalanx generated by 
Definition 1, matrix B’s element b[i,j]=Support, i, j is 
arbitrary item number in X, then we call B the 
978-1-4244-4507-3/09/$25.00 ©2009 IEEE
Corresponding binomial itemsets phalanx of itemset X and a 
subset of A matrix. 
Definition 3: Assume B is a phalanx defined by 2, if 
arbitrary b[i,j]?Minsupport, we call B frequent binomial 
itemsets phalanx of X. 
Theorem 1  Assume X is a database itemset and 
Length(x)=k(k?3), then the necessary condition for X of 
being frequent itemset is: all of its binomial subsets are 
frequent. 
Proof: According to reference [3]: ??X ,if YX ? , 
if Y is a frequent itemset, then X is also a frequent itemset, 
therefore, if a k-itemset is frequent, its all nonempty sets are 
frequent, it is obviously that all binomial itemsets are 
frequent.Proof is completed. 
Theorem 2  Assume X is a frequent itemset in D, and 
Length(X)=k(k?3), items of X are numbers defined by 4, 
then rule out the rows and columns of items in matrix A that 
do not belong to X, according to Definition 3: Sub-matrix B 
that consisted by the rest binomial itemsets will surely be a 
frequent binomial itemsets phalanx. 
Proof: According to Definition 2, this matrix is 
corresponding binomial itemsets matrix of X, we can see 
from the conditions that X is frequent, according to theorem 
1, we can see all binomial subsets of X are frequent, with the 
Definition 6 we know that B is frequent binomial itemset 
phalanx.  
Therefore, the proposition is established and the proof is 
completed 
For binomial set matrix is symmetrical to the main 
diagonal of matrix, we all use upper triangular matrix to 
improve budget speed, reduce storage space in practical 
applications,  
Theorem 3  Assume X is an itemset in D, and 
Length(X)=k(k?3), then the necessary condition for X is a 
frequent set is: binomial phalanx of itemset X is frequent 
phalanx. 
Proof of this theorem is a direct consequence from 
theorem 1 and will not be repeated. 
The significance of theorem 2 and theorem 3 is that we 
can get the binomial matrix of the database by scanning it 
once first, then search this matrix to get the number of 
frequent phalanx n which is number of maximum candidates 
of k(k?3) frequent itemsets; All itemsets collections 
corresponding to frequent phalanx is frequent candidate 
collections of D(excluding 1,2-frequency set), as the 
phalanx’ generation is based on frequent binomial itemsets, 
in essence, it’s a kind of “pruning” to the candidates based 
on frequent matrix, so the number of candidate sets is 
reduced greatly. The maximum crest reduction of 
unnecessary itemsets mining improved the mining 
efficiency. 
C.  frequent candidate itemsets generation algorithm 
For p×p matrix A defined in definition 1, we use 
breadth-first search algorithm to find out all possible 
full-frequency sub matrix, and store them with 
tree-structure(item candidate tree). 
Step 1, Search main diagonal elements, if 
a[i,i]?Minsupport, then we create a child node i under the 
super-root node, corresponding to the special frequent 
itemset a[i,i]. 
Step 2, Search elements a[i,j](j?p) in the right side of 
matrix a[i,i],if a[i,j]?Minsupport, then we create a child node 
j under node i(a[i,j] corresponding to frequent itemset a[i,j]); 
Step 3, Create grandchildren node for all child node of i 
in accordance with step 2, and so on, until we get the only 
child node. 
Through this search, we finally get a tree, it’s obviously 
that matrix that consisted by items of the tree’s each path 
might be a frequent binomial itemset matrix. The longest 
path of the tree is the maximum frequent itemset that may 
exist. 
Preorder traversal the tree, we find out all paths, all items 
of each path(except the root node) can form a binomial 
phalanx for each node is a frequent two sets, for example, 
we traverse to the third layer to get a 3×3 binomial matrix, to 
the third layer   we get a 4×4 binomial matrix, the k-th 
layer might get a k×k binomial matrix, Then, determine 
which of the phalanx is frequent according to Definition 3, 
itemsets corresponding to each frequent phalanx is the 
frequent candidate itemsets we are looking for, then we put 
the itemsets into a database of candidate frequent itemsets. 
For 1,2-item frequent itemsets can be easily found from 
binomial itemsets matrix, and 1,2-item frequent itemsets are 
generally large, so that in order to enhance the research 
speed, we don’t generate 1,2-item by tree, and 2-item 
frequent sets will not be stored in the database of frequent 
candidate itemsets neither. 
D.  frequent candidate itemsets support algorithm 
After the frequent candidate itemsets database has been 
gotten by 2.3, these itemsets need to scan transaction 
database for a second time, then we calculate the support 
degree, it can be  true frequent itemsets if it is no less than 
Misupport, we calculate support degree of candidate 
itemsets fast through vector inner product calculation.  
Definition 4 Assume total item number in transaction 
database D is p, serial numbers of items followed by 1,2, 
…,p, then the number of item of frequent candidate itemsets 
database will not exceed p, transaction bit vector Tran_Vec 
is defined as follows: 
 Support algorithm is calculated as follows: 
Assume Ficd is frequent candidate itemset database, the 
number of records(the number of candidate itemsets) is 
Fic_count. 
Step 1, Deine a Fic_count×p matrix Ficm, each row of 
the matrix is p-dimensional bit vector that generated 
according to Definition 4, corresponding to a candidate 
itemset of database; Define a Fic_count×1 matrix for the 
storage of each candidate itemset support in Ficd, set the 
initial value of each row of the matrix to 0; Define a 
Fic_count×1 matrix inner_product_Result_Matrax to store 
the results of temporary calculation. 
Step 2, Get the first record from the transaction database, 
transfer it to p-dimensional vector Trant_Vecr in accordance 
with the definition 4. 
Step3,Inner_product_Result_Matrax=Ficm·Tran_VecT 
Step4,Itemsets_Support_Matrax= 
Itemsets_Support_Matrax+ Inner_product_Result_Matrax 
Step 5, return to Step 2 until all transactions of records 
are processed. 
Algorithm takes a second scan on transaction database, 
calculate all candidate itemsets support in Ficd dand store 
them into matrix Itemsets_Support_Matrax, in this matrix, 
candidate itemsets corresponding to the elements whose 
values are bigger than Minsupport is the final frequent 
itemsets that we are looking for. 
E. Association rules discovery algorithm  
This paper raised a new frequent itemsets discovery 
algorithm and didn’t design special association rules 
discovery algorithm, in the following applications in this 
paper, association rules discovery algorithm uses the 
association rules discovery algorithm according to literature 
1 directly, and not repeat any more. 
  ?  ALGORITHM FOR SPECIFIC APPLICATION OF THE 
RESULTS 
The quality of automobile is the base of the a automotive 
manufacturers’ survival, a lot of decisions of enterprises are 
made around how to improve product quality, the quality of 
spare parts is good or bad and the design of product is good 
or bad are the main factors which affecting the quality of 
automobiles, how to make decisions has become a major 
problem. Using the method of association rules mining we 
can find the relationships among fault parts, the design 
department can find out the cause of problems according to 
the results of mining, then they change brand of parts or find 
out design flaws, and redesign it to achieve the objective of 
improving product quality.  
By discussing with related decision makers of a Sichuan 
Automobile Co., Ltd, using three bags of their enterprise 
data for 2004-2007, the system development environment is 
Microsoft Visual Studio 2008, developing language is C# 
and database system is Microsoft SQL Server 2005. The 
analysis result is shown in Table 1. 
?  CONCLUSION 
With the global economy is moving into an era of 
information analysis, the ability of information processing 
and utilization is the key to make an enterprise succeed or 
defeat. Enterprises hope to transfer the transaction data into 
reliable information, then transfer the knowledge into profit. 
Choosing the appropriate algorithm is very important, this 
paper improved mining efficiency by improving association 
(1)p21i
t1
t0
_ ][ ??? "=??
?
?
?
=
project
project
VecTran i
algorithm, and achieved the association mining of parts 
fault. 
Table 1.  the result of parts association faults mining. 
Name of Association Rules Support Confidence 
out oil-seal of rear?  inner oil-seal of rear 160 86.021% 
reducer set assembly?  basin gear 123 66.667% 
shortening shell?  basin gear 174 54.375% 
semi-axial bushing?  half axle 100 43.478% 
differential shell?  basin gear 152 42.817% 
inner oil-seal of rear? out oil-seal of rear 160 32.990% 
differential cross axle?  epicyclic gear 102 25.373% 
epicyclic gear?  differential cross axle 102 18.149% 
half axle? rear axle shell 174 18.087% 
rear axle shell? half axle 174 15.183% 
half axle?  semi-axial bushing 100 10.395% 
basin gear? shortening shell 174 6.214% 
basin gear? differential shell 152 5.429% 
basin gear? reducer set assembly 123 4.393% 
  
REFERENCES 
[1] R.Srikant, R.Agrawal. “Mining Quantiative association rules in 
large relational tables.” Proceedings of ACM SIGMOD International 
Conference on Management of Data?Montreal, Canada1.996,1-12 
[2] Mao Guo-jun. “Data Mining Techniques and Algorithms for 
Mining Association Rules.” Beijing Industrial University doctoral 
dissertation. 2003?63-64 
[3] li Xiong-fei?Li Jun. “Data Mining and Knowledge Discovery”. 
Higher Education Press.2003?118-120 
[4] Huang Liusheng, Chen Huaping. “A fast algorithm for mining 
association rules.” Journal of Computer Science and Technology. 
2000?15(6)?619-624 
[5] Zeng Wan-pin,Zhou Xu-bo,Dai Bo. “An Association Mining 
Algorithm Based on Matrix.” Computer Engineering,. 
2006,32?2?:45-47 
[6] Niu Xiao-fei, Shi Bing, Lu Jun. “A High Efficiency Algorithm of 
ABM for Mining Associataion Rules.” Computer Engineering,. 
2004?30(11):118-120 
