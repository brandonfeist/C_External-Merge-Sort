Mining Classification Rules via an Apriori Approach
S M Monzurur Rahman, Mohammed Rokibul Alam Kotwal, Xinghuo Yu† 
Department of CSE, United International University, Dhaka, Bangladesh. 
†School of Electrical and Computer Engineering, RMIT University, Melbourne, Australia. 
smr@cse.uiu.ac.bd, rokib_kotwal@yahoo.com, x.yu@rmit.edu.au 
Abstract 
Classification rules are the interest of most data miners 
to summarize the discrimination ability of classes 
present in data. A classification rule is an assertion, 
which discriminates the concepts of one class from oth-
er classes. The most classification rules mining algo-
rithm aims to providing a single solution where multiple 
solutions exist. Moreover, it does not guarantee the op-
timal solution and user has not any control over the 
classification error rate. In this paper, we addressed 
these problems inherent in mostly used classification 
algorithms. A solution has been proposed to solve these 
problems and it has been tested with experimental data. 
Keywords: Data Mining, Association Rules, Classifi-
cation Rules, Characteristic Rules, Classification Algo-
rithm, Apriori. 
I. INTRODUCTION 
The data mining has been a new and exciting research 
field receiving an increasing attention.  The first task of 
data mining is to construct a model that represents a 
huge data set of interest. One common representation of 
data is by means of rules.  There are mainly three kinds 
of rules; association rules, classification rules and cha-
racteristic rules. The acquisition of classification rules 
is of interest in this paper [1]. Classification is about 
classifying given data into categories or groups which 
have some common features. Typical examples of min-
ing classification rules include, for example, target mail-
ing, franchises location, credit approval, treatment, ap-
propriateness determination etc [1]. Classification is a 
well-studied problem in AI and Mining classification 
rules have been studied since data mining is introduced 
in [1-5]. There are several models for classification e.g. 
Neural Networks [4], Statistical models [6], Genetic 
models [7], Decision trees [8-10], among which deci-
sion trees and neural networks appear to be used very 
often. 
A decision tree is a directed graph consisting of nodes 
and directed arcs. The nodes frequently correspond to a 
question or test on an attribute of the given data set. The 
arcs contain values of attributes. Classes are at the leaf 
level. The most popular ID3 [9-10] algorithm can be 
used for generating decision trees. This algorithm parti-
tions the given data set into smaller and smaller subsets 
by growing the tree. Decision trees have many advan-
tages over other classifiers such as Neural Networks in 
that they are easy to construct, relatively fast, easy to be 
converted into SQL queries and can handle non-
numerical data. The rule extraction using decision trees 
is simple as well. However, its classification perfor-
mance may be compromised in the situation when the 
data set has complicated (nonlinear) domains. Since the 
mining goal of classification problem is to assign each 
example in the data set to one of many categories, the 
decision tree approach has to generate many branches 
for each node, and may result in large mining errors. On 
the other hand, Neural Networks can learn the classifi-
cation rules by many passes over the training data set, 
which takes a longer time for learning. The classifica-
tion functions generated by Neural Networks are buried 
in the weights which make the extraction of classifica-
tion rules difficult as well. Some attempts have been 
made in this direction [13]. Decision functions can be 
made of many forms e.g. equations, trees or production 
rules. Then each class can be classified or partitioned by 
the decision function associated to it. Efficient functions 
can be acquired by using, for example, the information 
theory. The weakness of this approach is that it is una-
ble to deliver all the solutions i.e. how many decision 
functions can be constructed for each class and which 
one is best on the basis of compactness. Another prob-
lem associated with mostly used classification methods 
is that they do not allow relaxation and tolerance in the 
mining process. Consequently these methods may end 
up with decision functions with larger classification 
errors. The present paper attempts to address the afore-
mentioned problems. Some new measures such as glob-
al and local measures have been introduced to study 
classification rules in this paper. Furthermore, it has 
provided a method to mine classification rules from real 
data based on well-known data mining algorithm apri-
ori. The proposed algorithm is also tested with real data 
and result is reported in this paper. 
This paper is organized as follows. Section II discusses 
the data-mining problem with examples and formalizes 
it. Section III describes our proposed method to mine 
sets of classification rules from submitted data. The 
proposed method is tested and results are reported in 
Section IV. Finally, Section V gives some concluding 
remarks. 
II. PROBLEM STATEMENT 
First, we will illustrate common problems associated 
with most currently available classifiers by considering 
a known animal world data [12]. There are three classes: 
Bird, Hunter and Peaceful animals, which are assumed 
to have been identified already. We choose the mostly 
used classifier C4.5 from Quinlan [9] as a typical clas-
sifier. The result of the classification using C4.5 is 
shown in Fig. 1. It is found in the experiments that this 
classifier C4.5  and other as well only gives a single 
solution for the classification problem as shown in Fig. 
Proceedings of 13th International Conference on Computer and Information Technology (ICCIT 2010) 
23-25 December, 2010, Dhaka, Bangladesh
978-1-4244-8494-2/10/$26.00 ©2010 IEEE 388
1 regardless of whether other solutions may exist. If we 
look at the data carefully, we can find that there is an 
alternative solution with the same size of tree as shown 
in another part of the figure, which cannot be obtained 
using C4.5. The downside of this single solution may 
have some consequence. For example, consider a test 
data of an imaginary animal, which has hooves, 2-legs, 
can fly and also has feather. If we want to determine the 
class of this animal using the tree from C4.5, we will be 
given the answer that this animal belongs to Peaceful 
animal. But we will be more convinced if it is classified 
as a Bird since it has most characteristics of a bird. In 
this situation, it may be helpful if the classifier can give 
multiple solutions to choose from. 
Fig. 1. Decisions tree and function for animal classifier 
produced and does not produce by C4.5. 
Another limitation is that the classifiers such as C4.5 do 
not allow enough relaxation in producing rules. This 
limitation restricts the classification ability of most clas-
sifiers as real data sets do contain noise and fuzziness 
and certain relaxation may help to produce more con-
vincing rules. For example, some members of one class 
may have overlaps with members of other classes. This 
may incur a classification error during constructing the 
classification model. This problem has been addressed 
to some extent in the existing classifiers, for example, 
C4.5 tries to keep this error as low as possible. When 
classification is not well defined by data given to C4.5, 
C4.5 constructs simple trees at the cost of complying an 
amount of classification. Note that C4.5 works accord-
ing to Information Theory in the construction of the root 
node to leaf nodes and users have no influence in guid-
ing the construction. Obviously it will be more desirable 
if users can have certain control in order to obtain their 
desired results. To demonstrate this point, let us consid-
er again the animal world data. If a user agrees to accept 
the classification error of 1%, a good classification solu-
tion may be given as follows, in comparison with the 
solutions in Fig. 1. 
Decision Function 3: 
IF Animal Size_Is (Small) THEN Class = Bird 
ELSE 
IF Animal Has (Hooves) Then Class = Peaceful 
IF Animal Has_Not (Hooves) Then Class = Hunter 
ENDIF 
We now look at the problem formulation. The classifi-
cation problem can be considered as finding the set of 
classification rules that partition the given data into dis-
joint sets or groups. The problem of inferring classifica-
tion rules from a set of given examples (data) has been 
formalized in many literature [11]. The formulation can 
be stated as follows. 
Let C  be a set of m  classes { }mCCC ,,, 21   and A  
be a set of n attributes ( )naaa ,21 , . Define )( iadom as 
the set of possible values for attribute ia . The conven-
tional classification problem formulation is given a 
large data set or database D , each example of which is a 
n -tuple of the form nvvv ,, 21 where )( ii adomv ? . 
Obtain an appropriate set of classification rules R whose 
member classification rule is of the form in conjunctive 
normal form kii Caar l ??1: , 
where nimk j ?? ,, , lj ,,= 1   and n li ?, . It is 
obvious that the member classification rules in R are 
disjoint, i.e. if ii r C ?  and jj r C?  and ji ? , 
then ji r r? . If a new example fires a classification rule 
ir  then it is said that this example is a member of iC
class. The problem is to obtain an appropriate set of 
classification rules, denoted as ? , from given data with 
decidability and classification accuracy ?  and ?  re-
spectively. The membership of set ?  is rule subset. For 
example, for the ith member subset   rule sets e.g. 
j 
i? where i denotes the class information and j denotes 
the member index of the class iG . 
j 
i? partition data 
between the iG  class and non-class and it consists of 
classification rules of the  form kji Gaa ?? , 
where jimk ?? ,,  and nji ?, . We will explain these 
two terms in the following sections.  If a new example 
fires a classification rule from ji?  then it is said that the 
example is a member of iG . 
III. MINING CLASSIFICATION RULES 
USING APRIORI APPROACH 
The mining task is to derive a set of classification rules 
for each class iC . Before proceeding further, we first 
introduce the following measures, which will be used 
later. 
Definition 1: Rule Class Support: An l-tuple classifica-
tion rule is expressed in the form of 
k
l
i
i a C 
=

1
where 
ia denotes ith attribute in the class kC  and   is the 
AND operator. Support of a classification rule ir of 
class kC is defined in sub example space kD as 
k
i 
k
ki N
n
Drs =),( , where 
i 
kn is the number of examples in 
kC that support the rule ir  and kN the number of ex-
amples in kC that support the rule ir and kN the num-
ber of examples in kC . For example, from the data we 
have, 
0.1 
7
7 ),)(( == birdDBirdfeatherHass  and 
389
85.0 
7
6 ),)(( == BirdDBirdflyDoess . 
Definition 2: Rule Class Global Support: A gklobal 
support of classification rule ir of class kC is as 
N
n
Drs
i 
k
i =),( , where 
i 
kn  is the number of examples 
of all class kC supporting the rule ir and N is the num-
ber of examples of all classes in D. For example, from 
the data we have, 
44.0
16
7 ),)(( == DBirdfeatherHass  and 
38.0
16
6 ),)(( == DBirdflyDoess . 
Defenition 3: Rule Set Class Support: Class support of 
a rule set is defined as )),((),(
1
i
l
i
ik
j
k drsMaxDRS 
=
= , 
where jkR is jth classification rule set of class kC and 
j
ki r R? . An example can only be considered once in the 
rule level support calculation i.e. ?=
=

l
i
id
1
 and 
k
l
i
i d D ?
=

1
. Here id is the subset of examples of the 
class Dk. All id are mutually exclusive. For example, 
from the data we have two classification rule sets for the 
class bird as follows: 
{ }bird)(bird;)(1 = huntdoessmallsizeRBird  
{ }bird)(bird;)(2 = flydoesmediumsizeRBird
Their rule set class support is then, 
0.1),( 1 =BirdRS bird , 85.0
7
)06(),( 2 =+=BirdRS bird .  
Definition 4: Rule Class Confidence: The rule class 
confidence is defined as 
N
nDrc kiki <>?= 1),( , where 
kin <>  is the number of examples from examples of 
kDD \ (D excluding kD ) that fires the rule ir  and N is 
the total number of examples of all classes. For example 
in Table I 0.100.1),)(( =?= BirdBirdswimDoesc  
8125.0
16
3 0.1),)(( =?= BirdBirdmediumSizec . 
Table I. Classification Example. 
Definition 5: Rule Set Class Confidence: The rule set 
class confidence is defined as )),(()( krcMinRC i
j
k = for 
li 1= and jki r R? . For example, from the data, con-
sider the classification rule set foe the class bird as 
{ }bird)(bird;)(1 = huntdoesmediumsizeRBird  its 
rule set class confidence is then calculated as 
6875.0)6875.0,8125.0()( 1 == MinRC bird . 
Definition 6: Compactness of Classification Rule Sets: 
The compactness of a classification rule set can be 
measured as follows [15]: 

=
×=
r
i i
i
Cn
t
r 
E
1
) 1(1 , where n is 
the total number of tuples in the set, r is the cardinality 
of the rule set, it the number of tuples classified by rule 
i in the rule set, and iC  the number of conditions in the 
antecedent part of rule i. It is obvious a higher value of 
E  indicates the goodness of the class ruleset. In the 
algorithm to be developed below, we will minimize the 
value of r as well as iC . As in our case we are able to 
generate different set of classification rules, so we can 
find out the best set i.e. compact with that measure. 
In the following we shall develop our algorithm for 
mining classification rules. This algorithm consists of 
three parts: Pattern Construction, Mining classification 
rules and Construction of Classifications Rule Sets. In 
our algorithm, we will allow users to control the accura-
cy of the classification rule mining solution. There may 
be two types of errors: one is the indecisive error due to 
less rules set support, the other the misclassification 
error due to rule set confidence. If the rule set confi-
dence is chosen too high, obviously the classification 
error will be small. However this may result in no solu-
tion at all. On the other hand, keeping the class rule set 
support we can get the closer picture of the class in 
terms of rules but it declines the prediction accuracy as 
discussed in the previous section. The other notable 
feature of our solution is that a rule cosseting of same 
logical predicates may appear for different classes. It 
does not mean that they do not have any discrimination 
power. They will have different confidence measure and 
from that we can have the idea it is more likely to be 
appeared in which class. 
3.1 Pattern Construction  
Before using our mining algorithm, data needs to be 
transformed into an appropriate format so that it can be 
processed. This transformation can be done by con-
structing logic predicates from attributes, which may be 
continuous or discrete. We produce rules in logical pre-
dicates from attributes and then truth-values of these m
attributes along with the class information to make the 
so-called patterns for next step of processing.  If the 
attribute is discrete then the predicate is constructed 
directly as j ii a v = , where 
j
iv  is the j 
th discrete value of 
attribute ia . Continuous attribute values can be discre-
dited via the existing approaches such as Chi-merge and 
Chi2 [14], and the resulting predicates would be of the 
form 1 +<<= j ii
j 
i vav , where 
1 +< ji
jv i v  are discrete 
values found, for example from Chi2. After that, all 
patterns will be further constructed with two parts the 
390
predicate and class information, from all examples. We 
now illustrate the above idea via examples given in Ta-
ble 1. From Table 1, we have three attributes Age, Sala-
ry and Sex where Age and Salary are continuous but 
Sex is discrete.  Arbitrarily we discretize Age and Sala-
ry in three ranges, that is ),40[),40,30[),30,[ ???  
),5000[),5000,3000[),3000,[ ??? , and construct the 
predicates as )30(1 <= Age 40p  ) 30(2 <<== Agep , 
)40(3 >= Agep , )3000(4 <= Salaryp ,  
)50003000(5 <<== Salaryp  and )5000(6 >= Salaryp . 
The other two predicates for Sex are constructed as 
)(7 MaleSexP ==  and )(8 MaleSexP == . In this ex-
ample we have three classes which are denoted as Pro-
grammer=C1, Secretary=C2 and Accountant=C3. The 
resulting corresponding patterns are given in Table II.  
Table II. Classification rules produced by C4.5 and X2r 
rule generator. 
The algorithm for normalizing attributes is given below: 
=1.Set 1 j  and predicate set P empty. 
2.Attribute normalization starts at here. For all attributes 
naa 1  do the following steps 
a.If ia  is discrete then find its discrete values and set 
them as il ii v v 
1 . Construct il  logical predicates of the 
form jva j ii ?=  , and add them to the predicate set P; 
b.If ia is ordinal then discretize its domain using 
Chi2 method [14] and get its discrete ranges. Define 
ranges as +??? ,, 1 il ii v v  . Construct il  logical predi-
cates of the form  1 +<<= j ii
j 
i vav where 
)10 +=<<= ilj , and add them to the predicate set P; 
3.Pattern construction starts at here. Given pn  predi-
cates in P, for all examples in the data set do the follow-
ing steps: 
a.Apply all predicates in P to the examples and get 
their truth-value (1 or 0) accordingly; 
b.Get the class member index of the example; 
c.Construct the pattern for the example by concate-
nating pn  truth-values of predicates to the class mem-
ber index e.g 
classvavvavvavvav mn
m
n
m 
m
n 
mmn m ,,,,,,, 11
1
1
12 
11
1 
1 1 <>=<>=<>=<>= 
where m is the number of attributes and in is the num-
ber of ranges of ith attribute. 
3.2 Mining Classification Rules  
We now mine classification rules from constructed pat-
terns obtained in a) using the popular data mining algo-
rithm apriori [1]. Assume that patterns have up to 
n attributes. The idea our algorithm is as follows. We 
start from 1-item classification rules (one condition in 
antecedent part) first, and then graduatelly to reach n-
item classification rules (n conditions in antecedent 
part).  The novelty of our method is each classification 
rule is derived using the apriori algorithm with their rule 
class support and rule class confidence levels given. 
This will enable us to discriminate those which are un-
likely to get the desirable level of support and confi-
dence of all classes. The main idea of the apriori algo-
rithm lies in constructing n-item classification rules 
from (n-1)-item classification rules. The rule of thumb 
is the present set of rules must have all of its subset 
rules in the previous iteration and there should not be 
conflict conditions for the same attribute in any rule as 
conditions of an attribute are disjoint. 
We use the data in Table I and II to explain it. In order 
to mine classification rules for C1 (Programmer class), 
the 1-itemset-classification rules with their associated 
rule class support and rule class confidence levels can 
be obtained as 
[0,0.75],1  ,[0.33,0.5]1  ,][0.66,0.751  ],75.0,0[  1 4321 JpJpJpJp   
[0,0.75]1  ,[1,0.75]1  ,[1.0,1.0]1  ],25.0,0[  1 8765 JpJpJpJp   
If we set the minimum rule class support to be larger 
than zero and minimum rule class confidence level to be 
0.5, we then have the classification rules for 1-itemset 
rules as  , 1  2 J , p   1  3 J 1p     , 1 76 JpJp  . 
Next, we can construct 2-itemset rules from the three 
groups of conditions }{},,{ 632 ppp and  from three 
attributes having a support higher than 0 in the data set 
and they are: 
],1,1[  1],1,1[  1],1,1[  1 637262 JppJppJpp ???  
]75.0,33.0[  173 Jpp ?  
The classification rules from 2-itemset 
is:  1,  1, 1 637262 JppJppJpp ??? 173 Jpp ?  
After 2-itemset rules we have still the three groups of 
conditions }{},,{ 632 ppp  and }{ 7p  from three 
attributes having a support higher than 0. We now create 
3-itemset rules as 
]1,33.0[  1],1,66.0[  1 763762 JpppJppp ????
and the classification rules are mined 
as   1, 1 763762 JpppJppp ???? . 
The next step is to mine 4-itemset rules and as we have 
only three group of conditions we have no rule in the 4-
itemset rules and hence it concludes the iteration. We 
now summarize the final result of classification rules for 
the programmer is  , 12 p J   
1  , 1 76 JpJp  1,  1, 1 637262 JppJppJpp ???  
173 Jpp ?   1, 1 763762 JpppJppp ????  
We can construct sets of classification rules from the 
above result as follows considering class set support 
0.95 and confidence 0.75 by combining one rule with 
other rules as long as the criteria is not met. The sets are 
391
6 1Set 1:  p J  [1, 1] Set 2:  17 p J  [1, 0.75]        
1Set 3:   62 Jpp ? ,  172 Jpp ? [1, 0.75] 
1Set 5:   , 1 763762 JpppJppp ???? [1, 1] 
Any set of the above has the discrimination ability to 
partition examples of the programmer from the rest. The 
algorithm for mining classification rules his given as 
below. The algorithm for Mining Classification Rule 
sets is given as below. 
Input: Examples of m classes, user defined class confi-
dence and user defined class rule support and set sup-
port. 
Output: Number of rule sets obtained in examples for 
each class. 
Steps: 
For class index  to m do the followings: 
1.Find the distinct attributes of class j which are at least 
present once in the example set of class j and lets say 
they are { }j m j j jbbbB ,, 21 = ; 
2.Construct all 1-item rules 1F  from B of the form 
ji 
jb g   for Bb ji ? ? ; 
3.Calculate rule class support, global support and rule 
class confidence of all rules found in step (b). 
4.Add all classification rules of 1F  to the solution set 
jG  which has class confidence and support equal or 
higher than user defined class confidence and support. 
=5.Set 1 k  and do the followings until kF  is empty 
I.Increment k and construct k-itemset rules as super-
sets of (k-1) itemset rules as Apriori algorithm as well as 
taking account that at most one condition may appear in 
a rule from each attribute; 
II.Calculate rule class support, global support and 
rule class confidence of all rules found in step I; 
III.Remove those rules which has class support lower 
than user defined support from kF ; 
IV.Add all classification rules of kF to the solution 
set jG  which has class confidence equal or higher than 
user defined class confidence. 
6.Finally, construct all rule sets ji? of class j from 
jG which has user defined class rule set support. 
The class rule set ji?  of each class gives us the whole 
picture of the class and we can partition the entire data 
set by ji? . The identification of an example pattern is 
straightforward. If the pattern obeys any rule from 
j 
i? but does not obey any other rule from 
kji 
k? ?, then it is obvious that the pattern is belong 
to the class j.  Since we are considering control parame-
ters class confidence and support, the test pattern has the 
chance to fire some rules from ji?  and some rules 
from kji 
k? ?, . In this case we will consider the rule 
level confidence to determine the membership of the 
test pattern and if any rule fired by the test pattern from 
j 
i? has the higher confidence than all rules fired from 
k 
i?  then we conclude that the test pattern belong to the 
class j. 
IV. EXPERIMENT RESULTS 
To study the performance of the proposed method for 
mining classification rules, we used the IRIS data set in 
[16]. It has four numeric continuous attributes: sepal-
length, sepal-width, petal-length and petal-width. IRIS 
data consists of three classes: Setosa, versicolor and 
virginica. It has 150 examples in total and each class 
has 50 examples.  Table III contains some available 
results about the data in [17] as well as the result about 
class support and classification error added. It should be 
noted that only single solution was given in [17], more-
over, the introduced default rules do not present the 
discrimination characteristics of the class. 
Table III. Intervals of attributes for IRIS data. 
Now we report the results using our method. First, each 
attribute was discretized. We used a method similar to 
Chi2 taking account of different stop criteria. As this 
paper is concerned about extracting classification rules 
from discretized attributes, details of the discretization 
algorithm used in this experiment is omitted here. How-
ever, any discretization algorithm can be used. The pre-
dicates for classification rules were constructed as 
shown in Table IV. Three experiments were set with 
high value for user defined class error and lower value 
for support.  The set support was kept the same for two 
experiments. The first experiment gave few rules as 
shown in Table V, VI and VII but with strong class sup-
port and confidence. We constructed the sets of rules 
from these rules and every set had the discrimination 
power to partition data from the associated class to the 
rest. The second experiment was conducted with less 
support and confidence value and it produced many 
rules and sets which have also the discrimination power. 
It should be noted that our result provides multiple solu-
tions without any default rule. 
Table IV. Classification rules produced proposed me-
thod with the parameters Min class support 80%, Min 
class confidence 99% (experiment 1). 
Table V. Classification rule sets from experiment 1 Min 
set support 95%, Min set confidence 99%. 
392
Table VI. Intervals Classification rules produced pro-
posed method with the parameters Min class support 
50%, Min class confidence 90% (experiment 2). 
Table VII. Classification rule sets produced from expe-
riment 2 Min set support 95%, Min set confidence 90%. 
V. CONCLUSION 
The classification problem in the context of data mining 
should be solved taking account that it should provide 
as many as solutions possible by meeting user-defined 
criteria. The more solutions means more classification 
rule sets which increase the discrimination, prediction 
and characteristics exhibition of classes found in the 
data set.  In the paper we have demonstrated multiple 
solutions construction with our algorithm to mine classi-
fication rules. Our algorithm can be tuned with many 
user-defined controls such as intervals, class support, 
class confidence, set support, set confidence etc. It re-
sults different solutions as well as it also guarantees the 
discrimination ability. Our method has been tested with 
widely used Iris data set for testing machine learning 
algorithms. The result has been reported which shows 
the effectiveness of our proposal in the classification 
rule-mining field. We would like to do more research at 
this field in near future. 
REFERENCES 
[1] Agrawal, R., Imielinski T., and Swami A., “Data-
base Mining: A Performance Perspective”, IEEE 
Trans. Knowledge and data Eng., vol. 5, no. 6, Dec. 
1993. 
[2] Weiss S. M., and Kuilikowski S. M., “Computer 
Systems that Learn: Classification and Prediction 
Methods from Statistics, Neural Nets, Machine 
Learning, and Expert Systems”, Morgan Kaufman, 
1991. 
[3] Chou P. A., “Application of Information Theory to 
Pattern Recognition and the Design of Decision 
Trees and trellises”, Ph.D. Thesis, Stanford Univer-
sity, California, June 1988. 
[4] Lippmann R. P., “An introduction to Computing 
with Neural Nets”, IEEE ASSP Magazine, PP 4-22, 
April 1987. 
[5] Lubinsky D. J., “Discovery from databases: A Re-
view of AI and Statistical Techniques”, IJCAI-89 
Workshop on Knowledge Discover in Databases:, 
pp. 204-218, Detroit, August 1989. 
[6] James M., “Classification Algorithms”, Wiley, 
1985. 
[7] Goldberg D. E., “Genetic Algorithms in Search, 
Optimization and Machine learning”, Morgan 
Kaufmann, 1989. 
[8] Breiman L., Friedman J. H., Olshen R. A., and 
Stone C. J., “Classification and Regression Trees”, 
Wadsworth, Belmont, 1984. 
[9] Quinlan J. R., “Induction of Decision Trees”, Ma-
chine Learning, 1:81-106,1986. 
[10] Quinlan J. R., “C4.5 Programs for Machine Learn-
ing”, Morgan Kaufman, 1993. 
[11] Lu, H., Setiono, R., and Liu, H., “Effective data 
mining”, IEEE Transactions on Knowledge Dis-
covery and Data Engineering, Vol. 8, No. 6, pp 
957-961, December 1996. 
[12] Kohonen T., “Self-Organizing Maps”, pp 116, 
Springer Series in Information Sciences, 1995. 
[13] Agrawal R., et al., “An Interval Classifier for Data-
base Mining Applications”, Proceedings of the 18th 
VLDB Conference Vancouver, British Columbia, 
Canada, 1992. 
[14] Yen S. J., Chen A., “An Efficient Algorithm Deriv-
ing Compact Rules From Databases”, In Proceed-
ings of the Fourth International Conference on Da-
tabase Systems for Advanced Applications, Singa-
pore, April, 1985. 
[15] Liu, H., and Setiono, R., “Discretization of Ordinal 
Attributes and Feature Selection”, Publication from 
Department of Information Systems and Computer 
Science, National University, Singapore. 
[16] Public domain IRIS data set at 
http://www.ncc.up.pt/liacc/ML/statlog/datasets.htm 
[17] Liu, H., and Setiono, R., “Dimension Reduction via 
Discretization”, Publication from Department of In-
formation Systems and Computer Science, National 
University, Singapore 
393
