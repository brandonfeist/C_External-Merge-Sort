A Document Clustering Technique Based on Term 
Clustering and Association Rules 
 
Yuepeng Cheng1      Tong Li2   
Computer Science and Engineering Department 
North China Institute of Aerospace Engineering 
LangFang, China, 065000 
cyp4173041@126.com, lflitong6207@sina.com 
 Song Zhu3 
 Computer Software Technology Department 
 Peking University Founder Technology College 
LangFang, China, 065000 
songe2006@qq.com
 
 
Abstract—With development of internet and database 
technology, web mining has got more and more attentions from 
information science domain. This paper proposes a document 
clustering technique based on term clustering and association 
rules. In this technique, extract words from document collection 
firstly, then construct term clustering according to AMI(Average 
Mutual Information) between terms, document VSM(Vector 
Space Model) is represented by term clustering, and use 
association rules to mine document clustering. Experiment 
results show that performance and clustering quality of this 
technique are improved than those of traditional methods in the 
clustering process. 
Keywords-term clustering; association rules; web mining; 
vector space model; document clustering 
I.  INTRODUCTION 
In general, with difference of contents, web mining has 
three types: web structure mining, web usage mining and web 
content mining [1]. Document clustering belongs to web 
content mining and content mining is to find some models or 
rules implied in document collection, if see the whole process 
of content mining is an I/O system, document collection D is 
the input and implied models or rules MR is the output, so the 
task of content mining is to find a shine upon § from D to 
MR: §D MR??? . 
Document clustering process is different from document 
classification, it is not based on kinds of table or category 
system but documents themselves, namely, it uses documents 
to get the kind of document, the content and extension of the 
kinds and the whole category system are all completely 
determined by document collection which needs to do 
clustering. Now the commonly document clustering methods 
are hierarchical method, partitioning method, etc. 
II. DOCUMENT FORMAL DESCRIPTION 
A. Document Participle Process and Pretreatment 
Firstly do participle process to all documents in D, all terms 
obtained have formed a term collection, then do pretreatment to 
the term collection, the main steps of pretreatment are: 
? According to far-between words table and inactive   
words table, remove all far-between words and inactive words 
from term collection;  
? Remove some words which have high appearance in lots 
of documents, such as some auxiliary verb, preposition, etc.  
? According to thesaurus to deal with some terms whose 
semantic are same or very related. choose formal terms in 
“Chinese Classification Themantic Words Table” to represent 
these terms whose semantic are same or very related,  seeing 
the formal terms are equal to these terms and operation on 
formal terms are equal to on these terms, such as “malingshu”, 
“tudou”, “yutou” are different names of potato, but only use 
“malingshu” to represent potato in this paper. 
B. Term Clustering 
Generally, there are some regulations in match process 
between terms, the representation is that there are some certain 
regular match relationship between terms and others, there are 
two type of similar relationship between two terms: semantic 
similar and semantic relevance, the semantic similar is about 
synonym and parasynonym, this has been processed before, 
and using  AMI (Average Mutual Information) to represent 
relevance degree of two terms. 
AMI? iT , jT ?= i j j2
j i j
n(T ,T ) n(T )
log  
n(T ) n(T ,T )
+ 
i j j
2
j i j
n(T ,T ) n(T )
log
n(T ) n(T ,T )
+ i j j
2
j i j
n(T ,T ) n(T )
log
n(T ) n(T ,T )
 
+ i j j
2
j i j
n(T ,T ) n(T )
log
n(T ) n(T ,T )
, 1?i, j?m. 
AMI? iT , jT ?is Average Mutual Information between 
iT and jT ; jn(T ) is number of documents where jT  appears in 
D; i jn(T ,T ) is number of documents where both iT  and jT  
appear in D; i jn(T ,T ) is number of documents where neither 
iT  nor jT  appears in D; i jn(T ,T ) is number of documents 
978-1-4244-6977-2/10/$26.00 ©2010 IEEE
where iT  appears but jT  not in D; i jn(T ,T ) is number of 
documents where jT  appears but iT  not in D. 
C. Term Weight 
This paper use Shannon’s reverse document frequency and 
Dennis’s information and noise rate but do some modifications 
on these two methods to form term weight, use relative 
frequency k, j
j
tf
f
 to replace absolute frequency k, jtf  in these 
two methods’ weight computing formula, then take the square 
root of the product of two weight value which are separately 
computed by these two modified weight-computing formula. 
k , jW =
idf inr
k j k jw w×, ,  
= k, j
j
tf
f
×
m
k,j j
2 2 j 2
j=1j j k, j
tf fn [(log )+1] [(log f ) - ( log )]
d f tf
× ×?  
idf
k jw , is weight value of jT  in kD which is computed by 
modified inverse document frequency; 
inr
k jw , is weight value 
of  jT  in kD which is computed by modified information and 
noise rate; t k, jf is total frequency of jT  in D; n is total number 
of documents in D; jd is number of documents which contain 
jT  in D. 
D. Document Vector Space Model 
Generally speaking, because terms in every term clustering 
are similar or relative, they are used to describe some certain 
questions or objects in the same or related domain; in the other 
aspect, documents themselves are also the detail description of 
some certain questions or objects in the same or related 
domain, so use term clustering to form the document vector 
space mode is a good method. 
kD =(< 1TC ? k,1r >?< 2TC ? k,2r >?…?< iTC ?
k,ir >?…?< tTC ? k,tr >??1?k?n. 
k,ir is relevance degree between document kD  and term 
clustering iTC . 
III. USE ASSOCIATION RULES TO MIN DOCUMENT 
CLUSTERING 
A. Association Rules 
Let set O is a group of objects, each object is also a subset 
of project set P, Let pO  is a subset of P, So we can define the 
support degree p(O )?  of pO  is number of objects which 
contain pO  in O, let min_sup is minimum support degree, if 
p(O )? ?min_sup, we say pO  is a frequent item-set. An 
association rule is an implication of the form pO ? qO , 
where pO ? P, qO ? P,and pO  qO =? .Support degree of 
association rule pO ? qO  is defined as: sup? pO ? qO ?
= p q
O O )
n
 ?(
(where, n is total number of objects in O); 
confidence degree of this association rule is defined as: conf?
pO ? qO ?= p q
p
O O )
(O )
 ?(
?
, let min_conf is minimum 
confidence degree, if conf? pO ? qO ??min_conf, we say 
association rule pO ? qO is confidently.  
B.  Document Similar Degree 
The square root of product of Jaccard coefficient and 
Cosine coefficient is chosen to be document similar degree. 
p qts(D ,D ) = p q p qjs(D ,D ) cs(D ,D )×  
=
1 1t t t t
2 22 4
p,i q,i p,i q,i p,i q,i p,i q,i
i=1 i 1 i 1 i=1
(r r ) [ (r r r r )] [ (r ) (r ) ]
? ?
= =
× + ? × ×? ? ? ?   
p qjs(D ,D ) and p qcs(D ,D )  separately represents Jaccard 
coefficient and cosine coefficient; t is total number of term 
clustering in T; p,ir  is relevance degree between pD and 
iTC ; q,ir  is relevance degree between qD  and iTC . 
C. DHP Algorithm 
This technique uses DHP [4] algorithm(Direct Hashing and 
Pruning) to min association rules, it is an advanced algorithm 
of Apriori, the using of hash technology of it has effectively 
reduced the number of candidate k-itemsets especially 2-
itemsets, so this algorithm has obviously improved the 
performance of Apriori. 
DHP algorithm: 
Input: r(D TC)× , min_sup, large_num; 
Output: k kL ,H ,k-itemsets; 
After DHP algorithm mined initial document clustering, 
then whole document collection D is formed by different 
document clusterings which are not relevance or have very few 
similar degree with each other, and every document clustering 
is formed by documents which are similar with each other. 
D=? 1DC , 2DC , …, xDC , …, dDC ?, 1?x?d. 
xDC is the 
thx document clustering in D; d is total number 
of document clustering in D. 
IV. EXPERIMENT RESULTS 
The document collection is composed of documents which 
come from CNKI (Chinese National Knowledge 
Infrastructure), there are 562 documents in   document 
collection. 
In experiment, comparing new technique with k-means and 
k-medoids to test its effectiveness, then compute ADD 
(Average Difference Degree) and ASD(Average Similar 
Degree) of document clustering and cost of time of clustering 
process of these three different methods on five different 
document collections, the results are show in Figure 1. 
 The experiment results show that average difference 
degree and average similar degree of document clustering in 
new technique are higher than these of k-means and k-medoids, 
namely, the performance and quality of document clustering 
are better. Cost of time of these three methods are show in 
Figure 2, it show that cost of time of these three methods are all 
rise with increase of number of documents in document 
collection, but cost of time of our new technique is fewer and 
its rise speed is slower than other two methods, there are the 
results of using DHP algorithm to min association rules and 
using term clustering to form document vector space model 
which are all reduce cost of time of clustering process. 
 
ACKNOWLEDGMENT 
This paper is supported by The Research Foundation of 
North China Institute of Aerospace Engineering (ZKY-2009-
06). 
REFERENCES 
[1] Kosala R, lockeel H, “Web mining research: a survey”, ACM SIGKDD 
Explorer Newslett[J], Vol.1,pp.1?15, 2000. 
[2] LIU Li-ping, YI Hua-rong, “A VSM-based Text Clustering Method”, 
Journal of Zhuzhou teachers college[J], in Chinese, Vol.9, pp.23?25, 
October 2004. 
[3] KANG Tie-gang, DAI Ru-wei,“A Novel Approach For Word Clustering 
Based On Large Tagged Corpus”, Journal of system simulation[J], in 
Chinese, Vol.15, pp.1439?1442, October 2003. 
[4] J. S. Park, M. S. Chen, and P. S. Yu, “An effective hash-based algorithm 
for mining association rules”, ACM SIGMOD Conference on 
Management of Data[C], San Jose, pp.175?186,May 1995. 
[5] Jin Yongtao, Cheng Yuepeng, “The Study on the Technology of 
Database Conneciton Pool in the Electronic Commerce”, Computer 
Knowledge and Technology [J], in Chinese, Vol.5:1794-1795, 2009. 
[6] ZHANG Rong, “A Fast and Efficient Web Document Clustering 
Method”, Computer application research[J], in Chinese, Vol.4, pp.174?
176, 2004. 
[7] SONG Qin-bao, SHEN Jun-yi, “A Web Document Clustering Algorithm 
Based on Association Rule”, Journal of Software[J], in Chinese, Vol.13, 
pp.417?423, 2002. 
[8] Cheng Yuepeng, Zhu Song, Jin Yongtao. “New Index Mechanism for 
XML Information Retrieval”, Journal of North China Institute of 
Aerospace Engineering [J], in Chinese, Vol.18: 15-18, 2008. 
[9] ZHAO Yan, WANG Xiao-long, and LIU Bing-quan, etc. “Solution 
Strategies for Word Sense Problems Based on Vector Space Model and 
Maximum Entropy Model”, Chinese High Technology Letters[J], in 
Chinese, Vol.15, pp.1?6, 2005. 
[10] YANG Min, DING Yue-hua, WEN Gui-hua, “Related Research about 
Mining Association Rules”, Computer Era[J], in Chinese, pp.5?7,2005. 
[11] Li Tong, Cheng Yuepeng, Liu Yuli. “Data Mining Technique and 
Application Based on Association Rules”, The 2009 International 
Symposium on Intelligent Information Systems and Applications[C], 
pp.304?306, October 2009. 
[12] LI Wei, SONG Han-tao,“Heuristic Way of Finding Association Rules 
Based on Similarity”, Journal of Beijing Institute of Technology[J], in 
Chinese, Vol.22, pp.98?100,February 2002. 
 
0
20
40
60P
E
R
C
E
N
T
A
G
E
?
%?
ADD ASD
new t echni que k-means k-medi ods
0
5
10
15
20
25
30
35
200 300 400 500 562
Document  Number
T
I
M
E
?
S
?
new t echni que k-means k-medoi ds
Fig. 2  Cost of time of new technique, k-means and k-medoids
Fig. 1  ADD and ASD of new technique, k-means and k-medoids
