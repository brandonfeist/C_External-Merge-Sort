         Classification Rules Mining based on Intent Reduction 
 
Ge Bin   
College of Computer Science and engineering 
 Anhui University of Science & Technology 
 Huainan , Anhui province ,China 
Email:bge@aust.edu.cn 
Meng Xiang-rui 
department of scientific research  
Anhui University of Science & Technology 
 Huainan , Anhui province ,China 
Email: Meng Xiang-rui@aust.edu.cn 
 
 
Abstract—Concept lattice is accurate and complete in 
knowledge representation and is an effective tool for data 
analysis and knowledge discovery. This paper focuses on 
classifcation rules mining based on concept lattices. By 
modifying incremental computation of intent reduction of 
concepts, it develops a method to mine affirmative 
classification rules and approximate classification rules 
using intent reduction. 
Key words—- concept lattice; classification rule;intent 
reduction 
?.  INTRODUCTION 
There are many extraction algorithms for classification 
rule based on concept lattice, but the number and form of 
extracted rules can not achieve satisfactory results. For 
example, Godin [1], Wang Zhi-Hai [2] and other scholars 
respectively proposed the algorithm to extract implication 
rules by the concept lattice, which are mostly deterministic 
rules. However, the actual database often contains a lot of 
errors or uncertain data, thus only a few or even no 
deterministic rules. And other algorithms may produce a 
large number of redundant rules of high confidence. This 
study, borrowing the idea from the references [3], carries 
out classification through determined and approximate 
classification rule bases, and proposes content-based 
synchronization and synchronization of pruning to reduce 
computing classification rule to extract these two types of 
classification rules, thus effectively reducing the number 
of classification rules without loss of useful information. 
   ?.  RELATED DEFINITIONS 
 First give the related definitions of classification 
rules for intent reduction extraction [4]  
 Definition 1  The determined classification rule 
base is: 
 ER B={r:red? c|c?B?red?IRed(B)?c?red} 
Where, B is the frequent closed itemset containing the 
classification property c (corresponding to the 
connotation of frequent concept), IRed(B) reduction for 
the B set. 
   The confidence of the determined classification rule 
r:red ? c is Conf(r)=1, namely, the Sup(red ?
{c})=Sup(red), which means that the red and c in the 
antecedent appear in connotation of the same concept. 
Therefore, only lattice node containing current 
classification c needs to be examined, to extract 
determined classification rules. 
Definition 2  The approximate classification rule 
base is   
 
 
ARB={r:red ? c | c?B?red?RED?f(g(red)) ? B} 
Where, B is the frequent closed itemset which 
contains the current class c (that is, the connotation of 
frequent concept), RED is reduction set for all the 
frequent closed itemsets.  
   The confidence of the approximate classification rule 
r:red ? c is that Conf(r)<1, namely, Sup(red ?
{c})<Sup(red), which means that the node containing the 
antecedent red should be the node that contains the 
precursor of type c node, which has been discussed in 
definition 1.Therefore, as for approximate classification 
rules, we only need to consider the precursor node 
without containing class c.  
   As reference [3] indicates, all valid determined 
classification rules, including its support and credibility, 
can be deduced from the class association and its support 
base. Therefore, we should find all determined and 
approximate classification rule bases which meet the 
user-specified minimum support and confidence, and then 
construct a classifier.  
To extract the determined classification rules, only the 
lattice nodes containing the current class need to be 
examined; to extract approximate classification rules, just 
examine the relationship between nodes containing the 
current class and its precursor nodes. This study suggests 
generating a section node which includes nodes of the 
current class and its precursor nodes. On this basis, 
classification rules are extracted. 
?.  IMPROVED SYNCHRONIZED ALGORITHM OF 
INTENT REDUCTION 
In reference [5], the solution to intent reduction based 
on incremental construction method is put forward. This 
paper considers extracting determined and approximate 
classification rule bases. Only intent reduction of two 
types of nodes are needed. The first class is the node 
which satisfies the threshold value and contains class c. 
The second class is the node which meets the threshold 
value and holds the first class node as super-concept node, 
without containing class c. In the process of solution to 
incremental intent reduction, redundant calculation may 
exist. In order to complete the extraction of classification 
rules, algorithm and its efficiency need to be further 
improved. It is necessary to study the change of these two 
classes of nodes in the incremental process.  
   Set the form background as?G?M?I?, record  
MinSup as minimum support threshold value, L as the  
corresponding concept lattice, and L~  as the 
978-1-4244-5143-2/10/$26.00 ©2010 IEEE
corresponding pruning section lattice, which is the part 
after deleting the nodes whose extension base is less than 
the minimum support threshold value MinSup in L. It is 
the the filter of L, and the supremum of sub-lattice; 
assume we add attribute m to the background, 
Gm=g(m) ? G  is a object set with attribute m, that is, the 
new background is (G?M ? {m}?I ? Gm×{m}). Set the 
concept lattices corresponding to the new background as 
*L , *~L  as section lattice after inserting property m  in 
L~  and *L  as the pruning section lattice corresponding 
to *
~L . 
Theorem 1, LBA ~),( ? ?and 21
~~),( LLBA ?? , 
Here, 1
~L  is the first class node set in L~ ?and 2
~L  is 
the second node set in L~ . Then, the update of ),( BA  
does not affect the reduction of node in 1
~L ? 2
~L .  
Prove that the descendants of the nodes do not change 
after update of ),( BA . Discuss two circumstances: if 
),( BA  is the updated node, the conclusion is trivial; If it 
is the generator, mark nCon as the corresponding new 
node, and it satisfies the threshold value, obviously, 
*~),( LBA ? . At this time, if pCon is the parent node in 
*~L . There are two cases: pCon is the new node, and then 
pCon can not belong to the set 1
~L . Since the extension 
of pCon is more than A only by x, it can not belong to set 
2
~L . Otherwise, the super-concept of pCon is also the one 
of ),( BA , thus ),( BA  being the second class node, 
which is contradictory to 21
~~),( LLBA ?? , so the 
conclusion holds up; pCon is a updated node, and through 
similar deduction, we know that pCon does not belong to 
21
~~ LL ? , so the conclusion is established.  
     According to theorem 1, the node not belonging to 
21
~~ LL ?  in L~  does not affect calculation of node 
reduction. If the node in 1
~L  is the generator which 
meets the threshold value or updated node, it’s easy to 
infer that corresponding new node and updated node still 
belong to the first class node; If the node in 2
~L  is the 
generator which meets the threshold value?the father 
node will be considered.  
 If the corresponding new node is not the second class 
node, then by the Theorem 1, it is inferred that if the new 
node needs not be involved in next incremental process, it 
does not belong to the updated pruning lattice. Then, the 
nodes not belonging to 21
~~ LL ?  in L~  can be directly 
deleted, and are not involved in the incremental process. 
Then, mark 21
~~:~ LLL ?= , *2
*
1
* ~~:~ LLL ?= , that is, the 
first and second class nodes are obtained after 
synchronized pruning, during which the dual case should 
considered. The algorithm is as follows: an improved 
synchronized algorithm MIIR for intent reduction, based 
on incremental construction of grid. Figure 1 is a flow 
chart of algorithm MIIR, and figure 2 is a flow chart of 
the intent reduction algorithm COMPUTE. 
 
    Fig. 1   Flow chart of algorithm MIIR 
 
    Fig. 2   Flow chart of algorithm COMPUTE 
 Function JUDGE (Con) in the quotient semi-lattice 
determines whether Con is the first class node with 
super-node, if yes, then return true. The return of JUDGE 
decides whether a certain generator needs producing a 
new node, thus reducing the computation of the 
next-round incremental process.  
   Through the above-mentioned synchronized pruning 
algorithm, the section lattice containing only the first and 
second class nodes can be obtained. It is still an ideal 
concept lattice *L , so the calculation of node reduction 
does not change. The above algorithm MIIR realizes the 
synchronized calculation of intent reduction for the 
required nodes. 
?. THE CLASSIFICATION RULE EXTRACTION BASED 
ON INTENT REDUCTION 
 
With the pruning lattice obtained from the above 
format (the reduction of each node storing in its ERed 
field), the two classification rules are extracted according 
to definition 1 and 2. Give a classification rule base on 
pruning lattice to extract CRPL. First, through the above 
MIIR, we can obtain the section lattice containing only 
the first and second nodes and the related reduction set. 
Next, save all the first class nodes with T1?and save the 
boundary nodes which have father nodes and belong to 
the second class nodes in T1 by use of T2? Call ERB for 
T1 (flow chart shown in Figure 4) to extract all the 
established rules-based classification and then on the T1, 
T2 call ARB (flow chart shown in Figure 5) to extract all 
of the approximate rule-based classification, thus 
obtaining the required set of classification rules.  
The flow chart of the extraction of algorithm CRPL 
based on the rule-based classification of pruning lattice is  
shown in Figure 3. CRPL input / output: Input: 
Lattice[],G, categorie c, support threshold value MinSup, 
confidence threshold value MinConf; Output: 
classification rule set Rset.  
 
 
     Fig. 3   Flow chart of algorithm CRPL 
 
     Fig. 4   Flow chart of algorithm ERB 
 
?.  EXPERIMENTAL ANALYSIS 
To verify the method of the classified association rule 
this study proposed, C + + is used to carry out the above 
algorithm. Classifier CSCR is constructed for the 
above-mentioned classified association rule set by use of 
method in reference [6].  And, six base set in machine 
learning base UCI is tested, the information of which is 
shown in Table 1. 
 
     Fig. 5   Flow chart of algorithm ARB 
Four cross-validation technique is adopted in this 
experiment. Minimum support value greatly affects 
accuracy of the classifier. In experiment, we find that 
when minimum support value is near 1%, we can obtain a 
moderate number of rules and the best classifier. 
Therefore, with the minimum support of 1%, minimum 
confidence of 60%, contrast experiment between the 
above six data sets and the standard classification method 
C4.5 is carried out, the result of which is shown in Table 
2. The first column is data set, the second, classification 
accuracy of C4.5, the third, classification accuracy of 
algorithm Classifier, the fourth, the time for extracting 
algorithm of association rules, and the fifth, the number 
of extracted classified association rules. 
    Table 1   Data sets used in experiments 
Date ets Number of attributes 
number 
of Category 
Number 
of objects 
breast-w 10 2 699
diabetes 8 2 768
glass 9 7 214
heart 13 2 270
iris 4 3 150
pima 8 2 768
 
As can be seen from Table 2, classification accuracy 
in algorithm CSCR is higher than that of C4.5 in four of 
the six data sets under experiment, especially in data set 
glass, and it’s lower than that of C4.5 in only two data 
sets. In the six data sets the mean classification accuracy 
is: CSCR 83.8%, and C4.5 81.58%, that is, the 
classification method put forward in this study is better 
than the standard C4.5 method. In addition, with regard to 
the time needed, four data sets are carried out fast, with 
the average time of 1.4 s. The needed time is a little 
longer, in the two data sets breast-w and heart, for the 
number of extracted rules is larger. However, the average 
time is still less than 8s, which is improved compared 
with the speed in reference [7]. 
Table 2   Results of algorithm CSCR and C4.5 
Data sets C4.5 CSCR Time Number of rules 
breast-w 95.0 96.3 28.7 307 
diabetes 74.2 73.1 2.6 35 
glass 68.7 81.1 1.3 49 
heart 80.8 83.2 10.9 528 
iris 95.3 96.1 0.4 19 
pima 75.5 73.0 1.2 25 
Average 
value 81.58 83.8 7.52 160.5 
 
?.  CONCLUSION 
To sum up, it is effective to propose a classified 
association rule method based on incremental lattice 
construction and sync calculations of intent reduction. 
Compared with that in references [3] and [5], this algorithm 
is improved in the following two aspects. Firstly, the 
range of nodes for computing intent reduction is restricted, 
namely, only nodes related to computation and classified 
rule extraction are needed. Secondly, since incremental 
computation was carried out ahead for the needed intent 
reduction, the extraction of these two classification rules 
can be given according to the foregoing definitions, thus 
improving the efficiency of rule extraction. In future, 
more data sets are to be tested to further improve the 
proposed method in this study.. 
 
REFERENCES 
 
[1] Godin R, Missaoui R. An incremental concept formation approach 
for learning from databases[C].Theoretical Computer Science, Special 
Issue on Formal Methods in Databases and Software Engineering, 
October 1994, 133: 387-419. 
[2] Wang Zhihai, Hu Keyun, Hu Xuegang, Etc. general rule extraction 
algorithms and incremental algorithms based on Concept lattices [J] 
Chinese Journal of Computers, 1999, 22(1): 66-70. 
[3] Qi Hong, Liu Dayou, Liu Yabo. User association mining based on 
Concept lattices [J].Computer Science, 2004, 31(Z2): 158-161. 
[4] Xie Zhipeng, Liu Zongtian. Concept Lattice and Association Rules 
[J]. Journal of Computer Research and Development, 2000, 37(12): 
1415-1421. 
[5] Liu B, Hsu W, Ma Y. Integrating classification and association rule 
mining[C].Proceedings of the Second International Conference on 
Knowledge Discovery and Data Mining, New York, 1998: 80-86. 
[6]Hu Keyun, Lu Yuchang, Shi Chunyi. classification and the 
integration of mining association rules based on Concept lattices 
[J]Journal of Software, 2000, 11(11): 1478-1484. 
 
 
 
