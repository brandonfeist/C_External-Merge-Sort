978-1-4244-5023-7/09/$25.00 ©2009 IEEE
September 14-16, 2009
METU
Northern Cyprus Campus199
Mining Quantitative Class-Association Rules for 
Software Size Estimation 
 
María N. Moreno, Joel P. Lucas, Saddys Segrera and Vivian F. López 
Department of Computing and Automatic 
University of Salamanca 
Salamanca, Spain 
 
 
Abstract—Associative models are usually applied in knowledge 
discovery problems in order to find patterns in large databases 
containing mainly nominal data. This work is focused on two 
different aspects, the predictive use of association rules and the 
management of quantitative attributes. The aim is to induce class 
association rules that allow predicting software size from 
attributes obtained in early stages of the project. In this 
application area, most of the attributes are continuous; therefore, 
they should be discretized before generating the rules. 
Discretization is a data mining preprocessing task having a 
special importance in association rule mining since it has a 
significant influence on the quality and the predictive precision of 
the induced rules. In this paper, a multivariate supervised 
discretization method is proposed, which takes into account the 
predictive purpose of the association rules. 
Keywords-Associative classification, class association rules, 
discretization, software size estimation 
I.  INTRODUCTION 
Estimation is one of the earliest and most important 
activities in software projects. It determines the project 
schedule and drives the decision making along the whole 
project. So, the reliability of estimations has a great 
repercussion in the duration and cost of the project as well as in 
the quality of the produced software. The most usual software 
attributes to be estimated are software size and project effort or 
cost. Traditional estimation methods, such as COCOMO or 
other methods based on software metrics (function points, use 
case points, design metrics, etc.), are widely known and used 
for many years, nevertheless, nowadays they are being replaced 
by prediction models based on data mining techniques whose 
superiority with regard to the first ones has been  demonstrated 
in numerous studies. 
Application of data mining techniques in the project 
management field has two main drawbacks: On the one hand, 
the number of available examples is scarce and, on the other 
hand, most of the involved attributes are continuous. First 
problem leads to a lesser precision of classification models, and 
second problem forces to carry out a discretization process 
previous to the application of many data mining techniques, 
which work with categorical and discrete attributes. In 
addition, the discretization of continuous attributes can provide 
many benefits: the predictive models induced by the algorithms 
are more accurate and more understandable; the induction 
process is faster and the analysis of the results can be more 
comprehensive and helpful [13]. 
Association analysis is one of the data mining areas where 
discretization has more relevance. Association rules can only 
be generated from discrete attributes, thus, an adequate 
discretization has a significant impact of on the rules’ quality. 
Association and classification techniques have been recognized 
as useful tools in making decisions in many application 
domains. Association is considered an unsupervised data-
mining task, thus it is mainly used in the area of knowledge 
discovery in databases, while classification problems are 
treated by means of supervised learning algorithms.  
Traditionally, both kinds of techniques have been used to 
solve different classes of problems. However, recent studies 
show that knowledge discovery algorithms, such as association 
rule mining techniques can be successfully used for building 
accurate and efficient classifiers [9] [12] [22] [15] [16]. 
Associative classification methods obtain a model of 
association rules that is used for classification. These rules are 
restricted to those containing only the class attribute in the 
consequent part. This subset of rules is named class association 
rules (CARs) [12].  
Association rule mining presents several problems that 
numerous proposed methods try to solve. The main are the 
high cost of generating the rules, the large number of rules 
generated, rules representing weak associations and 
uninteresting patterns, use of inadequate metrics for evaluate 
the validity of the rules, etc. The way of dealing with these 
problems depends on the application, because a rule may be 
useful for one application but not for another. In this paper we 
consider the predictive use of the association rules in the field 
of project management. In this area, as we commented before, 
the attributes involved are numeric, contrary to the traditional 
application domain of association rules where most of them are 
nominal. When attributes used for inducing the rules take 
continuous values an efficient data discretization contributes 
significantly to simplify the rule set, to generate strong and 
interesting patterns and to improve the algorithm performance. 
We propose a multivariate supervised discretization method 
suitable for the induction of CARs. The procedure selects the 
best attributes for classification in order to simplify the rules 
and the induction process as well as to obtain a more accurate 
classifier. All selected attributes are discretized simultaneously, 
200
including the class attribute, taking as reference clusters 
generated by a supervised clustering algorithm. 
The proposed approach is applied in order to estimate 
software size by using software attributes obtained in early 
stages of software life cycle. The method has been validated 
with data from real projects. 
The rest of the paper is organized as follows. Section II 
contains a review of relevant works in the literature concerning 
associative classification and discretization. In Section III the 
discretization algorithm is presented. The experimental study is 
showed in section IV and conclusions are given in section V. 
II. RELATED WORK 
A. Classification based on association 
Since Agrawal and col. introduced the concept of 
association between items [1][2] and proposed the Apriori 
algorithm [3], association rules have been the focus of 
intensive research. Most of the efforts have been oriented to 
simplify the rule set and improve the algorithm performance. 
The first pass of the Apriori algorithm consists of counting 
the number of occurrences of each item to determine the large 
itemsets, whose supports are equal or greater than the 
minimum support specified by the user. There are algorithms 
that generate association rules without generating frequent 
itemsets [9]. Some of them simplifying the rule set by mining a 
constraint rule set, that is a rule set containing rules with fixed 
items as consequences [5] [6]. The procedure causes loss of 
information, but it does not represent a drawback for predictive 
purposes when a single item, the class, constitutes the 
consequence part of the rules. Many other algorithms for 
obtaining a reduced number of interesting rules have been 
proposed, however this work is focused in the use of the rules 
in classification problems. The number of papers in the 
literature that consider this aspect is lesser. A proposal of this 
category is the CBA (Classification Based on Association) 
algorithm [12] that consists of two parts, a rule generator based 
on Apriori for finding association rules and a classifier builder 
based on the discovered rules. CMAR (Classification Based on 
Multiple Class-Association Rules) [10] is another two-step 
method, however CMAR uses a variant of FP-growth instead 
Apriori. Another group of methods, named integrated methods, 
build the classifier in a single step. CPAR (Classification Based 
on Predictive Association Rules) [23] is the most representative 
algorithm in this category. In [22], the weight of the evidence 
and association detection are combined for flexible prediction 
with partial information. The main contribution of this method 
is the possibility of making prediction on any attribute in the 
database. Moreover, new incomplete observations can be 
classified. The algorithm uses the weight of the evidence of the 
attribute association in the new observation in favor of a 
particular value of the attribute to be predicted. This approach 
uses all attributes in the observation, however in many domains 
some attributes have a minimal influence in the classification, 
so the process can be unnecessary complicated if they are taken 
in consideration. Our proposal evaluates the importance of the 
attributes on the classification. 
Our aim is to propose an effective procedure for 
classification problems that is very suitable for application 
domains, such as software project management, where many 
quantitative attributes are involved. 
B. Discretization 
Diverse types of attributes can be used in the data mining 
area. There are different ways for classifying them. A first 
division in categorical and quantitative (numerical) data is the 
most common. Categorical data can be subdivided in nominal 
and ordinal ones. Nominal data are named values without an 
order between them. Ordinal data are nominal data types with 
ordered values. Quantitative attributes can be continuous or 
discrete. While continuous data can take an infinite number of 
values, discrete attributes represent intervals in a continuous 
spectrum and therefore, they take a finite number of values 
[13].  
Data mining process involves a preprocessing step in order 
to assure the data have the quality and the format required by 
the algorithms. Data transformation is a preprocessing task 
including any procedure that modifies the original form of the 
data. A common transformation consists on splitting a 
continuous numerical domain into intervals, obtaining discrete 
attributes required by the algorithms. This procedure, 
denominated binning or discretization, is critical in 
classification and knowledge discovery. Discretization 
algorithms should be adapted to the data mining methods in 
order to improve the induced models.  
Several discretization approaches are available [13]. They 
can be classified in supervised and unsupervised techniques. 
The first ones are usually applied with knowledge discovery 
algorithms while the last ones are used by machine learning 
algorithms. The performance of such algorithms is significantly 
affected by the discretization process.  
Two simple unsupervised techniques commonly used are 
equal-width and equal-frequency, which consist on creating a 
specified number of intervals with the same size or with the 
same number of records respectively. The purpose of the 
discretized data and the statistical characteristics of the sample 
to be treated should be kept in mind when one of these 
algorithms is selected. For example, equal-with discretization 
give poor results when the sample distribution is not uniform, 
however it is more adequate for obtaining coherent and 
comprehensible intervals of values for some attributes like age, 
lines of code, etc., in order to achieve a better interpretation of 
the results. 
Supervised methods, used mainly in classification 
problems, consider entropy measures. These are often 
embedded within machine learning algorithms such as those of 
decision trees induction. In these cases, the supervised 
discretization carried out considers class information for 
generating the intervals while unsupervised discretization does 
not. The intervals obtained are those that better discriminate the 
classes. One known supervised method is the proposed by 
Fayyad and Irani [8], which is an entropy-based multi-interval 
discretization procedure. 
This work has been financed by the Spanish “Junta de Castilla y León”
201
On the other hand, discretization can be univariate or 
multivariate. Univariate binning quantifies one continuous 
attribute at a time while multivariate discretization considers 
concurrently multiple attributes, that is, the threshold values of 
the intervals are searched simultaneously for all continuous 
attributes. Though the univariate discretization is more used 
and simple than the multivariable one, the latter presents more 
advantages, particularly in discovery of association rules since 
all available attributes are concurrently involved in the mining 
process. Multivariate discretization allows to obtain 
interdependences between attributes that can improve the 
quality of the association rules, specially when they are used 
for classification. This one is the scenery of our work, in which 
we use a model of association rules for building a classifier. 
Extracting all association rules from a database requires 
counting all possible combination of attributes. Support and 
confidence factors can be used for obtaining interesting rules 
which have values for these factors greater than a threshold 
value. In most of the methods the confidence is determined 
once the relevant support for the rules is computed. 
Nevertheless, when the number of attributes is large, 
computational time increases exponentially. For a database of 
m records of n attributes, assuming binary encoding of 
attributes in a record, the enumeration of subset of attributes 
requires m x 2n computational steps. For small values of n, 
traditional algorithms are simple and efficient, but for large 
values of n the computational analysis is unfeasible. When 
continuous attributes are involved in the rules, the 
discretization process is critical in order to reduce the value of 
n and to obtain high confident rules at the same time.  
Attribute discretization methods for mining association 
rules have been treated in the literature. Nearly everyone take 
the support factor of the rules as the main feature for splitting 
the attribute values into intervals, that is, they consider the 
weight of the records in the interval in relation to the total 
number of records. In [18] the measure of “partial 
completeness”, based on confidence and support factors, is 
defined for quantifying the information lost by partitioning 
quantitative attributes. Values are fine-partitioned and then, 
adjacent intervals are combined if it is necessary. 
Lian et al. [11] introduce the concept of “density” to 
capture the characteristics of quantitative attributes and propose 
an efficient procedure to locate “dense regions” for mining 
association rules. The density of the regions representing rules 
is a measure used to evaluate the interestingness of the rules 
instead of using support and confidence. 
An alternative to the conversion of continuous attributes 
into discrete data is to consider the distribution of the 
continuous data, via standard statistical measures such as mean 
and variance [4]. In [4] a rule is considered a subset of a 
population. A validity testing is proposed taking account if the 
mean of the subset is significantly different to the mean of its 
complement in the database. The rules are also evaluated 
depending on other measure of probability distributions such as 
variance. 
Recently, several partition methods based on the fuzzy set 
theory have been proposed [20]. The mined rules are expressed 
in linguistic terms, which are more natural and understandable. 
In these works either both the antecedent and consequent parts 
of the rules are formed by a single item or the consequent part 
is not fixed. In our case the consequent part must be fixed 
because it is the class attribute and the antecedent parts is an 
itemset. So, it is more suitable a multivariate discretization that 
consider all the attributes. 
III. MINING CARS FROM QUANTITATIVE ATTRIBUTES 
In order to manage quantitative attributes we propose a 
multivariate discretization algorithm based on clustering (CBD, 
Clustering Based Discretization). When the number of 
available attributes is large a previous selection process is 
carried out, which take into account de classification purpose 
of the associative model. 
A. Finding the best attributes for classification 
The class label attribute is the target of prediction in 
classification problems. Class association rules have the 
consequent part formed only by this attribute. For the 
antecedent part we select, among all available attributes, the 
most influential in the prediction of the classes. Importance of 
columns is a technique that determines the degree of utility of 
the descriptive attributes (columns) in discriminating the 
different values of the label attribute. The purity measure (a 
number from 0 to 100) informs about how well the columns 
discriminate the classes. It is based on the amount of 
information (entropy) that the column provides [14]. The 
expression for that information (I) is: 
 
                                                                                  n 
                  I (P(c1), ..., P(cn)) = ?  - P(ci) logn P(ci)  (1) 
                                                                                  
i=1 
 
where P(ci) is the probability of the class i and n is the 
number of classes. 
The information is 1 when the classes have the same 
probabilities. 
The purity is defined as 1 – I. Every set in the partition has 
its own purity measure, and the purity of the partition is a 
combination of these individual measures. For a given set in 
the partition, the purity is 0 if each class has equal weight and 1 
(100%) if all records belong to the same class. The proposed 
selection procedure takes the attributes with higher values of 
purity for mining the rules. Although before generating the 
rules the continuous values of these attributes must be split into 
intervals according to the following procedure 
B. Discretization procedure 
A clustering technique was applied for discretizing the 
selected attributes simultaneously, including the class label. 
Clusters of similar records were built by using the k-means 
algorithm with an Euclidean distance metric [13]. This distance 
D(p,q) between two points p and q in a space of n dimensions 
is: 
                  [D(p,q)]2 = || p – q ||2 =   ?i  (pi - qi) 2  (2) 
202
Where pi and qi are the coordinates of the points p and q 
respectively. In our case, the points are the records to be 
compared, and the coordinates are the n attributes of each 
record. 
In our proposal the clusters are built giving more weight to 
the class attribute. This is a supervised way of obtaining the 
best intervals for classification. The distribution of attribute 
values in the clusters was used for making the discretization 
according to the following procedure, based on the algorithm 
that we proposed in a previous work for mining conventional 
quantitative association rules without classification purpose 
[17]: 
1. The number of intervals for each attribute is the same of the 
number of clusters. If m is the mean value of the attribute in 
the cluster and ? is the standard deviation, the initial 
interval boundaries are (m - ?) and (m + ?). 
2. For adjacent intervals i and i+1: 
If (mi > mi+1– ?i+1) or (mi+1 < mi+ ?i)  
the two intervals are merged into one:  
 (mi – ?i) , (mi+1 + ?i+1) 
else 
the cut point between intervals i and i+1:  
 (mi+1 – ?i+1+ mi+ ?i)/2 
This procedure was applied for creating intervals of values 
simultaneously for all attributes in order to generate the class 
association rules. 
IV. EXPERIMENTAL STUDY 
The proposed method was applied in the software project 
management area. This is a field where most of the attributes 
come from the application of software metrics and therefore, 
these attributes are numeric. The target of the CARs model 
built from project data is the estimation of software size. That 
is a critical issue in the project management area. Good early 
estimates are essential for a reliable prediction of project effort 
and cost as well as for an efficient planning and scheduling. 
It is convenient to specify that we propose a method for 
building estimation models, thus, the model obtained in this 
study is applicable to projects with similar characteristics, 
nevertheless, for other projects, it would be necessary to 
develop another model from suitable historical data. In 
addition, models must be regularly updated due to the dynamic 
nature of the software development field. 
A. Data Description 
The data set used in the empirical study was obtained from 
experiments carried out by Dolado [7]. The data comes from 
academic projects in which students developed accounting 
information systems that have the characteristics of 
commercial products.  
The information available was divided in two groups. One 
group containing global data from each project (42 records) 
and other group containing attributes from each component of 
the projects (1537 records). The code of the applications was 
classified into three types of components according to Verner 
and Tate [21]. 
The component attributes are: TYPECOMP (type of 
component, 1: menu, 2: input, 3: report/query), OPTMENU 
(number of choices, for menus), DATAELEMENT (number of 
data elements, only for inputs and reports/queries), 
RELATION (number of relations, only for inputs and 
reports/queries) and LOC (lines of code). 
The project attributes described below are those used in Mark 
II method (MKII) [19]: LOC (lines of code), NOC (number of 
components), NTRNSMKII (number of transactions MKII), 
INPTMKII (number of total inputs), ENTMKII (number of 
referenced entities), OUTMKII (number of total ouputs), 
UFPMKII (number of unadjusted functions points MKII). 
The target of the work was to estimate de software size of 
entire projects, hence new project attributes were calculated 
from the module attributes of each project: NOC-MENU (total 
number of menu components), NOC-INPUT (total number of 
input components), NOC-RQ (total number of report/query 
components), OPT-MENU (total number of menu choices), 
DATAELEMENT (total number of data elements), RELATION 
(total number of relations). These computed attributes and the 
global ones were used in the mining process whereas 
individual module attributes were discarded. Therefore, we 
used for mining the rules 42 records corresponding to the 42 
projects. Due to the sample had only three projects with LOC 
greater than 5,000 LOC, we removed the three corresponding 
records. 
B. Results 
We aim to build a model that relates some project attributes 
that can be obtained early in the life cycle with the final 
software size. Thus, LOC can be considered as the class 
attribute, but it must also be discretized. The first step is to 
search the best attributes for discriminating LOC by calculating 
their cumulative purity. The attributes found by means of 
Mineset, a Silicon Graphics tool [14], were RELATION, 
NTRNSMKII, OPT-MENU, DATAELEMENT and OUTMKII 
with a cumulative purity of 97.95%. These attributes were used 
in the discretization process in order to generate the class 
association rules later. 
The clusters were created with a weight for the class 
attribute LOC three times greater than for the other attributes, 
in order to obtain the most suitable clusters for predicting the 
class, which appear in the consequent part of the rules. In the 
study, the number of clusters used as input to the clustering 
algorithm was established considering the necessities of the 
problem domain. Finally five clusters were generated, which 
were used for splitting the values of the selected attributes 
according to the procedure described above. Figure 1 shows the 
intervals of values obtained for the class attribute (LOC). The 
distribution of records for each interval of values of the 
attributes is presented in figure 2. We can see in it that the 
algorithm CBD finds different number of intervals for the 
different attributes. We can also observe in this figure, by 
means of colors, the proportion of records of each class.  
203
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1.  Intervals of values for the class label (LOC) 
 
 
 
 
 
 
 
 
 
 
 
Figure 2.  Distribution of records for intervals of values of the attributes 
In order to validate the proposed Clustering Based 
Discretization (CBD) procedure we examined the precision 
obtained by applying several classifications methods with 
continuous data (except the class attribute) and several 
associative classification methods with discretized data by 
different discretization algorithms. 
Results from classification algorithm are showed in Table 
1. We applied Bayes Net, the decision tree J4.8 and two 
multiclassifiers, Bagging with RepTree and Staking with 
CeroR. For multiclassifiers we proved several base classifiers 
but we only report the most precise. These methods do not 
work with quantitative class attributes, therefore we split the 
LOC values into five equal width intervals since five was the 
number of obtained clusters. 
For associative classification we selected CBA and CMAR 
because our experience and the literature have demonstrated 
their superiority against others methods. Usually CMAR has 
more predictive precision than CBA, however it has higher 
computational cost. We also obtained better results with 
CMAR and the computational cost was similar, given that the 
data set is very small.  Obtained results are presented in Table 
2. We have applied CMAR with data discretized by means of 
four different algorithms: equal width, equal frequency, Fayyad 
and Irani method, and the proposed CBD procedure. For 
applying Fayyad and Irani method, five equal width intervals 
of LOC were used since the algorithm works with nominal 
class attributes. CMAR was executed with a minimum support 
of 10% and a minimum confidence threshold of  75%.  
TABLE I.  RESULTS FOR CLASSIFICATION METHODS 
CLASSIFICATION METHOD PRECISION 
Bayes Net 38.46% 
Decision Tree J4.8 58.97% 
Bagging (RepTree) 56.41% 
Stacking (CeroR) 33.33% 
TABLE II.  RESULTS FOR CMAR 
DISCRETIZATION METHOD PRECISION 
Equal width 27.50% 
Equal frequency 1.67% 
Fayyad and Irani 80.83% 
CBD 85.83%
 
As we can see in table I, traditional classifiers gave very poor 
results. It can be a consequence of the reduced number of 
records available in the data set, which is insufficient for the 
learning process. 
CMAR gave better results only with two discretized data 
sets, which were obtained by applying Fayyad an Irani method 
and the procedure proposed in this word, the CBD algorithm. 
The last one provided the best results, a precision of 85,83%, 
which can be considered a high precision, keeping in mind the 
small available data set. Precision differences between the two 
methods are not very significant, however, Fayyad an Irani 
method has the inconvenience that it works with nominal class 
attributes while CBD does not, but rather it discretizes 
simultaneously all attributes, including the class. Besides, CBD 
achieve a better precision with a lesser number of rules. 
V. CONCLUSIONS 
In the project management field it is very difficult to obtain 
a great quantity of historical data for making predictions. That 
fact leads to a low precision of classification models built from 
the available data sets. On the other hand, most of the involved 
attributes, obtained by applying software metrics, are 
continuous. Therefore, they cannot be directly used as input to 
the data mining algorithms. In this work we have used project 
management data in order to estimate the software size by 
means of an associative classification method. 
One of the main problems of quantitative association rule 
mining is the discretization of continuous attributes. In this 
204
work a supervised multivariate discretization procedure is 
presented, which is specially indicated for class association 
rules (CARs). Our proposal evaluates the importance of the 
attributes on the classification and split simultaneously their 
values into intervals by means of a procedure that takes as 
reference the clusters generated by a supervised clustering 
algorithm. The algorithm produces the most suitable rules for 
prediction, due to the supervised discretization and the use of 
the most important attributes in discriminating the different 
values of the class attribute, which constitutes the consequent 
part of the rules. Our experiments demonstrated that, in the 
studied application field, associative classification methods 
combined with a good discretization procedure overcome 
widely to the classic supervised learning methods. 
REFERENCES 
[1] R. Agrawal, T. Imielinski and A. Swami, “Database mining. A 
performance perspective”, IEEE Trans. Knowledge and Data 
Engineering, vol. 5, (6), pp. 914-925,1993. 
[2] R. Agrawal, T. Imielinski and A. Swami, “Mining associations between 
sets of items in large databases”, Proc. of ACM SIGMOD Int. 
Conference on Management of Data, Washinton, D.C., pp. 207-216, 
1993. 
[3] R. Agrawal and R. Srikant, “Fast algorithms for mining association rules 
in large databases”, Proc. of 20th  Int. Conference on Very Large 
Databases, Santiago de Chile, pp. 487-489, 1994. 
[4] Y. Aumann and Y. Lindell, “A statistical theory for quantitative 
association rules”, Journal of Intelligent Information Systems, 20 (3), pp. 
255-283, 2003.  
[5] R. Bayardo, R. Agrawal and D. Gunopulos, “Constraint-based rule 
mining in large, dense database”, Proc. of. 15th Int. Conference on Data 
Engineering, pp. 188-197, 1999. 
[6] R. Bayardo and R. Agrawal, “Constraint-based rule mining in large, 
dense database”. Proc. Mining the most interesting rules, Proc. of ACM 
SIGKDD Int. Conf. Knowledge Discovery in Databases, ACM Press, 
NY, pp. 145-154, 1999. 
[7] J.J. Dolado, “A validation of the component-based method for software 
size estimation”, IEEE Transactions on Software Engineering 26 (10), 
pp. 1006-1021, 2000.  
[8] U.M. Fayyad and K.B. Irani, “Multi-interval discretization of continuous 
valued attributes for classification learning”, Proc. of the Thirteenth 
International Joint Conference on Articial Intelligence, IJCAI93, 
Chambery, France, pp. 1022-1027, 1993.  
[9] J. Li and H. Shen and R. Topor, “Mining the smallest association rule set 
for predictions”, Proc. of IEEE International Conference on Data Mining 
(ICDM’01), 2001. 
[10] W. Li, J. Han and J. Pei, “CMAR. Accurate and efficient classification 
based on multiple class-association rules”, Proc. of the IEEE 
International Conference on Data Mining, (ICDM ’01), California, pp. 
369-376, 2001. 
[11] W. Lian, D.W. Cheung and S.M. Yiu, “An efficient algorithm for dense 
regions discovery from large-scale data streams”, Computers & 
Mathematics with Applications, 50, pp. 471-490, 2005. 
[12] B. Liu, W. Hsu and Y. Ma, “Integration classification and association 
rule mining”, Proc. of 4th Int. Conference on Knowledge Discovery and 
Data Mining, pp. 80-86, 1998. 
[13] H. Liu, F. Hussain, C.L.Tan and M. Dash, “Discretization. An enabling 
technique”, Data Mining and Knowledge Discovery, vol. 6, pp. 393-423, 
2002. 
[14] Mineset user’s guide, v. 007-3214-004, 5/98. Silicon Graphics,1998. 
[15] M.N. Moreno, L.A. Miguel, F.J. García and M.J. Polo, “Building 
knowledge discovery-driven models for decision support in project 
management”, Decision Support Systems, 38 (2), pp. 305-317, 2004. 
[16] M.N. Moreno, F.J. García, and M.J. Polo, “Mining interesting 
association rules for Prediction in the Software Project Management 
Area”, Proc. of 6th International Conference on Data Warehousing and 
Knowledge Discovery (DaWaK 2004). Lectures Notes in Computer 
Science, LNCS 3181, pp.341-350, 2004. 
[17] M.N. Moreno, I. Ramos, F.J.García and M. Toro, “An association rule 
mining method for estimating the impact of project management policies 
on software quality, development time and effort”, Expert Systems with 
Applications, 34 (2), pp. 522–529, 2008. 
[18] R. Srikant, and R. Agrawal, “Mining quantitative association rules in 
large relational tables”, Proc. of ACM SIGMOD Conf. pp. 1-12, 1996. 
[19] C.R. Symons, Software sizing and estimating MKII FPA, John Wiley 
and Sons, 1991. 
[20] H. Verlinde, M. De Cock and R. Boute, “Fuzzy versus quantitative 
association rules. A fair data-driven comparison”, IEEE Transactions on 
Systems, Man, and Cybernetics - Part B. Cybernetics, vol. 36, pp. 679-
684, 2006. 
[21] J. Verner and G. Tate, “A software size model”, IEEE Transaction of 
Software Engineering, 18 (4), pp. 265-278, 1992. 
[22] Y. Wang and A.K.C. Wong, “From association to classification. 
Inference using weight of evidence”, IEEE Transactions on Knowledge 
and Data Engineering, 15, pp. 764-767, 2003. 
[23] X. Yin and J. Han, “CPAR. Classification based on predictive 
association rules”, In. SIAM International Conference on Data Mining 
(SDM’03). pp. 331-335, 2003. 
 
