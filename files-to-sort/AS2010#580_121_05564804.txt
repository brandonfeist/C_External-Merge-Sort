Associative Web Document Classification Based on Word Mixed Weight 
Xingyi Li JunLan Huaji Shi 
Department of Computer Science and 
Telecommunication Engineering 
Jiangsu University 
Zhenjiang, China 
lixinyii@163 .com 
Department of Computer Science and 
Telecommunication Engineering 
Jiangsu University 
Zhenjiang, China 
Ij508508@163.com 
Department of Computer Science and 
Telecommunication Engineering 
Jiangsu University 
Zhenjiang, China 
hjshi001@163.com 
Abstract-There are two shortages when the method of 
classification based on association rules is applied to classify 
the web documents: one is that the method process the web 
document as a plain text, ignoring the HTML tags information 
of the web page; another is that either item of the association 
rules is only the word in the web page, without considering the 
weight of the word, or it quantifies the weight of the word 
frequency, ignoring the importance of the location of the word 
in the web document Therefore, a new efficient method is 
proposed in the paper. It calculates the word's mixed weight 
by the information of the HTML tags feature, and then mines 
the classification rules based on the mixed weight to classify the 
web pages. The result of experiment shows that the 
performance of this approach is better than the traditional 
associated classification methods. 
Keywords-web document classifICation; association rules; 
HTML tags; mixed weight. 
I. INTRODUCTION 
With the rapid development of the network, a number 
of pages are surging in the internet and the information has 
been explored in the form of them. Using the search engines 
to crawl, the result is usually very large and lots of 
information is irrelevant to the needs of the users. So it is a 
great challenge for technology that how to organize and 
process the large amount of web document data, and find the 
interesting information for the users quickly, exactly and 
fully. As a result, a possible strategy is presented by 
importing web document classification which has gradually 
been a hot spot in the field of machine learning as well as the 
text categorization. 
At present, the research about the technology of web 
document classification is mainly on the basis of the text 
classification methods, such as support vector machine 
(SVM) [2], Native Bayesian classification (NB) [3], k­
Nearest Neighbor algorithm (KNN) [4] and so on. Although 
these methods have achieved a certain effect, they still fall 
short of our expectations. And then the classification based 
on association rules has been recently presented which has a 
high degree of accuracy in classification and forecasting. 
Lots of literatures do the research about it. Taking the 
literatures [5,6] for instance, the idea of these methods is to 
use the existing mining algorithm of association rules to 
generate all the frequent item sets, and then build the 
classifier according to these sets in various categories. But 
these are just designed for classification of non-text area. A 
978-1-4244-5540-9/10/$26.00 ©2010 IEEE 
578 
approach is proposed in the studies [7,8] which mine the 
associative rules to classify the text through treating the text 
as a transaction and taking the word as a item. In spite of the 
fact that it is effective to some extent, there is still a 
drawback that it ignores the item's weight in the text. While 
the study [9] imports the word frequency as the weight, 
makes a concept of the weighted frequent item-sets and 
presents a corresponding mining algorithm, but it is only suit 
for text categorization. Compared with the normal text, web 
pages have their own characteristics, for example, web pages 
are rich in the tags, and the structural information is more 
clearly. Because of these, the classification of web 
documents is more difficult than the text classification. 
Therefore we must make further study in the view of the 
characteristic feature of the web pages. In brief, when the 
above methods are applied to classify the web documents, 
there exit two shortcomings: (1) to treat the web page as a 
plain text, without considering the information of the HTML 
tags; (2) for the items in the association rules, either ignoring 
the word's weight or only quantifying the weight of the word 
frequency, without considering the great effect of locations 
of the words in the web pages. 
So in order to truly classify the web pages, a method of 
associative web document classification based on word 
mixed weight is presented in this paper. Among the method, 
we express the web page of a collection of triples in 
accordance with the location feature embodied in the 
information of the HTML tags, word frequency and so on, 
then calculate the word mixed weight by the method of 
weighted frequency based on the word location feature, and 
use it to weight the item in the association rules, finally mine 
the sets of the frequent weighted items, moreover, build the 
classification rules to classify the unlabelled web pages. 
II. WEB DOCUMENT PREPROCESSING 
A. Expression of the web document 
As different from the common text, web document is a 
semi-structured data based on the HTML syntax rules. The 
structure of the web pages is embodied in the HTML tags. 
The content, belong to the different tags, has a different 
ability to express the subject of the web page. So it is a good 
instruction for the web document classification. For example, 
if the word "books" is contained in the title of the web page, 
it indicates that the category of this page is closely related to 
the kind of books. Therefore, it is necessary for us to not 
only take into account the infonnation of the words in the 
web pages, but also pay attention to the characteristics of the 
word location. Generally speaking, the locations, which are 
in accordance with HTML tags of the page, include Title, 
META, HI-H6, Strong, Anchor, and Plain Text and so on. 
The words in the Plain text are words that do not appear in 
the text enclosed by the title, header, or emphasized 
structures of the web document. 
According to the different infonnation of the categories 
contained in the locations [10], we have grouped locations 
into the following three classes (see Table I). 
TABLE I. THE THREE CLASSES AND ASSOCIATED HTML TAGS 
Class name: p HTMLTags 
I Title,Strong,HI-H6,META 
2 Anchor 
3 Plain text (N one of the above) 
Definition l(expression of the web document): a web 
document can be defined as a set of triples, such 
asR = {(Ci,p,/ci,P) I Ci E S,p E P,/ci,p E Z}, where Ci 
is a word in the web page, p is the class of the location for 
Ci, /ci, P is the frequency of the word Ci which is located in 
the location class p, S is a set of words in the page, P 
is a set of location class, Z is a set of integer. 
The method using sets of triples is a new process for 
expressing the web document. It has many advantages. On 
the one hand, this approach pays attention to the difference 
between the common text and the web document, and takes 
into account the characteristics of the web pages clearly. On 
the other hand, it makes a difference between the words 
located in different locations in the page, reflecting the 
importance of the location feature for the classification. 
B. Word frequency count 
When counting the word frequency in the traditional text 
classification methods, we often ignore the semantic 
relations of the tenns, like a situation that different words 
have the same or nearly meaning. As a result, for one thing it 
causes the dimension of the words so large that affect the 
efficiency of the classification, for other thing it also reduces 
the degree of accuracy in classifying. Consequently, .
a 
statistical method of the frequency based on semantIc IS 
presented in the paper. 
The steps are as follows: (1) to extract the text content 
from the web page according to the class of locations, obtain 
the words by segmenting them and process the words to a set 
of triples; (2) to get the synonyms by using the HowNet [11] 
to extend the words, scan the triples and combine two or 
more triples into a triple with adding the word frequency 
together if they have the same class of locations and mean?g; 
(3) finally obtain a collection of triples based on the semantIC 
which stands for the web page .. 
579 
C. Mixed weight of the word 
The weight of the word in the web page not only reflects 
the contribution degree of the word to identify the subject of 
the page, but also stands for a capacity of distinguishing 
between the different pages. As the characteristic of the web 
document, three factors are responsible for the word weight 
in the web page: (1) the word frequency in the web page; (2) 
the location feature of the word in the page; (3) the quantity 
of the web pages containing this word which is also called 
inverse document frequency. 
Definition 2(weighted frequency of the word): the 
calculating fonnula for the weighted frequency of the word 
C is defined as 
3 
tfc = ?)A(p)x/c,p) (1) 
p=l 
/C, p is the frequency of the word C which locates in the 
location class p, A is a function of location contribution. 
Let p denote a class of the locations, then 
{4 p = 1 
A(p) = 2 :p = 2 
1, p = 3 
(2) 
Therefore, let C denote a word in the web page, we 
propose a new calculating fonnula of the word mixed 
weight Wc , and it is defined as following: 
Wc = tfc x id/c = tfc x log(N / Nc + 0.01) (3) 
Where tfc is the weighted frequency of the word C 
calculated by the method of weighted frequency based on the 
word location feature, using the fonnula(1), id/c is the 
inverse document frequency, N is the total of the sample 
web pages, Nc is the number of these pages which contain 
the word c. 
For convenience of the classifying latter, after using the 
fonnula (3) to calculate the mixed weight of the words in the 
page, we will use a vector to represent the web page, just 
like V = {( Cl, WCI), (C2, WC2), (C3, WC3), . .   , (Cn, Wcn)} , where 
Ci is a word, and WCi is the mixed weight of this word. 
III. ASSOCIATIVE WEB DOCUMENT CLASSIFICATION 
BASED ON WORD MIXED WEIGHT 
As is well known, in the traditional methods of 
associative classification, the items, which compose the 
association rules, usually are only the words in the web page, 
ignoring its weight. Or the means of computing the weight ?s 
unreasonable, as they just use the word frequency as therr 
weight, without considering the characteristics of the web 
pages. In order to solve this problem, an efficient method is 
presented on the premise of pay a great attention to the web 
characteristic feature in the paper. Firstly, we compute the 
word mixed weight with the infonnation of the HTML tags, 
and then weight the items of the association rules by the 
mixed weight. At last, we mine the sets of the frequent 
weighted items; moreover, building the classification rules to 
classify the unlabelled web pages. 
Definition3:Let X = {(C!, Wc,), (C2, WC,), "', (Ci, WC;)} be a 
set which stands for a web page, then the element (Ci, We;) 
is called a weighted item, one or more elements are called a 
weighted item sets, such as X . If there are two weighted 
items, (Ci,We;) and (Ci,We;l) , and We;> We;' , then 
(Ci, We;) is called a supper set of (Ci, We; '). 
Definition 4: Let X={(a,l1a1(o,"W21";(a,Y1<l)} be a 
weighted item sets ande is a threshold of minimum support, 
then the support of X is the minimum mixed weight of the 
item which is contained in the set X , marked as 
stqpJrl(X)=Mn(We"l1b, .. ·,Wc;). If support(X) > e,  
then X is called a frequent weighted item sets. 
Associative web document classification based on word 
mixed weight contains two steps: 
Stepl: mining all the frequent weighted item sets from 
the sample pages. 
Step2: generating the classification rules based on the 
frequent weighted item sets, and classifying the unlabelled 
web pages. 
Apriori algorithm [12] is the classic algorithm in the 
mining of associate rule. In view of the first step, we 
improve this algorithm, and propose a new mining algorithm 
of frequent weighted item sets, called E-Apriori algorithm. 
E-Apriori algorithm 
Input: a kind of training web pages setD; a threshold of 
minimum support e 
Output: frequent weighted item sets L of this class pages 
(1) For each dE D 
(2) { To extract the text content from the web page d , 
count the word frequency, and then obtain the set R of 
triples based on the semantic; 
(3) To get the set of weighted items 1d = { (Ct, Wet) I Ct ESt} 
from R through the method of weighted frequency 
based on the word location feature; 
(4) add(Td, T ); Iladd the elements of Td into the set T 
(5) } C = {Ct I (Ct, Wet) E T}; II get the item sets from the 
weighted item sets 
(6) For each Ct E C 
LKd;(Ct) 
(7) Wet = 
{
d; 
} 
; II calculate the support of the 
I di I Ct E di I 
item Ct , function Kd;( Ct ) is stands for the mixed weight 
of item Ct in the web page d i  
(8) Ll={(Ct,Wet) IWet>e} ; II Ll is the frequent 
weighted I_ item sets 
(9) for(k = 2;Lk - d = null;k + +) 
580 
(10) { get k-l_ item sets G-l={CiI(Ci,Wc;)Ell-l} from 
Lk-l; 
(11) Ck = Ck -1 X Ck -1 ; Ilaccording to Apriori algorithm, 
get k_ item sets by the connecting of k-l_ item sets 
(12) For \;fX.Ct E Ck 
(13) { For each Ct E X 
LKdi(Ct) 
(14) Wet = 
{
d; 
} 
; Ilcalculate the weight over again 
I dilXEdi I 
(15) Lk = {(Ct, Wet) I (Ct, Wet) E X, Support(X) > e} 
Ilgenerate frequent weighted k_ item sets 
(16) }}return L = {L I i = 1, 2, .. · ,k} ; 
The initial classification rules of the web pages are 
derived from the frequent weighted item sets which are 
generated by the E-Apriori algorithm. When selecting some 
rules as meeting a certain threshold of the confidence, we 
can build a classifier by these rules. 
In general condition, using the classifier, we can classify 
different web pages. However, because of a lot of rules in it, 
the classifier will be inefficient if it matches the rules in turn. 
So with the property of the supper set defmed in the paper 
before, we adopt the CR-tree to store, search and match the 
rules, and then to classify the web pages. This is because all 
the child nodes in the tree are a supper set of its father node. 
If the rule in the father node can't match the unlabelled web 
pages, neither can the rules in its child nodes. In situation as 
this, we not only reduce many times of matching the rules, 
but also do not affect the accuracy of the classifying. The 
steps of web document classification is that firstly, to extract 
the words from the page and calculate the word's mixed 
weight, secondly, to use the rules in the CR-tree to match the 
page, and then get the category of the page. 
IV. EXPERIMENTAL RESULTS 
The experimental data is downloaded from the internet, 
and divided into six categories: political, military, computer, 
sports, entertainment and books. The total of the training 
sample web pages is 900, for each category, there are 150. 
And the quantity of testing samples is 300, for each category, 
there are 50. The tool of segmenting words is the ICTCLAS 
system. 
To evaluate our method, the measures are Precision, 
Recall and Fl. Their definitions are as follows: 
D 
11 number of correct positive predictions 
?eca = (4) 
number of positive examples 
P 
.. number of correct positive predictions 
( ) reClSIon= 5 
number of positive predictions 
F 1 = 2 x Recall x Precision 
Recall+ Precision 
(6) 
To evaluate the performance of the method presented in 
the paper, we make the approach of associative web 
document classification based on word mixed weight (called 
MW _ARC) in comparison with the traditional classification 
of association rules( called N _ ARC) which without 
considering the weight of the words . In the experiment, the 
threshold of minimum support is 12. And the results are as 
follows (see Figures 1, 2, 3). 
100 ,--------------, 
? 60 r----------? 
.:? 
'" . ? '10 
0.. 
20 r----------? 
poiil. nilil. conpu. sports cnte. books 
I-+-MW ARC --N ARCl 
Figure I. Precision Comparison 
100 ,-------------, 
80 ? 
? 60 
? 
? 40r-----------? 
'" 
20 ----------? 
pol i t. mi 1 i t. compu. sports entc. books 
I -+-MW_ARC __ N_ARC 1 
Figure 2. Recall Comparison 
pol i l. mi I i l. compu. sports enle. books 
I -+-MW ARC --N ARC I 
Figure 3. FI Comparison 
As a result, we can see from the figures above that the 
method of MW ARC is more accurate than the method of 
N ARC. 
V. CONCLUSIONS 
In view of the disadvantages of the classification based 
on association rules applied to classify the web documents, 
we have presented an approach of associative web document 
classification based on word mixed weight in the paper. And 
the experiment results show that it can enhance the accuracy 
by using the information of HTML tags to compute the 
mixed weight and weighting the items. Moreover, it will also 
promote the efficiency by using the CR-tree. In summation, 
581 
it sets off the faults of the traditional methods and proves that 
it is a better method. 
ACKNOWLEDGMENT 
This work is sponsored by National Natural Science 
Foundation of China under Grant No.1 0972027. 
REFERENCES 
[I] SunJiantao, ShenDow, LuYuchang, ShiChunyi.Web document 
classification techniques. Tsinghua Univ(Sci&Tech), 2004, 44(4), 
pp:65-68 . 
[2] Meijuan Gao, Jingwen Tian, Shiru Zhou. Research of web 
classification mining based on classifY support vector machine.2009 
ISECS International colloquinm on computing , communication, 
control, and management, 2009, vol.2, pp:21-24. 
[3] HeZhongli, LiuZhijing. A novel appraoch to Native bayes web page 
antomatic classification. Proceeding-5th International Conference on 
fuzzy system and knowledge, 2008, vol.2, pp:361-365. 
[4] Yin Jian, Tan Huanyun. A KNN text categorization algorithm base on 
x"2 statistic. Journal of chinese computer systems, 2007, 28(6), 
pp: I 094-1 097. 
[5] BingLiu, WynneHsu, YimingMa. Integrating classification and 
association rule mining. In:ACM International Conference on 
knowledge discovery and data mining(SIGKDD'98), 1998, pp:80-86. 
[6] Wenmin Li, Jiawei Han, Jian Pei. CMAR:Accurate and effcient 
classification based on multiple class-association rules. Icdm, pp:369-
376, First IEEE International Conference on Data Mining(ICDM'OI), 
2001. 
[7] GuoYuqin, YuanFang, LiuHaibo. Text categorization based on fuzzy 
classification rules tree. Journal of Southeast University(English 
Edition), 2008, 24(3), pp:339-342. 
[8] WangYuanzhen, QianTieyun, FengXiaonian. Association rules based 
automatic chinese text categorization. MINI-MICRO SYSTEMS, 
2005,26(8), pp:1380-1383. 
[9] QiuJiangtao, TangChangjie, Q.Shaojie, DuanLei, LiuQihong. Mining 
text classification rules based on weighted frequent item sets. Journal 
of SiChuan University(engineering science edition), 2008, 40(6), 
pp:110-114. 
[10] Michal Cutler, Yungming ShiH, Weiyi Meng. Using the structure of 
HTML documents to improve retrieval. Proceedings of the USENIX 
Symposium on Internet Technologies and Systems Monterey, 
California, 1997, pp:22-33. 
[II] DongZhendong, DongQ. HowNet[EB/OL].http: //www.keenage.com. 
2009. 
[12] Rakesh Agrawal, T.!., Arun Swami. Mining association rules between 
sets of items in large databases. Proceedings of the 1993 ACM 
SIGMOD Conference Washington DC, USA, 1993, pp:207-216. 
