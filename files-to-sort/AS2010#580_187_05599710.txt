Text Knowledge Representation Model based on 
Human Concept Learning 
Xiangfeng Luo1,2, Chuanliang Cai1,2, Qingliang Hu1 
1School of Computer Engineering and Science, Shanghai University 
2High Performance Computing Center, Shanghai University 
Shanghai 200072, P.R. China 
{luoxf, caichuanliang}@shu.edu.cn, hdhuql@163.com 
 
 
Abstract—The essential abilities of text knowledge representation, 
such as automatic construction, carrying abundant semantics 
and flexible reasoning, should be held due to the rapid growth of 
web resources and the requirements of the reasoning-based web 
services. However, current text knowledge representation models 
either lose many textual semantics or cannot be constructed 
automatically. To solve the above issues, text knowledge 
representation model is proposed based on the concept algebra of 
human concept learning. Then, the degree-2 power series 
hypothesis is developed and the reasoning ability of text 
representation is proposed. Finally, the results compared with 
current knowledge representation models show that our model 
performs better than other models in representing text 
knowledge. 
Keywords-text representation; power series; human concept 
learning; machine understanding 
I.  INTRODUCTION 
The essential abilities of text knowledge representation, 
such as automatic construction, carrying abundant semantics 
and flexible reasoning, should be held due to the rapid growth 
of web resources (e.g. web pages) and the requirements of the 
reasoning-based web services. 
Generally speaking, there are four main types of knowledge 
representation models. 1) Statistics model, such as vector space 
model (VSM), latent semantic analysis (LSA). 2) Cognition 
based model, such as element fuzzy cognitive map (EFCM), 
concept algebra based model [1] [2]. 3) Probability topic model, 
such as author topic mode (ATM) [3], author-recipient-topic 
model (ART) [4], correlated topic models (CTM) [5]. 4) 
Ontology based models, such as ontology inference layer (OIL) 
[6], ontology web language (OWL) [7]. 
From the analysis of the above models, we know that those 
models either are complicated computationally (e.g. CTM) or 
cannot be constructed automatically (e.g. OWL), even lose 
many textual semantics (e.g. VSM) and lack the ability of 
flexible reasoning. Herein, a text can be regarded as a concept 
and sentences contained in text can be regarded as objects 
belonging to the text, where the keywords are the attributes of 
these objects [8]. Based on concept algebra [9], power series 
representation of text knowledge is developed in this paper. 
The proposed model not only can carry more abundant 
semantics from text (e.g. web page) but also can be represented 
automatically, which adapts to the rapid growth of web 
resources and holds more abundant semantics than VSM and 
EFCM. 
This paper is organized as follows. The idea of concept 
algebra of human concept learning will be introduced in 
section II. In section III, we propose our text representation 
model base on the concept algebra of human concept learning. 
Then, the reasoning rules of text knowledge have been 
discussed in section IV. Finally, comparison results with other 
models and conclusions are given in section V and VI, 
respectively. 
II. CONCEPT ALGEBRA OF HUMAN CONCEPT LEARNING 
A. Concept Algebra 
Feldman pointed out that any set of data exhibits many 
patterns, some of which may be specified with only a small 
number of features, while others may be described by a larger 
number of features and other patterns involve an intermediate 
number of features to specify [9]. The key issue is how the 
arbitrary patterns of data can be described by the combinations 
of component patterns. In order to solve this issue, in the 
following parts, we will introduce the related work by an 
example (Fig.1) cited from [9]. 
The "world" W consisting of the five amoeba-like objects 
corresponds to a concept of human concept learning. We 
choose some "language" in which to express the structure of 
this "world" W, which we think of as simply as a list #  
ba1,a2,e,adc of abstract property tags, called the property 
 
Figure 1. A "world" W containing amoeba-like objects cited from [9]. This work is supported by the Shanghai Municipal Science and Technology 
Commission (project no.09JC1406200), the National Science Foundation of 
China (project no. 90612010), and the Shanghai Leading Academic Discipline 
Project (J50103). 
44
Proc. 9th  IEEE  Int. Conf. on Cognitive  Informatics  (ICCI’10) 
F. Sun, Y. Wang, J. Lu, B. Zhang, W. Kinsner & L.A. Zadeh (Eds.) 
978-1-4244-8040-1/10/$26.00 ©2010 IEEE
language [9]. For the "world" shown in Fig.1, an appropriate 
language might be #  bblob_shaped, shaded, has_nucleus, 
has_dotted_membrane, largec, which is abbreviated to #  ba, 
b, c, d, ec under the assignment:  
a=blob_shaped;  
b=shaded;  
c=has_nucleus;  
d=has_dotted_membrane; 
e=large, 
and the "world" W encoded into  # is denoted by W|#. 
In [9], Feldman figured out an idea which is to pick out a 
certain atomic set of rules as the building blocks of "concepts" 
or "patterns". These building blocks, together with the rules for 
combining them, constitute the description language "concept 
algebra". Then, there are two basic and important definitions 
described in the following parts. 
Definition 2.1: Implication 
One very obvious kind of rule - implicit in the notion of 
"causality" - is when one property's value determines another's. 
This distal causality will be reflected proximally in a pattern 
among observed variables, namely a logical implication: 
a1 
 a2, 
where a1 
 a2 means that a1 is never true unless a2 is also. 
Definition 2.2: Affirmation 
An even simpler type of consistent world structure is when 
one property holds consistently in all observed objects, that is a 
rule with the very simple form: 
a. 
Hence, the conceptual structure built up so far reduces to 
the following algebraic form. Write  for the set of constant 
properties;  for the set of pairwise implications; and @for the 
rest of #, the unconstrained properties [9]. Every possible 
world that can be produced by these types of concepts alone 
can be expressed as the Cartesian product of the lattice for 
with the lattice for @, conjoined with the properties , that is 
[9] 
d@, and W|# d@. 
Based on empirical evidence, Feldman proposed the 
linearity hypothesis. Human learners are biased towards linear 
concepts [9]. And cognitive scientists have done many 
researches so far. They find that the hypothesis accords with 
the cognitive behaviors of human beings.  
Therefore, according to the above analysis, the example in 
Fig.1 can be described as follows:  
 = {a}, 
@ = {b}, 
 = {e 
 c, c 
 d, d 
 c}, 
W|#{a}d [{e 
 c, c 
 d, d 
 c} {b}]. 
Because of the similarity between the concept and the text 
[8], an example about text fragments is given as follows. 
S1: That boy stands on the left, whose t-shirt is red. 
S2: Two girls stand on the right, whose skirts are also red. 
Referring to the above basic definitions, we find a property 
language #text which includes all the properties from the text 
fragment. 
#text = {boy, left, red}, and 
 #text = {a, b, c} under the following assignments: 
a = boy, 
b = left, 
c = red. 
After analyzing the text fragment, we find that the clothes 
of all people including one boy and two girls all are red. In 
addition, we can deduce that one person who is boy must be 
left, and one person who is left must be a boy. So the above text 
fragment can be described as: 
 = {c}, 
@ = {}, 
 = {a 
 b, b 
 a},  
W|#Text  {c}d [{a 
 b, b 
 a} {}]. 
B. Extended Concept Algebra of Human Concept Learning 
Based on definition 2.1 and 2.2, we model a "world" or 
"concept" with affirmation and implication; that is to say, the 
"world" or "concept" is described by linear terms. However, 
there are many higher-order terms in reality, so Feldman 
extended the concept algebra to more general hierarchy. 
Definition 2.3: Extensive implication 
According to definition 2.1, the "implication" is linear and 
means the paired causality. However, there are higher-order 
causal regularities among more than two attributes in reality. 
So the paired causality should be extended to a general form. 
The general form of an implication is  
a1a2eak
a0. 
The number k here, called the degree of the implication, is 
the number of properties that are antecedent to the implication, 
which serves as a measure of the complexity of the implication. 
Obviously, the paired implication proposed in the previous 
section a1 
 a2 is the special form of the general case. 
Definition 2.4: Power series expansion 
After defining the extensive implication, the "world" or 
"concept" set can be described more fruitfully. So, Feldman 
proposed the power series expansion of concept. For an object 
set x, the set of implication of degree k that is contained in the 
series expansion of x will be denoted as . And the full 
power series for observations x (denoted as 
k
x
)(xE ) looks like: 
)degree minimum(:0 0x  
4
)degree maximum(:1
:
:1
1
1



D
x
k
x
x
D
K


 
Then, a theorem power series representation is proposed, 
that is: for any object set # defined over a language of size D, 
there is a minimal irredundant set of implications:  
(x)
|
1
0
110
E

ddd



#
k
x
D
k
D
xxxXW
 
Contrast to the lack that the simple algebra is only able to 
express linear concepts; the full power series can describe any 
discrete featured pattern. On the web, there are a huge number 
of web pages, which are discrete and hold all kinds of 
relationships, so it is an important task to distinguish these 
different web pages and represent the relationships among 
them. After studying the feature of concepts and that of web 
pages, we find that the power series representation of concepts 
can describe discrete featured pattern of concepts and the 
relationships among them. Considering that there are some 
similarities between the concepts and the web page, we have 
tried to study the representation of text knowledge (e g. web 
pages) based on human concept learning. Therefore, the power 
series representation of text knowledge will be discussed in the 
next section. 
III. TEXT KNOWLEDGE REPRESENTATION BASED ON 
HUMAN CONCEPT LEARNING 
Simply speaking, text knowledge is composed by 
sequential sentences, a sentence composed by some word’s 
combinations. When we regard the keywords as attributes and 
a sentence as an object in a text, the understanding of text can 
be regarded as the process of concept learning [8]. In this paper, 
we regard the keywords as attributes of describing text 
knowledge, a sentence or paragraph as an object, so a text 
which consists of many sentences and paragraphs can be 
regarded as a concept. Then, we can represent the text 
knowledge according to the concept algebra of human concept 
learning.  
A. Machine-oriented Text Representation 
According to the related thesis of human cognitive process 
[10], machine-oriented text understanding should aim at 
gaining text knowledge or relationships among keywords. 
Referring to concept algebra of human concept learning, in this 
paper, different knowledge from text can be regard as different 
sets. There is a simple description as follows. 
: {} denotes text assertion, the common sense belonging to 
the text; 
f{} denotes text association rules, which contain the 
relationships among keywords from text. 
@f{} denotes isolated keywords, which are not related with 
other keywords in text, and affect the understanding 
difficulty of text. 
We utilize above three sets to represent text knowledge, 
which can be represented as  
d@, 
where d denotes adding  to every element in the set (@);  
denotes the Cartesian product of two sets (@ and ).  
If @ is O, @ equals to , which means that no isolated 
keywords affect the relationships  among keywords from text. 
For the text fragment in section?, if we regard the text 
fragment as a text T and suppose that {boy, girl, red, left, right} 
is the set of keywords of this text fragment, the knowledge of 
this text fragment can be represented as follows: 
: {red}, 
: {boy 
 left, girl 
 right}, 
@:  {}. 
So the text can be represented as: 
T = d@ = {red}d [{}  {boy 
 left, girl 
 right}]. 
From the above simple and qualitative analysis, we know 
that the representation method of concept algebra can describe 
knowledge from text. Therefore, the next section will develop 
text knowledge representation model based on concept algebra 
of human concept learning. 
B. Power Series Representation of Text Knowledge 
In order to describe the power series representation of text 
knowledge conveniently, some terms will be given in advance. 
Term 3.1: Text assertion 
Text assertions are certain keywords that stand for the 
common sense knowledge, which is the simplest and the most 
easily-understood information in certain field.  
For instance, WSDL, UDDI are common sense knowledge 
in the field of web service. 
Term 3.2: Text association rules 
Text association rules are association relationships among 
keywords, and these keywords have adjacent location and co-
occurrence appearances, which are causations and reflect the 
latent semantic relationships of text. 
According to human concept learning [9], different text 
association rules are represented as the following forms: 
ba
r1

 , 
cab
r2

 , 
 , 
4C
mab
irk

ddd


, 
where are keywords in a text;  denote 
corresponding confidences of text association rules. 
mcba ,,,, ddd irrr ,,, 21 ddd
Term 3.3: Degree of text association rule 
Degree of text association rule is the number of its 
antecedent keywords.  
For example, the degree of text association rule a 
 b is 1; 
the degree of text association rule a,b 
 c is 2. From the point 
of view of human concept learning, the degree of implication 
reflects the understanding difficulty of concepts and 
relationships among them. Similarly, the degree of text 
association rule measures the complexity of relationships 
among keywords. The greater the degree of text association 
rule is, the more complicated the text association rule is; vice 
verse. For instance, there are two text association rules in the 
field of web service: 
WSDLService Description; 
WSDL, UDDI, SOAPService Framework. 
The degree of the former is 1, and that of the latter is 3. It is 
obvious that the latter is more difficult to be understood, 
because there are more complicated associations among WSDL,
UDDI and SOAP. Specially, the largest degree of a D-keyword 
text is D-1. Text assertion is a special text association rule 
whose degree is 0. 
Term 3.4: Degree-k text association rule set 
One text, containing keywords #  bK1, K2, …, KDc, is 
denoted as T|#. And the set of degree-k text association rules is 
denoted as , where x  #. kx
In previous section, the concept is expanded as the form of 
power series, so we expand similarly the text association rules 
in the following terms. 
Term 3.5: Power series expansion of text association rules 
In a text containing D keywords, power series expansion of 
text association rules is a set of text association rules whose 
degrees are from 0 to the D-1. Referring to the expansion of 
power series of human concepts, power series expansion of text 
association rules is represented as 
degree)largest   theof rulesn associatio(text :1
:
:1
)assertions(text :0
1
1
0




D
x
k
x
x
x
D
k


 
Term 3.6: Power series representation of text knowledge 
Referring to concept algebra of human concept learning [9], 
there is a set of text association rules corresponding to any text 
T, so there is the formula: 
k
x
D
k
D
xxxT

ddd



#
1
0
110
       
|
 
where #  denotes all keywords in text T; D denotes the number 
of keywords;  denotes a set of degree-k text association 
rules. 
k
x
After analyzing the power series representation of text 
knowledge, we find it is complicated to represent text 
knowledge. Therefore, it is an important task to find the special 
features of web texts. So the simplicity of power series 
representation will be discussed in the following parts. 
C. Simplicity of Power Series Representation 
According to the definition of the degree of text association 
rule, text association rules are classified into different types. 
After analyzing the distribution of text association rules in a 
huge amount of texts, we find that text association rules mainly 
consist of degree-1 and degree-2 text association rules. And 
degree-3 text association rules are few; text association rules of 
degree more than 2 are seldom. We calculate the average 
number of all degrees' text association rules from 500 web 
pages, and the results are showed in Fig. 2. 
In Fig. 2, degree-1 text association rules reach to 78.5%, 
degree-2 text association rules cover 18.9%, degree-3 text 
association rules hold 1.2% and degree-4 text association rules 
is about 0.4%. 
The distribution of text association rules supplies the 
important reference for machine extracting association rules 
automatically. Therefore, text association rules of degree-1 and 
degree-2 are the most important parts, which should be 
extracted by machines. So, we propose degree-2 hypothesis in 
machine-based text understanding. 
Degree-2 hypothesis: If one text can be composed by text 
assertion, degree-1 text association rules and degree-2 text 
association rules, that is to say, if one text can be represented 
by , the most semantics of one text can be 210 xxx 










   
GHJUHHRIDVVRFLDWLRQUXOH
DY
HU
DJ
H
UD
WH
R
I
DV
VR
FL
DW
LR
Q
UX
OH



 
Figure 2. The distribution of all degrees' association rules of 500 texts 
4B
extracted by machines. 
Base on the above hypothesis, power series representation 
of text knowledge is simplified as 
210| xxxT # . 
Compared with linear hypothesis theory, degree-2 
hypothesis is the expansion of linear hypothesis in human 
concept learning. Linear hypothesis reflects the cognitive 
behaviors of human concept learning. The complex 
relationships among the concept's attributes are hard to be 
obtained by human beings because of some physiological and 
psychological restrictions such as brain storage capacity, 
logical thinking ability, etc. In fact, for the majority of concept 
learning process, it is sufficient to obtain a concept's 
knowledge with degree-1 relationships between attributes 
because human beings have the associated ability and the prior 
knowledge to explain the concept's content with the help of 
degree-1 association rules [9]. However, to the machine, as 
there is no storage capacity limitation and logical thinking 
limitation, it is able to obtain more complex semantic 
relationships among keywords, making up of the lack of 
associated ability and prior knowledge like human beings. 
IV. REASONING BASED ON DEGREE-2 HYPOTHESIS OF 
TEXT KNOWLEDGE 
According to the above analysis, especially degree-2 
hypothesis, any text can be represented as: 
210| xxxT # . 
And this formula not only simplifies the representation of 
text knowledge, but also makes our model's semantic reasoning 
possible. Herein, we will introduce the basic structure of this 
model, and then analyze the essential features of power series 
representation of text knowledge based on human concept 
learning (PSR). Therefore, the reasoning rules of textual 
knowledge will be discussed in the following. Above of all, we 
choose two texts denoted as: 
210
aaa xxxa
T   
210
bbb xxxb
T   
Then we propose three types of basic reasoning rules, 
which are intersection, union and subtraction between two 
different texts in the same domain.  
The common information can be reasoned by the 
intersection rule, the domain information can be collected by 
the union rule among certain texts belonging to the same 
domain, and the differences between two different texts can be 
extracted by the subtraction rule between two different texts.  
In this point, our model performs better than other models. 
Vector space model hasn’t the reasoning ability because the 
resources are just represented by keywords. Element fuzzy 
cognitive map just considers the degree-1 association rules, so 
the reasoning rules of this model is too simple. Ontology based 
models can express rich semantics, but the reasoning rules of 
these models are too strict, which cannot meet with the 
diversity and irregularity of web resources. Probability topic 
models are based on probability and statistic, where many 
mathematical assumptions are given. However, these 
assumptions are not suitable, because the web resources are 
massive, dynamic and uncertain. To our model, text knowledge 
can be represented automatically, and can accord with the mass, 
dynamic and uncertainty of the web resources. At the same 
time, the reasoning process is easier to be implemented in our 
model, which also contains richer semantics. 
Definition 4.1: Intersection reasoning of two texts Ta, Tb which 
stands for the special information that appears not only in Ta 
but also in Tb, is denoted as I (Ta, Tb). 
0 1 2 0 1 2
0 0 1 1 2 2
0 1 2
0 1 2
( , ) ( ) ( )
           ( ) ( ) ( )
             = 
            
a a a b b b
a b a b a b
a b a b a b
a b x x x x x x
x x x x x x
x x x x x x
x x x
I T T           
        
    
     
  
  

  
 
where ax  is the set of keywords of text ; and aT bx  is the set of 
keywords of text ;bT a bx x x   is the intersection of ax   and 
bx .  
Intersection of two texts contains the common information, 
including text assertions and text association rules. 
Definition 4.2: Union reasoning of two texts Ta, Tb, which 
stands for all of information from two texts, is denoted as U (Ta, 
Tb). 
0 1 2 0 1 2
0 0 1 1 2 2
0 1 2
0 1 2
( , ) ( ) ( )
            ( ) ( ) ( )
             
             
a a a b b b
a b a b a b
a b a b a b
a b x x x x x x
x x x x x x
x x x x x x
x x x
U T T           
        
     
     
  
  

  
 
where  is the set of keywords of text ; and ax aT bx  is the set of 
keywords of text ; bT U a bx x x   is the union of  ax  and bx . 
When intersection reasoning between two texts isn’t O, 
union reasoning can extract comprehensive semantics from two 
texts. 
Definition 4.3: Subtraction reasoning of Ta to Tb, which stands 
for the special information that appears in Ta but not in Tb, is 
denoted as Sa-b (Ta, Tb).
0 1 2 0 1 2
0 0 1 1 2 2
0 1 2
0 1 2
( , ) ( ) ( )                 
                 ( ) ( ) ( )
                 
                 
a a a b b b
a b a b a b
a b a b a b
a b a b a b
a b a b x x x x x x
x x x x x x
x x x x x x
x x x
S T T
  

  
           
           
     
     
a
 
where x  is the set of keywords of text ; and aT bx  is the set of 
keywords of text ; bT a b a bx x x    is the subtraction of  ax  to 
bx . 
4
TABLE I. COMPARISONS OF REASONING ABILITY, AUTOMATIC CONSTRUCTION ABILITY, COMPLEXITY COMPUTATION, AND SEMANTIC RICHNESS 
Representation models VSM EFCM PSR PLSA, LDA OWL 
Intersection  ×   ×  
Union ×   ×  
 
Reasoning 
Subtraction ×   ×  
Automatic construction    Manual Semi-automatic 
Complexity Computation Not Easy Easy Difficult Not 
Semantic richness Poor Media Rich Rich Rich 
Subtraction reasoning between two texts means that there 
are some differences between them. 
The above three types of reasoning are the basic 
components of the reasoning rules of power series 
representation.  
With , we are able to find all knowledge of the two 
texts; with the , we could find the shared information 
between two texts; with the , we can find 
the main section of two different texts. So, three types of basic 
reasoning rules play an important role in the reasoning of 
power series text representation.  
( , )a bU T T
I T( , )a bT
( , ),   ( , )a b a b b a a bS T T S T T 
In addition, there are many other reasoning rules in our 
model, such as "exclusive or", "inclusive or". However, these 
two reasoning rules may be deduced by the above three types 
of basic reasoning rules. So, intersection, union and subtraction 
constitute the principal part of the reasoning rules of our model. 
V. COMPARISON OF TEXT REPRESENTATION MODELS 
In section IV, we have studied the reasoning rules of our 
model. Then, we will compare our model with other models in 
the following parts, in order to verify that our model performs 
better than other models in representing text knowledge.  
A. Summary comparasions among different kinds of models 
In this part, we will compare our model with current 
knowledge representation models in reasoning ability, 
automatic construction ability, complexity computation, and 
semantic richness (Table I). 
In Table I, we discuss vector space model, element fuzzy 
cognitive map, power series representation, probability topic 
models, and ontology representation models. From this table, 
we can find that PSR, PLSA, LDA, and OWL are all of 
semantic richness, but only PSR can be constructed 
automatically. Because of the rapid growth of web resources, 
the automatic construction ability is one of the most important 
properties. So, in the following part, we focus on these models, 
which can be constructed automatically. 
B. Experiment Verification 
In the following part, six texts divided into two groups 
(Table II, III) from web pages (Appendix 1) will be represented 
by the three different models, which are VSM, EFCM and PSR.  
TABLE II. GROUP ONE CONTAINS THE SAME KEYWORDS {SUN, APPLE}, AND THESE ARE SEARCHED FROM GOOGLE 
Webpage One (NSA Offers Security Guidelines for Sun, Apple and Red Hat, denoted as T1) 
VSM {sun, apple, Schaeffer, policymaker, warfighter, guideline, NSA, harden, assurance, infrastructure, baseline, spectrum, 
subcommittee, institution, operator, homeland, Solaris, demand, commerce, situation} 
EFCM {sunT1: 0.067,appleT1: 0.067, SchaefferT1: 0.133, assuranceT1: 0.133, infrastructureT1: 0.133, NSAT1:
0.467}·{assuranceSchaeffer; Schaefferassurance; infrastructure NSA; SchaefferNSA} 
PSR {sun, apple}·{assuranceSchaeffer; Schaefferassurance; infrastructure NSA; SchaefferNSA} 
Webpage Two (NSA helps Apple, Sun and Red Hat harden their systems, denoted as T2) 
VSM {sun, apple, NSA, SCAP, guideline, operating, hearing, automation, protocol, subcommittee, development, agency, effort, 
homeland, assistance, Solaris, director, terrorism, door ,attack, range} 
EFCM {sun T2: 0.042,apple T2: 0.042, hearing T2: 0.083, NSA T2: 0.333, agency T2: 0.083, director T2: 0.125,
operating T2: 0.125, developmentT2: 0.083, attack T2: 0.083}·{hearingNSA; agencyNSA; 
directorNSA;operatingNSA;operatingdevelopment;hearingoperating;developmentoperating;developmentNSA}
PSR {sun, apple}·{hearingNSA; agencyNSA; directorNSA; operatingNSA; operatingdevelopment; 
hearingoperating; developmentoperating; developmentNSA}·{NSA, hearingdirector; development, 
operatingNSA; NSA, operatingdevelopment; NSA, developmentoperating; attack, NSAoperating; attack, 
NSAdirector; attack, NSAdevelopment}
Webpage Three (Planting Apple Trees, T3) 
VSM {sun, apple, planting, soil, root, fruit, diameter, grass, hole, pocket, graft, watering, nutrient, drainage, scion, stake, bloom, 
lawn, water, everybody, standing} 
EFCM {sun T3: 0.024, apple T3: 0.195, root T3: 0.146, soil T3: 0.146, grass T3: 0.049, planting T3: 0.268, diameter
T3: 0.049, water T3: 0.073, pocket T3: 0.049}·{rootsoil; soilroot; grassplanting; diametergrass; 
grassdiameter; diameterwater; waterdiameter} 
PSR {sun, apple}·{rootsoil;soilroot;grassplanting; diametergrass; grassdiameter; diameter water;
waterdiameter}·{ pocket, soilroot} 
4
TABLE III. GROUP TWO CONTAINS THE SAME KEYWORDS {INTERPRETER}, AND THREE PAGES ARE SEARCHED FROM GOOGLE 
Webpage One (Meaning of Computer Interpreter and Compiler, denoted as T4) 
VSM {interpreter, bcc32, binary, conversion, compilers, Borland, literate, task, package, programmer, editor,  person, string, rule, 
operating, character} 
EFCM {interpreter T4: 0.313, person T4: 0.063, programmer T4: 0.156, editor T4: 0.094, conversion T4: 0.063, binary
T4: 0.313}·{personprogrammer; personeditor; editorperson; conversionbinary; editorprogrammer} 
PSR {interpreter}·{personprogrammer;personeditor;editorperson;conversionbinary;editorprogrammer}·{editor,
personprogrammer; programmer, personeditor; programmer, editorperson} 
Webpage Two (Interpreter Vs. Compiler, denoted as T5) 
VSM {interpreter, BYTECODE, Perl, assembler, processor, 3gl, assembly, PASCAL, platform, compilation, output, instruction, 
object, QBASIC, COBOL, advantage, programmer, visual, execution, Basic, manipulation} 
FCM {interpreter T5: 0.214, platform T5: 0.143, bytecode T5: 0.119, advantage T5: 0.048, programmer T5: 0.143, 
output T5: 0.071, compilation T5: 0.048, execution T5: 0.048, processor T5: 0.071, Perl T5: 0.095}·{platform
bytecode; bytecodeplatform; advantageprogrammer; outputcompilation; compilationoutput; execution bytecode; 
bytecodeexecution; processorinstruction; instructionprocessor; executionPerl} 
PSR {interpreter}·{platform bytecode; bytecodeplatform; advantageprogrammer; outputcompilation; 
compilationoutput; execution bytecode; bytecode execution; processorinstruction; instructionprocessor; 
executionPerl} 
Webpage Three (Interpreters and Translator-- Job Outlook, denoted as T6)
VSM {interpreter, prospect, increasing, localization, specialty, translator, employment, population, growth, globalization, 
shortage, demand, relay, Spanish, employer, occupation, skill, decade, expansion} 
EFCM {interpreterT6: 0.268, employmentT6: 0.107, prospectT6: 0.071, demandT6: 0.125, translatorT6: 0.232,
increasingT6: 0.018, growthT6: 0.036, populationT6: 0.036, localizationT6: 0.036, specialtyT6: 0.036, 
SpanishT6: 0.036}·{employmentprospect; prospectemployment; demandtranslator; increasingtranslator; 
prospecttranslator; employmenttranslator; growthtranslator ;populationtranslator; localizationdemand} 
PSR {interpreter}·{employmentprospect; prospectemployment; demandtranslator; increasingtranslator;
prospecttranslator; employmenttranslator; growthtranslator ;populationtranslator; 
localizationdemand}·{increasing, translatordemand; translator, specialtyprospect; translator, Spanishdemand; 
translator, employmentprospect; prospect, employmenttranslator; prospect, translatoremployment} 
The titles of six pages are respectively denoted as T1, T2,e,
T6, respectively.  In the two groups, there is kiTj: Hi, which 
means that contribution rate of the keyword ki to the page Tj is 
HI in a certain EFCM. At the same time, there is the support 
rate of keywords in the three types of models, and confidence 
of association rules in EFCM, PSR.  
In Table II, although there are the common keywords {sun,
apple} in three pages, the former two pages tell us the one 
similar event and the latter one tells us another. The former two 
pages tell that the National Security Agency (NSA) helps apple, 
sun and red hat harden their systems. Herein, sun and apple are 
different companies. But, the latter article tells something about 
planting apple tree. Just judging the content of keywords, these 
three pages are not easily distinguished. However, with the 
degree-1 and degree-2 association rules, the similarity and 
difference among them become obvious. In page one, there is 
{Schaeffer
assurance; infrastructure
NSA}, which is 
related with the assurance and the infrastructures supported by 
NSA.  In page two, there is {development
operating; 
development
NSA}, which denotes the development 
supported by NSA. The two pages have something to NSA, 
which aims at the computer system security. In page three, 
viewing {soil
root; grass
planting}, readers can infer that it 
is different from the former two pages, and there is something 
about soil and planting. So, PSR expresses the difference 
among them very well, which is generated readily 
automatically by machines. 
As similar to group one, there are three pages which 
contain some common keywords (here, only one "interpreter"). 
So, how to distinguish these pages becomes more difficult 
because of the lack of clue information. However, with the help 
of text association rules, it can be determined whether 
interpreter is a human or machine. In page one, interpreter can 
be regarded as a computer program with the help of the 
association rules {conversion
binary; editor
programmer}. 
And, in page two, there is {platformbytecode; 
bytecodeplatform; instructionbytecode}, which 
distinguishes the meanings of interpreter more easily. So, in 
the former two pages, interpreter is a computer program. But, 
in page three, {translator, employmentprospect} poses 
interpreter as a certain person whose job is to exchange 
information from one language to another language. That is to 
say, it is further proved that some issues that cannot be solved 
by other knowledge representation models can be easily solved 
by PSR, which can be generated automatically. 
In conclusion, there are summary comparisons and 
experiment verification given in above part. From summary 
comparisons of different models, PSR, PLSA, LDA, and OWL 
are all of semantic richness, but only PSR can be constructed 
automatically. VSM, EFCM, and PSR can be constructed 
automatically, but EFCM and PSR contain richer semantics 
than VSM. From experiment verification, we find that poor 
semantics can be extracted by VSM, but EFCM and PSR can 
extract more semantics from texts.  In this point, both EFCM 
4
and PSR perform better than other models in expressing 
semantics.  
However, EFCM can only express the whole contribution 
of all keywords, which mixes the common knowledge and 
association rules included in texts. That is to say, PSR can 
distinguish these different semantics from texts, which 
represents the common knowledge as text assertion and text 
association rules holding different degrees, which have 
different understanding complexity. Therefore, PSR is better 
than EFCM. 
VI. CONCLUSION 
In this paper, we have proposed power series representation 
of text knowledge based on the human concept learning, which 
makes some contributions to the development of the 
knowledge representation. 
1) Firstly, power series representation of text knowledge is 
proposed based on the latest research of human concept 
learning, and many related theories of cognitive psychology 
about text understanding are introduced.  
2) Secondly, we propose the machine-oriented degree-2 
hypothesis, in order that text knowledge can be represented 
automatically, efficiently and compactly. 
3) Thirdly, based on the machine-oriented degree-2 hypothesis, 
we have studied the reasoning rules of our model, which 
perform better than other models. Through the implement 
process of the reasoning rules, we can find the common 
information, differences and the collected domain 
knowledge, which supply the theoretical foundation for the 
discovery of latent semantics. 
ACKNOWLEDGMENT 
The authors acknowledge the anonymous reviewers for the 
valuable remarks and advices given to this paper. Research 
work is supported by the Shanghai Municipal Science and 
Technology Commission (project no.09JC1406200), the 
National Science Foundation of China (project no. 90612010), 
and the Shanghai Leading Academic Discipline Project 
(J50103). 
REFERENCES 
[1] Tian, Y., Y. Wang, and K. Hu (2009), A Knowledge Representation 
Tool for Autonomous Machine Learning Based on Concept Algebra, 
Transactions of Computational Science, Springer, 5, 143-160. 
[2] Feldman J. Minimization of Boolean complexity in human concept 
learning. Nature, 2000, 407(5): 630-633. 
[3] Rosen-Zvi,Michal, Thomas Griffiths,Mark Steyvers, and Padhraic 
Smyth. 2004. The author-topic model for authors and documents. In 
Proc. UAI, pp. 487–494. 418,523, 530, 531. 
[4] McCallum A., Corrada-Emmanuel A., Wang X. The author-recipient-
topic model for topic and role discovery in social networks. IDIAP, 2004. 
[5] Blei, D. M. and Lafferty, J. D. (2006). Correlated topic models. InWeiss, 
Y., Schölkopf, B.,and Platt, J., editors,Advances in Neural Information 
Processing Systems 18.MIT Press, Cambridge, MA. 
[6] Fikes R., McGuinness D. An Axiomatic Semantic Semantics for RDF, 
RDF Schema and DAML+OIL, 2001(11). 
[7] Michael K. Smith, Deborah McGuinness, Raphael Volz, and Chris 
Welty.Web ontology language (owl) guide version 1.0. www.w3c.org, 
nov 2002. 
[8] Fang, N; Luo, X; Xu, W. Measuring Textual Context Based on 
Cognitive Principles. Journal of Software Science and Computational 
Intelligence, Vol. 1, Issue 4. Pages: 61-89. 
[9] Feldman J. (2006). An Algebra of human concept learning. Journal of 
Mathematical Psychology. (50), 339-368. 
[10] Sloman, S. A., Love, B. C., & Ahn, W.-K. (1998). Feature centrality and 
conceptual coherence. Cognitive Science, 22, 189–228. 
APPENDIX 1 
TABLE IV. THREE ARE SIX WEBPAGES DIVIDED INTO TWO GROUPS FOR THIS EXPERIMENT FROM GOOGLE 
 Title Website 
Page one 
NSA Offers Security 
Guidelines for Sun, Apple and 
Red Hat 
http://sun.systemnews.com/articles/141/4/Government/
22604 
Page two NSA helps Apple, Sun and Red Hat harden their systems
http://www.h-online.com/security/news/item/NSA-
helps-Apple-Sun-and-Red-Hat-harden-their-systems-
863889.html 
Group one 
Page three Planting Apple Trees http://www.aboutappletrees.com/planting_apple_trees.shtml 
Page one Meaning of Computer Interpreter and Compiler 
http://hubpages.com/hub/Meaning-of-Computer-
Interpreter-and-Compiler 
Page two Interpreter Vs. Compiler http://www.bookrags.com/research/interpreter-vs-compiler-wcs/ Group two 
Page three Interpreters and Translator – Job Outlook http://www.bls.gov/oco/ocos175.htm 
 
 
4
