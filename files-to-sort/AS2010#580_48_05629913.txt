 
 
The authors wish to thank the 085 Innovation Projects Foundation of 
Shanghai Education Committee for contract J20902 under which the 
present work was possible. 
Research On Decision Of air cargo strategy based on 
Data Mining 
 
LI Yuewen
School  of Management, Shanghai University of Engineering Science 
Shanghai, China 
  
Abstract—This article mines the air cargo customer information 
and establishes the decision tree of the ID3 association rules from 
the ID3 algorithm. It finds frequent customer information using 
Apriori algorithm and finally gets the best combination of 
customer information. The combination has some significance on 
the development of air cargo.  
          
  Keywords-ID3 algorithm; Apriori algorithm; customer 
information 
I. INTRODUCTION  
During the recent years, great achievements of the research 
on data mining application have been achieved, whose broad 
application prospect have attracted many researchers and 
business organizations. A batch of data mining systems have 
been developed, and have been applied in commerce, 
economy, financial management and other fields among which 
the most representative systems are the Quest System 
researched by R. Agrewal of the IBM Almaden and the data 
mining prototype system DB Miner developed by the team led 
by the famous international data mining professor Han Jiawei 
who comes from Canada, Simon Fraster university. The 
successful application of the commercial software’s greatly 
arouse the enthusiasm for data mining technology in the 
academic circle and promote the application of data mining 
technology in industry, such as ERP system and various 
management information system, geographic information 
system, etc.  
However, as for domestic airlines, they have already 
introduced some advanced management experience, but still in 
the beginning stages. They haven’t applied it yet, but with the 
increasingly fierce competition, to use the data mining 
technologies like ID3 algorithm and Apriori algorithm in 
airline industry will definitely become a trend of the air freight 
company development. 
 
II.        DECISION TREE ALGORITHM 
Decision tree is the most widely used method for data 
classification. Decision tree classification is a method to 
deduce the classification rules of the decision tree expression 
forms from disorder and irregular sample sets. It adopts the 
top-down recursive method, which first make comparisons of 
the property values between the inner nodes, then judge the 
branch followed and finally draw conclusions at the leave 
node. Therefore, there is a choice rule to correspond to the path 
from the root to the leave node, and thus a set of classification 
rules to correspond to the decision tree. 
Among all the decision tree classification algorithms, the 
most influential one is ID3 algorithm, the main ideas of which 
are as follows: 
Every non-leave node corresponds to a non-category 
property, whose value is represented by the branch. A leave 
node represents the category that corresponds to the path 
between the root to the leave, and it’s property value is 
recorded by the leave. 
In the decision tree, every non-leave node associates with 
the most informative non-category property. 
Entropy is a term to measure the amount of information that 
a non-leave possesses. 
Here is the definition for Information Entropy: 
The entropy of discrete random variables: It reflects 
expected value of the variable. In general, there are two kinds 
of entropy: unconditional entropy and conditional entropy. 
Unconditional entropy, entropy for short, can be expressed 
in the following formula: 
 
?
=
?=
n
i
ii xpxpXH
1
2 )(log)()(
              (1) 
In this formula, ?? (i = 1,2 ..., n) stands for X value, and n 
stands for the number of values; ?????expresses the Probability 
of ? ? ???  in which 2 is the substrate of the logarithmic 
function because entropy is encoded in binary. H (X) reflects 
the expectation of X with a certain value, that is to say, it 
stands for the uncertainty of X. 
The conditional entropy of the discrete random variable X 
to a given discrete random variable Y can be referred to as the 
conditional entropy X to Y and recorded as H (X|Y),  which 
can be expressed using the following formula: 
??
= =
?=
n
i
m
j
jiji yxpyxpYXH
1 1
2 )|(log),()|(
             (2) 
In this formula, ????, ???  is Joint probability of 
ji yYxX == , ; ji yx |  stands for Conditional probability 
of Y ? y?, X ? x? , and n, m, respectively stand for the values 
2010 International Conference on Electrical and Control Engineering
978-0-7695-4031-3/10 $26.00 © 2010 IEEE
DOI 10.1109/iCECE.2010.993
4085
 
 
number of X, Y.  Similarly  2 is the substrate of the logarithmic 
function because entropy is encoded in binary. 
H (X|Y) reflects the uncertainty of variable X under certain 
conditions. It can be proved that X can effectively lower its 
uncertainty if certain conditions are added. Therefore, 
H(X|Y)?H(X)                                   (3) 
    So generally we use the conditional entropy value of X 
so as to lower its uncertainty. 
    Through the characteristics showed in the data collected, 
we can find an accurate description or model for each category. 
What’s more, we can use the category description it generates 
to classify the future test data. Therefore, we will have a better 
understanding of each category in the data. In other words, we 
obtain some knowledge about this category. 
 
III.     ASSOCIATION RULES 
A. Mining Algorithm 
Definition 1: Mining Association Rules data sets recorded 
for the D (Generally, it is a transaction database); 
D= ???, ??, … , ??, … , ??? , ?? ? ???, ??, … , ??, … , ????  , 
?? ?k=1,2,…,n) is called Transaction, and i??m=1,2, …p? is 
called item.   
Definition 2: Suppose I?{ i?, i?, … , i?  } is a collection 
made up of all the items in D, therefore X , any subset of I, is 
called an itemset of D, and |X| ? k  means collection X is a k 
itemset. Suppose t?  and X respectively are Transaction and 
Itemset in D. If   exists, Transaction t?  will contain 
itemset X. Each Transaction has a unique identifier, called 
TID. 
Definition 3: The number of itemsets X contained in data 
set D is called the supporting number of itemset X, recorded 
as ??. The degree of supporting of itemsets X is recorded as 
support (X): 
          support(X) ??|?|=×100? ?or support(X)= 
??
|?|? 
In this formula, The number of Transaction in the data sets 
is|D|; If support (X) is not less than the minimum support 
specified by users, X is called frequent itemset (or “Large 
Itemset”), otherwise, X is non-frequent itemset (or “Small 
Itemset”). 
Definition 4: If X, Y are Itemsets, and X ? Y ? ? , the 
implication type X ? Y  is called the association rule, in which 
X, Y are respectively known as association rule premise and 
conclusion of the association rule. The support degree of 
itemsets is called the degree of association rules, recorded as: 
support(X ? Y ), support(X ? Y )=support(X ? Y). Association 
rules X ? Y .Confidence level of the association rules is 
recorded as confidence( X ? Y  ) ? confidence ? ? ? ?? ?
??? ?????????
??? ??????? ? 100 . Usually the minimum confidence level 
specified by users’ mining need is recorded as minconfidence. 
     Support and confidence levels are two important 
concepts which describe association rules; the former is for 
measuring the importance of the association rules in the entire 
statistical data, and the latter is used to measure the credibility 
of the association rules. Generally speaking, only the 
association rules of high support and confidence levels can be 
attractive and useful to users. 
   The task of mining association rule is to dig out all of the 
strong rules in D. Strong rules X ? Y  of the corresponding 
itemsets ? X ? Y  ? must be the “Frequency Itemsets”; 
Association rules X ? Y  of the confidence level which is 
derived by “Frequency Itemsets”? X ? Y ?can be calculated 
by the level support of “Frequency Itemsets” X and ?X ? Y ?
. Therefore, mining association rules can be divided into the 
following two sub-questions: 
1)According to the smallest degree of support, we can find 
out all “Frequency Itemsets” in data sets of D. 
2) According to “Frequency Itemsets” and the minimum 
“Confidence Level”, we can generate association rules. 
The basic model of mining Association Rules is shown in 
Figure 1.In this figure, D is data sets; Algorithm-1 is search 
algorithm of the “Frequent Itemsets”; Algorithm-2 is the 
Generated algorithm of the “Association Rules”; R is the 
mined association rules set. Users by specifying minsupport,  
minconfidence separately with the algorithm Algorithm-1, 
Algorithm-2 interaction, and through the results of interaction 
with R excavation interpretation and evaluation. 
 
 
Figure 1. The basic model of mining association rules 
B. Discovery Algorithm of Association Rule  
In order to find out all the full level of support and 
confidence of association rules, we can divide this question 
into the following two sub-questions: 
z Looking for all combinations of items which are from 
those services support more than minimum support, 
these combinations are called large itemsets, while 
other combinations are called small itemsets. 
z The necessary rules which are generated by the large 
itemsets. The general idea: If ABCD and AB are large 
itemsets, we can calculate the ratio of r which is equal 
to the number of supporting for (ABCD) the number 
kX t?
X Y?
4086
 
 
of supporting for (AB), and determine whether the 
rules are strong or not. Only r minconfidence, it is 
strong rules. Attention to this rule, it has the minimum 
level of support, because ABCD is the largest. 
 
IV.    CASE ANALYSIS 
Suppose here is the customer information of a certain air 
cargo company (See Table 1). 
Table 1 The information of the customers of an air cargo company 
Customer Type Customer value 
Customer 
satisfaction Lost or not 
Regular cargo owner Low High No 
Individual cargo 
owner Low High No 
Individual cargo 
owner High High No 
Regular cargo owner Low Average No 
Regular cargo owner High Average No 
Individual cargo 
owner High Average Yes 
Individual cargo 
owner Low Average Yes 
Regular cargo owner Low Low Yes 
Individual cargo 
owner High Low Yes 
Individual cargo 
owner Low Low Yes 
 
Now suppose: 
1) D1={A,B},A stands for regular cargo owner, and B for 
Individual cargo owner; D2={C,D,E,F,G},C?D?E?F?G 
stands for Customer category 1?Customer category 2?
Customer category 3?Customer category 4?and Customer 
category 5 respectively(See Table 2). 
 
Table 2 Customer classification 
Customer 
category Type Meaning 
Category 1 Potential customers 
Patronize one or two times or even no 
patronage 
Category 2 Disloyal customers Patronage occasionally 
Category 3 General customers Patronize at a comparatively high frequency 
Category 4 Loyal customers Patronize at an extremely high frequency 
Category 5 Disconnected customers 
Used to patronize at a high frequency, 
but no patronage now 
D3={H,I,J},H stands for Door-to-door delivery service, I 
for expedited service and J for booking service. 
2) There are 20 Services in the database,namely, |D|=20 
3) The smallest support is 3, and minconfidence is 60?. 
A.  The application of ID3 algorithm 
1) Calculation process 
a) Calculate the initial entropy 
Let lost or not be the category property, therefore there are 
only two categories: Yes or No, that is to say, m=2.The number 
of the sample sets N=10, thus the number of the No category N
?=5 and the number of the Yes category N2=5. According to 
the previous formula:  
H (Loss)= ? ??? ? ???? 5/10 ?
?
?? ? ???? 5/10 ? 1 
b) Calculate the entropies of the non-category properties 
respectively 
? Calculate property A, namely Customer type 
There are 4 regular cargo owners, so n11=4, and the 
number of the No category n11(1)=3 and that of the Yes 
category n11(2 )= 1. 
There are 6 individual cargo owners, so n12=6, and the 
number of the No category n12(1)=2 and that of the Yes 
category n12(2)=4. 
Now A’s conditional entropy can be calculated: 
H(Loss|A)= ??? ? ?
?
? ? ????
?
? ?
?
?? ? ?
?
? ? ???? ?
?
?? ?
?
?? ?
? ?? ? ???? ?
?
?? ?
?
?? ? ?
?
? ? ???? ?
?
?? ? 0.876  
? Calculate property B, namely Customer value 
There are 6 low customer values, so n2=6, and the number 
of the No category n21(1)=3 and that of the Yes category 
n21(2)=3 
There are 4 high customer values, so n22=4, and the 
number of the No category n22(1)=2 and that of the Yes 
category n22(2 )=2 
Now such conclusion H(Loss |B)=1 can be drawn using the 
same method. 
 ? Calculate property C, namely Customer satisfaction 
There are 3 high customer satisfaction, so n31=3, and the 
number of the No category n31(1)=3 and that of the Yes 
category n31(2)=0 
There are 4 average customer satisfaction, so n32=4, and 
the number of the No category n32 (1)=2 and that of the Yes 
category n32 (2)=2 
There are 3 low customer satisfaction, so n33=3, and the 
number of the No category n33 (1)=0 and that of the Yes 
category n33 (2)=3 
So H(Loss |C)=0.4 
?
4087
 
 
c) Calculate the increased information value of each 
property, and select the highest as the root node of the decision 
tree. 
G(A)=H(Loss)- H(Loss |A) =1-0.876=0.124 
G(B)=H(Loss)- H(Loss |B) =1-1=0 
G(C)=H(Loss)- H(Loss |C) =1-0.4= 0.6 
Obviously, what causes the entropy to reduce the most is 
the attribute “Customer satisfaction”, therefore this property is 
made the root of the decision tree. 
B.   The application of Apriori algorithm 
1) TID list in the database 
{{A,C,I,H},{A,C,H},{A,C,I},{A,D,I},{A,E,I},{A,E,H},{A,F,I,
J},{A,F,J},{A,F,J,H},{A,G,I},{B,C,H},{B,C,I},{B,D,I},{B,E,I},{
B,E,J},{B,E,I,J},{B,F,H},{B,F,H,I},{B,F,J},{B,G,I}} 
In order to get ?? by ??, we can apply Apriori algorithm. 
The specific process is as follows: 
z Connect. 
z Prun. 
z Compare the candidate support count and minimum 
support count, and delete the designate option 
which does not meet minimum support count, and 
we can get ??. 
???{{ A,F },{ A,H },{ A,I },{ A,J },{ B,E }, 
{ B,F },{ B,I },{ B,J },{ C,H },{ E,I }} 
This step is repeated until ??cannot get ??, and stop mining 
association rules.  
Once frequent itemsets are obtained in the transactions of 
database D, it is straightforward to generate strong association 
rules by them (Strong association rules satisfy minimum 
support and minimum confidence). Confidence level can be 
calculated from Definition 4. The following will be generated 
by the association rules: 
Each frequent itemset  L generates its all nonempty sets. 
For each of L’s non-empty subset s, if confidence (A B)
min, conf, “s (l-s)” is the output rule, in which min_conf 
stands for minconfidence. 
The frequent item set deduced above is L={A,F}, therefore 
{A},{F} are L’s non-empty subsets. Thereby, the following 
connection rule can be derived from L. 
A F, confidence =3/10=30% 
F A, confidence =3/6=50% 
The minconfidence is 60%, so no rule can be deduced. 
A H, confidence =5/10=50% 
H A, confidence =5/7=71% 
The minconfidence is 60%, so only the second rule can be 
deduced. 
Similarly the results shown in the following Table are 
obtained. 
 
Table 3  Conclusions drawn by connection rule algorithm 
 Rules obtained 
Measures taken 
 (services offered) 
H A 
Generally those who want 
door-to-door delivery service 
are regular cargo owners 
Offer door-to-door delivery 
service to the regular cargo 
owners, but only to high value 
customers because not all of 
them can bring high profits 
E B General customers are 
individual cargo owners 
No measures taken because it 
has nothing to do with the 
subject 
B I 
Individual cargo owners 
usually demand expedited 
service 
The profits individual cargo 
owners bring are not high, 
therefore offer expedited 
service to loyal or high value 
individual cargo owners 
C H 
Potential customers usually 
demand door-to-door 
delivery service 
Offer door-to-door delivery 
service to high value potential 
customers only 
E I General customers usually 
demand expedited service 
The profits general customers 
bring are low, therefore no 
expedited service offered 
 
V.   CONCLUSIONS 
Now that decision tree for the customers of a certain air 
cargo company, we can see from it:  
z It is more likely to keep the customers with high 
customer satisfaction and lose those with low 
customer satisfaction.   
z If the customer satisfaction is average, it is more 
likely to keep the regular cargo owners and lose 
individual cargo owners.  
According to the decision tree knowledge and Table 3, we 
can obtain the specific and necessary measures to solve 
different problems we are face with. For example, individual 
cargo owners generally will require expedited services, and we 
can provide expedited services for the customers with average 
customer satisfaction to keep those who are liable to lose. 
REFERENCES 
[1]  R. Agrawal, R. Srikant. “Fast algorithms for mining association rules.”  
Proceeding of the 20th International Conference on Very Large 
Databases?1994, pp.487-499? 
[2] Jiawei Han, Micheline Kamber, translated by Ming Fan, Xiaofeng 
Meng. “Data mining concepts and skills.” Beijing: Machinery Industry 
Press.  August 2001. 
[3] Zhijing Liu. “A new itemset expression method.” Computer Engineering 
and Design, vol23, 2002,pp.42-44. 
?
? ?
?
?
?
?
?
?
?
?
?
4088
[4] Yuming Qu. “One method to improve the efficiency of Apriori 
algorithm.”  Computer Engineering and Design, vol25, 2004. 
[5] R. Agrawal. “Database Ming: A Performance Prospective.” IEEE 
Transaction on knowledge and data engineering, May 1993,pp.914-925. 
 
  
 
4089
