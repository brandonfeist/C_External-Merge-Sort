Sampling Based N-Hash Algorithm for Searching 
Frequent Itemset 
 
Chen Yong-ming    Zhu Mei-ling 
College of Mathematics, 
Chengdu University of Information Technology, 
Chengdu, China. 
E-mail: chenym@cuit.edu.cn 
 
 
Abstract—Searching frequent itemsets is the critical problem in 
generating association rules in data mining, classic Hash-based 
technique, put forward by J. S. Park, for searching frequent 
itemsets has two shortcomings: one is that it is difficult to choose 
an appropriate hash function; the other is that it is liable to cause 
hash colliding. In order to solve the two problems, Chen Y.M. 
proposed N-Hash algorithm which needn’t to choose hash 
function and avoided hash colliding. In this paper, the sampling 
technique is employed to improve the efficiency of N-Hash 
algorithm. 
Keywords- data mining; association rule; frequent itemset; N-
Hash algorithm;  sampling. 
I.  INTRODUCTION 
Association rule[1] is widely used in business management, 
for example, it can help a supermarket manager to plan 
marketing or advertising strategies, so association rule mining 
has become an important data mining task and a focused theme 
in data mining research. In general, association rule mining can 
be viewed as a two-step process [2]: 
• Step 1 Find all frequent itemsets: By definition, each 
of these itemsets will occur at least as frequently as a 
predetermined minimum support count, min_sup. 
• Step 2 Generate strong association rules from 
frequent itemsets: By definition, these rules must 
satisfy minimum support and minimum confidence. 
Searching frequent itemsets is the critical problem in 
generating association rules, there are several algorithms for 
the task, such as the Apriori algorithm [3], Hash-based technique 
[4]. In 2007, Chen Y.M.[5] pointed out that the classic Hash-
based technique has two shortcomings: one is that it is difficult 
to choose an appropriate hash function; the other is that it is 
liable to cause hash colliding. In order to solve the two 
problems, Chen Y.M. (2007)[5] proposed N-Hash algorithm 
which needn’t to choose hash function and avoided hash 
colliding. But the efficiency of N-Hash method still has room 
to be improved, we will employ sampling technique to make 
N-Hash algorithm more efficient. 
II. PRELIMINARIES 
    For convenience, we firstly list some basic concepts for 
later use, and then we give the Hash-based technique and N-
Hash algorithm a brief introduction. 
A. Basic Concepts[2] 
Let },...,,{ 21 miiiI = be a set of items, Let D , the task-
relevant data, be a set of database transactions where each 
transaction T  is a set of items such that IT ? . Each 
transaction is associated with an identifier, called TID. Let A  
be a set of items. A transaction T is said to contain A  if and 
only if TA ? . An association rule is an implication of the 
form BA?  , where IA ? , IB ? , and ?=? BA . The 
rule BA?  holds in the transaction set D  with support 
s (sometimes is referred to as relative support), where s  is 
the percentage of transactions in D  that contain BA ? , i.e. 
( ) ( )support A B P A B? = ? . The rule BA?  has 
confidence c in the transactions set D , where c is the 
percentage of transactions in D  containing A  that also 
contain B , i.e. ( ) ( | )confidence A B P B A? = . Rules 
that satisfy both a minimum support threshold (min_sup) and a 
minimum confidence threshold (min_conf) are called strong. A 
set of items is referred to as an itemset. An itemset that 
contains k items is a k -itemset. The occurrence frequency 
(or support count) of an itemset is the number of transactions 
that contain the itemset. If the relative support of an itemset I  
satisfies a prespecified minimum support threshold, then I  is a 
frequent itemset. The set of frequent k -itemsets is commonly 
denoted by kL . 
B. Hash-based Technique 
In 1995, by experiments Park discovered that the 
computation of searching the frequent itemsets is mainly 
dominated by searching the 2-itemsets 2L , they proposed 
Hash-based technique[4], which means hashing itemsets into 
corresponding bucket. The Hash-based technique can be used 
to reduce the size of the candidate k -itemsets, kC , for 1k > . 
It performs as follows: When scanning each transaction in the 
database, we can generate all of the 2-itemsets for each 
transaction, by a hash function, hash (i.e. map) those 2-
Supported by the Scientific Research Foundation of CUIT under Grant 
KYTZ201001 and by the Folk Culture Research Centre of Philosophy and
Social Science Key Research Institute in Sichuan Province (MJ09-03). 
978-1-4244-5143-2/10/$26.00 ©2010 IEEE
itemsets into the different buckets of a hash table structure, 
and increase the corresponding bucket counts. The following 
example shows its usage. 
Table 1 gives out the transactional data we considered. 
TABLE I.  TRANSACTIONAL DATA 
TID List of item_IDs 
1 i1, i2, i5 
2 i2, i4 
3 i2, i3 
4 i1, i2, i4 
5 i1, i3 
6 i2, i3 
7 i1, i3 
8 i1, i2, i3, i5 
9 i1, i2, i3 
    Here, the function ( , ) (10 ) mod 7h x y x y= +  is used as 
the Hash function, as for the transaction 1, we can generate 2-
itemsets { i1, i2}, { i1, i5} and { i2, i5}. As for the 2-itemset { i1, 
i2}, calculate (1, 2) (10 1 2) mod 7 4h = × + = , then 4 is the 
bucket address of the 2-itemset {i1, i2}, the 2-itemset {i1, i2} is 
sometimes called bucket content of bucket address 4. After the 
other transactions and 2-itemsets have been dealt with, we can 
get a Hash table (see table 2). 
TABLE II.      HASH TABLE GOT BY CLASSIC HASH TECHNIQUE 
Address 0 1 2 3 4 5 6 
Count 2 2 4 2 2 4 4 
Content {i1, i4} {i1, i5} {i2, i3} {i2, i4} {i2, i5} {i1, i2} {i1, i3} 
 {i3, i5} {i1, i5} {i2, i3} {i2, i4} {i2, i5} {i1, i2} {i1, i3} 
   {i2, i3}   {i1, i2} {i1, i3} 
   {i2, i3}   {i1, i2} {i1, i3} 
   From table 2, we see that in address 0, there are two 
different bucket contents, content 1 4{ , }i i and content 3 5{ , }i i , 
this phenomenon is Hash colliding.  
    To avoid Hash colliding, Chen Y.M.[5] put forward N-Hash 
Algorithm to generate Hash table. 
C. N-Hash Algorithm 
1) The main idea of N-Hash Algorithm 
    The k-dimensional vector is employed to stand for k-itemset, 
as for the bucket content which need to be put into a bucket 
address, if the bucket address to contain it had been existed, 
this bucket is called old bucket, we put the content into 
corresponding old bucket address, at the same time, the bucket 
count increased by 1; if the bucket address to contain it had 
not been existed, a new bucket address is added to contain it, 
at the same time, the bucket count of the new bucket address is 
1. The idea comes from our daily life: a technique to check 
whether a playing card is lost or not. So the method is called 
Hash algorithm based on naturally assigning bucket, for 
brevity, it is called N-Hash Algorithm.  
2) Process  of N-Hash Algorithm 
Step 1 Denote the set of existed bucket address as A , firstly 
let A = ? ; 
Step 2 Scan the transaction database, take out the transaction 
one by one, when got the 2-itemset{ , }s ti i from the 
transaction that is being dealt with, use 2-dimensional 
vector ( , )s t  as the bucket address of { , }s ti i , let stn  
be the count of { , }s ti i , check A , if ( , )s t A? , then 
let ( , )s t A?  and 1stn = ; if ( , )s t A? , then let 
1st stn n= + .  Repeat step 2 until all transaction have 
been dealt with, then we obtain a Hash table  
Steps 3 According to prespecified minimum support, frequent 
itemsets are generated. 
3) Example of N-Hash Algorithm 
    Take the data in Table 1 as an example, we will show how 
N-Hash Algorithm works. 
    Step 1 Set A = ? ; 
    Step 2 Scan the transaction database, take out the 
transaction one by one. As for transaction 1, generate the 
following 2-itemsets: 1 2{ , }i i , 1 5{ , }i i  and 2 5{ , }i i . As for 
1 2{ , }i i ,  since (1, 2) A? , then let {(1, 2)}A =  and 
12 1n = .  As for 1 5{ , }i i , since (1,5) {(1,2)}A? = , then let 
{(1,2), (1,5)}A =  and 15 1n = . As for 2 5{ , }i i , since 
(2,5) {(1, 2), (1,5)}A? = , then let 25 1n =  and 
{(1, 2), (1,5), (2,5)}A = . 
    As for transaction 2, generate the 2-itemset 2 4{ , }i i , as for 
2 4{ , }i i , since (2, 4) {(1, 2), (1,5), (2,5)}A? = , then let 
{(1,2), (1,5), (2,5), (2, 4)}A =  and 24 1n = . 
    As for transaction 3, generate the 2-itemset 2 3{ , }i i , as for 
2 3{ , }i i , since (2,3) {(1,2), (1,5), (2,5), (2,4)}A? = , 
then let {(1, 2), (1,5), (2,5), (2,4), (2,3)}A =  and 23 1n = . 
As for transaction 4, generate the following 2-itemsets: 
1 2{ , }i i , 1 4{ , }i i  and 2 4{ , }i i . As for 1 2{ , }i i , since (1, 2) A? , 
then let 12 1 1 2n = + = . As for 1 4{ , }i i , since 
(1, 4) {(1, 2), (1,5), (2,5), (2, 4), (2,3)}A? = , then let 
{(1,2), (1,5), (2,5), (2, 4), (2,3), (1, 4)}A =  and 14 1n = . 
As for 2 4{ , }i i , since 
)}4,1(),3,2(),4,2(),5,2(),5,1(),2,1{()4,2( =? A , then 
let 24 1 1 2n = + = . 
As for the other transactions and 2-itemsets, we can deal 
with them similarly. Finally, we obtain a Hash table shown in Table 3 
TABLE III.   HASH TABLE GOT BY N-HASH ALGORITHM 
Address (1, 2) (1, 5) (2, 5) (2, 4) (2, 3) (1, 4) (1, 3) (3, 5) 
Count 4 2 2 2 4 1 4 1 
Content {i1, i2} {i1, i5} {i2, i5} {i2, i4} {i2, i3} {i1, i4} {i1, i3} {i3, i5} 
 {i1, i2} {i1, i5} {i2, i5} {i2, i4} {i2, i3}  {i1, i3}  
 {i1, i2}    {i2, i3}  {i1, i3}  
 {i1, i2}    {i2, i3}  {i1, i3}  
III. IMPROVEMENT ON N-HASH ALGORITHM 
A. Aadvantages and disadvantages of N-Hash Algorithm 
N-Hash algorithm has some advantages. Its rationale is 
simple and clear, it can be carried out without any Hash 
function and will not encounter Hash colliding problem, it 
absorbed the merits of 2-dimensional Hash algorithm [6] which 
employ 2-dimensional vector to represent bucket address. N-
Hash algorithm generate the appropriate bucket address just 
for the bucket content, the number of address is the most but 
necessary and the least but sufficient number needed to avoid 
Hash colliding, that is it is the best value.  
However, N-Hash algorithm also has disadvantages, there 
is still room for it to be improved. When we put some bucket 
content, i.e. a itemset, into its corresponding bucket address, 
we need to check those existed addresses to see whether one 
of them matches or not, the process to do this is fairly time-
consuming. Take Table 3 as example, suppose that we have a 
2-itemset on hand to be handled, say 1 3{ , }i i , we have to check 
the previous 6 addresses: (1, 2) , (1,5) , (2,5) , (2,4) , 
(2,3) and (1, 4) , the 7th address (1,3)  is the one we want, to 
find the address (1,3)  for 1 3{ , }i i  cost us 7 times of matching, 
so much time is wasted. In the next, we use sampling methods 
to solve the problem and to further improve the efficiency of 
N-Hash algorithm.  
B. Sampling method 
Sampling method is a popular method in computational 
statistics; two important terminologies related to it are 
population and sample.  The population is defined in keeping 
with the objectives of the study, a sample is a subset of 
population. Usually, when the population is large, if the 
sample is scientifically chosen, it can be used to represent the 
population, because the sample reflects the characteristics of 
the population from which it is drawn.  
C. Main idea of  sampling based N-Hash algorithm 
Here, sampling method is employed to improve N-Hash 
algorithm. Usually, in data mining, the population is large, so 
the sampling method is appropriate. Take the above database 
as an example, suppose that the transaction data in Table 1 is a 
carefully chosen sample of some population P (Certainly the 
sample here is too small, in practice, a sample will often be 
larger, we just use a small sample to demonstrate our idea), 
after the sample has been dealt with, we got the Hash table 
(see Table 3). From table 3, we discovered that the occurrence 
frequency of 1 3{ , }i i  is high, but there are so many addresses 
before 2-itemset 1 3{ , }i i ’s address (1,3) , so much time is 
wasted in finding 1 3{ , }i i ’s corresponding address (1,3) . If 
there are less addresses before the address (1,3) , not so much 
time will be needed. Therefore using sampling method can 
save much time, if the sample is carefully chosen, the sample 
can represent the population, and then the Hash table that 
comes from the sample can represent that comes from the 
population, the 2-itemsets with high frequency in sample’s 
Hash table are liable to be the one with high frequency in 
population’s Hash table. So we propose a method that 
combines sampling method and N-Hash algorithm to improve 
the efficiency of N-Hash algorithm, the method is called 
sampling based N-Hash algorithm. To search for frequent 
itemsets, the sampling based N-Hash algorithm works as 
follows: 
Step 1 Carefully draw a sample S  from the population P , 
usually by random sampling; 
Step 2  Use N-Hash algorithm to deal with the sample S  
to  get a Hash table, denoted as Hash table SH ;  
Step 3  Rank the Hash table SH with respect to the 
frequency of bucket content in order to make the bucket 
address with high frequency lie in the former and that with 
low frequency the latter, then we get a new Hash table SRH ; 
Step 4 Based on SRH , Use N-Hash algorithm to deal with 
the rest sample of the population P , i.e. P S? , when 
finished, get a Hash table denoted as PH ; 
Step 5 Obtain frequent itemsets according to 
predetermined minimum support count. 
D. Demonstration of Sampling Based N-Hash algorithm 
In the above step 1, in practice, draw sample usually by 
random method, but our sample is rather special here, the 
reason why we do like this is that our main purpose is to  
make it convenient to demonstrate how our algorithm works in 
a simple way, and convenient to show our algorithm’s 
efficiency.  
After our above explanation, suppose we have database to 
be dealt with shown in Table 4. 
TABLE IV.      DATABASE USED TO SHOW SAMPLING BASED N-HASH 
ALGORITHM 
TID List of item_IDs 
1 i1, i2, i5 
2 i2, i4 
3 i2, i3 
4 i1, i2, i4 
5 i1, i3 
6 i2, i3 
7 i1, i3 
8 i1, i2, i3, i5 
9 i1, i2, i3 
10 i1, i2, i5 
11 i2, i4 
12 i2, i3 
13 i1, i2, i4 
14 i1, i3 
15 i2, i3 
16 i1, i3 
17 i1, i2, i3, i5 
18 i1, i2, i3 
    We use sampling based N-Hash algorithm to get Hash table 
denoted as PH  with respect to database in Table 4 
Step 1 Draw a sample S  from the population P , shown 
in Table 4; 
Step 2  Use N-Hash algorithm to deal with the sample S  
to  get a Hash table, denoted as Hash table SH , shown in 
Table 3;  
Step 3  Rank the Hash table SH with respect to the 
frequency of bucket content in order to make the bucket 
address with the high frequent content lie in the former and 
that with low frequent content the latter, get a new Hash table 
SRH , shown in Table 5; 
Step 4 Based on SRH , Use N-Hash algorithm to deal with 
the rest sample of the population P , i.e. P S? , when 
finished, get a Hash table denoted as PH , shown in Table 6; 
Step 5 Obtain frequent itemsets according to 
predetermined minimum support count. If we set support 
count as 6, we find that 2-itemset 1 2{ , }i i , 2 3{ , }i i  and 1 3{ , }i i  
are frequent. 
In this example, comparing with N-Hash, approximately 
24.6% matching times ((86-69)/69?24.6%) has been saved in 
dealing with the transactions in P-S.
TABLE V.      HASH  TABLE   SRH  
Address (1, 2) (2, 3) (1, 3) (1, 5) (2, 5) (2, 4) (1, 4) (3, 5) 
Count 4 4 4 2 2 2 1 1 
Content {i1, i2} {i2, i3} {i1, i3} {i1, i5} {i2, i5} {i2, i4} {i1, i4} {i3, i5} 
 {i1, i2} {i2, i3} {i1, i3} {i1, i5} {i2, i5} {i2, i4}   
 {i1, i2} {i2, i3} {i1, i3}      
 {i1, i2} {i2, i3} {i1, i3}      
TABLE VI.      HASH  TABLE   PH  GOT BY  SAMPLING BASED N-HASH ALGORITHM 
Address (1, 2) (2, 3) (1, 3) (1, 5) (2, 5) (2, 4) (1, 4) (3, 5) 
Count 8 8 8 4 4 4 2 2 
Content {i1, i2} {i2, i3} {i1, i3} {i1, i5} {i2, i5} {i2, i4} {i1, i4} {i3, i5} 
 {i1, i2} {i2, i3} {i1, i3} {i1, i5} {i2, i5} {i2, i4} {i1, i4} {i3, i5} 
 {i1, i2} {i2, i3} {i1, i3} {i1, i5} {i2, i5} {i2, i4}   
 {i1, i2} {i2, i3} {i1, i3} {i1, i5} {i2, i5} {i2, i4}   
 {i1, i2} {i2, i3} {i1, i3}      
 {i1, i2} {i2, i3} {i1, i3}      
 {i1, i2} {i2, i3} {i1, i3}      
 {i1, i2} {i2, i3} {i1, i3}      
REFERENCES 
[1] R. Agrawal, T. Imielinski and A. Swami. “Mining association rules 
between sets of items in large databases”, In Proceedings of the ACM-
SIGMOD International Conference on Management of Data, pages 
207-216, Washington D.C.,  May 1993. 
[2] Jiawei Han and Micheline Kamber, Data Mining Concepts and 
Techniques (2nd edition), Beijing: China Machine Press, 2006. 
[3] R. Agrawal, R.Srikant. “Fast algorithms for mining association rules”, 
In Proc. 1994 Int. Conf. Very Large Data Bases, pages 487-499, 
Santiago, Chile, Sept. 1994. 
[4] J. S. Park, M. S. Chen, and P. S. Yu. “An effective hash-based 
algorithm for mining association rules”. In Pro. 1995 ACM-SIGMOD 
Int. Conf. Management of Data (SIGMOD’95), pages 175-186, San 
Jose, CA, May 1995.. 
[5] Chen Yong-Ming, Xie Hai-Ying, “Hash Algorithm Based on Naturally 
Assigning Bucket for Searching Frequent Item-set”,  Journal of Data 
Analysis, vol. 3, No. 5, pp. 47-54, 2008. (in Chinese) 
[6] HE Xiao-wei, “Two Dimension Hash Algorithm of Large Itemsets of 
Apriori Algorithm”, Computer and Modernization, No. 4, pp. 10-12, 
2003. (in Chinese) 
Sam
ple 
Population 
