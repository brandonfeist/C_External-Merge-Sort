 An Adaptive Method for Discovering Maximal Frequent Itemsets 
To large databases 
 
     V.Chandra Shekhar Rao  P. Geetha    P.K.Vaishali 
      2-1-29/b, Saraswathinagar            1-7-1329, Advocates colony  2-10-1286. 
      Hanamkonda, Dist Warangal Hanamkonda, Dist Warangal  Jyothinagar, Karimnagar 
      Andhra Pradesh, India, 506001 Andhra Pradesh, India, 506001 Andhra Pradesh, India             
      vcsrao.kitswgl@gmail.com Geetha_pallepati@yahool.com    vaishali5599@gmail.com 
      09052452294                    09866661646    0970255599 
           
        
              
Abstract 
 
A novel adaptive method included two phases 
for discovering maximal frequent itemsets is roposed. A 
flexible hybrid search method is given, which exploits 
key advantages of both the top-down strategy and the 
bottomup strategy. Information gathered in the bottom-
up can be used to prune in the other top-down 
direction. Some efficient decomposition and pruning 
strategies are implied, which can reduce the original 
search space rapidly in the iterations. The compressed 
bitmap technique is employed in the counting of 
itemsets support. According to the big space 
requirement for the saving of intact bitmap, each bit 
vector is partitioned into some blocks, and hence every 
bit block is encoded as a shorter symbol. Therefore the 
original bitmap is impacted efficiently. Experimental 
and analytical results are presented in the end. 
 
Key words: frequent items, apriori, support count. 
Association rule mining, Datamining,  
 
1.Introduction 
 
Frequent itemsets mining was first proposed by 
Agrawal[1] for market basket analysis in the form of 
association rule mining. A major challenge in mining 
frequent itemsets from a large data set is the fact that 
such mining often generates a huge number of patterns 
satisfying the min_sup threshold, especially when 
min_sup is set low. This is because if a pattern is 
frequent, each of its subsets is frequent as well. A large 
pattern will contain an exponential number of smaller, 
frequent subpatterns. To overcome this problem, closed 
frequent itemsets(CFI) and maximal frequent 
itemsets(MFI) mining were proposed[2]. An itemset _ is 
a maximal frequent itemset (or max-itemset) in set D if 
_ is frequent, and there exists no super-pattern _ such 
that _ • • _ and _ is frequent in D. For the same min_sup 
threshold, the set of max-itemsets, which is more 
compact, contains the complete information regarding 
to its corresponding frequent itemsets.  
 
1.1 Existing methods 
 
In this paper first discussed on association rule 
mining, compared some algorithms on frequent itemsets 
mining. Frequent itemsets mining is first proposed by 
Agrawal[1] for market basket analysis in the form of 
association rule mining. A major challenge in mining 
frequent itemsets from a large data set is the fact that 
such mining often generates a huge number of patterns 
satisfying the min_sup threshold, especially when 
min_sup is set low. This is because if a pattern is 
frequent, each of its subsets is frequent as well. A large 
pattern will contain an exponential number of smaller, 
frequent sub patterns. 
 
To achieve this problem, closed frequent 
itemsets   CFI) and maximal frequent itemsets (MFI) 
mining ere proposed [2]. An itemset á is a maximal   
requent itemset (or max-itemset) in set D if á is 
frequent, and there exists no super-pattern â such that á 
• • â and â is frequent in D. For the same min_sup 
threshold, the set of max-itemsets, which is more 
compact, contains the complete information regarding 
to its corresponding frequent itemsets. Mining max- 
itemsets was first studied by Bayardo[3], where an 
Apriori-based, level-wise, breadth first search method 
was proposed to find max-itemset by performing 
superset frequency pruning and subset infrequency 
pruning for search space reduction.  
 
The above  zlgorithms approach is bottom up, 
but the Pincer Search algorithms approach is both the 
bottom up and top down method. Pincer- Search[4] uses 
horizontal data format. It not only constructs the 
candidates in a bottom-up manner like Apriori, but also 
starts a top-down search at the same time, maintaining a 
candidate set of maximal patterns. Depth-Project [5] 
finds long itemsets using a depth first search of a 
lexicographic tree of itemsets, and uses a counting 
method based on transaction projections along its 
branches.  
 
2010 International Conference on Advances in Recent Technologies in Communication and Computing
978-0-7695-4201-0/10 $26.00 © 2010 IEEE
DOI 10.1109/ARTCom.2010.56
421
 Mafia [6] is another efficient method 
formining the MFI, which uses three pruningstrategies 
to remove non-maximal sets and a vertical bit-vector 
data format to improve performance. Both Depth 
Project and Mafia mine a superset of the MFI, and 
require a post pruning toeliminate non-maximal patterns 
this algorithm isespecially efficient when the itemsets in 
the database are very long..  
 
FP-Growth[7] uses the novel frequent pattern tree 
(FP-tree)structure, which is a compressed representation 
of all the transactions in the database, and a recursive 
divide-and conquers and database projection approach 
to mine long patterns. A portioning-based, divide and 
conquer method is used to decompose the mining task 
into a set of smaller tasks for mining confined patterns 
in conditional databases, which dramatically reduces 
the search space. 
 
The complexity of enumerating maximal itemsets 
is shown to be NP-hard. Ramesh et al.[9]characterized 
the length distribution of frequent and maximal frequent 
itemset collections. 
 
1.2 Proposed System 
 
A modern adaptive approach method proposed 
included two phases for discovering maximal frequent 
itemsets is proposed. A flexible hybrid search method is 
given, which exploits key advantages of both the top-
down strategy and the bottom up strategy. Information 
gathered in the bottom-up can be used to prune in the 
other topdown direction. Some efficient decomposition 
and pruning strategies are implied, which can reduce 
the original search space rapidly in the iterations. The 
compressed bitmap technique is employed in the 
counting of itemsets support. According to the big 
space requirement for the saving of intact bitmap, each 
bit vector is partitioned into some blocks, and hence 
every bit block is encoded as a shorter symbol. 
Therefore the original bitmap is impacted efficiently.. 
 
 
1.2 Contributions 
 
Firstly, support counting is a confessed bottleneck in 
the association rules mining, which requires a great I/O 
and computing cost. A novel compressed bitmap index 
technique to speed up the counting process is employed. 
The presented algorithm could reduce both the number 
oftimes the database is scanned and the search space 
rapidly. Secondly, a flexible hybrid search method is 
given, which exploits key advantages of both top-down 
and bottom-up strategies.  The rest of this paper is 
organized as follows. Section 2 discusses the problem 
of itemset support counting based on compressed 
bitmap technology. Section 3 introduces the description 
and decomposition strategies of search space. Section 4 
presents the algorithm and some pruning strategies. The 
feasible optimizations and experimental results are 
presented Section 5. Finally, we conclude this paper in 
Section 6. 
 
2. Itemset Support Counting 
 
The bitmap technique was proposed in the 1960’s, 
and has been used by a variety of products. A typical 
context is the modern relational DBMS[10]. It also has 
been applied in association mining. The key idea of the 
approach is to use a bitmap index to determine which 
transactions contain which itemsets. Each transaction 
has one unique offset position in the bitmap. A bit 
vector ) X,bit=(b1,b2 bn)  is associated to each itemset X. 
In X.Bit . the ith bit i bi is set to 1 if the transaction I 
contains the itemset X, and otherwise i b is set to 0. It 
should be noted that i b is set to null if itemsets or 
transaction does not exist. The ordered collection of 
these bit vectors composes the bitmap. In a bitmap, a 
line represents a transaction, while a column 
corresponds to a given k-itemset. During the supports 
counting, Intensively manipulates bit vectors requires a 
lot of disk space and main memory. So, it is necessary 
to compress the bitmap in advance. A new block 
strategy is proposed to encode and decode the bitmap, 
which is similar to the pagination technology in 
operating systems. In this approach, every bit vector is 
partitioned into fractions, called blocks, that can be 
encoded respectively, so that a bitmap is divided into 
granules. Each block should have an appropriate size, if 
the size is too small, the impact is not remarkable; 
otherwise the encoding is not straightforward. In order 
to take full advantage of Logical Calculation Units, the 
block size should be an exponential to 2. Each block is 
represented as  (p:W), p is the number of the block, and 
W is the block bit vector. Let l be the block size, m the 
number of transactions in database D. In this way each 
bit vector B=(b1,b2,…bi…bm) can be partitioned into p= 
INT( mll)  blocks ( INT is the integer function). The kth 
block vector  Wk=(w1,w2….wj),j=MOD(I,l)=i-i*(k-l) 
(MOD is the mode function)  
 
Encoding each block as a shorter code can reduce the 
space demanding. The encoding principle which should 
be conformed is that each block can be represented 
uniquely. As a part of the initial bit vector B, each block 
vector W is also a binary bit vector. The conversion 
between binary, octal, decimal and hexadecimal can be 
implemented conveniently, hereby every block can be 
represented a binary, octal, decimal or hexadecimal 
code. We use a hexadecimal code, i.e. every four bits in 
a block encode a hexadecimal code. As each itemset is 
associated to a binary bit vector, the support of a given 
itemset is the total number of 1 in the vector. For the 
sake of efficient counting the number of 1, we 
previously store the binary block in a bit array Bit[1..l] 
(l is the block size), and the hexadecimal 
422
 blocks in an array ABit[1..p] (p is the number of 
blocks). The value in  ABit[i] is the hexadecimal code 
of the ith block. Implementation of this support 
counting algorithm follows. 
 
2.1 Proposed algorithm to Itemset support 
Counting 
 
Algorithm 1 Itemsets Support Counting 
Algorithm Countsupport(X1, X2) 
Begin 
Support=0; 
For (i=1; i=p; i++) do 
If X1.ABit[i]<>0 and X2.ABit[i]<>0 then 
For (j=1; j=l; j++) do 
X1.Bit[j]<>= X2. Bit[j]& X2. Bit[j]; 
Support+= X.Bit[j]; 
Endfor; 
X.ABit[i]= X1. ABit[i]& X2. ABit[i]; 
Else 
X.ABit[i]=0; 
endif;   end; 
 
3. Description of the Search Space 
 
With only a limited amount of main memory 
in practice, we should decompose the original search 
space into some smaller pieces, such that each one can 
be solved independently in main memory. Following 
the above description and partition mechanisms, the 
original enormous search space could be partitioned 
into some little ones as flexibly as possible. 
Furthermore, theorem1 can be used to prune the search 
space. The search space for item set I={a,b,c,d} is 
S=[:I]. It can be partitioned into some little ones step by 
step. The iterative results of a hierarchy of search space 
are shown in figure 1. 
 
 
 
 
 
For a given item set I in database D and the minimum 
support threshold min_sup, the task of mining FI or MFI 
is in the follow: finding the set (x\x?[:l] and 
Supp(x)>=minsup) in the search space S=[:I]. 
 
4. Discovering Max-itemsets 
 
4.1 Search and Pruning Strategy 
In general, it is possible to search for the maxitemsets 
either top-down or bottom-up. The bottom-up approach 
is good for the case when all max-itemsets areshort, and 
the top-down approach is good when all maxitemsetsare 
long. If some max-itemsets are long and some are short 
in the mining database, then both search approaches 
will not be efficient. A key idea of our hybrid approach 
is the use of information gathered in the search in the 
bottom-up to prune search space during the top down 
search. It uses infrequent itemsets found in the search in 
the bottom-up to prune search space during the top-
down search For search space S=[X:Y], if the number of 
infrequent itemsets containing X is large, the scale of S 
will be reduced rapidly.  
 
4.2 The Hybrid Max-Itemsets Search 
Algorithm 
There are two phases in this hybrid approach: the search 
in bottom-up direction and the other in top-
downdirection for every pass. Max-itemsets are 
enumerated in both bottom-up and top-down directions. 
Consider a pass k, the set of frequent k-itemsets Lk and 
the set of infrequent k-itemsets ~Lk are to be classified 
in the bottom-up direction. This procedure repeatedly 
uses Apriori-gen algorithm to generatecandidates like 
the Apriori[1]. During the kth pass, every search space 
S• •[X:Y] where X is an itemset of size k-1 can be 
decomposed into some little pieces, whose ancestors are 
k-itemsets. For search space S• [X:Y], the top-down 
procedure check whether the border element (i.e.  XUY 
) of S is frequent firstly, if not, S is decomposed. 
Implementation of this hybrid approach is 
shown in algorithm2. 
 
Algorithm2. Algorithm for Max-itemsets Mining 
Procedure: MFI Search (Transaction Set: D, Item 
 
423
  
 
 
 
5. Experimental and Analytical Results 
 
In this algorithm bitmap technology is used to 
count the support of every itemset instead of scanning 
the entire transaction database. Figure 2 shows the 
relative times at varying numbers of transactions for 
databases where the average size of transactions is 10 
and the average size of potential max-itemsets is 4. 
 
 
When the average size of transactions or the average 
size of max-itemsets increase, there has much more 
itemsets (or search spaces) to be tested. therefore the 
total time will increase. Figure 3 shows the relative 
times of this hybrid algorithm at varying minimal 
supports on the datasets of T15.I8.D10K. 
 
 
 
 
 
To illustrate expandability of this algorithm, we 
performed an experiment varying the database size 
from 5K to 20K. The average size of transactions is 10, 
and the average size of potential max-itemsets is 6. For 
the experiment we fixed a minimum support of 4%. 
Figure 4 shows the result for the datasets. 
 
 
 
As we have shown, there are three search strategies 
for discovery MFI. Figure 5 shows the relative times of 
the three approaches for the tests at varying minimal 
supports on T10.I6.D10K. From the experiments, we 
could see that when minimal support is greater than 2%, 
the performances of the bottom-up is little better than 
the hybrid. The main reason is that the number of 
itemsets generated is small with the increasing of 
minimal support. As the minimal support decreases, 
MFI becomes longer, which results in an increase in the 
number of counting itemsets. In such a case, the hybrid 
has performances. We can also see performance of the 
bottom-up approach is lower than the others. The most 
primary factor is almost all the max-itemsets are 
expected to not be long in this T10.I6.D10K dataset. 
The experiment illustrates the fact that top-down search 
might be efficient for the long maxi temsets. 
 
 
 
 
 
 
424
 6. Conclusions 
 
We use an improved compacting bitmaps database 
format. Support of itemset can be counted by means of 
binary bit vectors intersections, which minimizes the 
I/O and computing cost. To reduce the disk and main 
memory space demanding, we break the bitmap down 
into some little blocks, which can be encoded as a 
shorter code. The blocks of bitmaps are fairly adaptable. 
Hence the additional space decreases rapidly. The 
hybrid approach exploits key advantages of both the 
top-down strategy and the bottom-up ones, which can 
discovery both longer max-itemsets and the shorter 
ones in earlier passes. And the infrequent (or frequent) 
itemsets discovered in the bottom-up can also be used 
to prune the search space in the other top-down 
direction. Furthermore, this algorithm can be 
parallelized easily on this hierarchical search space 
organization. We note that using ~Lk to prune the 
search space is not the only technique. If  it 
would be more efficient to decompose and prune the 
search space using Lk rather than ~Lk. 
  
 Too many candidate itemsets and a large 
database would create a performance bottleneck,  if we 
apply the pipeline methodology using hardware.  So we 
effectively reduce the frequency of loading the database 
into the hardware. 
 
Acknowledgements 
 
Thnks very much to prof Fu-zan Chen,and prof 
Min-qiang Li published lot of research work in this rea.  
 
References 
 
[1]. R. Agrawal, T. ImielinSki, A. Swami, Mining 
Association rules between sets of items in large 
database. Proc. of the ACMSIG2 MOD 
International Conference on Management of Data, 
Washington, DC.1993, 2 : 207-216 
 
[2]. Jia wei Han, et al, Frequent pattern mining: current 
status and future directions, Data mining and 
Knowledge Discovery, 2007 V(15) 55-86 
 
[3]. Bayardo R. Efficiently mining long patterns from 
databases. In: Proc. of the ACM  IGMOD, Int’l Conf. 
On Management of Data. New York: ACM Press. 1998. 
85-93  
 
[4]. Dao-I Lin, Zaki M. Kedem, Pincer Search: A New 
Algorithm for Discovering the Maximum Frequent Set, 
Proceedings of the 6th International Conference on 
Extending Database Technology. 1998. 105-119 
 
[5]. R. Agrawal, C. Agarwal, Depth first generation of 
Long patterns, 7th International conference on 
Knowledge discovery and Data mining. 2000. 108-118 
[6]. D. Burdick, M. Calimlim and J. Gehrke, MAFIA: A 
Maximal Frequent Itemset Algorithm for Transactional 
Databases, Proc. of the 17th Int' l Conf. on Data 
Engineering. 2001. 443-452 
 
[7]. J.P. Han, Y. Yin. Mining frequent patterns without 
candidate generation. In ACM SIGMOD Conf., May 
2000. 1-12 
 
[8]. Guizhen Yang. The complexity of mining maximal 
frequent itemsets and maximal frequent patterns. In: 
Proceeding of the 2004 ACM SIGKDD international 
conference on knowledge discovery in databases 
(KDD’04), Seattle,WA, 344–353 
 
[9]. Ramesh G, Maniatty WA, Zaki MJ (2003) Feasible 
itemset distributions in data mining: theory and 
application. In: Proceeding of the 2003 ACM 
symposium on principles of database systems 
(PODS’03), San Diego, CA, 284–295  
 
[10]. Mikolaj Morzy, Hierarchical Bitmap Index An 
Efficient and Scalable Indexing Technique for Set- 
Valued Attributes, ADBIS 2003, LNCS 2798, 2003, 
236–252 
 
[11] Rymon, E,1992. Search through Systematic,Set 
Enumeration, In Proc. Of tjird Int conf on Principles of 
Knowledge representation and Reasong.593-550 
 
Agarwal R,Srikant R919950 Mining Sequential patterns 
in proceedings of the 1995 International conference on 
data engineering(ICDE’95) Taipei, Taiwan,pp 3-14. 
 
Han J.Kamber M(2006) Data Mining concepts 
techniques, 2nd edn. Morgan Ksufmann 
 
Kamber. M. Han J.Chiang JY(1997) Metarule-guidd 
mining of multi-dimensional association rules using 
data cubes, In: Proceeding of the 2005 ACM SIGKDD 
international conference on knowledge discovery and 
data mining(KDD’97),Newport,Beach,CA,pp 207-210  
 
A Hybrid method for discovering Maximal frequent 
Itemsets Fu-zan Chen, Min-quang Li,Fifth International 
conference on fuzzy systems and knowledge discovery 
970-07695-3305-6108 2008 IEEE 
 
A Two-way Hybrid Algorithm for Maximal Frequent 
Itemsets Mining, Fu-zan Chen, Min-qiang Li, School of  
Management, Tianjin University Tianjin, 300072, 
China 
 
 
 
 
 
 
 
425
