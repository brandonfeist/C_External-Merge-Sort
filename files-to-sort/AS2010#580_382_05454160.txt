A Fuzzy Associative Classification System with
Genetic Rule Selection for High-Dimensional
Problems
J. Alcala´-Fde´z, R. Alcala´ and F. Herrera
Department of Computer Science and Artificial Intelligence
CITIC-UGR, University of Granada
Granada, Spain, 18071
Email: {jalcala, alcala, herrera}@decsai.ugr.es
Abstract—The learning of Fuzzy Rule-Based Classification
Systems for High-Dimensional problems suffers from exponential
growth of the fuzzy rule search space when the number of
patterns and/or variables becomes high.
In this work, we propose a fuzzy association rule-based classi-
fication method with genetic rule selection for high-dimensional
problems to obtain an accurate and compact fuzzy rule-based
classifier with low computational cost. The results obtained
from the comparison with other two genetic fuzzy systems over
nine real-world datasets with different characteristics show the
effectiveness of the proposed approach.
I. INTRODUCTION
There are many real applications in which Fuzzy Rule-
Based Classification Systems (FRBCSs) [1] have been em-
ployed. However, in most of them, the available data consists
of a high number of patterns and/or variables. In this situation,
the learning of FRBCSs suffers from exponential growth
of the fuzzy rule search. This growth makes the learning
process more difficult and, in most cases, leads to problems
of scalability and/or complexity [2].
Association discovery is one of the most common Data
Mining techniques used to extract interesting knowledge from
large datasets [3]. Many efforts have been made to use its
advantages for classification under the name of associative
classification [4], [5], [6].
A typical associative classification system is constructed in
two stages:
1) Discovering the association rules inherent in the
database.
2) Selecting a small set of relevant association rules to
construct a classifier.
In order to enhance the interpretability of the obtained
classification rules and to avoid unnatural boundaries in the
partitioning of the attributes, different studies have been
presented to obtain classification systems based on fuzzy
association rules [7], [8].
Supported by the Spanish Ministry of Education and Science under grant
no. TIN2008-06681-C06-01.
In this paper, we present a Fuzzy Association Rule-based
Classification method for High-Dimensional problems (FARC-
HD) with genetic rule selection to obtain an accurate and
compact fuzzy rule-based classifier with a low computational
cost. This method is based on three stages:
1) Fuzzy association rule extraction for classification: A
search tree is employed to list all possible frequent
fuzzy itemsets and generate fuzzy association rules for
classification.
2) Candidate rule prescreening: We consider the use of
subgroup discovery to select the most interesting rules
by means of a pattern weighting scheme [9].
3) Genetic fuzzy rule selection: We consider the use of
Genetic Algorithms (GAs) [10], [11] to select a compact
set of fuzzy association rules with high classification
accuracy.
In order to assess the performance of the proposed approach,
we have used nine real-world datasets with a number of vari-
ables ranging from 19 to 60 and a number of patterns ranging
from 208 to 7400. We have shown the results obtained from
the comparison with other two genetic fuzzy systems (GFSs)
and we have made use of some non-parametric statistical tests
for multiple comparison [12], [13], [14] of the performance of
these classifiers. Furthermore, we have analyzed the scalability
of the proposed approach.
This paper is arranged as follows. The next section briefly
introduces the fuzzy association rules for classification. Sec-
tion III describes in detail each stage of the proposed approach.
Section IV shows and discusses the results obtained on the
nine real-world datasets. Finally, in Section V some conclud-
ing remarks are made.
II. PRELIMINARIES: FUZZY ASSOCIATION RULES FOR
CLASSIFICATION
Fuzzy set theory has been used more and more frequently in
Data Mining to represent and to identify dependencies between
attributes in a database [1]. The use of fuzzy sets to describe
associations between data extends the types of relationships
that may be represented [15]. For this reason, in recent years
different studies have proposed methods for mining fuzzy
association rules from quantitative data [16], [17].
Let us consider a simple database N with two attributes (age
and weight) and three linguistic terms with their associated
MFs (see Fig. 1). Based on this definition, a simple example
of a fuzzy association rule could be:
Age is Low ?Weight is Middle
Fig. 1. Attributes and linguistic terms in a simple problem
Support and confidence are the most common measures of
interest of an association rule. These measures can be defined
for fuzzy association rules as follows:
Support(A? B) =
?
xp?N µAB(xp)
| N | (1)
Confidence(A? B) =
?
xp?N µAB(xp)?
xp?N µA(xp)
(2)
where | N | is the total number of transactions (patterns) in
the database, µA(xp) is the matching degree of the transaction
xp with the antecedent part of the rule and µAB(xp) is the
matching degree of the transaction xp with the antecedent and
consequent of the rule.
In the last few years, different studies have proposed meth-
ods to obtain fuzzy association rule-based classifiers [7], [8].
The task of classification is to find a set of rules in order
to identify the classes of undetermined patterns. A fuzzy
association rule can be considered a classification rule if the
antecedent contains fuzzy item sets and the consequent part
contains only one class label.
A fuzzy associative classification rule A ? Clasj could
be measured directly in terms of support and confidence as
follows:
Support(A? Clasj) =
?
xp?Clasj µA(xp)
| N | (3)
Confidence(A? Clasj) =
?
xp?Clasj µA(xp)?
xp?N µA(xp)
(4)
III. FARC-HD: FUZZY ASSOCIATION RULE-BASED
CLASSIFIER FOR HIGH-DIMENSIONAL PROBLEMS
In this section, we will describe our proposal to obtain a
fuzzy association rule-based classifier for high dimensional
problems. This method is based on three stages:
1) Fuzzy association rule extraction for classification.
2) Candidate rule prescreening.
3) Genetic fuzzy rule selection.
A scheme of this algorithm is shown in Fig. 2.
Fig. 2. Scheme of FARC-HD method
In the following subsections we will introduce the three
mentioned stages, explaining in detail all their characteristics.
A. Fuzzy association rule extraction for classification
To generate the initial rule base (RB) we employ a search
tree to list all the possible fuzzy itemsets of each class. The
root or level 0 of a search tree is an empty set. All attributes
are assumed to have an order, first setting the fuzzy items of
the variable 1, then the fuzzy items of the variable 2, so on.
The one-itemsets corresponding to the attributes are listed in
the first level of the search tree according to their order. The
children of a one-item node for an attribute A are the two-item
sets that include the one-item set of attribute A and a one-item
set for another attribute after attribute A in the order, and so
on.
An example with two attributes (V1 and V2) with two
linguistic terms (L and H) is detailed in Figure 3. Notice that
we can not joint the items set of a node with the items set
of another previous node in the level because we will repeat
nodes generated for this level. For instance, if we joint V2, L
with V1, L we will obtained the node V2, LV1, L, which is the
same as the node V1, LV2, L.
An itemset with a support higher than the minimum support
is a frequent itemset. If the support of an n-item set in a
node A is less than the minimum support, it does not need
to be extended more because the support of any item set in a
node in the subtree led by node A will also be less than the
minimum support. Likewise, if a candidate item set generates
a classification rule with confidence higher than the minimum
confidence, it is again unnecessary to extend it further.
Once all frequent fuzzy itemsets have been obtained, the
candidates fuzzy association rules for classification can be
generated setting the frequent fuzzy itemsets in the antecedent
of the rules and the corresponding class in the consequent. This
process is repeated for each class.
Fig. 3. The search tree for two quantitative attributes V1 and V2 with two
linguistic terms L and H
The number of frequent fuzzy itemsets extracted depends
directly on the minimum support. The minimum support is
usually calculated considering the total number of patterns in
the dataset, however the number of patterns for each class
in a dataset can be different. For this reason, our algorithm
recalculates the minimum support for each class using the dis-
tributions of the classes over the dataset. Thus, the minimum
support of the class Clasj is defined as:
MinimumSupportClasj = minSup ? fClasj (5)
where minSup is the minimum support determined by the
expert and fClasj is the pattern ratio of the class Clasj .
In this stage we can generate a large number of rules. It is,
however, very difficult for human users to handle such a large
number of generated fuzzy rules. It is also very difficult for
human users to intuitively understand long fuzzy rules with
many antecedent conditions. For this reason, the depth of the
trees is limited to a fixed value (Depthmax) determined by an
expert.
B. Candidate rule prescreening
In order to decrease the computational costs, we consider
the use of subgroup discovery to select the most interesting
rules from the RB obtained in the previous stage by means
of a pattern weighting scheme [9]. Thus, in each iteration
the rules are ordered according to a rule evaluation criteria
from best to worst. The best rule is selected, covered patterns
are reweighted, and the procedure repeats these steps until all
patterns have been covered more than kt times or there are no
more rules in the RB. This process is to be repeated for each
class.
This scheme treats the patterns in such a way that covered
positive patterns are not deleted when the currently best rule
is selected. Instead the algorithm stores for each pattern a
count i of how many times the pattern has been covered by a
selected rule and decreases its weight according to the formula
w(ej , i) = 1i+1 (in the first iteration all target class patterns
are assigned the same weight w(ej , 0) = 1). Covered patterns
are completely eliminated when these have been covered more
than kt times (for more information, see [9]).
In this paper, we have modified the measure used in [9]
(wWRAcc’) to enable handling fuzzy rules. Thus, this measure
for a rule A? Clasj is then defined as
n??(A · Clasj)
n?(Clasj)
· (n
??(A · Clasj)
n??(A)
? n(Clasj)
N
) (6)
where n??(A) is the sum of the product of the weight of each
covered pattern by its matching degree with the antecedent
part of the rule, n??(A · Clasj) is the sum of the product of
the weight of each covered positive pattern by its matching
degree with the rules, and n?(Clasj) is the sum of the weights
of patterns of class Clasj . Moreover, the first term in the
definition of wWRAcc’ has been replaced by n
??(A·Clasj)
n(Clasj)
to
reward rules which cover uneliminated patterns of the class
Clasj .
C. Genetic Fuzzy Rule Selection
We consider the use of GAs to select a compact set
of fuzzy association rules with high classification accuracy
from RB obtained in the previous stage. We consider the
CHC genetic model [18] which has achieved good results
for binary selection problems [19]. In the following, the main
characteristics of this genetic approach are presented:
• Genetic model.
• Codification and initial gene pool.
• Chromosome evaluation.
• Crossover operator.
• Restarting approach.
1) CHC Genetic Model: The genetic model of CHC makes
use of a “Population-based Selection” approach. P parents
and their corresponding offspring compete to select the best
P individuals to take part in the next population. The CHC
approach makes use of an incest prevention mechanism and
a restarting process to provoke diversity in the population,
instead of the well known mutation operator.
This incest prevention mechanism will be considered in
order to apply the crossover operator, i.e., two parents are
crossed if their hamming distance divided by 2 is over a
predetermined threshold, L. This threshold value is initialized
following the original CHC scheme as:
L = (#Genes)/4.0 (7)
where #Genes is the number of genes in the chromosome (for
more information, see [20]). In order to make this procedure
independent of #Genes, in our case, L will be decremented
by a ?% of its initial value (being ? determined by the user,
usually 10%). The algorithm restarts when L is below zero.
A scheme of this algorithm is shown in Fig. 4.
Fig. 4. Scheme of CHC
2) Codification and Initial Gene Pool: Each chromosome
is a binary vector that determines when a rule is selected or
not (alleles ‘1’ and ‘0’ respectively). A chromosome C =
{c1, ..., cM} represents a subset of rules, so that:
If ci = 1 then (Ri ? RB) else (Ri 6? RB),
with Ri being the corresponding i-th rule in the initial RB.
To make use of the available information, all the candidates
rules are included in the population as an initial solution. To
do so, the initial pool is obtained with an individual having all
genes with value ‘1’ and the remaining individuals generated
at random in {0, 1}.
3) Evaluation: To evaluate a determined chromosome we
compute the classification rate (clas rat) and maximize the
following function (defined for maximization):
Fitness(C) = clas rat? w1 ·NR (8)
where NR is the number of selected rules (to penalize a
large number of rules) and w1 is computed from the clas rat
considering all the candidate rules,
w1 = ? · (clas ratinitial/NRinitial) (9)
with ? being a weighting percentage given by the system
expert that determines the tradeoff between accuracy and
complexity. We have empirically tested that a good neutral
choice is for example 1.0 (good accuracy and not too many
rules).
The fitness value of a chromosome will be ?w1 if there is
at least one class without selected rules.
4) Crossover Operator: The half uniform crossover scheme
(HUX) is employed [20]. This operator exactly interchanges
the mid of the alleles that are different in the parents, ensur-
ing the maximum distance of the offspring to their parents
(exploration).
Figure 5 depicts the behavior of this operator.
5) Restarting Approach: To get away from local optima,
this algorithm uses a restart approach. In this case, the best
chromosome is maintained and the remaining are generated at
random in {0,1}.
The algorithm restarts when L is below zero, which means
that all the individuals coexisting in the population are very
similar.
Fig. 5. Scheme of the behavior of the HUX operator
IV. EXPERIMENTAL STUDY
In order to analyze the performance of the proposed ap-
proach we have considered 9 real-world datasets. Table I
summarizes the main characteristics of the 9 datasets, where
Attributes is the number of attributes, Patterns is the
number of patterns and Classes is the number of classes.
Furthermore, this table shows the link to the KEEL project
webpage [21] from which they can be downloaded. Notice
that we have removed the instances with any missing value in
the dataset Dermatology.
TABLE I
DATA SETS CONSIDERED FOR THE EXPERIMENTAL STUDY
Name Attributes Patterns Classes
Segment 19 2310 7
Ringnorm 20 7400 2
Thyroid 21 7200 3
Wdbc 30 569 2
Ionosphere 34 351 2
Dermatology 34 358 6
SatImage 36 6435 6
Spambase 57 4597 2
Sonar 60 208 2
Available at http://sci2s.ugr.es/keel/datasets.php
This section is organized as follows:
• First, we describe the experimental set-up in subsec-
tion IV-A.
• Second, we show a statistical analysis obtained from the
comparison with another two GFSs in subsection IV-B.
• Finally, we analyze the scalability of the proposed ap-
proach in subsection IV-C.
A. Experimental Set up
In this study, we have compared the performance of our
approach with another two GFSs, including FH-GBML [22]
and SGERD [23] algorithms.
• FH-GBML: This method follows a Genetic Cooperative-
Competitive Learning (GCCL) approach and consists of
two processes. The first process is used for generating
good fuzzy rules while the second one is used for
finding good combinations of generated fuzzy rules. This
method simultaneously use multiple fuzzy partitions with
different granularities for fuzzy rule extraction, using four
homogeneous fuzzy partitions with triangular fuzzy sets
and a don’t care condition.
• SGERD: It is a steady-state GA to generate a prespecified
number of Q rules per class following a GCCL ap-
proach. In each iteration, parents and their corresponding
offspring compete to select the best Q rules for each
class. Simultaneously, this method also uses multiple
fuzzy partitions with different granularities and don’t care
conditions for fuzzy rule extraction.
To develop the different experiments we consider a 10-fold
cross-validation model, i.e., we randomly split the data set into
10 folds, each containing 10% of the patterns of the data set,
and used nine folds for training and one for testing 1. For each
of the ten partitions, we executed three trials of the algorithms.
For each data set, we therefore consider the average results of
30 runs.
The initial linguistic partitions are comprised by five lin-
guistic terms with uniformly distributed triangular membership
functions giving meaning to them. The values considered for
the input parameters of each method are:
• FARC-HD: MinSup = 0.05, MinConf = 0.8,
Depthmax = 3, kt = 2, Pob = 50, Eval = 5, 000, ? = 0.02
• FH-GBML: Nrules = 20, Fsets = 200, Gen = 1000, Pc =
0.9, Pdcare = {0.5, 0.8, 0.95}, Pmichigan = 0.5
• SGERD: Q = heuristics
These values were selected according to the recommenda-
tion of the corresponding authors within each proposal, which
are the default parameters settings included in the KEEL
software tool [21]. Notice that in the FH-GBML algorithm the
authors used three different probabilities of don’t care (0.5, 0.8
and 0.95 depending on the size of the dataset) to obtain fuzzy
rules with a few antecedent fuzzy sets. In these experiments,
we have used these three probabilities of don’t care in each
dataset and have shown in the tables the best average result
obtained in each one.
B. Results and Statistical Analysis
The results obtained by the analyzed methods are shown in
Table II, where #R stands for the average number of rules,
#L stands for the average number of linguistic terms in the
antecedent of the rules, and Tra and Tst respectively for
the average accuracy obtained over the training and test data.
The results presented show that our proposal obtains the best
global results for test. Furthermore, the average number of
rules is low (13.5 rules on average) and the rules obtained only
involve three or less attributes in their antecedents, giving the
advantage of easier understanding from an user’s perspective.
In order to compare the results, we have used non-
parametric statistical tests for multiple comparison [12], [13],
[14] to find the best approach, considering the results ob-
tained in the test partitions (Tst). First of all, we have used
Friedman and ImanDavenport tests [24], [25] in order to find
out whether significant differences exist among all the mean
values. Tables III shows the Friedman and ImanDavenport
statistics and it relates them to the corresponding critical
values for each distribution by using a level of significance
? = 0.05. The p value obtained is also reported for each test.
Given that the statistics of Friedman and ImanDavenport are
clearly greater than their associated critical values, there are
1The corresponding data partitions (10-fold) for these
data sets are available at the KEEL project webpage [21]:
http://sci2s.ugr.es/keel/datasets.php
significant differences among the observed results with a level
of significance ? ? 0.05.
TABLE III
RESULTS OF THE FRIEDMAN AND IMANDAVENPORT TESTS (? = 0.05)
Friedman test ImanDavenport test
Statistic (X 2F ) Critical Value p value Statistic (FF ) Critical Value p value
14.001 5.991 <0.001 28.001 3.634 <0.001
Table IV shows the rankings (computed using a Friedman
test) of the different methods considered in this study.
TABLE IV
AVERAGE RANKINGS OF THE METHODS
Method Ranking
SGERD 2.666
FH-GBML 2.333
FARC-HD 1.000
We now apply Holm’s test [26] to compare the best ranking
method (FARC-HD) with the remaining methods. In Table V,
the methods are ordered with respect to the z-value obtained.
Holm’s test rejects the hypothesis of equality with the rest of
the methods (p < ?/i). Therefore, analyzing the statistical
study shown in Tables IV and V we conclude that our model
has shown itself to be the best performing method when
compared with the remaining fuzzy GFSs applied in this study.
C. Analysis of Scalability
Table VI shows the average runtime of the analyzed meth-
ods on the 9 real-world problems. The methods were imple-
mented using Java and all of the experiments were performed
using a Pentium Corel 2 Quad, 2.5GHz CPU with 4Gb of
memory and running Linux.
Analyzing the results presented we can see that:
• The SGERD algorithm presents a very low average run-
time in all datasets, obtaining a good scalability when we
increase the size of the problem. This method, however,
obtains the worst ranking in the Friedman’s test when we
compare the results obtained in the test partitions (see
table IV).
• The FH-GBML algorithm expends a large amount of time
when the number of attributes and patterns in the dataset
is high.
• The FARC-HD approach presents a low computational
cost in all datasets, obtaining a good scalability and the
best performance in accuracy.
V. CONCLUSION
In this paper we have proposed a fuzzy associative classifi-
cation method with genetic rule selection for high-dimensional
datasets to obtain accurate and compact fuzzy associative
classifiers with a low computational cost.
Analyzing the results obtained we can conclude that our
model has shown itself to be the best performing method in
the experimental study and presents a low computational cost
in all datasets, obtaining a good scalability. Furthermore, our
proposal obtains models with a reduced number of rules with
few attributes in the antecedent, giving the advantage of high
interpretability from an user’s perspective.
TABLE II
RESULTS OBTAINED BY THE ANALYZED METHODS
FH-GBML SGERD FARC-HD
Dataset #R #L Tra Tst #R #L Tra Tst #R #L Tra Tst
Segment 10.3 10.6 72.8 73.6 8.50 1.98 76.49 76.78 22.2 2.4 91.1 89.9
Ringnorm 6.9 11.3 87.3 86.9 6.83 2.00 73.21 72.63 9.8 1.5 88.2 87.9
Thyroid 3.0 6.7 93.5 93.2 2.13 2.00 92.89 92.84 3.7 2.5 93.6 93.4
Wdbc 7.2 4.9 95.1 92.3 3.70 2.00 91.79 90.68 6.3 1.2 95.4 94.4
Ionosphere 12.2 3.7 74.4 73.2 3.97 1.61 80.40 79.22 15.2 1.9 95.4 90.0
Dermatology 10.3 3.4 85.2 83.5 8.20 1.80 81.70 78.88 23.5 2.4 99.3 90.2
SatImage 5.8 19.6 75.0 74.4 21.80 2.00 64.08 64.05 19.0 2.7 75.6 74.9
Spambase 3.9 18.5 77.9 77.2 3.70 2.00 72.90 72.98 9.8 1.7 84.8 84.8
Sonar 10.3 4.7 80.6 68.2 3.17 2.00 74.22 71.90 11.6 2.4 93.8 78.9
Mean 7.8 9.3 82.4 80.3 6.9 1.9 78.6 77.8 13.5 2.1 90.8 87.1
TABLE V
HOLM TABLE FOR THE SELECTION METHODS WITH ? = 0.05
i Method z p ?/i Hypothesis
2 SGERD 3.53 4.069E-4 0.025 Rejected
1 FH-GBML 2.82 0.0046 0.05 Rejected
TABLE VI
RUNTIME OF THE ANALYZED METHODS (HH:MM:SS)
Dataset FH-GBML SGERD FARC-HD
Segment 02:03:51 00:00:03 00:00:26
Ringnorm 10:05:13 00:00:09 00:01:06
Thyroid 07:48:19 00:00:08 00:00:13
Wdbc 00:50:01 00:00:01 00:00:09
Ionosphere 00:41:28 00:00:01 00:00:04
Dematology 00:26:14 00:00:01 00:00:04
SatImage 12:04:02 00:00:19 00:04:51
Spambase 13:01:47 00:00:16 00:03:29
Sonar 00:21:02 00:00:01 00:00:36
Mean 05:15:46 00:00:06 00:01:13
REFERENCES
[1] H. Ishibuchi, T. Nakashima, and M. Nii, Classification and Modeling
with Linguistic Information Granules: Advanced Approaches to Linguis-
tic Data Mining. Berlin: Springer-Verlag, 2004.
[2] W. Combs and J. Andrews, “Combinatorial rule explosion eliminated
by a fuzzy rule configuration,” IEEE Transactions on Fuzzy Systems,
vol. 6, no. 1, pp. 1–11, 1998.
[3] J. Han and M. Kamber, Data Mining: Concepts and Techniques. Second
Edition. San Fransisco: Morgan Kaufmann, 2006.
[4] B. Liu, W. Hsu, and Y. Ma, “Integrating classification and association
rule mining,” in Proceedings of the International Conference on Knowl-
edge Discovery and Data Mining (SIGKDD), New York, USA, 1998,
pp. 80–86.
[5] G. Dong, X. Zhang, L. Wong, and J. Li, “Caep: Classification by aggre-
gating emerging patterns,” in Proceedings of the Second International
Conference on Discovery Science (DS), ser. Lecture Notes in Artificial
Intelligence, S. Arikawa and K. Furukawa, Eds. Tokyo, Japan: Springer
Berlin-Heidelberg, 1999, vol. 1721, pp. 30–42.
[6] B. Liu, Y. Ma, and C. Wong, “Classification using association rules:
weaknesses and enhancements,” in Data Mining for Scientific and Engi-
neering Applications, ser. Massive Computing, R. Grossman, C. Kamath,
P. Kegelmeyer, V. Kumar, and R. Namburu, Eds. Springer Berlin-
Heidelberg, 2001, vol. 2, pp. 591–602.
[7] Y.-C. Hua and G.-H. Tzeng, “Elicitation of classification rules by fuzzy
data mining,” Engineering Applications of Artificial Intelligence, vol. 16,
no. 7-8, pp. 709–716, 2003.
[8] Z. Chen and G. Chen, “Building an associative classifier based on fuzzy
association rules,” International Journal of Computational Intelligence
Systems, vol. 1, no. 3, pp. 262–273, 2008.
[9] B. Kavsek and N. Lavrac, “Apriori-sd: Adapting association rule learning
to subgroup discovery,” Applied Artificial Intelligence, vol. 20, no. 7, pp.
543–583, 2006.
[10] D. Goldberg, Genetic algorithms in search, optimization, and machine
learning. New York: Addison-Wesley, 1989.
[11] J. Holland, Adaptation in natural and artificial systems. London: The
University of Michigan Press, 1975.
[12] J. Dems?ar, “Statistical comparisons of classifiers over multiple data sets,”
Journal of Machine Learning Research, vol. 7, pp. 1–30, 2006.
[13] S. Garc?´a and F. Herrera, “An extension on statistical comparisons of
classifiers over multiple data sets for all pairwise comparisons,” Journal
of Machine Learning Research, vol. 9, pp. 2579–2596, 2008.
[14] S. Garc?´a, A. Ferna´ndez, J. Luengo, and F. Herrera, “A study of statis-
tical techniques and performance measures for genetics-based machine
learning: Accuracy and interpretability,” Soft Computing, vol. 13, no. 10,
pp. 959–977, 2009.
[15] T. Sudkamp, “Examples, counterexamples, and measuring fuzzy associ-
ations,” Fuzzy Sets and Systems, vol. 149, no. 1, pp. 57–71, 2005.
[16] J. Alcala´-Fdez, R. Alcala´, M. Gacto, and F. Herrera, “Learning the
membership function contexts for mining fuzzy association rules by
using genetic algorithms,” Fuzzy Sets and Systems, vol. 160, no. 7, pp.
905–921, 2009.
[17] T. Hong and Y. Lee, “An overview of mining fuzzy association rules,”
in Studies in Fuzziness and Soft Computing, H. Bustince, F. Herrera,
and J. Montero, Eds. Springer Berlin/Heidelberg, 2008, vol. 220, pp.
397–410.
[18] L. Eshelman, “The chc adaptive search algorithm: How to have safe
serach when engaging in nontraditional genetic recombination,” in
Foundations of Genetic Algorithms, G. Rawlin, Ed. Morgan Kaufmann,
1991, vol. 1, pp. 265–283.
[19] J. Cano, F. Herrera, and M. Lozano, “Using evolutionary algorithms
as instance selection for data reduction in kdd: an experimental study,”
IEEE Transactions on Evolutionary Computation, vol. 7, no. 6, pp. 561–
575, 2003.
[20] L. Eshelman and J. Schaffer, “Real-coded genetic algorithms and interval
schemata,” in Foundations of Genetic Algorithms, D. Whitley, Ed.
Morgan Kaufmann, 1993, vol. 2, pp. 187–202.
[21] J. Alcala´-Fdez, L. Sa´nchez, S. Garc?´a, M. del Jesus, S. Ventura,
J. Garrell, J. Otero, C. Romero, J. Bacardit, V. Rivas, J. Ferna´ndez, and
F. Herrera, “KEEL: A software tool to assess evolutionary algorithms
to data mining problems,” Soft Computing, vol. 13, no. 3, pp. 307–318,
2009.
[22] H. Ishibuchi, T. Yamamoto, and T. Nakashima, “Hybridization of fuzzy
gbml approaches for pattern classification problems,” IEEE Transactions
on Systems and Man and Cybernetics - Part B: Cybernetics, vol. 35,
no. 2, pp. 359–365, 2005.
[23] E. Mansoori, M. Zolghadri, and S. Katebi, “Sgerd: A steady-state genetic
algorithm for extracting fuzzy classification rules from data,” IEEE
Transactions on Fuzzy Systems, vol. 16, no. 4, pp. 1061–1071, 2008.
[24] M. Friedman, “The use of ranks to avoid the assumption of normality
implicit in the analysis of variance,” Journal of the American Statistical
Association, vol. 32, no. 200, 1937.
[25] R. Iman and J. Davenport, “Approximations of the critical region of
the friedman statistic,” Communications in Statistics. Part A Theory and
Methods, vol. 9, pp. 571–595, 1980.
[26] S. Holm, “A simple sequentially rejective multiple test procedure,”
Scandinavian Journal of Statistics, vol. 6, pp. 65–70, 1979.
