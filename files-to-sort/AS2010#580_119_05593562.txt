A new algorithm of association rules mining 
 
Gang FANG, Ji-Ping ZENG, Jiang XIONG, Xiao-Feng CHEN 
Chongqing Three Gorges University 
Chongqing, P.R. China 
cqwzjsjfg@163.com 
 
 
Abstract—To reduce the number of candidate itemsets and 
the times of scanning database, and to fast generate candidate 
itemsets and compute support, this paper proposes an algorithm 
of association rules mining based on attribute vector, which is 
suitable for mining any frequent itemsets. The algorithm 
generates candidate itemsets by computing nonvoid proper 
subset of attributes items, it uses ascending value and descending 
value to compute nonvoid proper subset of the weights of 
attributes items, the method may be used to reduce the number of 
candidate itemsets to improve efficiency of generating candidate 
itemsets. And the algorithm gains support by computing attribute 
vector module, the method may be used to reduce the time of 
scanning database, and so the algorithm only need scan once 
database to search all frequent itemsets. The experiment 
indicates that the efficiency of the algorithm is faster and more 
efficient than presented algorithms of congener association rules 
mining. 
Index Terms—data mining, association rules, attribute vector, 
proper subset, attributes items weights 
I.  INTRODUCTION 
Apriori mainly wants to solve two key problems: one is 
how to reduce the number of candidate itemsets and the times 
of scanning database, the other is how to fast generate 
candidate itemsets and compute support. Aiming to the first 
problem, people present some algorithms, DMFIA as in [1] and 
IDMFIA as in [2]. And then, to solve the second problem, 
some algorithms based on binary were presented, B-Apriori as 
in [3], B_ARDSM as in [4] and B_UDMA as in [5]. These 
algorithms improved the method of generating candidate 
itemsets and computing support by binary logic operation to 
indeed improve efficiency. However, they need repeated scan 
database when computing support. These algorithms based on 
binary are inefficient because they still use traditional way of 
generating candidate itemsets and computing support. 
Therefore, this paper proposes an algorithm of association rules 
mining based on attribute vector, denoted by ARMBAV, which 
is suitable for mining any long frequent itemsets. 
II. BASE NOTIONS AND PROPERTIES 
Let I= {i1, i2…im} be a set of attribute items, suppose each 
ik has been changed into Boolean quantity from multiattribute. 
Definition 1 Binary Transaction (BT), a transaction is 
expressed as binary, binary transaction of transaction T is 
expressed as BT= (b1 b2…bm), bk?  [0, 1], k=1…m, if ik?T, 
and then bk =1, otherwise bk =0. 
Example, let I={1,2,3,4,5,6} be a set of attribute items, if 
T={2,5,6}, and then BT=(010011)2. 
Definition 2 Transaction Weights (TW), it is an integer, its 
value is equal to decimal integer of binary transaction. 
Example, if BT = (010101)2, and then TW (BT) =21. 
Definition 3 Relation of transaction weights accords with 
relation of transaction set.  
Example, suppose transaction weights of a transaction T1 is 
denoted by TW1, transaction weights of a transaction T2 is 
denoted by TW2. If T1?T2, and TW1?TW2, namely, TW1 & 
TW2 = TW1. TW1 is regarded as subset of TW2, which is 
regarded as superset of TW1. 
Definition 4 Attribute Vector (AV), it is constituted by the 
m-order integer, m is called vector dimension, and each integer 
is called vectorial component. 
Example, 3-AV= {23, 45, 33}, here 23, 45 and 33, they are 
also called vectorial component. 
Definition 5 Vectorial Component Module (VCM), it is an 
integer, which is equal to the number of “1” contained by 
binary code of vectorial component. 
Example, here, 23 is one of vectorial component of above 
3-AV, VCM (23) = VCM (010111) = 4. 
Definition 6 Attribute Vector Module (AVM), it is an 
integer, which is equal to the summation of vectorial 
component module. 
Example, 3-AV= {23, 45, 33}, AVM (3-AV) = VCM (23) 
+ VCM (45) + VCM (33) = 4 + 4 + 2 = 10. 
Definition 7 Attribute Vectors Cross Product (AVCP), the 
result of operation is a vector. Each component is the value that 
components corresponding to all Attribute Vectors will operate 
“and” logical operation. 
Example, if here are two Attribute Vectors, 3-AV1= {23, 45, 
33}, 3-AV2= {20, 36, 35}, and AVCP (3-AV1, 3-AV2) is equal 
to the result: {23&20, 45&36, 33& 35} = {20, 36, 33}. 
Property 1 Let p and q be binary transactions with m bits, 
let Tp be a transaction about p, let Tq be a transaction about q, 
and p & q=p, let F be frequent itemsets. 
?If Tq? F, then Tp? F.  
?If Tp? F, then Tq? F. 
978-1-4244-6005-2/10/$26.00 ©2010 IEEE 
The 5th International Conference on
Computer Science & Education
Hefei, China. August 24–27, 2010
 511 
ThP1.8
         
III. ASSOCIATION RULES MINING ALGORITHM BASED ON 
ATTRIBUTE VECTOR 
Suppose there are N transactions in database, which 
contains m attributes item, and these items are Boolean 
variable. There are n different transactions in database. 
A. The method of generating candidate itemsets 
We let I = {i1, i2…im} be a set of attribute items, If we let I 
be a transaction, and TW (I) = 2m-1, this value is also denoted 
by the weights of attributes items. According to this knowledge 
as in [6], the algorithm computes nonvoid proper subset of the 
weights of attributes items to generate candidate itemsets by 
ascending value and descending value, this course is expressed 
as follows: 
Min= 1, Max= 2m-2. 
The course of generating candidate itemsets by ascending 
value is expressed as follows: 
VMin=1, 2, 3... 
The course of generating candidate itemsets by descending 
value is expressed as follows: 
VMax= 2m-2, 2m-3, 2m-4… 
Example let I= {a, b, c, d}, and then TW (I) = 24-1=15, 
namely, 15 is the weights of attributes items. Let NPS be a 
nonvoid proper subset, and all NPS of the weights of attributes 
items is expressed as follows: 
Min=1, Max=14. 
NPS1=1, it denote candidate itemsets {d}. 
NPS2=14, it denote candidate itemsets {a, b, c}. 
NPS3=2, it denote candidate itemsets {c}. 
NPS4=13, it denote candidate itemsets {a, b, d}. 
NPS5=3, it denote candidate itemsets {c, d}. 
NPS6=12, it denote candidate itemsets {a, b}. 
NPS7=4, it denote candidate itemsets {b}. 
NPS8=11, it denote candidate itemsets {a, c, d}. 
NPS9=5, it denote candidate itemsets {b, d}. 
NPS10=10, it denote candidate itemsets {a, c}. 
NPS11=6, it denote candidate itemsets {b, c}. 
NPS12=9, it denote candidate itemsets {a, d}. 
NPS13=7, it denote candidate itemsets {b, c, d}. 
NPS14=8, it denote candidate itemsets {a}. 
According to the example, we know all candidate itemsets 
were generated by computing nonvoid proper subset of the 
weights of attributes items. 
B. The method of computing support 
According to this knowledge as in [7], the method of 
computing attribute vector module is used to gain support by 
the algorithm, which is expressed as follows: 
Step1: Turning original transaction into Binary Transaction 
according to definition 1. 
Step2: Aiming to the column corresponding to each 
attribute item, its corresponding Binary code is turned into 
integer in fragment, and these integers will constitute Attribute 
Vector of attribute item according to definition 4. 
Step3: Each attribute item in candidate itemsets has own 
Attribute Vector, and then we will use these Attribute Vectors 
to operation by Attribute Vectors Cross Product. Finally, a new 
Attribute Vector will be gained. 
Step4: Computing Attribute Vector Module of new 
Attribute Vector according to definition 6. The result is equal to 
support of candidate itemsets. 
For example, there are ten transactions in database. The 
course of computing support is expressed as follows: 
Step1: Turning original transaction into Binary Transaction. 
T1: {a, b, e, f}, BT1 = (110011)2; 
T2: {b, c, d, e}, BT2 = (011110)2; 
T3: {a, b, d, e, f}, BT3 = (110111)2; 
T4: {a, b, c, d, e, f}, BT4 = (111111)2; 
T5: {c, d, e, f}, BT5 = (001111)2; 
T6: {a, c, d, f}, BT6 = (101101)2; 
T7: {a, c, d, e, f}, BT7 = (101111)2; 
T8: {a, b, c, e, f}, BT8 = (111011)2; 
T9: {b, d, e, f}, BT9 = (010111)2; 
T10: {a, b, c, d, f}, BT10 = (111101)2; 
Setp2: Computing Attribute Vector of each attribute item in 
database is expressed as table I. 
TABLE I.  COMPUTING ATTRIBUTE VECTOR 
No. a b c d e f 
BT1 1 1 0 0 1 1 
BT2 0 1 1 1 1 0 
BT3 1 1 0 1 1 1 
BT4 1 1 1 1 1 1 
BT5 0 0 1 1 1 1 
component 1 22 30 11 15 31 23 
BT6 1 0 1 1 0 1 
BT7 1 0 1 1 1 1 
BT8 1 1 1 0 1 1 
BT9 0 1 0 1 1 1 
BT10 1 1 1 1 0 1 
component 2 29 7 29 27 14 31 
Attribute 
Vectors 
{22, 
29} 
{30, 
7} 
{11, 
29} 
{15, 
27} 
{31, 
14} 
{23, 
31} 
Step3-step4: Let candidate = {b, e, f} be a candidate 
itemsets, and then we use these three Attribute Vector of 
attribute item to compute support by Attribute Vectors Cross 
Product, denoted by Support (candidate): 
AV ({b}) = {30, 7}; 
AV ({e}) = {31, 14}; 
 512 
ThP1.8
         
AV ({f}) = {23, 31}; 
AVCP ({b}, {e}, {f}}) = {30&31&23, 7&14&31} 
= {22, 6}; 
Support (candidate) = AVM (AVCP ({b}, {e}, {f}})) 
= AVM ({22, 6})  
=VCM (22) +VCM (6)  
= 3 + 2 = 5. 
C. The program of generating frequent itemsets 
Let I= {i1, i2…im} be a set of attribute items, we define 
some signs expressed as follows: 
D: saving all Binary Transactions. 
F1: saving frequent itemsets by ascending value.  
F2: saving frequent itemsets by descending value. 
NF: saving non frequent itemsets by ascending value. 
(1) Min =1, Max=2m-2; 
(2) While (Min<Max) { 
(3)  If ((Min? F1) && (NF?Min) { 
(4)   If (Support (Min)>=minimal support)  
(5)     Write Min to F1 and delete its subset in F1; 
(6)   Else  
(7)     Write Min to NF; 
(8)  } 
(9)  If ((Max? F2) && (NF?Max) { 
(10)    If (Support (Max)>=minimal support) 
(11)     Write Max to F2 and delete its subset in F1 ;} 
(12)  }  
(13)  Min++; 
(14)  Max-- ;} 
(15) Output F1 and F2; 
Input: a Candidate; 
Output: support of Candidate; 
Support (int Candidate) 
(1)  int i = 0, j = 0, Support = 0, exit = 0; 
(2)  int [] Set = new int[Row]; 
(3)  i = 0; 
(4)  While ((i < Column) && (Exit == 0)) { 
(5)     If ((Candidate & 1) == 1) { 
(6)        Exit = 1; 
(7)        For (j = 0; j < Row; j++) { 
(8)         Set[j] = D [j, i] ;} 
(9)     } 
(10)     Else { 
(11)         Candidate = Candidate >> 1; 
(12)         i++;} 
(13)   } 
(14)   j = i; 
(15)   While ((j < Column) && (Candidate! = 0)) { 
(16)       If ((Candidate & 1) == 1) { 
(17)         For (i = 0; i < Row; i++) { 
(18)           Set[i] = Set[i] & D [i, j] ;} 
(19)       } 
(20)       Candidate = Candidate >> 1; 
(21)       j++;} 
(22)   For (i = 0; i < Row; i++) { 
(23)     Support = Support + VCM (Set[i]) ;} 
(24)  Return Support; 
IV. COMPARING THE CAPABILITY OF ALGORITHMS 
Here we use B-Apriori as in [3] and B_ARDSM as in [4] to 
compare with ARMBAV presented by this paper. 
A. Analyzing the Capability of Algorithms 
B-Apriori: The algorithm generates candidate itemsets 
which contain the number of items from fewness to many. 
However, the algorithm use traditional method to compute 
support of candidate, namely, it need scan once database when 
computing once support, and so there are many times of 
scanning database in the course of mining association rules, the 
efficiency of algorithm would be affected because of this. 
B_ARDSM: Generating candidate frequent digital 
transaction DTL of which is from maximum and minimum to 
middle. The algorithm is suitable for mining general database 
where frequent itemsets aren’t confined, but the method of 
computing support is the same as B-Apriori, the algorithm 
scans once database when computing support. And so it need 
scan all these transaction several times, and so the efficiency of 
algorithm still isn’t fast and efficient. 
ARMBAV: The algorithm is suitable for mining any 
frequent itemsets. It generates candidate itemsets by computing 
nonvoid proper subset of attributes items, it uses ascending 
value and descending value to compute nonvoid proper subset 
of the weights of attributes items, the method is used to reduce 
the number of candidate itemsets to improve efficiency of 
generating candidate itemsets. The algorithm gains support by 
computing attribute vector module, the method is used to 
reduce the time of scanning database, and so the algorithm only 
need scan once database to search all frequent itemsets. 
B. Comparing the experiment of algorithms 
Now we use result of experiment to testify above analyses 
and conclusion. Above three algorithms are used to generate 
 513 
ThP1.8
         
frequent itemsets from these transaction weights, which are 
from 3 to 8191, these transactions don’t include any single 
items, and so the number of attributes is expressed as m=13, 
the sum of transactions is expressed as n=8178. 
Our experimental circumstances are expressed as follow: 
Intel(R) Celeron(R) M CPU 420 ? 1.60 GHz, 1.24G, language 
of the procedure is Visual C# 2005.NET, OS is Windows XP 
Professional.  
The experimental result is expressed as Fig. 1 and 2, where 
support is absolute value. The runtime of algorithms B-Apriori 
and ARMBAV is expressed as Fig. 3 as support and length of 
itemsets change. The runtime of algorithms B_ARDSM and 
ARMBAV is expressed as Fig. 4 as support and length of 
itemsets change. 
 
Figure 1.  The experimental result 
 
Figure 2.  The experimental result 









       
6XSSRUW/HQJWK
5X
QW
LP
H
0L
OO
LV
HF
RQ
G
%$SULRUL
$50%$9
 
Figure 3.  Runtime of algorithms B-Apriori and ARMBAV 






       
6XSSRUW/HQJWK
5X
QW
LP
H
0L
OO
LV
HF
RQ
G
%B$5'60
$50%$9
 
Figure 4.  Runtime of algorithms B_ARDSM and ARMBAV 
V. CONCLUSION 
This paper proposes an algorithm of association rules 
mining based on attribute vector, which is suitable for mining 
any long frequent itemsets. The experiment indicates its 
efficiency is faster and more efficient than presented congener 
algorithms. 
ACKNOWLEDGMENT 
This work was fully supported by science and technology 
research projects of Chongqing Education Commission (Project 
No. KJ091108), and it was also fully supported by science and 
technology research projects of Chongqing Three Gorges 
University (Project No. 10QN-22). 
REFERENCES 
[1] Song YQ, Zhu YQ, Sun ZH, Chen G. An algorithm of its updating 
algorithm based on FP-Tree for mining maximum frequent itemsets. 
Journal of Software, 2003, vol. 14(9), pp.1586-1592. 
[2] Ji GL, Yang M. Song YQ. Sun ZH. Fast Updating Maximum Frequent 
Itemsets. Journal of Computers, 2005, vol. 28(1), pp.128-135. 
[3] Chen G, Zhu YQ, Yang HB, Study of Some Key Techniques in Mining 
Association Rule, Journal of Computer Research and Development, 
2005, vol. 42 (10), pp. 1785-1789. 
[4] Fang Gang, Wei Zu-Kuan, Yin Qian. An Algorithm of Association 
Rules Double Search Mining Based on Binary. In Proc. of 7th 
International Conference on Machine Learning and Cybernetics, IEEE 
Press, China, 2008, pp.184-189. 
[5] Fang G, Wei ZK, Yin Q. The Research of Association Rules Mining 
Algorithm Based on Binary. In Proc. of International Conference on 
Cybernetics and Intelligent Systems and International Conference on 
Robotics, Automation and Mechatronics, IEEE Press, China. 2008, pp. 
406-410. 
[6] Liu YL, Fang G, Xiong J, Wu YB. An Efficient Algorithm of Mining 
Association Rules Based on Digital Pure Subset. In Proc. of 6th 
International Conference on Fuzzy Systems and Knowledge Discovery, 
IEEE Press, 2009, China, pp.43-46. 
[7] Fang G, Wei ZK, Liu YL. An Algorithm of Improved Association Rules 
Mining. In Proc. of 8th International Conference on Machine Learning 
and Cybernetics, China. IEEE press, 2009, pp.133 - 137. 
 
 514 
ThP1.8
