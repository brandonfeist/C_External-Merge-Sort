Ranking Evaluation Functions to Improve Genetic Feature Selection in
Content-Based Image Retrieval of Mammograms?
Se´rgio Francisco da Silva, Agma J. M. Traina,
Marcela Xavier Ribeiro, Joa˜o do E. S. Batista Neto, Caetano Traina Jr.
Department of Computer Science, University of Sa˜o Paulo at Sa˜o Carlos USP
{sergio, agma, mxavier, jbatista, caetano}@icmc.usp.br
Abstract
The ranking problem is a crucial task in the information
retrieval systems. In this paper, we take advantage of single
valued ranking evaluation functions in order to develop a
new method of genetic feature selection tailored to improve
the accuracy of content-based image retrieval systems. We
propose to boost the feature selection ability of the genetic
algorithms (GA) by employing an evaluation criteria (fit-
ness function) that relies on order-based ranking evalua-
tion functions. The evaluation criteria are provided by the
GA and has been successfully employed as a measure to
evaluate the efficacy of content-based image retrieval pro-
cess, improving up to 22% the precision of the query an-
swers. Experiments on three medical datasets containing
breast cancer diagnosis and breast tissue density analysis
showed that fitness functions based on ranking evaluation
functions occupy an essential role on the algorithms’ per-
formance, obtaining results significatively better than other
fitness function designs. The experiments also showed that
the proposed method obtains results superior than feature
selection based on the traditional decision-tree C4.5, naive
bayes, support vector machine, 1-nearest neighbor and as-
sociation rule mining.
1. Introduction
Breast cancer is the second most common type of cancer
in women [15]. Mammograms are a valuable tool to the
early detection of the disease and may contribute to increase
the patient survival rates. Due to the decreasing costs of
mammographies and storage devices, the volume of digital
mammograms has grown exponentially. The exploration of
such data has become of great importance as they normally
contain useful information on past cases, which can help
?This work has been supported by FAPESP, CNPq and CAPES.
specialists to retrieve similar cases and also contribute to
their analysis task and treatments.
Content-Based Image Retrieval (CBIR) refers to the
technique to retrieve images based on their content, as op-
posed to textual descriptions. In medical domain, the ob-
jective of a CBIR system is to aid the specialist on diag-
nosing patients by retrieving relevant past cases with proven
pathology, along with the associated clinical diagnostic, and
other information [1].
In traditional approaches of CBIR, each image in the
dataset is represented by a set of feature values, the so-
called feature vector, which conveys the essence of the im-
age regarding specific criteria. Therefore, feature vectors
can hold one or more sets of features generated by one or
more image feature extractors, and they are used during the
indexing and retrieval process. At retrieval, the images that
are most similar to the query image according to some dis-
tance measure (e.g. Euclidean distance) are returned. In this
context, well-suited features are very important to improve
the accuracy of similarity queries results. For most CBIR
applications, including medicine, it is well-known that a
set of image features computed from a single extractor is
not usually the most appropriate way of characterizing im-
ages. However, multiple feature extractors usually provide a
large number of features, many times containing correlated
and irrelevant information that deteriorates the efficiency of
similarity queries and impairs data mining techniques, lead-
ing to the dimensionality curse problem [12]. Feature selec-
tion methods have been proposed in the literature aimed at
dealing with the dimensionality curse.
Feature selection is one of the most important and fre-
quently used preprocessing technique for data mining [13].
It reduces the number of features, removes irrelevant, re-
dundant, or noisy data, and brings immediate benefits to ap-
plications: speeds up data mining algorithm, improves min-
ing performance, such as predictive accuracy, and improves
results comprehensibility.
Genetic algorithms (GAs) have become quite popular
due to their ability in dealing with very large search spaces
978-1-4244-4878-4/09/$25.00 ©2009 IEEE
[18]. Genetic algorithms [8] were inspired on Darwin’s evo-
lutionary theory: “the better adapted individuals tend to sur-
vive, transferring their genetic material to the next genera-
tion, while the less adapted tend to disappear”.
For each problem kind solved by a GA, a fitness function
must be supplied; and this choice is of crucial importance to
maximize the GA’s performance [18]. Given an individual
(represented as a chromosome), the fitness function must
return a numerical value that represents how well adapted
the individual is. This score will be used in the parents se-
lection and survival selection processes for the next gener-
ation thus that the better adapted individuals will have the
greater likelihood of being chosen. Therefore, the fitness
function must be tailored to the problem being dealt with.
The GA’s effectiveness is, in a large degree, determined by
how faithfully the fitness function characterizes the problem
to be optimized.
To better understand our proposal, it is necessary to de-
fine the concepts of similarity query and relevant element.
A similarity query returns a set of elements ordered accord-
ing to a similarity measure. This order is known as a rank-
ing. In this work, feature selection is considered a super-
vised task that uses a set of pre-labeled samples distributed
among a set of different classes. A relevant element (an
image, for example) is a sample returned by a query (re-
gardless of its position in the ranking) that belongs to the
expected class.
This work proposes using single valued ranking evalua-
tion functions, henceforth referred to only as ranking eval-
uation functions, in the design of evaluation criteria for ge-
netic feature selection algorithms in CBIR. A ranking eval-
uation function provides a measure of the quality of a simi-
larity query result. This work employs order-based ranking
evaluation functions which share the utility concept. The
utility of a relevant element is proportional to its position in
the ranking, i.e. the higher its position in the ranking (more
similar), the higher its utility. If an element is not relevant
(does not belong to the expected class) its utility score is set
to zero.
To the best of our knowledge, this is the first work using
ranking evaluation functions for feature selection. Perform-
ing feature selection in CBIR using a measure of ranking
quality as evaluation criteria is more adequate to evaluate
the retrieval effectiveness than the actual approaches that
use classification error, class separability or measures of in-
trinsic information. Here, we propose to use a measure of
ranking quality (ranking evaluation functions) to build fit-
ness functions for genetic feature selection algorithms. Due
to space limitations, we present results for three medical
datasets showing that this new feature selection approach
brings superior results when compared to evaluation cri-
teria derived from classification (decision-tree C4.5, naive
bayes, support vector machine, 1-nearest neighbor) and as-
sociation rule mining. We also show that it considerably
increases the precision of the similarity queries while de-
creasing the number of features selected. The cost to train
the genetic algorithm is high, however it can be done of-
fline, since the cost that effectvely counts to the user is the
searching one.
The remainder of this paper is structured as follows. Sec-
tion 2 gives the main concepts concerning this work. Sec-
tion 3 presents related works. Section 4 describes the pro-
posed framework for feature selection. Section 5 details the
experimental evaluation, and Section 6 presents the conclu-
sions.
2. Background
This Section discusses the main concepts necessary to
follow this paper.
2.1. Feature Selection
Feature selection (FS) aims at choosing a reduced num-
ber of features that preserves the most relevant information
of the dataset. Consequently, it performs a dimensionality
reduction of the dataset. Feature selection is usually applied
as a preprocessing step in data mining tasks: remove irrel-
evant or redundant features (dealing also with the curse of
dimensionality), leading to more efficient and effective clas-
sification, clustering and similarity search processes. It also
diminishes the cost to process the queries, as well as the
memory required.
A typical feature selection process consists of four basic
steps, namely, subset generation, subset evaluation, stop-
ping criterion, and result validation [13]. Subset generation
is a search procedure that produces candidate feature sub-
sets for evaluation based on a certain search strategy. Each
candidate subset is evaluated and compared with the previ-
ous best subsets according to a certain evaluation criterion.
If the new subset turns out to be better, it replaces the previ-
ous one. The process of subset generation and evaluation is
repeated until a given stopping criterion is satisfied. Then,
the selected best subset usually needs to be validated by a
test dataset.
In the wrapper feature selection process, the evaluation
criterium is usually a performance measure of a predeter-
mined mining algorithm. Works concerning feature selec-
tion for CBIR with the wrapper approach were originally
intended to optimize classification [22] and clustering per-
formance [5]. In our work, we take advantage of traditional
classifiers such as 1-nearest neighbor (1-NN), decision-tree
(C4.5), Support Vector Machine (SVM) and Naive Bayes
(NB) to build FS wrappers. These feature selectors are used
as baselines for comparison.
Another approach for feature selection is the use of sta-
tistical association rules, which led to the development of
the StARMiner algorithm [14]. The goal of StARMiner is
to implement statistical association rule mining to find fea-
tures that best discriminate images into categorical classes.
It is also employed as a baseline for comparison with our
method.
2.2. Genetic Algorithms
Genetic Algorithms (GAs) [8, 9] are based on the princi-
ples of biological inheritance and evolution . Each potential
solution is called an individual (i.e. a chromosome) in a
population. GAs work iteratively applying the genetic op-
erations of selection, crossover and mutation, to a popula-
tion of individuals to create more diverse and better adapted
individuals in subsequent generations. The fitness function
assigns a fitness value for each individual, and it is known
for playing an essential role in genetic evolution.
Associated with the characteristics of exploitation and
exploration search, GAs can efficiently deal with large
search spaces, and hence are less prone to stuck with a lo-
cal optimal solution than other algorithms. This is due to
its ability to allow multiple solutions representations (indi-
viduals) in the search space and by applying probabilistic
genetic operators.
2.3. Ranking Evaluation Functions
Ranking evaluation functions are measures of the rank-
ing accuracy (a set of sorted elements according to a simi-
larity measure). Ranking evaluation functions belong to two
categories: order-based and non order-based. Non order-
based ranking evaluation functions are measures in which
the score of an element in the ranking has a fragile relation-
ship to its position. An example is the R-precision measure
(percentage of relevant images between the first R in rank-
ing). Order-based ranking evaluation functions are based
in the utility concept, where the score value of a relevant
element in the ranking is usually inversely proportional to
its position. The fact that users would rather see relevant
elements appearing in the initial positions of the ranking,
suggests that order-based ranking evaluation functions are
more likely to be successful.
Various ranking evaluation functions have been proposed
in the literature. However, as far as we know, ranking eval-
uation functions have never been applied to the feature se-
lection domain. A ranking evaluation function that presents
promising results is [6]:
Fr(q, C) =
?
?i?I
(
r(i)
1
A
(
(A? 1)
A
)(pos(i)?1))
(1)
where Fr(q, C) denotes the score value of the individual
C (given by the chromosome coding, explained in the next
section) for the image query q. I represents the entire im-
age dataset, r(i) returns the relevance of the image i, where
r(i) = 1 if the image i is relevant for that query q, and
r(i) = 0 otherwise. A is a user-defined parameter with
values larger than or equal to 2. It determines the relative
importance of the positioning of an element in the ranking
position pos(i). For small values of A, greater importance
is given to relevant elements better positioned in the rank-
ing (i.e. those at the initial positions). When A takes high
values, i.e. the factor (A?1)A leads to values near 1, the rela-
tive position of the elements in the ranking are not strongly
reflected in the final score value. We have set A to 10,
which leads to an intermediate behavior of the score value
Fr(q, C). That is, the score values computed for all rel-
evant elements at increasing positions are more uniformly
distributed in the range (0, 1).
3. Related Works
Most of the works in data mining field regarding fea-
ture selection technique aim at maximizing the classifica-
tion accuracy. Also, there is various researches aim at opti-
mizing the information retrieval process. Many works em-
ployed evolutionary approaches to perform feature selec-
tion [20, 21, 13, 5, 18] and others to improve information
retrieval (IR) [7, 17, 4]. A wide review of the application
of evolutionary computation to boost information retrieval
is presented in [4].
The main focus of this work is to provide effective infor-
mation retrieval (IR), specifically image retrieval of mam-
mograms, employing feature selection. It is well-known
that an image retrieval system can be affected by any of
the three subsystems: images representations, queries rep-
resentations and similarity functions. Previous approaches
for effectively improving IR performance by manipulating
queries representations have been developed in [2], and also
dealing with relevance feedback techniques [3, 10, 16].
Recently, similarity functions and ranking functions
have received more attention of the researchers. Many rank-
ing functions were developed and applied to Web informa-
tion retrieval, as well as similarity functions to image re-
trieval. However, recent studies show that these functions
do not perform consistently well across different contexts
[7, 17]. According to [17], an image descriptor is a pair
composed of an image feature extractor and a distance func-
tion, since the similarity functions and the image represen-
tations (feature vectors) have a crucial role in the retrieval
performance.
The approaches presented in [7] and [17], respectively,
showed solutions about how to combine multiples ranking
functions and descriptors using evolutionary computation
optimization and an evaluation criterion based on ranking
quality. Those works consider that there is not a best rank-
ing function nor a best descriptor for a given problem. In the
classification context, ensemble techniques combine predic-
tion of multiple models to allow higher accuracy, which are
often not achievable when using just a single model. The
works presented in [18, 19] employed genetic algorithms to
select feature subsets to construct ensembles using one base
classifier.
Another approach to improve an image retrieval tech-
nique is to modify the images representations. It is well-
known that the choice of an adequate feature vector im-
proves the accuracy of the image retrieval. Many filter fea-
ture selection techniques are applied to improve image re-
trieval. Such techniques are based on the idea that corre-
lated or inconsistent feature deteriorate the ability of data
differentiating. Wrapper feature selection techniques are
also applied to improve image retrieval. However, the latter
does not use an adequate evaluation criteria for image re-
trieval, which normally is based on classification error or a
measure of clustering separability [5].
In this is work it is proposed a method to improve im-
age retrieval through feature selection. The quality of fea-
tures subsets (chromosomes) is evaluated using a measure
of ranking quality given by a ranking evaluation function
and evolved by a genetic algorithm.
4. The Proposed Framework
The present framework uses GA with a fitness function
based on the ranking evaluation concept to perform feature
selection for CBIR. This decision stemmed from three rea-
sons: (i) the large size of the search space in feature selec-
tion; (ii) previous successfully usage of GA in the feature
selection domain; and (iii) no prior works on feature selec-
tion based on optimizing ranking quality applied for CBIR.
Figure 1 illustrates the steps from the pipeline that im-
plemented the proposed method. Here, the feature selection
step is a supervised process. In the training phase, image
features are extracted from the image training set and then
submitted to the feature selection process. The feature se-
lection process guided by GA searchs for the best features
subset according to an evaluation criteria based on the rank-
ing quality – given by a single valued ranking evaluation
function (Fc). In the test phase, the features selected indi-
cated by the best chromosome chosen the GA in the train-
ing phase, will be used for representing the images of the
test set. Similarity queries are performed considering each
image from the test set as a query image, and finally the
average precision-recall curve is built.
The corresponding GA designed for feature selection is
described as follows.
The chromosome coding – In order to apply GA to a given
Figure 1. Pipeline of the proposed method.
problem, it is necessary to define the genotype required by
the problem, i.e. the chromosome representation. In other
words, a decision must be made on how the parameters of
the problem will be mapped into a finite string of symbols
(genes), encoding a possible solution in the problem space.
In this work, a chromosome was coded by an n-bit string
or binary-valued vector (n is the initial number of features),
C = (g1, g2, . . . , gn), where gi takes value 0, if the i-th
feature is excluded from the subset, and 1, if it is kept in the
subset. We refer to C as a feature selection vector and gi as
a feature selection variable.
The genetic operators – GA searches for better solutions
by genetic operations, including selection, crossover and
mutation. Selection implements the survival of the best fit-
ted individuals according to some predefined fitness mea-
sure. Therefore, high-fitted individuals have a better chance
of surviving and reproducing, while low-fitted ones are
more likely to disappear. Crossover and mutation opera-
tions represent an analogy to natural reproduction and ex-
plore the solution space to find the best solution. The ge-
netic operations used in this study are:
• Selection for recombination: applied to select pairs of
individuals that will go on reproducing (mating pool).
Linear Ranking Selection was used: the individuals are
sorted according to their fitness and the last position is
assigned to the best individual, while the first position
is allocated to the worst one. The selection probability
is linearly assigned to the individuals according to their
ranks.
• Selection for reinsertion: a total of (Sp ? 2) best off-
springs and 2 best parents according to their fitness
values survive from two consecutive generations. Sp
is the population size.
• Crossover: represents the mating of two individuals
to form two new individuals (offsprings). Uniform
crossover was used in this work. Uniform crossover
is a sort of multiple points crossover taken to the ex-
treme, where instead of raffling crossover points, raffle
a mask with the size of the chromosome that indicates
which Chromosome-Parent will supply each gene to
Offspring 1. Offspring 2 is generated by the comple-
ment of the mask.
• Mutation: a gene (a feature selection variable) selected
for mutation is substituted by its complement, i.e. each
chosen bit will be changed from 0 to 1 and vice-versa.
This is known as uniform mutation.
Fitness function: The fitness function plays a very im-
portant role in guiding a GA to obtain the best solutions
within a large search space. Good fitness functions help a
GA to explore the search space more effectively and effi-
ciently. Not proper fitness functions, on the other hand, can
easily make the GA get trapped in a local optimum solu-
tion and lose the discovery power. Two main designs have
been analysed in this work: classification error and fitness
functions based on ranking evaluation functions. From the
order-based ranking evaluation function Fr(q, C) (Eq. 1), a
mechanism to derive the fitness function Fc(Q,C) for the
GA has been devised. Fc(Q,C) (Eq. 2) is given by the
average of the score values obtained from each image q of
the training set Q as an image query. nQ is the number of
images in the set Q. The output of the fitness function has
been normalized within the range [0, 1], where 1 indicates
maximum accuracy. According to this criterion, the prob-
lem consists of seeking for the highest values. On the other
hand, this can also be seen as a minimization problem if the
output is subtracted from 1 (1 - output). This is the approach
adopted in this work.
Fc(Q,C) =
?
?q?Q Fr(q, C)
nQ
(2)
We recall that the fundamental principle in wrapper fea-
ture selection approaches is the minimization of the number
of features, while optimizing (or preserving) the quality of
the result. For a CBIR system, the best approach is to re-
trieve images with a minimum amount of features and high-
est accuracy. This concept led to the proposal of two distinct
fitness functions FcA(Q,C) and FcB(Q,C) (Equations 3
and 4) that combine two optimization criteria. The first cri-
terion indicates the quality of the query results given by the
term Fc(Q,C) present in both Equations. The second crite-
rion, the minimization of the number of features, are given
by the terms (|C|?d)n and
|C|
n in Equations 3 and 4, respec-
tively.
In both Equations, |C| is the number of selected features
coded by chromosome C, Q represents the training set, d
is the number of desired features specified by the user, and
n is the dimensionality (number of features) of the entire
dataset. The term (|C|?d)n yields high values when the num-
ber of selected features |C| widely differs from the number
of desired features d. The term |C|n of Equation 4 is also
a penalizing factor that takes into account the number of
selected features only. Finally, ? is an adjustment parame-
ter in the range [0,1], which determines the importance as-
signed to each criterion, in a complementary way.
FcA(Q,C) = ?(Fc(Q,C)) + (1? ?)
(
(|C| ? d)
n
)
(3)
FcB(Q,C) = ?(Fc(Q,C)) + (1? ?)
( |C|
n
)
(4)
Control parameters: the experiments described in this
work employed crossover probability pc = 0.8, mutation
probability pm = 0.01 for each gene (a feature selection
variable).
5. Experiments and Analysis
This section presents three experiments in which the pro-
posed technique is employed to perform content-based im-
age retrieval of mammograms. The datasets were split into
training and test subsets. Feature selection has been car-
ried out on the training subset, while performance evalu-
ation has been conducted through precision–recall (P&R)
curves [2] over the test subset for different feature selec-
tion techniques, grouped as: (a) traditional methods (1-NN,
C4.5, SVM and Naive Bayes - NB); (b) non order-based
ranking evaluation functionR-precision (FR-Precision); (c)
the association rules (StARMiner algorithm) ; (d) the pro-
posed technique (Fc, FcAand FcB) and (e) all features
combined (no feature selection). A rule of thumb when an-
alyzing P&R curves is the closer to the top the better the
technique.
All methods evaluated appear in the legends of Figures 2,
3 and 4. The values in parenthesis indicate the numbers of
features selected (|C|). The methods of group (a) employ
classification errors as minimization criteria for the GA.
Group (b) employs a fitness function based on R-precision.
Group (c) is based on association rules. Group (d) are
those related to the proposed framework. Fc, FcAand FcB
correspond to the proposed fitness functions Fc(Q,C),
FcA(Q,C), FcB(Q,C) given by the Equations 2, 3, 4 and
respectively.
The evolutionary search was set to 100 individuals,
evolving along 400 generations in experiments 1 and 2. Ex-
periment 3 employed 50 individuals and 250 generations.
According to the dimensionality of the datasets, the parame-
ter d employed in FcA (Equation 3) was set to 50 in the first
and second experiments, and to 20 in the third experiment.
The Euclidean distance was employed in the experiments to
measure the similarity between the feature vectors.
5.1. Experiment 1 - ROI-102 image dataset
This dataset consists of 102 images with regions
of interest (ROI), taken from mammograms collected
from the Breast Imaging Reporting and Data System of
the Department of Radiology of University of Vienna
(http://www.birads.at) classified into three levels of BI-
RADS (3, 4 and 5). The BIRADS (Breast Imaging Re-
porting and Data System) categorization was developed by
the American College of Radiology to standardize mam-
mogram reports and procedures. The BIRADS categoriza-
tion is summarized in Table 1. The dataset has been di-
vided into a 68–image training set and a 34–image test set.
Each image has been characterized by a 850–dimensional
feature vector, including features generated by Haralick de-
scriptors (140 features), wavelets (64 features), zernike mo-
ments (255 features), histogram (256 features), features of
first order derived of histogram (6 features), Run length (44
features) and Edge Histogram MPEG7 (80 features). Fig-
ure 2 shows the P&R curves for the test dataset and also
the number of features selected for each method. It can be
seen that the proposed framework with fitness function de-
rived from order-based ranking evaluation function yielded
superior results when compared with the traditional evalu-
ation criteria given by the average classification error, im-
proving up to 22% the precision of the query answers. The
proposed framework also outperforms the fitness function
derived from non order-based ranking evaluation function
(FR-Precision) and all features combined. Also, the pro-
posed framework finds the sets with the fewest features in
comparison with the other methods.
5.2. Experiment 2 - ROI-250 image dataset
This experiment investigates the performance of
the proposed technique for a 250–image mammog-
raphy dataset with ROIs comprising lesions, taken
from the Digital Database for Screening Mam-
Table 1. BIRADS categorization.
value description
0 Need Additional Imaging Evaluation.
1 Negative.
2 Benign Finding.
3 Probably Benign Finding. Short Interval Follow-
Up Suggested.
4 Suspicious Abnormality. Biopsy recommended.
5 Highly Suggestive of Malignancy. Proper Action
Must be Taken.
Figure 2. Precision-recall curve in ROI-102
image dataset.
mography of the University of South Carolina
(http://marathon.csee.usf.edu/Mammography/), classi-
fied into two classes: mass-benign and mass-malign. A
739–dimensional feature vector has been computed for
each sample, including features generated by Haralick
descriptors (140 features), zernike moments (255 features),
histogram (256 features), features of first order derived
of histogram (6 features), Run length (44 features) and
invariant moments (38). The dataset was divided into a
166–image training subset and a 64-image test subset.
Figure 3 shows the precision-recall curves obtained and
also the number of features selected in each method.
The graphs of Figure 3 show that the proposed methods
(FcA, FcB and Fc) increased the precision of the queries
in about 15% in the region of 5% of recall in comparison
with the other methods, while decreasing the number of fea-
tures from 739 to around 50. This is, using about 7% of
the previous memory space for the images representation.
These results indicate that ranking evaluation functions are
well-suited to be employed in genetic feature selection for
CBIR.
Figure 3. Precision-recall curve in ROI-250
image dataset.
5.3. Experiment 3 - Mammograms-1080 im-
age dataset
This experiment employed a dataset composed by 1080
mammograms images collected in the Clinical Hospital of
University of Sao Paulo at Ribeiro Preto. The dataset was
previously classified into 4 levels of breast tissue density:
(1) mostly fatty (362 images); (2) partly fatty (446 images);
(3) partly dense (200 images); and (4) mostly dense (72 im-
ages).
Figure 4. Precision-recall curve in
Mammography-1080 image dataset.
Breast density is an important risk factor in the devel-
opment of breast cancer. In this experiment, the images
are represented by the feature set proposed in [11], build-
ing a vector of 85 features, including shape and size of the
breast, the conditions of the breast contour; nipple position,
and the distribution of fibroglandular tissue. This dataset
was divided in training set and test set. The training set is
composed of 720 images and test set is composed of 360
images.
Figure 4 shows the P&R curves over test dataset and also
the number of features selected in each method. Again, the
proposed methods reached the highest values of precision
and select the smallest number of features.
(a)
(b)
(c)
(d)
Figure 5. Queries in Mammography dataset:
(a) – is the query image; (b) using the fea-
tures selected through fitness function FcB;
(c) using the features selected through clas-
sification error of C4.5; and (d) using the all
features extracted
Results for the retrieval of the 5 most similar images
from a query image are also provided in this experiment,
as illustrated in Figure 5. The image 5.(a) is the query im-
age taken from the mostly fatty image class. Images shown
in 5.(b) are the 5 most similar images retrieved for the pro-
posed fitness function FcB. The row (c) shows the results
for C4.5 classifier, whereas (d) illustrate the images result-
ing from all features (no selection applied). In Figure 5,
the images surrounded by dashed lines are false positives
(not relevant images). For this query, the proposed method
achieved the highest precision (100%) when compared the
results of C4.5 and the original feature vector (precision of
40%).
6. Conclusions
This work proposed a novel genetic feature selection
framework for CBIRs. It employs a wrapper strategy that
searches for the best reduced feature set, while optimizing
(or preserving) the quality of the solution. From a rank-
ing evaluation function, three new fitness functions namely,
FcA, FcB and Fc have been proposed and evaluated in
three experiments.
The proposed genetic feature selection approach, which
encompasses FcA, FcB and Fc, has been compared with
(a) traditional methods found in the literature, (b) the
StARMiner feature selector and (c) the whole feature vec-
tor, and significantly outperformed them.
The proposed approach has been able to optimize the ac-
curacy of similarity queries while selecting a significatively
reduced number of features. Additionally, the proposal of
combining the quality of the query results with the crite-
rion of minimizing the number of selected features, FcA
and FcB, led to high accurate query answers while reduc-
ing the number of features more than the fitness function
Fc. Therefore, the final processing cost of the queries is
also reduced.
References
[1] P. M. d. Azevedo-Marques, N. A. Rosa, A. J. M. Traina,
C. Traina-Jr., S. K. Kinoshita, and R. M. Rangayyan. Re-
ducing the semantic gap in content-based image retrieval in
mammography with relevance feedback and inclusion of ex-
pert knowledge. International Journal of Computer Assisted
Radiology and Surgery, 3(1-2):123–130, June 2008.
[2] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information
Retrieval. Addison-Wesley, Essex, UK, 1999.
[3] B. Bartell, G. Cottrell, and R. Belew. Optimizing similar-
ity using multi-query relevance. Journal of the American
Society for Information Science, 49:742–761, 1998.
[4] O. Cordo´n, E. Herrera-Viedma, C. Lo´pez-Puljalte,
M. Luque, and C. Zarco. A review on the application of
evolutionary computation to information retrieval. Interna-
tional Journal of Approximate Reasoning, 34:241–264, July
2003.
[5] J. G. Dy, C. E. Brodley, A. Kak, L. S. Broderick, and
A. M. Aisen. Unsupervised feature selection applied to
content-based retrieval of lung images. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 25(3):373–
378, March 2003.
[6] W. Fan, E. A. Fox, P. Pathak, and H. Wu. The effects of fit-
ness functions on genetic programming-based ranking dis-
covery for web search. Journal of the American Society for
Information Science and Technology, 55(7):628–636, 2004.
[7] W. Fan, P. Pathak, and M. Zhou. Genetic-based approaches
in ranking function discovery and optimization in informa-
tion retrieval - a framework. Decision Support Systems,
2009.
[8] D. E. Golberg. Genetic algorithms in search, optimization
and machine learning. Addison Wesley, 1989.
[9] R. L. Haupt and S. E. Haupt. Practical Genetic Algorithms.
John Wiley & Sons, New Jersey, United States, second edi-
tion edition, 2004.
[10] J. Horng and C. Yeh. Applying genetic algorithms to query
optimization in document retrieval. Information Processing
& Management, 36:737–759, 2000.
[11] S. K. Kinoshita, P. M. d. Azevedo-Marques, R. R. Pereira-
Jr., J. A. H. Rodrigues, and R. M. Rangayyan. Content-
based retrieval of mammograms using visual features re-
lated to breast density patterns. Journal of Digital Imaging,
20(2):172–190, June 2007.
[12] F. Korn, B. Pagel, and C. Faloutsos. On the ’dimensional-
ity curse’ and the ’self-similarity blessing’. IEEE Trans. on
Knowledge and Data Engineering, 13(1):96–111, 2001.
[13] H. Liu and L. Yu. Toward integrating feature selection algo-
rithms for classification and clustering. IEEE Transactions
on Knowledge and Data Enginnering, 17(4):491–502, April
2005.
[14] M. X. Ribeiro, A. J. M. Traina, C. Traina-Jr, and P. M.
Azevedo-Marques. An association rule-based method to
support medical image diagnosis with efficiency. IEEE
Transactions on Multimedia, 10(2):277–285, 2008.
[15] U. S. Cancer Statistics Working Group. United states
cancer statistics: 1999-2005 incidence and mortality web-
based report. atlanta (ga): Department of health and hu-
man services, centers for disease control and preven-
tion, and national cancer institute., 2009. Available in
http://apps.nccd.cdc.gov/uscs/.
[16] L. Tamine, C. C., and M. Boughanem. Multiple query
evaluation based on an enhanced geneticnext term algo-
rithm. Information Processing & Management, 39(2):215–
231, 2003.
[17] R. S. Torres, A. X. Falca˜o, M. A. Gonc¸alves, J. P. Papa,
Z. B., W. Fan, and E. A. Fox. A genetic programming
framework for content-based image retrieval. Journal of the
American Society for Information Science and Technology,
42(2):283–292, 2009.
[18] A. Tsymbal, P. Cunningham, M. Pechenizkiy, and S. Pu-
uronen. Search strategies for ensemble feature selection in
medical diagnostics. In Proceedings of the 16th IEEE Sym-
posium on Computer-Based Medical Systems, pages 124–
129, June 2003.
[19] A. Tsymbal, M. Pechenizkiy, and P. Cunningham. Sequen-
tial genetic search for ensemble feature selection. In Pro-
ceedings of the International Joint Conferences on Artificial
Intelligence, pages 877–882, August 2005.
[20] C.-M. Wanga and Y.-F. Huang. Evolutionary-based feature
selection approaches with new criteria for data mining:: A
case study of credit approval data. Expert Systems with Ap-
plications, 36(3 - Part 2):5900–5908, 2009.
[21] H. Yan, J. Zheng, Y. Jiang, C. Peng, and S. Xiao. Select-
ing critical clinical features for heart diseases diagnosis with
a real-coded genetic algorithm. Applied Soft Computing,
8:1105–1111, 2008.
[22] T. Zhao, J. Lu, Y. Zhang, and Q. Xiao. Feature selection
based on genetic algorithm for cbir. In IEEE Congress on
Image and Signal Processing, volume 2, pages 495–499,
2008.
