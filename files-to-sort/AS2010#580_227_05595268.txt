Temporal Pattern Mining
(Invited Paper)
Rudolf Kruse?, Matthias Steinbrecher†, Christian Moewes‡
Department of Knowledge Processing and Language Engineering
Otto-von-Guericke University of Magdeburg
Universitätsplatz 2, 39106 Magdeburg, Germany
{ ?kruse, †msteinbr, ‡cmoewes }@iws.cs.uni-magdeburg.de
Abstract—Data analysis has become an integral part in many
economic fields. In this paper, we present several real-world
applications occurring in the fields of automobile development
and manufacturing, finance, and online communities. The given
examples share one aspect in common: time. It is not only the
fact to find patterns inside data volumes but also to identify them
based on their temporal behaviour.
We will give examples of dealing with different models of in-
corporating the temporal aspects. Furthermore some new results
in the area of Visual Data Analysis are presented. These methods
offer intuitive methods of guiding the user through the process
of data and model inspection and assist in drawing conclusions
with the help of meaningful graphical representations.
I. INTRODUCTION
Automatic data analysis is a crucial part in strategic plan-
ning for business organizations around the world. Global com-
petition, shorter production cycles and increasing customer
requirements put high demands on the responsiveness of every
company.
One of the core concepts of visual analytics is to integrate
automatic and visual analysis techniques to explore complex
unknown data sets. The vital objective is to quickly identify
meaningful patterns within the collected data volumes. Such
patterns comprise changed customer interests that should be
addressed in order to prevent churning. A car manufacturer
may want to detect gradually occurring failures in delivered
vehicles to prevent expensive callbacks. Financial service
providers are highly engaged in detecting fraudulent behavior
of customers and co-contractors.
But change is not necessarily related to problems. Change
can also mean an opportunity (like an evolving group of
customers) to a business. Therefore, (pattern) change detection
is a vital task in order to survive or to win.
Many data collected are already time-stamped. Due to its
temporal nature business data reflect external influences like
management decisions, economic and market trends and thus
captures the changes a business is interested in.
All these above-mentioned scenarios share a common prop-
erty: the respective patterns are not likely to arise out of a
sudden but will rather evolve slowly over time. Conversely,
after a problem has been addressed, it would be naive to
expect the patterns disappear immediately. Again, an effect
of a counteraction will need time to become visible. Thus,
the temporal evolution of such patterns carries valuable—if
not vital—information about the urgency of the underlying
problem (or the effectiveness of the treatment).
However, a fully automatic approach has its limitations. In
order to minimize response times to problems, data analysis
results must be interpretable by technical staff that normally
has no statistical background. In addition, the analysis should
be as transparent as possible to comprehend all inferences and
conclusions that were drawn.
To address this, we introduced an interactive visual analysis
approach [3], [16], [13] for so-called association rules [1]—
one of the most prominent data mining techniques.
The next section will briefly review the announced visu-
alization technique. Section III will discuss results from two
cooperations with the automotive industry whereas we sketch
a project carried out in the financial setting in section IV.
A last success story from the analysis of user behavior in an
3D online environment is told in section V before we conclude
the paper in section VI.
II. RULE VISUALIZATION
In order to visualize a single (association) rule, our method
of choice is an icon-based representation [3], [18]. This is
justified in the fact that usually we have a finite and rather low
number of antecedent attributes together with a low number of
derived measures. This icon shall contain visual information
about the support of the rule (that is the number of covered
database cases), the confidence of the rule (indicating the
validity or degree of applicability), the consequent attribute
value and the antecedent attributes values. Such an icon is
depicted in Figure 1: Every rule is represented as a circle the
size of which represents the support of the rule.
The interior is solidly filled with a color denoting the
consequent attribute value. The saturation of this color is
used to indicate the confidence of the rule: full saturation
represents 100% confidence (that is a deterministic rule) while
white would technically correspond to 0% validity. In some
applications, customers asked for another representation: we
3
Copyright © 2010 by Institute of Electronics, Silesian University of Technology
Fig. 1. The visualization of a single association rule as it is used in this
paper. The outer ring encodes the values of the antecedent attributes whereas
the interior represents the class value and the confidence of the rule.
also used pie charts to denote the confidence as we will see
later.
The antecedent of the rule is shown in the border of the
circle. For every possible attribute (i. e. for every attribute
except the class attribute) a unique fragment is reserved.
The fragments are equally sized and are ordered clockwise,
starting at the right. Every fragment is filled according the
following policy: If the respective attribute is referenced in the
antecedent, the corresponding fragment is filled with a color
that uniquely represents the value of the attribute’s domain.
This way of representation, of course, is only feasible if the
number of attributes and the size of their domains is small.
Otherwise, we simply omit the other antecedent ring.
Now that we have an icon for a single rule, we need to
arrange them in a chart. We propose to assign as coordinates
the value of association rule evaluation measures [19]. More
specific, we recommend the lift value of a rule as its y-
coordinate and the recall as its x-coordinate (the confidence as
a third interesting measure is represented by the (de-)saturated
inner color and the support is represented by the circle area).
Doing so, we are able to encode four numeric dimensions
into a two-dimensional image without redundancy. Of course,
every other selection of four measures is possible, however,
we found the above measures intuitive and will use them in
every figure of this work.
III. AUTOMOBILE MANUFACTURING AND DEVELOPMENT
A. Fault Detection
A major cooperation with an automobile manufacturer led
to the investigation of a vehicle database containing ap-
proximately 300 000 cases that exhibited 180 attributes. The
manufacturer kept record of the configuration of every car that
left the production plant. Whenever a failure occurred, the
database was updated. Every tuple in the dataset represents a
unique car that left the production plant of the manufacturer.
Since for every car the time of a failure was logged as
well, we were able to partition the full set of cars into data
sets of (in this case) equal length. We used a preprocessing
technique [13] based on Bayesian networks [12] to induce a
set of attributes that should serve as antecedent attributes of
the rules to be visualized. It was possible to identify a small
set of meaningful attributes (out of the 180) that were used
to generate rules whose temporal trajectories were visualized.
Figure 2 shows a set of 760 rules at three different times.
There are numerous rules that were interesting to experts.
We selected four rules, two showing an evolving problem
and two representing a vanishing group of failed cars. To
simplify the assessment, we superposed the rule locations of
the second and third chart with the first and indicated the
motion with arrows. Four rules that showed an interesting
behavior and could be assigned a meaning by experts are
numbered in the figure: rule 1 and 2 represent shrinking sets
of cars whose confidence is also dropping. More interesting,
however, is the rapidly lessening lift which gave rise to the
conjecture that the cause for the failure had been successfully
addressed. Contrary, rules 3 and 4 represent sets of cars with
increasing failure rate (confidence is increasing indicated by
the darkening of the interior of the icons).
B. Safety Systems
Every millisecond todays cars must decide based on dozens
of signals whether (and if so which) components of the
restraint systems shall be deployed or not. The airbag e. g.
shall only deploy in severe crash situations, so-called fire
crashes. Unfortunately in the first moments of a crash, the
signals during a no-fire crash are very similar to those obtained
by a fire crash. Matters are complicated further by the crash
test costs that usually come up to a car’s price. In addition,
dissimilar vehicles behave different in a standardized crash
test. The decision logic developed by automobile suppliers
must therefore be accurately aligned to every car platform.
Decision rules, e. g. based on fuzzy logic, are formulated
by domain experts doing tedious detail work to meet most
requirements of a car manufacturer and technical inspectors.
The rule base to be generated must be both highly interpretable
and very accurate regarding crash discrimination. to learn such
decision logic is still open. Many attempts to automatically
induce fuzzy rules from observations (see e. g. [11], [5], [17])
fail since they generate either way too many rules composed
of many clauses or too simple rules leading to a high error-
proneness.
We try to find an algorithm which is capable to induce a
decision logic that is accurate and interpretable. The problem
to maintain high accuracy has been already solved using
statistical learning methods, e. g. SVMs [6], [9], [10]. Efforts
so far to induce interpretable and accurate fuzzy rules from
SVMs failed [7].
Note that each algorithm must deal with the preference
structure of this binary classification problem. That is, at each
point in time which is labeled negatively (i. e. no-fire) must
be predicted correctly. Every false positive may lead to severe
injuries. On the contrary it is enough to classify one positive
instance before a certain time has past. Recognizing a fire
crash after this point in time could be fatal. Each signal can
be seen as criterion, i. e. an attribute with preference-order
domain. Clearly, the higher one of the signals, the more likely
it is to fire at that point in time. So, both decision classes (i. e.
fire and no-fire) are preference-ordered as well. We conclude
4
Fig. 2. Real-world application of a set of vehicles with a binary class
variable: failure and no failure. Only rules indicating a failure are depicted.
Two attributes were used to form the rules, hence there are two filled regions in
the outer ring of every rule. The three charts shows the rules at the beginning,
the middle and the end of the production period. To assess the motion of the
rules, we superposed the final locations of the rules with the first image and
indicated the corresponding rule with an arrow. Bold arrows indicate the four
rules that were found interesting by experts.
Fig. 3. Fuzzy partitioning of attributes: First, the crisp rule thresholds ?i,j
are clustered with k-means and k = 6. Then, the cluster centroids are used
as intersecting points of neighboring fuzzy sets. The arbitrary widths wi,j of
the overlapping fuzzy sets are found by an EA.
that the task to solve is an ordinal classification problem
with monotonicity constraints. Due to that we apply the
Variable Consistency Dominance-based Rough Set Approach
(VC-DRSA) [4]. It enables us to directly incorporate the above
mentioned preference structures. Its key idea is to approximate
the given data table by dominance relations. These relations
induce decision rules of the preferential information in the
data. Here, all rules predicting the fire class shall be certain,
i. e. they must not cover any negative instance. Certain rules
can be written as
Ri : if x1 ? ?i,1 and . . . and xp ? ?i,p then class = 1.
We use the algorithm VC-DomLEM [2] to induce crisp rules
of that kind. Note that the obtained rule base only covers
positive examples. The negative class is predicted when no
positive rule fires.
Every rule is fuzzified by a fuzzy partitioning of all
attributes in order to further compress the rule base. The
partitioning of every attribute xj with 1 ? j ? p is performed
by several heuristic steps. First, the clause thresholds ?ij of
all rules Ri with 1 ? i ? r are collected. Second, an arbitrary
number of splits kj is determined, e. g. by the user. Note that
xj is partitioned into kj + 1 fuzzy sets. Here, suppose that
kj = k = 6 for all j ? {1, . . . , p}. Then, we group all elements
of {?ij | 1 ? i ? r}, e. g. using k-means clustering. Finally,
the cluster centroids c1,j , . . . , ck,j represent the intersecting
points of neighboring trapezoidal fuzzy sets µi,j , µi+1,j such
that
µi,j(ci,j) = µi+1,j(ci,j) = 0.5.
The complete process of fuzzification is shown exemplarily
for x1 in Figure 3.
The only parameters left for tuning are the intersecting
widths wi,j of the overlapping trapezoidal fuzzy sets. We
simply apply a steady-state evolutionary algorithm to find
good assignments of every width. Each candidate solution is
represented by a vector of reals, i. e. all wi,j’s. The following
procedure is called for evaluating the fitness of a candidate
solution: First, we partition the attributes according to the fixed
centroids and the variable widths. Second, we fuzzify every
rule’s clause using the fuzzy partition. Finally, the fitness is
5
given by the specificity of the fuzzy rule-based classifier. That
is, the ratio of true negatives divided by the sum of true and
false negatives.
We finally obtained a minimal set of certain rules that
in average only use 25% of all possible attributes as fuzzy
clauses. Whereas the specificity of the crisp classifier is nearly
perfect, the one of the fuzzy classifier is significantly lower
as expected. The discretization of all attributes, however,
increases the readability of the rules. Further improvements
can be made using, e. g. multiobjective optimization. We plan
to incorporate temporal dependencies of the data points into
the VC-DRSA. We expect temporal rules of a more general
form to further exploit the problem description. The integration
of time series data mining techniques, e. g. motif discovery [8],
might additionally boost the performance.
IV. FINANCIAL APPLICATIONS
The ongoing financial crisis has revealed how sensitive and
almost unpredictable a system the financial market is. Subtle
changes in customer behavior or credit loss rates may be
indicators that lead to premonitions upon which one may react
in order to narrow down monetary failures or losses.
An international capital company raised a project in which
we were to reassess credit contracts from a past trading
year. For every credit contract that was still to be repayed
the company used an in-house business intelligence system
(in connection with human experts) to infer the likelihood
of the contract to default. While this predictive system is
highly confidential, we were asked to assess the temporal
development of consecutive creditworthiness assessments.
Technically speaking, we employed a decision tree induc-
tion algorithm to arrive at an intuitive starting position. Since
every path of a decision tree can be considered an association
rule, we ended up with a rule set for visualization. Since
the target variable of the decision tree was chosen to be the
(boolean) failure indicator variable, we arrived at association
rules with consequents referring to exactly that variable of
interest.
Figure 4 shows one of the results. For the sake of brevity
we only show the first and last time step which are one fiscal
year apart. The confidence was encoded as a pie chart instead
of a shaded interior as this was considered more intuitive by
the customer. The two arrows in Figure 4 point out three rules
that showed a rather rapid increase in both lift and confidence
and the respective antecedent attributes were found worth a
further investigation (e. g. to use them as predictive features
in a future assessment).
V. ONLINE COMMUNITY ANALYSIS
This application stems from a cooperation with a company
creating content for the 3D online community SecondLife.1
In order to evaluate whether online content such as buildings
mimicking online stores or museums function as intended,
a set of virtual sensors is deployed in that environment.
1http://www.secondlife.com/
Fig. 4. Rules regarding credit failure rate at the beginning and the end
of one fiscal year. The two arrows mark three rules that showed a strong
lift and confidence increase. The respective contracts underwent a deeper
investigation.
Whenever a user passes by such a sensor (within a prespecified
vicinity) a visit event is logged. We analyzed such a log file for
100 sensors that had been logged for six months. The dataset
was then discretized month-wise. A common representation of
such a logfile is a so-called cooccurrence graph. The nodes
comprise the sensors whereas the edges represent that the
sensors connected by that edge have been visited by some set
of common users (within a certain timespan). Edge weights are
used to denote the cardinality of such sets. The cooccurrence
graph of the dataset discussed so far is depicted in Figure 5.
Even though we have proposed methods to deal with tem-
poral changes within series of cooccurrence graphs [14], [15],
we can extract more information when we induce rules from
these graphs and apply our visualization. This is advantageous
because of the following reasons: A rule of the form “If
sensor X was visited, sensor Y was also visited in p% of all
cases” is more meaningful than just an edge with an associated
weight (which corresponds to the absolute support). The above
rule depicted with our visualization immediately tells how
6
Fig. 5. Cooccurrence graph representing six months of visiting events of
100 sensors deployed on certain islands in SecondLife.
many visitors compared to other rules are covered and how
that number compares to the general visiting activity of a
sensor. Further, a certain rule set will have a fixed visual
representation whereas the corresponding graph needs to be
layouted which can create different impressions based on
the layout algorithms. However, coordinates from any graph
representation cannot carry the information of the rule icons.
Figure 6 shows the rules induced from the visiting events
of the above-mentioned log file at three different time steps.
Rules have been induced that covered at least 1% of all visits
and had a minimum confidence of 1%. Note that this low
confidence value is not unusual as we can expect a meaningful
lift value since this is the ratio between confidence and
consequent probability. Note also, that since a rule represents
an edge, the antecedent contains exactly one attribute. Hence,
we have omitted the border of the rule icons.
We focus our findings on two subsets of rules. The subset
marked by the ellipse in Figure 6 corresponds to a triangle in
the original graph.2 All rules show a lift increase to the middle
of the time period after which it decreased again. Semantically,
this corresponds to an uprise and loss of predictability of the
sensors in that subset: a high lift tells that given a visit at
one sensor, we can conclude a more likely visit at the sensor
described by the rule consequent. Of course, not necessarily
by the same user. An inquiry at the company confirmed that a
certain modification had been undertaken at the 3D content
in the vicinity of these sensors during the middle of the
2As the graph is undirected, it is possible that two rules are induced for
a single edge if they match the specified criteria. Hence, a triangle might be
represented by up to six different rules.
Fig. 6. Rules induced from the cooccurrence graph in Figure 5 at three
different time steps. The two rule sets that are marked by the ellipse
and rectangle were identified to correspond to modified artifacts inside the
3D world: The rule set marked by the ellipse showed a collective lift increase
towards the middle of the logging period and diminished after. The rules
marked by the rectangle exhibit a lift decrease and support increase.
log period. The rules marked by the rectangle in Figure 6
correspond to a single edge that was heavily visited as can be
seen from the rule icon size. The lift is decreasing, which is
not surprising in the underlying 3D setting. The two sensors
were placed along a frequented pathway that more and more
users were accepting to use. If the general usage increases
(and thus the consequent probability) the lift will decrease, as
we observe here.
7
VI. CONCLUSION
Data collection is a process in time. Hence, it is almost
always possible to attach to a database entry a time stamp.
We have shown that real-world applications can greatly benefit
when the time dimension is taken into account.
This article has demonstrated how complex data analysis
tasks can be greatly simplified by appropriate visualization and
temporal change mining methods. We therefore encourage to
focus research onto the augmentation of other soft computing
techniques in this respect.
REFERENCES
[1] Rakesh Agrawal, Tomasz Imielinski, and Arun N. Swami. Mining
Association Rules between Sets of Items in Large Databases. In Peter
Buneman and Sushil Jajodia, editors, Proc. ACM SIGMOD Int. Conf.
on Management of Data, Washington, D.C., May 26-28, 1993, pages
207–216. ACM Press, 1993.
[2] Jerzy B?aszczyn´ski, Roman S?owin´ski, and Marcin Szela?g. Sequential
covering rule induction algorithm for variable consistency rough set
approaches. Submitted to Information Sciences in 2009.
[3] Christian Borgelt, Matthias Steinbrecher, and Rudolf Kruse. Graphical
Models — Representations for Learning, Reasoning and Data Mining.
John Wiley & Sons, United Kingdom, 2nd edition, 2009.
[4] Salvatore Greco, Benedetto Matarazzo, Roman S?owin´ski, and Jerzy
Stefanowski. Variable consistency model of Dominance-Based rough
sets approach. In Rough Sets and Current Trends in Computing, pages
170–181. 2001.
[5] Hisao Ishibuchi, Tomoharu Nakashima, and Tadahiko Murata. Three-
objective genetics-based machine learning for linguistic rule extraction.
Information Sciences, 136(1-4):109–133, 2001.
[6] Christian Moewes. Application of support vector machines to discrimi-
nate vehicle crash events. Master’s thesis, School of Computer Science,
University of Magdeburg, Universitätsplatz 2, 39106 Magdeburg, Ger-
many, October 2007.
[7] Christian Moewes and Rudolf Kruse. Unification of fuzzy SVMs
and rule extraction methods through imprecise domain knowledge. In
José Luis Verdegay, Luis Magdalena, and Manuel Ojeda-Aciego, editors,
Proceedings of the International Conference on Information Processing
and Management of Uncertainty in Knowledge-Based Systems (IPMU-
08), pages 1527–1534. University of Málaga, June 2008.
[8] Christian Moewes and Rudolf Kruse. Zuordnen von linguistischen
Ausdrücken zu Motiven in Zeitreihen. at-Automatisierungstechnik,
57(3):146–154, March 2009.
[9] Christian Moewes, Clemens Otte, and Rudolf Kruse. Tackling multiple-
instance problems in safety-related domains by quasilinear SVM. In
Didier Dubois, M. Asunción Lubiano, Henri Prade, María Ángeles Gil,
Przemyslaw Grzegorzewski, and Olgierd Hryniewicz, editors, Soft Meth-
ods for Handling Variability and Imprecision, volume 48 of Advances
in Soft Computing, pages 409–416. Springer Berlin/Heidelberg, October
2008.
[10] Christian Moewes, Clemens Otte, and Rudolf Kruse. Simple machine
learning approaches to safety-related systems. In Rajat Kumar De,
Deba Prasad Mandal, and Ashish Ghosh, editors, Machine Interpretation
of Patterns: Image Analysis and Data Mining, volume 11 of Statistical
Science and Interdisciplinary Research, chapter 12, pages 231–249.
World Scientific Publishing Co. Inc., Hackensack, NJ, USA, June 2010.
[11] Detlef Nauck and Rudolf Kruse. A neuro-fuzzy method to learn fuzzy
classification rules from data. Fuzzy Sets and Systems, 89(3):277–288,
1997.
[12] Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of
Plausible Inference. Morgan Kaufmann, San Mateo, California, 1988.
[13] Matthias Steinbrecher and Rudolf Kruse. Identifying temporal trajecto-
ries of association rules with fuzzy descriptions. In Proc. Conf. North
American Fuzzy Information Processing Society (NAFIPS 2008), pages
1–6, New York, USA, May 2008.
[14] Matthias Steinbrecher and Rudolf Kruse. Assessing the strength of
structural changes in cooccurrence graphs. In Bärbel Mertsching, Marcus
Hund, and Zaheer Aziz, editors, KI 2009: Advances in Artificial Intel-
ligence, 32nd Annual German Conference on AI, Paderborn, Germany,
volume 5803 of Lecture Notes in Computer Science, Lecture Notes in
Artificial Intelligence, pages 476–483. Springer Verlag, 2009.
[15] Matthias Steinbrecher and Rudolf Kruse. Fuzzy descriptions to identify
temporal substructure changes of cooccurrence graphs. In Proceedings
of 2009 IFSA/EUSFLAT, pages 1177–1182, July 2009.
[16] Matthias Steinbrecher and Rudolf Kruse. Visualizing and fuzzy filtering
for discovering temporal trajectories of association rules. Journal of
Computer and System Sciences, 76(1):77–87, February 2010.
[17] Jeen-Shing Wang and C.S.G. Lee. Self-adaptive neuro-fuzzy inference
systems for classification applications. IEEE Transactions on Fuzzy
Systems, 10(6):790–802, December 2002.
[18] Matthew O. Ward. Handbook of Data Visualization, chapter Multivariate
Data Glyphs: Principles and Practice, pages 179–198. Springer Hand-
books Comp. Statistics. Springer Berlin/Heidelberg, 2008.
[19] Y. Y. Yao and Ning Zhong. An Analysis of Quantitative Measures
Associated with Rules. In Methodologies for Knowledge Discovery and
Data Mining. Springer-Verlag Berlin, 1999.
8
