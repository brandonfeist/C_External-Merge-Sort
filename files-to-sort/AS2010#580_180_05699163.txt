Classifying using Specific Rules with High Confidence
R. Herna´ndez-Leo´n?†, J. A. Carrasco-Ochoa?, J. Fco. Mart?´nez-Trinidad? and J. Herna´ndez-Palancar†
?Computer Science Department,
National Institute of Astrophysics, Optics and Electronics, Puebla, Mexico
Email: {raudel,ariel,fmartine}@inaoep.mx
†Data Mining Department,
Advanced Technologies Application Center, La Habana, Cuba,
Email: {rhernandez,jpalancar}@cenatav.co.cu
Abstract—In this paper, we introduce a new strategy for
mining the set of Class Association Rules (CARs), that allows
building specific rules with high confidence. Moreover, we
introduce two propositions that support the use of a confidence
threshold value equal to 0.5. We also propose a new way for
ordering the set of CARs based on rule size and confidence
values. Our results show a better average classification accu-
racy than those obtained by the best classifiers based on CARs
reported in the literature.
Keywords-Data Mining; Supervised Classification; Class As-
sociation Rules; Association Rule Mining;
I. INTRODUCTION
Classification Association Rule Mining (CARM) or as-
sociative classification, introduced in [1], is a well-known
Data Mining technique for the extraction of classification
rules. CARM integrates Classification Rule Mining (CRM)
[2] and Association Rule Mining (ARM) [3] for mining a
special subset of association rules called Class Association
Rules (CARs). A classifier based on this approach usually
consists of an ordered CAR list l, and a mechanism for
classifying unseen transactions using l [1], [7], [11].
Associative classification has been used in several tasks
including text segmentation [18], determination of DNA
splice junction types [5], mammalian mesenchymal stem
cell differentiation [16] and prediction of protein-protein
interaction types [17], among others.
In CARM, similar to ARM, we have a set of items
I = {i1, i2, . . . , in}, a set of classes C and a set of labeled
transactions D, where each transaction in D comprises a
set of items X ? I and one class c ? C. The support of
an itemset X ? I (denoted as Sup(X)) is the fraction of
transactions in D containing X (see Eq. 1). The idea of
CARM aims at extracting a set of CARs, where a CAR is
an implication of the form X ? c, being X ? I and c ? C.
In CARM, as well as in ARM, two threshold values are
commonly used to determine the interest of a CAR:
1) Support: The support of a CAR X ? c (see Eq. 2) is
the fraction of transactions in D that contains X?{c}.
2) Confidence: The confidence of a CAR X ? c (see
Eq. 3) is the probability of finding c in transactions
that also contain X , which represents how “strongly”
the rule antecedent X implies the rule consequent c.
Sup(X) =
|DX |
|D| (1)
where DX is the set of transactions in D containing X .
Sup(X ? c) = Sup(X ? {c}) (2)
Conf(X ? c) = Sup(X ? c)
Sup(X)
(3)
Many studies [3], [12] have pointed out the combinatorial
number of association rules that could be obtained when a
small support threshold is used. To address this problem,
some works [4], [9], [10] propose to prune the search space
each time a CAR satisfies predefined support and confidence
thresholds, this strategy allows us to obtain general (small)
rules. However, this strategy has two main drawbacks:
1) It does not allow to generate specific (large) rules,
some of which could be more interesting (i.e. “with
higher confidence”).
2) It extends a candidate CAR satisfying the support
threshold until this CAR also satisfies the confidence
threshold, which implies that many branches of the
CARs search space could be explored in vain.
In this paper, we introduce a new pruning strategy to
generate the set of CARs. The rules computed by this
strategy are specific (large) rules with high confidence. For
mining the set of CARs, an appropriate confidence threshold
value supported by two propositions, is used. Besides, we
propose a new way for ordering the set of CARs based
on the size of the CARs and their confidence value. The
ordered CARs together with the “Best K rules” mechanism
are integrated in a CAR based classifier (called CAR-IC)
which is more accurate than CBA, CMAR, CPAR, TFPC
and HARMONY classifiers.
This paper is organized as follows: The next section
describes the related work. The third section introduces our
classifier. In the fourth section the experimental results are
Ninth Mexican International Conference on Artificial Intelligence
978-0-7695-4284-3/10 $26.00 © 2010 IEEE
DOI 10.1109/MICAI.2010.24
75
shown. Finally, the conclusions as well as the future work
are given in section five.
II. RELATED WORK
Several classifiers based on CARs have been developed
[1], [4], [5], [7], [11], [19]. In general, these classifiers can
be divided in two groups according to the strategy used for
computing the set of CARs:
1) Two Stage classifiers. In a first stage all the CARs
satisfying the support and confidence thresholds are
mined and later, in a second stage, a classifier is built
by selecting a small subset of CARs that fully covers
the training set. The classifiers CBA [1], CMAR [7]
and MCAR [8] follow this strategy.
2) Integrated classifiers. In these classifiers the subset
of CARs is generated directly. The classifiers CPAR,
[11], TFPC [4] and HARMONY [19] follow this
strategy.
In the literature, regardless of the strategy used for com-
puting the set of CARs, there are five main schemes for
ordering CARs:
a) CSA (Confidence - Support - Antecedent size): The
CSA ordering scheme sorts the rules in a descending
order according to their confidence. Those CARs that
share a common confidence value are sorted in a
descending order according to their support, and in case
of tie, CSA sorts the rules in ascending order according
to the size of their rule antecedent. This scheme has
been used by the CBA classifier [1].
b) ACS (Antecedent size - Confidence - Support): The
ACS ordering scheme is a variation of CSA. But it
takes into account the size of the rule antecedent as first
ordering criterion followed by confidence and support.
The classifier TFPC [4] follows this scheme.
c) WRA (Weighted Relative Accuracy): The WRA order-
ing scheme, proposed in [13], assigns to each CAR a
weight and then sorts the set of CARs in a descending
order according the assigned weights. The WRA has
been used to order CARs in two versions of the TFPC
classifier [9], [10]. Given a rule A ? B the WRA is
computed as follows:
WRA(A ? B) = Sup(A)(Conf(A ? B)?Sup(B))
d) LAP (Laplace Expected Error Estimate): The LAP
ordering scheme was introduced by Clark and Boswell
[14] and it has been used to order CARs in CPAR
classifier [11]. Given a rule A ? B, in [11] the LAP
is defined as follows:
LAP (A ? B) = Sup(A ? B) + 1
Sup(A)+ | C |
where C is the set of predefined classes.
e) ?2 (Chi-Square): The ?2 ordering scheme is a well
known technique in statistics, which can be used to
determine whether two variables are independent or
related. After computing an additive ?2 value for each
CAR, this value is used to sort the set of CARs in a
descending order in the CMAR classifier [7].
There are three main satisfaction mechanisms reported in
the literature [1], [7], [10].
1) Best rule: This mechanism selects the first (“best”)
rule in the order that satisfies the transaction to be
classified (unseen data), and then the class associated
to the selected rule is assigned to this transaction [1].
2) Best K rules: This mechanism selects the best K
rules (for each class) that satisfy the transaction to
be classified and then the class is determined using
these K rules, according to different criteria [10].
3) All rules: This mechanism selects all rules that satisfy
the transaction to be classified and then these rules are
used to determine the class of the new transaction [7].
Algorithms following the “Best rule” mechanism could suf-
fer biased classification or overfitting since the classification
is based on only one rule. On the other hand, the “All rules”
mechanism includes rules with low ranking for classification
and this could affect the accuracy of the classifier. Since
the “Best K rules” mechanism has been the most used
satisfaction mechanism for CAR based classifiers, and it
reports the best results, we will use it in our work.
III. PROPOSED CLASSIFIER
First of all, in subsection III-A, we propose two propo-
sitions that support the use of a confidence threshold value
equal to 0.5 for mining the CARs. An important part of
our classifier is the pruning strategy used during the mining
of the set of CARs, thus in subsection III-B we introduce
this strategy. In section III-C, we propose a new way for
ordering the set of CARs and describe how the pruning
strategy and the new ordering are used to define a new CAR
based classifier.
A. Determining the confidence threshold
All the CAR based classifiers use different support and
confidence thresholds for mining the set of CARs. The
threshold values used in those works must be carefully
defined by the user, there is not a guideline that helps to
the user to choose these values. In this paper, we propose to
use a confidence threshold that allows to obtain CARs with
different antecedent, avoiding ambiguity at the classification
stage.
In order to determine an appropriate confidence threshold
we introduce two propositions. The proposition 1 guarantees
that the sum of the confidence values of all CARs having
identical antecedent is 1. Later, in proposition 2 we show
that only one CAR can have a confidence value greater than
0.5.
76
Proposition 1: Let X be an itemset and C =
{c1, c2, . . . , cm} be the set of predefined classes, the fol-
lowing equation is fulfilled:
m?
i=1
Conf(X ? ci) = 1 (4)
The demonstration is immediate from the definitions of
itemset support (Eq. 1), CAR support (Eq. 2) and CAR
confidence (Eq. 3).
Proposition 2: Let X be an itemset and C =
{c1, c2, . . . , cm} be the set of predefined classes, only one
CAR X ? ck (ck ? C) can have a confidence value greater
than 0.5.
Since Conf(X ? c) takes values between 0 and 1, the
demonstration is immediate from the proposition 1.
Based on proposition 2, if we set the confidence threshold
to 0.5, for each itemset X we can obtain at most one CAR
having X as antecedent, and in this way, ambiguity at the
classification stage is avoided. For our experiments, we will
use this value as confidence threshold.
B. Mining the set of CARs
In order to generate the set of CARs, we follow the
ideas of CA [6], a frequent itemset mining algorithm, which
according to the experiments shown in [6], outperforms
other efficient algorithms for mining frequent itemsets, like
Apriori (used in CBA), Fp-growth (used in CMAR) and TFP
(used in TFPC).
In [12], for mining ARs the authors propose partitioning
the itemset space into equivalence classes grouping itemsets
of the same size k which have a common (k ? 1)-length
prefix. An equivalence class grouping k-itemsets will be
denoted as ECk. In our proposal, unlike the algorithm
proposed in [12] we consider each predefined class c ? C as
another item and we propose to divide the CAR space into
equivalence classes defined by the following equivalence
relation: “The CARs of size k that share the consequent
(the same class) and the first k ? 2 items of the antecedent
(which has k ? 1 items) belong to the same equivalence
class”.
Similar to CA [6], in order to compute support values,
we take advantage of bit-to-bit operations by representing
the dataset as an m x n binary matrix, being m the number
of transactions and n the number of items including the class
items.
In the literature, regardless of the used strategy for mining
the CARs, each CAR satisfying the support and confidence
thresholds is extended by adding a new item. The extended
CAR, called candidate CAR, is searched in the transaction
dataset to obtain their support and confidence values. Recent
algorithms for mining the set of CARs [4], [9], [10] prune
the CAR search space each time a CAR satisfying the
support and confidence thresholds is found, it means that
CARs satisfying both thresholds are not extended anymore.
This strategy produces more general rules. Besides, these
algorithms do not prune the CAR search space when a
CAR only satisfies the support threshold; instead of it they
continue extending the CAR until it satisfies the confidence
threshold. With this pruning strategy, at most one CAR is
obtained for each branch of the CARs search space but many
branches could be explored in vain.
In order to allow generating more specific rules with high
confidence, we introduce the following pruning strategy.
Let X be an itemset, c be a class and i be an item,
if the candidate CAR X ? c does not satisfy either
support threshold or confidence threshold we do not ex-
tended the CAR anymore, i.e., we prune the CARs space
avoiding to generate candidate CARs from CARs that not
satisfy the support and confidence thresholds. Otherwise,
if the candidate CAR X ? c satisfies the support and
confidence thresholds we follow extending the CAR while
Conf(X ? {i} ? c) >= Conf(X ? c), thus allowing to
obtain, for each branch, many CARs with high confidence.
C. Ordering and Classifying
Once the set of CARs has been generated, the CAR list
is sorted. As it was mentioned earlier, for classifying we
use more specific rules with high confidence; therefore, we
propose a new way for sorting the set of CARs. First we
sort the set of CARs in a descending order according to the
size of the CARs and in case of tie, we sort the tied CARs
in a descending order according to their confidence.
Algorithm 1 CAR-IC (training phase)
Input: training dataset db
Output: the classifier
1: Answer ? ?
2: CARs ? Generating CARs(db)
3: Answer ? Ordering CARs(CARs)
4: return Answer
Algorithms 1 and 2 show respectively the pseudo code
of the training phase and classification phase of CAR-IC. In
the training phase (Alg. 1), the Generating CARs function
computes the set of CARs from the training dataset, and after
that, the Ordering CARs function sorts the obtained set
of CARs, in the way described above.
Algorithm 2 CAR-IC (classification phase)
Input: set of sorted CARs, unseen transaction t
Output: the assigned class
1: Answer ? ?
2: BestK ? Select BestK(t)
3: Answer ? Classify(BestK)
4: return Answer
For classifying unseen transactions, we decided to use the
77
“Best K rules” satisfaction mechanism, since it has reported
the best results [4], [7], [10].
In the classification phase (Alg. 2), to classify an unseen
transaction t, for each class, the “Best K rules” (we used
K=5 as in the other evaluated classifiers) covering t are
selected using the Select BestK function. Later, the class
having the greatest average of the confidence values of their
selected K rules, is assigned to t (Classify function). If
there is a tie, one of the tied classes is randomly assigned.
If no rule covers t, the default class is assigned (in our
algorithm, we use the majority class as default class).
IV. EXPERIMENTAL RESULTS
In this section, we report some experimental results com-
paring our classifier (CAR-IC) against the best classifiers
based on CARs reported in the literature: CBA [1], CMAR
[7], CPAR [11], TFPC [4] and HARMONY [19]. Other
good classifiers like RCBT [22] and DDPMine [23] were
not included in the experiments because the authors of
these works did not provide their programs, and from the
description of the algorithms given in their papers it is
impossible to implement them. Moreover, the first one was
evaluated using only four gene expression datasets (which
were not provided by the authors either) and the second one
was evaluated using only 8 unusual datasets from the UCI
repository.
Table I
DATASET CHARACTERISTICS.
Dataset # instances # items # classes
adult 48842 97 2
anneal 898 73 6
ecoli 336 34 8
flare 1389 39 9
glass 214 48 7
heart 303 52 5
hepatitis 155 56 2
horseColic 368 85 2
iris 150 19 3
led7 3200 24 10
letRecog 20000 106 26
mushroom 8124 90 2
pageBlocks 5473 46 5
pima 768 38 2
waveform 5000 101 3
The first four classifiers were downloaded from the Frans
Coenen’s homepage (http://www.csc.liv.ac.uk/?frans); and
for HARMONY, we used the accuracy values reported in
[19] since the program was not provided by the authors and
it is impossible to implement the classifier from the paper.
Experiments were done using ten-fold cross-validation, re-
porting the average accuracy of the ten folds. Our tests were
performed on a PC with an Intel Core 2 Duo at 1.86 GHz
CPU with 1 GB DDR2 RAM, running Windows XP SP2.
As in other works [1], [4], [7], [11], experiments were
conducted using several datasets, 15 in our case (see char-
acteristics in Table I). The chosen datasets were originally
Table II
CLASSIFICATION ACCURACY.
Dataset CBA CMAR CPAR TFPC HARMONY CAR-IC
adult 84.21 79.72 77.24 80.79 81.90 82.11
anneal 94.65 89.09 94.99 88.28 91.51 91.80
ecoli 83.17 77.01 80.59 58.53 63.60 82.06
flare 84.23 83.30 64.75 84.30 75.02 85.98
glass 68.30 74.37 64.10 64.09 49.80 68.12
heart 57.33 55.36 55.03 51.42 56.46 53.21
hepatitis 57.83 81.16 74.34 81.16 83.16 84.56
horseColic 79.24 80.06 81.57 79.06 82.53 82.47
iris 94.00 92.33 94.70 95.33 93.32 96.06
led7 66.56 72.31 71.38 68.71 74.56 72.71
letRecog 28.64 26.25 28.13 27.57 76.81 73.14
mushroom 46.73 100.00 98.52 99.03 99.94 98.54
pageBlocks 90.94 87.98 92.54 89.98 91.60 91.82
pima 75.03 72.85 74.82 74.36 72.34 75.23
waveform 77.58 72.22 70.66 66.74 80.46 73.06
Average 72.56 76.27 74.89 73.96 78.20 80.72
Table III
RANKING POSITION BASED ON ACCURACY.
Dataset CBA CMAR CPAR TFPC HARMONY CAR-IC
adult 1 5 6 4 3 2
anneal 2 5 1 6 4 3
ecoli 1 4 3 6 5 2
flare 3 4 6 2 5 1
glass 2 1 4 5 6 3
heart 1 3 4 6 2 5
hepatitis 5 3 4 3 2 1
horseColic 5 4 3 6 2 1
iris 4 6 3 2 5 1
led7 6 3 4 5 1 2
letRecog 3 6 4 5 1 2
mushroom 6 1 5 3 2 4
pageBlocks 4 6 1 5 3 2
pima 2 5 3 4 6 1
waveform 2 4 5 6 1 3
Average 3.13 4.00 3.73 4.53 3.13 2.27
taken from the UCI Machine Learning Repository [15], and
their numerical attributes were discretized by the author of
[20] using the LUCS-KDD discretized/normalized ARM and
CARM Data Library. The discretization technique used in
LUCS-KDD is different from those used in [1], [7], [11];
thus, the performance reported in tables II and III may be
different from those reported in previous studies, even for
the same classifier and the same dataset [21].
For CBA, CMAR, CPAR and TFPC classifiers we used
the support threshold set to 0.01 and the confidence thresh-
old set to 0.5, as their authors suggest. In [19], the authors
of HARMONY obtained the best results using a support
threshold of 0.5. In our classifier CAR-IC we used the
confidence threshold also set to 0.5, based on our previous
analysis (see section III-A).
Table II shows the accuracy of all tested classifiers on the
15 datasets and Table III shows the ranking position obtained
by each classifier according to its accuracy value. Analyzing
these tables, we can see that CBA has the worst performance
in average accuracy while it has a good performance in
78
average ranking; this is because, although CBA has low
accuracy values for some datasets, it reaches the first place
in 3 of the tested datasets and the second place in other
4 datasets. On the contrary, CMAR has a good average
accuracy but a poor average ranking w.r.t. the other evaluated
classifier.
Our proposed classifier, CAR-IC, has the best average
classification accuracy outperforming all other classifiers and
having in average a difference in accuracy of more than
2.5% w.r.t. the classifier in the second place. Additionally,
CAR-IC has the best average ranking position w.r.t. the
other evaluated classifier. The classifier with the second best
performance was HARMONY, which was the second best
in average accuracy as well as in average ranking.
Although the original implementations of CBA, CMAR
and CPAR use different discretization/normalization tech-
niques, we consider interesting to show, in Table IV, a
comparison of the accuracies obtained by our classifier,
CAR-IC, against the best reported accuracies of all the
evaluated classifiers. In the case of HARMONY, the authors
did not report which technique was used for discretiza-
tion/normalization.
Despite the discretization/normalization technique is not
the same, CAR-IC obtains the best average accuracy being
1.09% better than the second best.
V. CONCLUSION
In this paper, we have proposed an accurate classifier
based on CARs. This classifier, called CAR-IC, introduces
a new pruning strategy for obtaining specific rules with
high confidence; moreover, we propose two propositions
that support the use of a confidence threshold equal to 0.5
for mining the rules. Besides, we propose a new way for
ordering the set of CARs using the size of the CARs and
their confidence value. The experimental results show that
CAR-IC has better performance than CBA, CMAR, CPAR,
TFPC and HARMONY classifiers. In general, CAR-IC has
the best average classification accuracy as well as the best
average ranking.
As future work, we are going to study the problem of
producing rules with multiple labels, it means rules with
multiple classes in the consequent. For this, we will study
the use of confidence thresholds smaller than 0.5 allowing
to obtain more than one rule with the same antecedent.
ACKNOWLEDGMENT
This work was partly supported by the National Council
of Science and Technology of Mexico under the project CB-
2008-01-106443 and grant 228056.
REFERENCES
[1] B. Liu, W. Hsu and Y. Ma: Integrating classification and
association rule mining. In: Proceedings of the 4th International
Conference on Knowledge Discovery and Data Mining, pp. 80-
86, New York, NY, USA, 1998.
[2] J. R. Quinlan: C4.5: programs for machine learning. In: Mor-
gan Kaufmann Publishers Inc., San Francisco, CA, USA, 1993.
[3] R. Agrawal and R. Shrikant: Fast Algorithms for Mining
Association Rules. In: Proceedings of the 20th International
Conference on Very Large Data Bases, pp. 487-499, Santiago,
Chile, 1994.
[4] F. Coenen, P. Leng and L. Zhang: Threshold Tuning for
Improved Classification Association Rule Mining. In: Proceed-
ings of Pacific-Asia Conference on Advances in Knowledge
Discovery and Data Mining (PAKDD ’05), Vol. 3518, pp. 216-
225, Hanoi, Vietnam, 2005.
[5] F. Berzal, J. C. Cubero, D. Sa´nchez and J. M. Serrano: ART:
A Hybrid Classification Model. In: Machine Learning, Vol. 54,
Number 1, pp. 67-92, 2004.
[6] R. Herna´ndez, J. Herna´ndez, J. A. Carrasco and J. Fco.
Mart?´nez: Algorithms for Mining Frequent Itemsets in Static
and Dynamic Datasets. In: Intelligent Data Analysis, Vol. 14,
Number 3, pp. 419-435, 2010.
[7] W. Li, J. Han and J. Pei: CMAR: accurate and efficient
classification based on multiple class-association rules. In:
Proceedings IEEE International Conference on Data Mining
(ICDM ’01), pp. 369-376, 2001.
[8] F. Thabtah, P. Cowling and Y. Peng: MCAR: multi-class
classification based on association rule. In: Proceedings of the
3rd ACS/IEEE International Conference on Computer Systems
and Applications, pp. 33+, 2005.
[9] Y. J. Wang, Q. Xin and F. Coenen: A Novel Rule Order-
ing Approach in Classification Association Rule Mining. In:
Proceedings of the 5th international conference on Machine
Learning and Data Mining in Pattern Recognition, pp. 339-
348, Leipzig, Germany, 2007.
[10] Y. J. Wang, Q. Xin and F. Coenen: Hybrid Rule Ordering
in Classification Association Rule Mining. In: Transaction on
Machine Learning and Data Mining (MLDM ’08), Vol. 1,
Number 1, pp. 1-15, 2008.
[11] X. Yin and J. Han: CPAR: Classification based on Predictive
Association Rules. In: Proceedings of the Third SIAM Interna-
tional Conference on Data Mining, San Francisco, CA, 2003.
[12] M. Zaki, S. Parthasarathy, M. Ogihara and W. Li: New
Algorithms for Fast Discovery of Association Rules. In: 3rd
International Conference on Knowledge Discovery and Data
Mining, pp. 283-286, USA, 1997.
[13] N. Lavracˆ, P. Flach and B. Zupan: Rule Evaluation Measures:
A Unifying View. In: Proceedings of the 9th International
Workshop on Inductive Logic Programming (ILP’ 99), pp. 174-
185, 1999.
[14] P. Clark and R. Boswell: Rule Induction with CN2: Some
Recent Improvments. In: Proceedings of European Working
Session on Learning (ESWL’ 91), pp. 151-163, Porto, Portugal,
1991.
[15] A. Asuncion and D. J. Newman:
UCI Machine Learning Repository. In:
http://www.ics.uci.edu/?mlearn/MLRepository.html, 2007.
79
Table IV
COMPARISON OF THE ACCURACIES OBTAINED BY CAR-CI AGAINST THOSE ACCURACIES REPORTED BY THE METHODS CBA, CMAR, CPAR, TFPC
AND HARMONY.
Dataset CBA-R CMAR-R CPAR-R TFPC-R HARMONY-R CAR-IC
adult 84.20 80.10 76.70 80.80 81.90 82.11
anneal 97.90 97.30 98.40 88.30 91.51 91.80
ecoli 83.17 77.01 80.59 58.53 63.60 82.06
flare 84.20 84.30 64.75 84.30 75.02 85.98
glass 73.90 70.10 74.40 64.50 49.80 68.12
heart 81.90 82.20 82.60 51.40 56.46 53.21
hepatitis 81.80 80.50 79.40 81.20 83.16 84.56
horseColic 82.10 82.60 84.20 79.10 82.53 82.47
iris 94.70 94.00 94.70 95.30 93.32 96.06
led7 71.90 72.50 73.60 57.30 74.56 72.71
letRecog 28.64 25.50 28.13 26.40 76.81 73.14
mushroom 46.70 100.00 98.52 99.00 99.94 98.54
pageBlocks 90.90 90.00 92.54 90.00 91.60 91.82
pima 72.90 75.10 73.80 74.40 72.34 75.23
waveform 80.00 83.20 80.90 74.40 80.46 73.06
Average 76.99 79.63 78.88 73.66 78.20 80.72
[16] W. Wang, Y. J. Wang, R. Ba nares, Z. Cui and F. Co-
enen: Application of Classification Association Rule Mining
for Mammalian Mesenchymal Stem Cell Differentiation. In:
Proceedings of the 9th Industrial Conference on Advances in
Data Mining. Applications and Theoretical Aspects (ICDM’
09), pp. 51-61, Leipzig, Germany, 2009.
[17] S. H. Park, J. A. Reyes, D. R. Gilbert, J. W. Kim and S. Kim:
Prediction of protein-protein interaction types using association
rule based classification. In: BMC Bioinformatics, Vol. 10,
Number 1, 2009.
[18] E. Cesario, F. Folino, A. Locane, G. Manco and R. Ortale:
Boosting text segmentation via progressive classification. In:
Knowledge and Information Systems, Vol. 15, Number 3, pp.
285-320, 2008.
[19] J. Wang and G. Karypis: On Mining Instance-Centric Classi-
fication Rules. In: IEEE Transactions on Knowledge and Data
Engineering, Vol. 8, Number 11, pp. 1497-1511, 2006.
[20] F. Coenen: The LUCS-KDD discretised/normalised
ARM and CARM Data Library. Department of
Computer Science, The University of Liverpool, UK. In:
http://www.csc.liv.ac.uk/?frans/KDD/Software/LUCS-KDD-
DN, 2003.
[21] J. Wang and G. Karypis: HARMONY: Efficiently mining the
best rules for classification. In: Proceedings of SDM, pp. 205-
216, 2005.
[22] G. Cong, K. L. Tan, Anthony K. H. Tung and X. Xu:
Mining top-K covering rule groups for gene expression data.
In: Proceedings of the 2005 ACM SIGMOD international
conference on Management of data, pp. 670-681, 2005.
[23] H. Cheng, X. Yan, J. Han and S. Y. Philip: Direct Discrimina-
tive Pattern Mining for Effective Classification. In: Proceedings
of the 2008 IEEE 24th International Conference on Data
Engineering, pp. 169-178, 2008.
80
