Mining Product Features from Online Reviews 
 
 
Weishu Hu, Zhiguo Gong, Jingzhi Guo 
Faculty of Science and Technology 
University of Macau 
Macau, China 
{ma76523, fstzgg, jzguo}@umac.mo 
 
 
Abstract—With the advance of the Internet, e-commerce 
systems have become extremely important and convenient to 
human being. More and more products are sold on the web, 
and more and more people are purchasing products online. As 
a result, an increasing number of customers post product 
reviews at merchant websites and express their opinions and 
experiences in any network space such as Internet forums, 
discussion groups, and blogs. So there is a large amount of data 
records related to products on the Web, which are useful for 
both manufacturers and customers. Mining product reviews 
becomes a hot research topic, and prior researches mostly base 
on product features to analyze the opinions. So mining product 
features is the first step to further reviews processing. In this 
paper, we present how to mine product features. The proposed 
extraction approach is different from the previous methods 
because we only mine the features of the product in opinion 
sentences which the customers have expressed their positive or 
negative experiences on. In order to find opinion sentence, a 
SentiWordNet-based algorithm is proposed. There are three 
steps to perform our task: (1) identifying opinion sentences in 
each review which is positive or negative via SentiWordNet; (2) 
mining product features that have been commented on by 
customers from opinion sentences; (3) pruning feature to 
remove those incorrect features. Compared to previous work, 
our experimental result achieves higher precision and recall. 
Keywords-text mining; sentiment classification 
I.  INTRODUCTION 
As internet has become one part of people’s daily lives, 
e-commerce has been developed into one of the most import 
methods in business deal. More and more products are sold 
on the Web, and more and more people are shopping on the 
Web. In order to share their shopping experiences and 
feedbacks, an increasing number of customers post product 
reviews at merchant websites and express their opinions and 
experiences in any network space such as internet forums, 
discussion groups and blogs, which is a great wealth of 
opinions about products. As a consequence, mining opinion 
has become a perspective research topic since [20, 6, 14]. It 
is quite different from traditional text summarization [7, 21] 
in many ways: (1) Opinion mining is a structured 
summarization rather than a free document which is 
generated by most text summarization systems. (2) Users are 
mainly interested in the product features that customers have 
positive or negative opinions on. A systematic procedure of 
opinion mining is formed in many researchers’ effort, and 
Hu and Liu in [14] propose one useful method which is 
feature-based opinion summarization of reviews. It is 
performed in three steps: 
1. Mining the features of the product that customers have 
expressed opinions on, and then ranking the features 
according to their frequencies that they appear in the reviews. 
2. For each feature, identifying opinion sentences in each 
review and determining each opinion sentence’s semantic 
orientation. The specific reviews that express opinions are 
attached to the feature, which facilitates browsing the 
reviews by potential customers. 
3. Summarizing the results using the discovered 
information. 
A feature-based opinion summary is helpful for a 
potential customer to find out the feeling of the existing 
customers. If potential customers are very interested in a 
particular feature, they can click the link to see what existing 
customers like or why they make a complaint. 
Extracting product features is the foundational step of 
opinion mining. So this paper focuses on mining product 
features. We propose a different method to extract the 
product features from online reviews. In this paper, we aim 
to mine product features that the reviewers have commented 
on. This task mainly involves three subtasks: (1) Identifying 
opinion sentences in each review which are positive or 
negative via SentiWordNet [3]; (2) Mining product features 
that have been commented on by customers from opinion 
sentences; (3) Pruning feature to remove those incorrect 
features. 
This paper proposes using SentiWordNet to distinguish 
the opinion sentences which is a new attempt and utilizes a 
number of techniques based on data mining and natural 
language processing methods to mine product features. Our 
experimental results show that these techniques are highly 
effective. 
The remainder of this paper is organized as follows: 
Section 2 describes related work, Section 3 gives an 
overview of our system, describes and evaluates the main 
components, Section 4 shows the experiment results and 
Section 5 makes a conclusion. 
IEEE International Conference on E-Business Engineering
978-0-7695-4227-0/10 $26.00 © 2010 IEEE
DOI 10.1109/ICEBE.2010.51
24
II. RELATED WORK 
Many researchers have proposed different methods to 
solve the problem of mining product features. Our work is 
closely related to Hu and Liu’s work in [4, 14, 15] on mining 
opinion features in customer reviews. In [14], they design a 
system to perform the summarization in two main steps: 
feature extraction and opinion direction identification. The 
inputs to the system are a product name and an entry page for 
all the reviews of the product. The output is the summary of 
the reviews. 
The system performs five tasks: (1) POS tagging [13], 
which parses each sentence and yields the part-of speech tag 
of each word (whether the word is a noun, verb, adjective, 
etc) and identifies simple noun and verb groups (syntactic 
chunking). (2) Frequent feature generation. Hu and Liu use 
association rule mining [1] to find all frequent itemsets, 
which is a set of words or a phrase that occurs together. The 
association rule miner CBA [12] based on the Apriori 
algorithm in [1], finds all frequent itemsets in the transaction 
set with the user-specified minimum support 1%. (3) Feature 
Pruning aims to remove those incorrect features. Two types 
of pruning are presented: (a) Compactness pruning checks 
features that contain at least two words, which are named 
feature phrases, and removes those that are likely to be 
meaningless. (b) Redundancy pruning removes redundant 
features that contain single words. (4) Opinion words 
extraction with all the remaining frequent features after 
pruning. (5) Infrequent feature identification. Hu and Liu 
suppose people like to use the same opinion word to describe 
different features. So they can use the opinion words to look 
for features that cannot be found in (2). If one sentence 
contains no frequent feature but one or more opinion words, 
find the nearest noun or noun phrase of the opinion word as 
an infrequent feature. 
The proposal in [14] can produce a number of features, 
but only explicit features could be found and implicit feature 
does not be extracted. The irrelevant sentences may be 
thought as opinion sentences, and the nouns in irrelevant 
sentence would be extracted as features. In addition, the 
premise (people use the same pinion word to describe 
different features) of finding infrequent features is not so 
reasonable. Different features trend to be described by 
different opinion words. 
In [2], Popescu and Etzioni introduce an unsupervised 
information-extraction system (OPINE), which mines 
reviews in order to build a model of important product 
features, the evaluation by reviewers and the relative quality 
across products. OPINE performed four main tasks: (1) 
Identify product features. (2) Identify opinions regarding 
product features. (3) Determine the polarity of opinions. (4) 
Rank opinions based on their strength. 
OPINE is built on top of KnowItAll, a Web-based, 
domain-independent information extraction system [16], 
which instantiates relation-specific generic extraction 
patterns into extraction rules to find candidate facts. 
KnowItAll’s Assessor assigns a form of Point-wise Mutual 
Information (PMI) between phrases that is estimated from 
Web search engine hit counts [19]. It computes the PMI 
between each fact and automatically generated discriminator 
phrases. Given fact f and discriminator d, the computed PMI 
score as (1): 
)()(
)(),(
fHitsdHits
fdHitsdfPMI
+
+
=  (1) 
The PMI scores are converted to binary features for a 
Naive Bayes Classifier, which outputs a probability 
associated with each fact [16]. 
OPINE extracts explicit feature for the given product 
class from parsed review data. (1) The system recursively 
identifies both the parts and the properties of the given 
product class and their parts and properties, in turn, 
continuing until no candidates are found. (2) The system 
finds related concepts and extracts their parts and properties. 
Opine achieves 22% higher precision than Hu’s, but has 3% 
lower recall. 
Scaffidi presents a probability-based algorithm and 
compares it to an existing support-based approach in [5]. 
Specially, he uses each algorithm to extract features from 7 
Amazon.com product categories, and then asks end users to 
rate the features in terms of helpfulness for choosing 
products. The end users prefer the features identified by the 
probability-based algorithm. This probability-based 
algorithm can identify features that comprise a single noun 
or two successive nouns (which end users rated as more 
helpful than features comprising only one noun), yet even for 
collection of tens of thousands of reviews, it still executes 
fast enough (at around 1ms per review) for practical use. 
OpinionFinder [18] is a system that processes documents 
and automatically identifies subjective sentences as well as 
various aspects of subjectivity within sentences, including 
agents who are sources of opinion, direct subjective 
expressions and speech events, and sentiment expressions. It 
outputs files using inline Standard Generalized Markup 
Language (SGML) markup. It’s not suitable for indentifying 
the opinion sentences in reviews, because identifying opinion 
sentence does not care the sources of the opinion and the 
sentiment expressions are free styles to process in reviews. 
So we propose a new method to find opinion sentences using 
SentiWordNet. 
III. THE PROPOSED TECHNIQUES 
In this section, we give an architectural overview for our 
product feature extraction system in Figure 1. The system 
performs the extraction in two main steps: opinion sentence 
identification using SentiWordNet-based algorithm and 
product feature extraction. The inputs to the system are all 
the reviews of the product in the database. The output is the 
feature set of the reviews as Figure 2. 
We download (or crawl) all the reviews, and put them in 
the review database. The feature extraction function, which 
is the focus in this paper, firstly identify the opinion sentence 
that a lot of people have expressed their positive or negative 
opinions on, and then extract the product feature including 
explicit feature and implicit feature. Finally we filter the 
result via pruning irrelevant feature. Below, we discuss each 
of the functions in feature extraction in turn. 
25
Review Database 
Further Processing 
Explicit Feature
Implicit Feature
Feature Pruning 
POS Tagging 
Feature Generation 
Infrequent Feature
Opinion Sentence 
Identification Feature Set
Frequent Feature
Digital_camera_1: 
Photo quality:  
Frequency: 5 
Ratio: 0.013454 
Weight: 
Frequency: 7 
Ratio: 0.034127 
… 
Figure 1.  The product feature extraction system. 
Figure 2.  An example of output. 
A. Part-of-Speech Tagging (POS) 
POS tagging is the part-of-speech tagging [13] from 
natural language processing. Firstly, we give some sample 
sentences from some reviews to describe what kinds of 
opinions are handled, and the application of part-of-speech 
tagging from natural language processing will be discussed 
later. 
For a given product, the aim of our system is to find what 
people like and dislike. Therefore it is an important step to 
find out the product features that people talk about. However, 
it is difficult to understand natural language and deal with 
some types of sentences [14]. The following are some easy 
and hard sentences from the reviews of a digital camera: 
“The images are very vague.” 
“Totally an undoubted very smart camera.” 
In the first sentence, the user is satisfied with the picture 
quality of the camera, image is the feature that the user talks 
about. Similarly, the second sentence shows that camera is 
the feature that the user expresses one’s opinion on. While 
the features of these two sentences are explicitly mentioned 
in the sentences, some features are implicit and hard to find. 
For example, 
“While light, it will not easily fit in pockets.” 
This customer is talking about the size of the camera, but 
the word “size” is not explicitly mentioned in the sentence. 
To find such implicit features, semantic understanding is 
needed, which requires more sophisticated techniques. Thus 
in this paper, we propose a simple method to find the implicit 
feature, called Feature Mapping using associative spread. For 
example, it is too heavy to carry. And we can imply the 
weight of this subject from the adjective word. The detail 
will be shown in Feature Generation section. 
However, implicit features occur much less frequent than 
explicit ones. So we focus on finding features that appear 
explicitly as nouns or noun phrases in the reviews. To 
identify nouns/noun phrases from the reviews, we use the 
part-of-speech tagging. 
In this work, we use the openNLP [17] linguistic parser, 
which to parse each sentence and yield the part-of-speech tag 
of each word (whether the word is a noun, verb, adjective, 
etc) and identifies simple noun and verb groups (syntactic 
chunking). The following shows a sentence with the POS 
tags. 
I/PRP am/VBP absolutely/RB in/IN awe/NN of/IN 
this/DT camera/NN ./. 
The openNLP system generates XML output. For 
instance, camera/NN indicates a noun. Each sentence is 
saved in the review database along with the POS tag 
information of each word in the sentence. 
A transaction file is then created for the identification of 
frequent features in the next step. In this file, each line 
contains words from a sentence, which includes only 
preprocessed noun, verb, adjective, adverb phrases of the 
sentence. The reason is that other components of a sentence 
are unlikely to be used in expressing opinions. Here, pre-
processing includes the deletion of stopwords, stemming 
(Stanford POS Tagging [11] and JWI [10] for WordNet [22]) 
and fuzzy matching. Fuzzy matching [9] is used to deal with 
word variants or misspellings. For example, “redeye” and 
“red-eye” actually refer to the same feature. All the 
occurrences of “redeye” are replaced with “red-eye”. 
B. Opinion Sentences Identification 
In opinion mining, users primarily care about what the 
customers like and dislike. So we only need to extract these 
sentences called Opinion sentences that people use to express 
a positive or negative opinion. Observing that people often 
express their opinions of a product feature in opinion 
sentences, we can extract opinion sentences from the review 
database using all the opinion words. In a review, there are 
many sentences. Some are opinion sentences, others are 
irrelevant sentences. For instance, let us look at the following 
two sentences: 
“The performance of this camera is perfect.” 
“I bought this camera yesterday.” 
In the first sentence, the feature, performance, is in the 
opinion sentence. But in the second sentence, it’s not an 
opinion sentence, and no feature exists. 
In order to find the opinion sentences, we use opinion 
words which are sentiment words that people used to express 
their positive or negative attitudes. Only four kinds of words 
can express the sentiment, they are nouns, adjectives, 
adverbs and verbs. Because we use nouns as product features, 
and few opinion sentences use nouns to express the 
sentiment. So in this paper, we only think about adjectives, 
adverbs and verbs as opinion words. Sentiment includes 
three types: positivity, negativity and neutrality. Neutrality is 
usually used to describe a fact, and users pay much attention 
to the positivity and negativity. In review extraction, we take 
26
more care of the positivity and negativity of the opinion 
words. For evaluating the sentiment of each sentence, we 
make the quantization of opinion words to calculate the 
sentiment score of each sentence. The sentiment score of 
each opinion word is acquired from SentiWordNet. 
SentiWordNet is a lexical resource for opinion mining 
[3]. SentiWordNet assigns to each synset of WordNet three 
sentiment scores: positivity, negativity, objectivity. The 
assumption that underlies SentiWordNet’s switch from terms 
to synsets is that different senses of the same term may have 
different opinion-related properties. Each of the three scores 
ranges from 0.0 to 1.0, and their sum is 1.0 for each synset. 
This means that a synset may have nonzero scores for all the 
three categories, which would indicate that the corresponding 
terms have, in the sense indicated by the synset, each of the 
three opinion-related properties only to a certain degree. For 
example, the synset [estimable(3)], corresponding to the 
sense “may be computed or estimated” of the adjective 
estimable, has an Obj score of 1.0 (and Pos and Neg scores 
of 0.0), while the synset [estimable(1)] corresponding to the 
sense “deserving of respect or high regard” has a Pos score 
of 0.75, a Neg score of 0.0 and an Obj score of 0.25.  
From this observation, we can extract opinion sentences 
in the following way: for each sentence in the review 
database, if it satisfies that the positive or negative score is 
greater than certain score, to extract this sentence as opinion 
sentence. The positive and negative score can be calculated 
in below formulas. 
Firstly, calculate the opinion word’s sentiment. In 
SentiWordNet, each opinion word has different sentiment 
scores in different scenes. It’s difficult to classify the opinion 
word into the right scene. Here, we use the sentiment average 
of the opinion word in each scene as its final sentiment score 
in review. 
?
=
=
n
i
iScore
n
WordScore posadvadjvpos
1
)(1)( .).,.,(    (2) 
?
=
=
n
i
iScore
n
WordScore negadvadjvneg
1
)(1)( .).,.,(  (3) 
After acquiring the sentiment of each opinion word, we 
need to get the adjectives, adverbs and verbs’ sentiment 
score in the sentence respectively. The opinion word in the 
same part of speech has the same sentiment weight in the 
sentence, so we think the sentiment average in each part of 
speech as the sentiment score of adjectives, adverbs and 
verbs. 
?
=
=
n
i
advadjvWordScore
n
advadjvScore ipospos
1
.)).,.,((1.).,.,(  
?
=
=
n
i
advadjvWordScore
n
advadjvScore inegneg
1
.)).,.,((1.).,.,(  
And then we calculate the positivity and negativity of 
each sentence as follow: 
3
.)(.)(.)( advScoreadjScorevScoreScore pospospospos ++=  
3
.)(.)(.)( advScoreadjScorevScoreScore negnegnegneg ++=  
It’s an opinion sentence, if Max(Scorepos, Scoreneg) ? ?; 
It’s not an opinion sentence, if  Max(Scorepos, Scoreneg) < 
?. In our experiment, we find ? equals 0.08. 
As shown in the previous example, perfect is an adjective 
that can be considered as the opinion word. So we can use 
opinion sentence identification to decide the sentence is an 
opinion or not. We explain the process in the follow example: 
The/DT pictures/NNS are/VBP razor-sharp/JJ and 
clear/JJ ./. 
In this sentence, we have three opinion words, including 
are, razor-sharp and clear. First we use stemmer to get each 
opinion word’s base form, and then calculate its sentiment 
score. Verb be is a stopword, so we omit it. Firstly, using 
formula (2) and (3), razor-sharp is 0.313 in positivity and 0.0 
in negativity, and clear is 0.410 in positivity and 0.049 in 
negativity. Secondly, using formula (4) and (5), we get that 
the sentiment score of adjective is 0.362 in positivity and 
0.025 in negativity. At last, under formula (6) and (7), the 
sentence sentiment score is 0.362 in positivity and 0.025 in 
negativity. Max(0.362, 0.025) > 0.08, so it is a opinion 
sentence. 
C. Features Generation 
This step is to find features that people are most 
interested in. The features are classified into explicit and 
implicit. 
1) Explicit feature. 
Explicit feature is the product features that appear in 
noun form in the reviews. Most of the features are expressed 
explicitly in the reviews. So we focus on extracting the 
majority of the explicit features which means finding most of 
the feature in the reviews. Instances below are the explicit 
features in the opinion sentence. In order to extract the 
explicit features, we do not use association rule mining [1]. 
We use probability-based algorithm to extract the product 
feature. Explicit features are divided into frequent and 
infrequent features. In the previous step, we have filtered the 
irrelevant sentences, so the infrequent feature also can have a 
high probability to appear in the result of probability-based 
algorithm. We do not need to extract the infrequent features 
independently, because the amount of the infrequent features 
which cannot be found in our algorithm is very small. 
In [14], Hu and Liu use Association rule mining to find 
all frequent itemsets, which are a set of words or a phrase 
that occur in one sentence. However, its computation is too 
large, and the result still needs compact pruning. We use a 
much rigorous method to finish this step. Each noun in the 
same sentence should appear nearby, and then the noun 
phrases can be considered as the candidates of product 
features. 
The picture is great, but the flash is terrible. 
The battery life is not so good. 
The quality of the picture is perfect. 
Here ‘picture’ and ‘flash’ will be misunderstood as one 
feature in [14]. We will distinguish the two nouns into two 
features, because they are not nearby. In sentence 2, ‘battery’ 
(4) 
(5) 
(6) 
(7) 
27
and ‘life’ is appearing nearby, so they are considered to 
organize one feature. In sentence 3, ‘quality’ and ‘picture’ 
separated by the preposition are also considered nearby and 
describe one feature. 
For acquiring the better nouns used for product feature, 
we employ the nearby principle to extract the nouns are 
described by the adjective or verb. One adjective or verb 
only can be used for one object. 
The picture is beautiful in this camera. 
The lens and flash are very good. 
In sentence 1, picture has the ‘beautiful’ to describe, but 
camera does not have any description. So we only extract 
picture as candidate feature. In sentence 2, it is a special case 
(conjunction sentence), so lens and flash can be described by 
one adjective or verb. Lens and flash are two features. 
2) Implicit feature. 
The amount of implicit feature is few in the reviews, and 
most of them use some specific adjectives to express implicit 
opinions. The adjective almost has clear meaning, which is 
one adjective mapped to one noun. So we can create a 
mapping database to obtain the implicit feature. 
It is too heavy to carry out. 
It is too expensive. 
The feature ‘weight’ can be understood in Sentence 1, 
and the price also can be gotten in Sentence 2. So we map 
heavy to weight, and expensive to price. In order to 
understand more adjectives, we use WordNet (WordNet 3.0) 
to extend the adjective set. The synonymset and antonymset 
of one adjective are used to describe the same feature. So if 
one adjective belongs to one synonymset or antonymset, the 
mapping noun is the feature expressed implicitly. If there are 
no explicit features in the opinion sentence, we use this 
method to find the implicit feature. In order to get implicit 
feature, we first stem the adjective, and then use JAWS [8] to 
get the the synonymset and antonymset from WordNet. If the 
adjective belongs to one synonyset or antonymset, the 
corresponding feature will be extract. 
After getting the candidate features, we can calculate 
their probability of occurring in the reviews. 
Sentence
Occurrenceobability =Pr   (8) 
Occurrence: the occurrence number of the candidate 
feature in the reviews; 
Sentence: the number of opinion sentences. 
The experiment shows the occurrence probability of the 
feature is bigger than 0.6%, which is much smaller than 1% 
in [14]. 
3) Feature Pruning. 
The probability-based algorithm still can generate useless 
or fake features, which are some uninteresting and redundant 
ones. So feature pruning aims to remove these incorrect 
features. We use the pruning method proposed in [14]. 
Redundancy pruning focuses on removing redundant 
features that contain single words. Hu and Liu in [14] define 
that p-support (pure support) of feature ftr is the number of 
sentences that ftr appears in as a noun or noun phrase, and 
these sentences must contain no feature phrase that is a 
superset of ftr. For example, feature manual has the support 
of 10 sentences, which is a subset of feature phrases manual 
mode and manual setting in the review database. The support 
of the two feature phrases are 4 and 3 respectively, and the 
two phrases do not appear together in any sentence. Then the 
p-support of manual would be 3. 
If a feature has a p-support lower than the minimum p-
support and the feature is a subset of another feature phrase, 
it is pruned. Here, the minimum p-support is 3. In the 
previous example of manual with a p-support of 3, it is not 
pruned. Thus all the three features, manual, manual mode, 
manual setting, could be meaningful. 
IV. EXPERIMENTS 
In our experiments we use sets of reviews for 5 product 
classes from Hu and Liu [21, 22]. There are five electronics 
products: 2 digital cameras, 1 DVD player, 1 mp3 player, 
and 1 cellular phone. Hu’s system and OPINE are the review 
mining system most relevant to our work. Hu’s system uses 
association rule mining to extract frequent review noun 
phrases as features. Frequent features are used to find 
potential opinion words (only adjectives) and the system 
uses WordNet synonyms/antonyms in conjunction with a set 
of seed words in order to find actual opinion words. Finally, 
opinion words are used to extract associated infrequent 
features. The system in [21] only extracts explicit features. 
OPINE is built on top of KnowItAll, a Web-based, domain-
independent information extraction system. OPINE’s Feature 
Assessor uses PMI assessment to evaluate each candidate 
feature and incorporates Web PMI statistics in addition to 
review data in its assessment. Our system uses pre-
processing in identifying the opinion sentences which can 
reduce the interference of irrelevant sentence. By addition, 
we use Linguistic pattern and probability-based algorithm to 
extract the feature instead of Associated Rule and PMI, this 
simple method gains a good result. In the following, we 
analyse the experiment’s result. 
TABLE I.  RESULT OF OPINION SENTENCE IDENTIFICATION 
Product Name No. of manual Opinion Sentences 
SentiWordNet Approach 
Recall Precision 
Digital cameral 1 466 0.903 0.931 
Digital cameral 2 274 0.923 0.966 
Cellular phone 398 0.915 0.922 
Mp3 player 1219 0.882 0.904 
DVD player 488 0.891 0.912 
Average 569 0.908 0.927 
Table I show the recall and precision of SentiWordNet 
Approach are very high, the average of recall is 0.908 and 
the average of precision is 0.927, which means this approach 
is effective to identify the opinion sentences. The errors 
happen in the following cases: 
This camera keeps on autofocussing in auto mode with a 
buzzing sound which can’t be stopped. 
Get a “system error” problem 30 days after purchase. 
Awesome! 
28
In sentence 1, the opinion words (buzzing sound) which 
are nouns cannot be marked as an opinion sentence in our 
method, but these situations are very limited. So we can 
neglect using nouns as opinion words. In sentence 2, the 
sentence does not have any opinion words, but it still 
expresses the opinion implicitly, and the feature also can be 
gotten from this sentence. This expression style is seldom in 
reviews. In sentence 3, it is considered as an opinion 
sentence, but there are no features. These situations do not 
interfere with the feature extraction. 
TABLE II.  COMPARISON OF HU’S, OPINE’S AND OUR RESULT 
Product 
Name 
No. of 
manual 
Features 
Hu’s OPINE Opinion Sentence 
Re Pr Re Pr Re Pr 
Digital 
cameral 1 79 0.822 0.747 -0.02 +0.19 +0.10 +0.05 
Digital 
cameral 2 96 0.792 0.710 -0.06 +0.22 +0.11 +0.07 
Cellular 
phone 67 0.761 0.718 -0.03 +0.23 +0.11 +0.10 
Mp3 
player 57 0.818 0.692 -0.03 +0.25 +0.09 +0.06 
DVD 
player 49 0.797 0.743 -0.02 +0.21 +0.12 +0.09 
Average 69 0.80 0.72 -0.03 +0.22 +0.10 +0.07 
a. Re: recall Pr: precision 
From table II, we know that our method is better than 
Hu’s either in precision or in recall. Compared with OPINE, 
the precision of our system is not so good, but the recall is 
much better. Overall our proposal will have better 
performance than OPINE. 
In order to show that our system’s performance is robust 
across multiple product classes, we used two sets of reviews 
downloaded from Epinions.com for Movie and Laptop. In 
Movie’s reviews, we get 0.78 in precision and 0.89 in recall. 
In Laptop, we get 0.83 in precision and 0.91 in recall. So the 
average of the two products is 0.805 in precision and 0.90 in 
recall. 
In summary, with the average of recall of 90% and the 
average precision of 79%, we believe that our techniques are 
quite promising, and can be used in practical settings. 
V. CONCLUSION 
In this paper, we propose a different method to extract 
the product feature, which employs data mining and natural 
language processing methods. Compare to previous works, 
we propose a new pre-process to identify the opinion 
sentence using SentiWordNet, which can get 0.927 in 
precision and 0.908 in recall. The simple extraction based on 
probability also can get a better result than Hu’s. Although 
OPINE has the best precision, our method improves the 
precision and recall at the same time. So our proposed 
techniques are effective in performing the feature extraction. 
This paper leaves many avenues for future work, we plan to 
further improve these techniques and classify features 
according to the opinions’ strength that have been expressed, 
e.g., to sort the features from the strongest to the weakest. 
Feature extraction is the first step, and the summarization 
will be done in the future work. 
REFERENCES 
[1] Agrawal R. and Srikant R., “Fast algorithm for mining association 
rules”, VLDB’94, 1994. 
[2] Ana-Maria Popesu and Oren Etzioni, “Extracting Product Features 
and Opinions from Reviews”, Proceeding of Human Language 
Technology Conference and Conference on Empirical Methods in 
Natural Language, ACL, Vancouver, October 2005, pp. 339-346. 
[3] Andrea Esuli and Fabrizio Sebastiani, “SentiWordNet: A Publicly 
Available Lexical Resource for Opinion Mining”, In Proceedings of 
LREC-06, 5th Conference on Language Resources and Evaluation, 
Genova, IT, 2006, pp. 417-422. 
[4] Bing Liu, Mingqing Hu and Junsheng Cheng, “Opinion Observer: 
Analyzing and Comparing Opinions on the Web”, WWW, ACM, 
Chiba Japan, 2005. 
[5] Christopher Scaffidi, “Application of a Probability-based Algorithm 
to Extraction of Product Features from Online Reviews”, USA, 2006. 
[6] Dave K., Lawrence S., and Pennock D., “Mining the Peanut Gallery: 
Opinion Extraction and Semantic Classification of Product Reviews”, 
WWW, 2003. 
[7] Hovy E., and Lin C.Y., “Automated Text Summarization in 
SUMMARIST”, ACL Workshop on Intelligent. Scalable Text 
Summarization, 1997. 
[8] JAWS, the Java API for WordNet Searching. 
http://lyle.smu.edu/cse/dbgroup/sw/jaws.htm 
[9] Jokinen P., and Ukkonen E., “Two algorithms for approximate string 
matching in static texts”, In A. Tarlecki, (ed.), Mathematical 
Foundations of Computer Science, 1991. 
[10] JWI, the MIT Java Wordnet Interface. 
http://projects.csail.mit.edu/jwi/ 
[11] Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram 
Singer, “Feature-Rich Part-of-Speech Tagging with a Cyclic 
Dependency Network”, In Proceedings of HLT-NAACL, 2003, pp. 
252-259. 
[12] Liu B., Hsu W., Ma Y., “Integrating Classification and Association 
Rule Mining”, KDD’98, 1998. 
[13] Manning C. and Schutze H., Foundations of Statistical Natural 
Language Processing, MIT Press, May 1999. 
[14] Mingqing Hu and Bing Liu, “Mining Opinion Features in Customer 
Reviews”, American Association for Artificial Intelligence, 2004. 
[15] Mingqing Hu and Bing Liu, “Mining and Summarizing Customer 
Reviews”, KDD’ 04, ACM, Seattle USA, 2004. 
[16] O. Etzioni, M. Cafarella et al., “Unsupervised named-entity extraction 
from the web: An experimental study”, Artificial Intelligence, 2005, 
pp. 91-134. 
[17] OpenNLP, open source toolkit for natural language processing. 
http://opennlp.sourceforge.net/ 
[18] Opinionfinder. 
http://www.cs.pitt.edu/mpqa/opinionfinderrelease/ 
[19] P. D. Turney, “Mining the Web for Synonyms: PMI-IR versus LSA 
on ToEFL”, In Procs. Of the 20th European Conference on Machine 
Learning (ECML’2001), Freiburg Germany, pp. 491-502. 
[20] P. D. Turney, “Thumbs up or thumbs down? Semantic orientation 
applied to unsupervised classification of reviews” In Procs. Of the 
40th Annual Meeting of the Association for Computational 
Linguistics (ACL’ 02), pp. 417-424. 
[21] Radev D. and McKeown K., “Generating natural language summaries 
from multiple on-line sources”, Computational Linguistics, 24(3) pp. 
469-500, September 1998. 
[22] WordNet. http://wordnet.princeton.edu/ 
29
