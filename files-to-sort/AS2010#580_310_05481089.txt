A new robust classification method for CRM.
                                                       
* Corresponding author. E-mail address: xixilove0917@yahoo.cn
Li Xiaoyu
Business School
Sichuan Uni.
Chengdu, China
hechangzheng@scu.edu.cn
He Changzheng*
Business School
Sichuan Uni.
Chengdu, China
xixilove0917@yahoo.cn
Panos Liatsis
School of Engineering and 
Mathematical Sciences
City University
London UK
p.liatsis@city.ac.uk
Abstract: —Customer classification is a key step in customer 
relationship management(CRM), and there are many methods 
used for it, such as Neural Net, association rules, SOM model,
etc. However, most existing methods don’t take noise which is 
very common in reality into consideration. In this paper , we 
combine Croup Method of Data Handling(GMDH) with
Takagi and Sugeno fuzzy model(TS) to form a new 
classification method TS-GMDH. The experimental result 
shows that TS-GMDH outperforms the benchmark classifiers 
when the noise level is high.
Keywords-TS-GMDH, experiment, noise, customer
classification
I. INTRODUCTION,
In recent years, customer relationship management
(CRM) becomes an important tool by which companies
can face the challenges brought by the changing market. 
To cater to the customers? customer analysis plays an 
important role, for companies hope to improve customer 
repurchasing rate, promote sales and win reputation with
limited resources and ability. In fact, data used in CRM is 
always affected by noise, while the commonly used CRM 
customer classification methods are generally sensitive to 
noise, misleading decision.
Group Method of Data Handling (GMDH) which is 
proposed by Ivakhnenko [1].  GMDH has been 
recognized as a noise-immunity approach[2] and 
successfully applied to many real-world data mining 
applications[3]. Fuzzy model is a powerful model in 
system modeling. There are mainly two kinds of 
rule-based fuzzy model: Mamdani fuzzy model and TS 
fuzzy model
Reference [3] proposes Self-Organizing Fuzzy Rule 
Induction (FRI), which combines advantages of Mamdani 
fuzzy model and GMDH techniques. When applied to the 
market survey data analysis, it automatically and 
objectively extracts the consumer segment market 
characteristics to make up for the deficiency of existing 
customers analytical methods.
TS model is a rule-based fuzzy model proposed by Takagi 
and Sugeno in 1985[4]. Compared with Mandani model, 
TS model can achieve higher accuracy with fewer rules. 
Therefore,
in recent years, many identification methods of TS model 
have been proposed. In this paper , we combine TS model 
and GMDH technique forming TS-GMDH model, which 
makes the TS model more useful and shows the strong 
noise-immunity.
II. TS-GMDH MODEL,
The modeling of TS-GMDH is very similar to FRI, the 
only differences is that the partial functions becomes TS 
fuzzy models instead of Mamdani fuzzy rules and the 
sample data set N is divided to a training set A and a 
testing set B( UN = A B ).The main steps of TS-GMDH 
modeling are as follows:
1) Fuzzification of variables. The initial variables are 
transformed into fuzzy linguistic variables. If we 
define L fuzzy sets in every variables, there will be 
2010 Asia-Pacific Conference on Wearable Computing Systems
978-0-7695-4003-0/10 $26.00 © 2010 IEEE
DOI 10.1109/APWCS.2010.26
75
1 1
0
5
mL inputs in the first layer in the network (see [2] 
and Fig.1).
2) Forming of the first generation TS models. All the 
input fuzzy sets are combined in pairs to form the 
first generation TS models of. For example, if two 
fuzzy sets isiA and j
s
jA defined in the space of input 
variable ix and jx ( , 1,2, , )i j m  are
combined ,we get following TS model:
1
1 10 1
1 2
1 10 1
21 2 1 2
i
j
s
i i i il
s
j j j j
mn
if x is A then y a a x
R :
if x is A then y b b x
i, j , ,...,n,i j;l , ,...,C
       

      
 	 
        (1)
where 2mnC is the number of combination? the
parameters in the consequents parts are estimated by  
Ordinary Least Square(OLS)in the training set A.
3) Selecting of  Models. 1F best TS models are 
selected in the testing set B by external criterion(see 
the selecting of the first generation TS models in 
Fig.1). Regularity criterion in following form is 
applied in our algorithm[1]:
21 2
1 2
1 11 2 1 2
1
B
i
G Gˆ ˆy ( y y )
G G G G

 
    
        (2)
where y is the real output , B denotes the number of 
data in the set B, 11yˆ and
2
1yˆ represent the outputs of the 
two rules in the model, 
1 is
i iG A ( x ) and 2 j
s
j jG A ( x ) are firing strength of the 
two rules.
4) Rules fusion. The two rules of 1
lR are merged into 
one rule which is still denoted by 1
lR :
1
1 10 1 1 11
ji ssl
i i j j
l
i i j j
R : if x is A and x is A
then y a a x a x ,l ,...,F
      
    
      (3)
where fuzzy set isiA ? j
s
jA will do the conjunction 
operation? the coefficients of the consequent linear 
function remain to be decided?
5) Generating the second generation TS models. After 
merging, the 1F rules are combined in pairs to form 
following second generation TS models denoted 
as 2
lR :
1
2 20 2 2
2
2
2 20 2 2
ji
k h
ss
i i j j
i i j jl
s s
k k h h
k k h h
if x is A and x is A
then y a a x a x
R :
if x is A and x is A
then y b b x b x
       

     

      

    
          (4)
where the parameters in the consequents are still
estimated in the training set A.
6) Generating and merging of the second generation TS 
models. For the second generation TS models
obtained in step 5), step 3) and step 4) are repeated to 
get  2F second generation fuzzy rules as follows?
2
2 20 2 2 2 2
ji
k h
ssl
i i j j
s s
k k h h
l
i i j j k k h h
R : if x is A and x is A and
x is A and x is A
then y a a x a x a x a x
     
      
     
       (5)
7) Forming of the final rules. Step 5) and step 6) are 
repeated to generate TS models of the 3th?4th?
…generation until the external criterion begin to 
increase. If the increasing happens in the kth layer, 
kF best TS fuzzy models are chosen and then the 
kF TS models are integrated into kF fuzzy rules 
according to step 4). These kF fuzzy rules make up 
of the final TS model, and parameters in the 
consequent part are estimated by OLS.
8) Calculation of the final output. The output y is
computed by taking the weighted average of the 
outputs of  the kF rules in the final model.
761
Fuzzification
1x
    
      
     
ix
    
nx
    
1
1x   
mx1  
1
ix   
    
m
ix  
1
nx  
    
m
nx  
TS
TS
TS
TS
TS
TS
TS
TS
TS
TS
TS
TS
Selecting of 
1th generation 
Selecting of 
2th generation 
Selecting of 
kth generation 
Weighted
Average
y
1th generation
TSmodel
2th generation
TSmodel
kth generation
TSmodel
Initial input
Figure1 Network of TS-GMDH modeling
III. EXPERIMENT
A. Experimental setting
The experiment starts with comparison between 
TS-GMDH and FRI in Matlab, for TS-GMDH is proposed 
based on FRI. Next, we compare TS-GMDH with four 
classification algorithms: Naïve Bayes(NB), Support 
Vector Machine(SVM), C4.5 decision tree(C4.5) and K 
Nearest Neighbour(KNN) under four levels of noise,
which  automatically run in Weka, for they are used for 
classification in our daily life and work for a long time and 
perform generally well.
We use classification accuracy to measure classifiers’ 
performance?and introduce 10-fold cross-validation to 
finish algorithm comparison on 9 UCI data sets. Data sets 
used in the experiment come from UCI database, basic
information of which is listed on TABLE1
TABLE1 Data sets used in experiment
Data set Abbr. #instance #attribute
Credit Approval credit-a 690 15
Echocardiogram Data Echo 132 12
German Credit credit-g 1000 20
Haberman's Survival Haberman 306 3
MAGIC gamma
telescope
Magic 19020 10
Mammographic Mass 
Data
Mammo 961 6
Mammographic Mass 
Data
Mammo 961 6
Wisconsin Breast 
Cancer 
Breast 699 10
Wisconsin Diagnosis 
Breast Cancer 
WDBC 699 10
B. Result
Comparison between TS-GMDH and FRI
To test whether TS-GMDH model makes some 
improvement on FRI model, we compare the two models, 
and TABLE2shows the result.
TABLE 2 demonstrates that TS-GMDH model 
achieves higher accuracy on 8 sets out of 9 data sets in 
comparison with FRI. We do paired t-test on the result and 
set confidence level as 95%. It shows that the difference 
on all sets except “ Haberman” and “Credit-g” is 
significant
TABLE2 Mean and Variance
Data set FRI TS-GMDH
Breast 93.15±0.60 93.95±0.62
Credit –a 85.51±0.10 84.91±0.59
Credit-g 71.06±0.62 71.06±0.96
Echo 95.22±1.82 96.68±0.88
Haberman' 73.12±1.64 74.14±0.39
Magic 72.39±5.8E-15 79.8±0.13
Mammo 79.52±0.44 81.47±0.85
Pima 73.94±0.45 75.04±0.60
WDBC 92.02±0.63 93.69±0.77
To test the overall performance of TS-GMDH, we treat 
the average results in 10-fold cross-validation of 
TS-GMDH and FRI on each data set as a group, forming 9 
groups, then tested by Wilcoxon-Signed-Rank Test. It 
shows that there is significant difference between the two 
algorithms on the whole.
Therefore we come to the conclusion that TS-GMDH 
can get higher classification accuracy than FRI.
Comparison between TS-GMDH and benchmark
classifiers
  In this subsection, we compare TS-GMDH with 
NB, SVM, C4.5 and KNN. It demonstrates that the 
average rank of TS-GMDH is 3 ranking 4th among them 
without noise. However, it ranks 1st when there is 5% 
noise and keeps ahead when the level of noise gets higher 
showing that TS-GMDH outperforms other four 
classifiers under high level of noise.
  We record the accuracy of each algorithm and draw
line chart to show the trend directly. TABLE3andFig.2 
772
show the result on credit-g T?he line charts on other 8 sets 
are similar to that on credit-g.
TABLE3 Result on credit-g
Algorithm
Noise level
0.00% 5% 10% 20%
TS-GMDH 71.06% 68.81% 69.25% 66.87%
C4.5 70.70% 65.10% 66.10% 55.40%
SVM 75.20% 72.40% 71.30% 65.30%
NB 74.60% 71.20% 67.60% 63.10%
Figure2 Performance of each algorithm on credit-g
And then, we test the result under no and 20% noise 
with Friedman test, which is often used to test the 
performance of multiple algorithms. It turns out that there 
is no significant difference between each algorithm 
without noise ,while there is significant difference with 
20% noise.
Next, we do post-hoc test on the result of 20% noise 
with Bonferroni-Dunn test. The result shows that 
TS-GMDH outperforms C4.5 and KNN, while we can’t 
tell whether there is significant difference between SVM 
and TS-GMDH, or TS-GMDH and NB.Therefore, we 
adopt Wilcoxon test, and it shows that there is significant 
difference, that is TS-GMDH outperforms SVM and NB.
In summary, we come to another conclusion that, 
TS-GMDH gets similar accuracy as others, however, 
when noise level gets higher, it performs outstanding.
IV. CONCLUSIONS
In this paper, we firstly make comparison between 
TS-GMDH model and FRI model with 10-fold cross 
validation in Matlab. The following conclusion can be 
drawn, TS-GMDH model is better than FRI model on
classification accuracy, showing the improvement of 
TS-GMDH model.
And then, we compare TS-GMDH model with popular 
classification algorithms :SVM, C4.5,NB and KNN. We 
introduce 3 different level of noise: 5%,10% and 20% to 
how the TS-GMDH performs in comparison with other 
classification algorithms with noise. We draw the second 
conclusion that, as noise level gets higher, the 
performance of TS-GMDH is far superior to others, 
showing extra capability of anti-interference.
As we all know, the real life and work is filled with 
noise, we can’t identify the true information when making 
decision, easily causing serious consequence. In view of 
strong noise immunity of TS-GMDH, we recommend a 
new classification method in customer analysis, help 
companies to get useful information.
ACKNOWLEDGMENT
This work was supported by National Natural Science 
Foundation of China (Grant No. 70771067) and the 
NSFC/RS (Royal Society of the UK) International Joint 
Project (Grant No. 70911130133).
REFERENCES
[1] Ivakhnenko A.G. The group method of data handling in 
prediction problems. Soviet Automatic Control c/c of Avtomatika,
1976, 9(6):21-30.
[2] Mueller J.A., Lemke F. Self-organizing data mining. Herstellung. 
Berlin?Libri Books on Demand, 2000.
[3] Mueller J.A. and F. Lemke, Self-organizing Data Mining. 2000, 
Berlin, Hamburg: Libri Books.
[4] T.Takagi, M. Sugeno. Fuzzy identitication of systems and its 
application to modeling and control. IEEE Trans. Syst., Man, 
Cybern, 1985:vol. 15, pp. 116-132.
[5] Mueller J.A., Lemke F. Self-organizing data mining. Herstellung. 
Berlin?Libri Books on Demand, 2000.
783
