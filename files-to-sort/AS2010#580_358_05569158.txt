                                 978-1-4244-5934-6/10/$26.00 ©2010 IEEE                                 1152
2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2010)                       
A New Fuzzy Associative Classification Based on
Axiomatic Fuzzy Set Theory
Xiaojuan Tian
Department of mathematic,
Dalian Maritime University
Dalian 116026, P. R. China
Email: txj0728@sohu.com
Guangfei Hu
China Higher-education Student
Information and Career Center,
Beijing 100083, P. R. China
Jing Li
Department of mathematic,
Dalian Maritime University
Dalian 116026, P. R. China
Abstract—Classification is one of the most popular data mining
techniques applied to many scientific and industrial problems.
Recently,fuzzy association rule has been extensively studied in
classification. In this paper,a new classification model is pro-
posed,which is based on interpretable fuzzy association rules
and automatic generating membership functions. In addition,a
modified algorithm for classification is presented. The results on
five data sets indicate that the proposed classifier can be regarded
as an accurate and effective classification technique. Compared
with other classification approaches and fuzzy logics, a relative
error rate is lower in our results.
1. INTRODUCTION
Today is an era of data collection and storage. However,
knowledge representation and information acquisition from
huge data are a bottleneck for many engineering applications.
Currently many researchers are working hard to extract useful
information via designing effective classifiers. One way to
create a classifier is to use a learning algorithm to construct a
classifier from a training set of objects whose classification(s)
is known.
In real-world applications, many data may be fuzzy, varied
data types or conflict but useful, discovering quantitative
association rules has been regarded meaningful and important.
Since Agrawal et al.[1] introduced the notion of association
rules, association rules mining has become a focal point of
research in knowledge discovery. The algorithm proposed by
Srikant and Agrawal [8] proceeded by partitioning attribute
domains into several intervals and transforming quantitative
values into binary ones in order to apply the classical mining
algorithm. Generally, this method always leads to the so-called
”sharp boundary” problem because it may under-emphasize or
over-emphasize the elements near the boundaries of intervals
in the mining process. A natural way to deal with this problem
is that an element around the boundary could partly belong
to two adjacent intervals, which is fuzzy in nature. The use
of fuzzy sets in connection with association rules has been
motivated by numerous authors ([9]—[14]). Compared with
traditional association rules, fuzzy association rules provide
linguistic explanation and overcome the sharp boundary prob-
lems in continuous (numerical) attributes, and also can be used
to analysis the mixture relations both in discrete (nominal)
attributes and continuous attributes.
One of the earliest automated methods for finding fuzzy
sets in association rule mining has been proposed in [15].
Apart from common clustering techniques such as k-means,
approaches based on genetic algorithms have recently been
developed [16]. An extension of the equi-depth partitioning
algorithm, which allows for combining crisp values, intervals
and fuzzy sets in the antecedent and consequent part of
association rules, has been proposed in [17]. As a disadvantage
of purely data-driven approaches to partitioning, they cannot
guarantee the linguistic interpretability of the resulting fuzzy
sets. In principle, this problem can be avoided by using
predefined fuzzy partitions [18]. However, it is impossible for
the user to specify all fuzzy partitions by hand, if the data set
comprises a large number of attributes.
In order to cope with the above problems, we propose a new
algorithm based on the AFS (Axiomatic Fuzzy Set) theory
([19]—[26]). In order to verify the quality and efficiency
of AFS fuzzy associative classification(AFSFAC), we apply
the proposed algorithm to find the classification rules for
five benchmark data sets—the Iris data, the Wisconsin breast
cancer data, the Wine data, the Sonar data and the Pima
classification data and compare its results with other rule-
based or associative classifiers. The illustrative results show
that AFSFAC outperforms the other classifiers on almost each
data set presented.
The rest of the paper is organized as follows: Section
2 gives an overview of AFS theory. Section 3 introduces
fuzzy association rule and the modified degree of support.
Section 4 presents an algorithm based on AFS to design a
fuzzy classifier. The algorithm is applied to five data sets
and the experiment results are showed in section 5. Finally,
conclusions are presented in Section 6.
2. OVERVIEW OF THE AFS THEORY
In this section, we recall some notations and properties of
AFS fuzzy logic that will be of immediate use in the study. For
more details the reader is referred to [4], [5], [7]. In essence,
the AFS framework supports the studies on how to convert
the information in databases into the membership functions
and their fuzzy logic operations.
Definition 1: ([21]) Let M be a non-empty set. Then the
set EM? is defined by
                                                                                                                                          1153
EM? = {?i?I(?m?Ai m)|Ai ? 2M , i ? I, I is any
non-empty indexing set }.
A binary relation R on EM? is defined as follows:for any?
i?I(
?
m?Ai m),
?
j?J(
?
m?Bj m) ? EM?,[?
i?I(
?
m?Ai m)
]
R
[?
j?J(
?
m?Bj m)
]
?? (i)
?Ai (i ? I), ?Bh (h ? J) such that Ai ? Bh; (ii) ?Bj
(j ? J), ? Ak (k ? I), such that Bj ? Ak.
It’s clear that R is an equivalence relation. EM?/R
is denoted by EM . The notation
?
i?I(
?
m?Ai m) =?
j?J(
?
m?Bj m) means that
?
i?I(
?
m?Ai m) and?
j?J(
?
m?Bj m) are equivalent under equivalence relation
R. In what follows, each
?
i?I(
?
m?Ai m) ? EM is called
a fuzzy concept.
Theorem 1: ([23]) Let M be a non-empty sets. Then
(EM,?,?) forms a completely distributive lattice under the
binary compositions ? and ? defined as follows. For any?
i?I(
?
m?Ai m),
?
j?J(
?
m?Bj m) ? EM,?
i?I
(
?
m?Ai
m) ?
?
j?J
(
?
m?Bj
m) =
?
k?IunionsqJ
(
?
m?Ck
m) (2.1)
?
i?I
(
?
m?Ai
m) ?
?
j?J
(
?
m?Bj
m) =
?
i?I,j?J
(
?
m?Ai?Bj
m) (2.2)
where for any k ? I unionsq J (the disjoint union of I and J , i.
e. , every element in I and every element in J are always
regarded as different elements in I unionsq J), Ck = Ak if k ? I ,
and Ck = Bk if k ? J .
(EM,?,?) is called the EI (expanding one set M )
algebra over M—one type of AFS algebra. For ? =?
i?I(
?
m?Ai m), ? =
?
j?J(
?
m?Bj m) ? EM , ? ? ??? ? ? ? = ? ? ?Ai (i ? I), ?Bh (h ? J) such that
Ai ? Bh.
Definition 2: ([5]) Let ? be any concept defined on the
universe of discourse X . R? is called a binary relation (i. e. ,
R? ? X ×X) of ? if R? satisfies: x, y ? X , (x, y) ? R? ?
x belongs to concept ? at some degree and the degree of x
belonging to ? is larger than or equal to that of y, or x belongs
to concept at some degree and y does not at all.
Definition 3: ([5]) Let X be a set and R be a binary relation
on X . R is called a sub-preference relation on X if for x, y,
z ? X , x 6= y, R satisfies the following conditions:
1. If (x, y) ? R, then (x, x) ? R;
2. If (x, x) ? R and (y, y) /? R, then (x, y) ? R;
3. If (x, y), (y, z) ? R, then (x, z) ? R;
4. If (x, x) ? R and (y, y) ? R, then either (x, y) ? R or
(y, x) ? R.
A concept ? is called a simple concept on X if R? (defined
by Definition 2 ) is a sub-preference relation on X . Otherwise
? is called a complex concept on X .
Definition 4: [23] Let X, M be sets, 2M be the power set
ofM, ? : X×X ? 2M . (M, ?,X) is called an AFS structure
if ? satisfies the followings
AX1: ?(x1, x2) ? X ×X, ?(x1, x2) ? ?(x1, x1);
AX2: ?(x1, x2), (x2, x3) ? X×X, ?(x1, x2)??(x2, x3) ?
?(x1, x3).
Then X is called the universe of discourse, M is called the
attribute set and ? is called the structure.
Let X be a set of objects andM be a set of simple concepts
on X . If ? : X × X ? 2M is defined as follows: for any
(x, y) ? X ×X ,
?(x, y) = {m | m ?M, (x, y) ? Rm} ? 2M (2.3)
where Rm is the binary relation of simple concept m ? M
(refer to Definition 2). Then (M, ?,X) is an AFS structure.
we define the membership functions of fuzzy concepts in
EM via the AFS structure of data and EI algebra EM . Let
(M, ?,X) be an AFS structure and S ? 2X be an ?-algebra
over X . We assume that for each simple concept ? ?M , there
exists m? a measure on S with 0 ? m?(U) ? 1 for any
U ? S. For the fuzzy concept ? =?i?I(?m?Ai m) ? EM ,
if ?x ? X,?i ? I , A?i (x) ? S, then the membership function
of fuzzy concept ? is defined as follows. For any x ? X ,
µ?(x) = sup
i?I
???
??Ai
m?(A?i (x))
?? (2.4)
where
A? (x) = { y ? X | ?(x, y) ? A } (2.5)
A? (x) is the set of all elements in X whose degrees of
belonging to concept
?
m?Am are less than or equal to that
of x.
In general, the measure m? for a simple concept ? is
constructed to weight the completeness of the set A ? S
as a extent of concept ?. In practice, the measure m? can
be induced by a function ? : X ? [0,?) as the following
Definition 5.
Definition 5: ([23]) Let X be a set, S is a ?-algebra over
X . ? : X ? R+ = [0,?). 0 < ?x?X ?(x) < ?. For any
A ? S, we define a measure m over ?-algebra S on X ,
m(A) =
?
x?A ?(x)?
x?X ?(x)
. (2.6)
3. FUZZY ASSOCIATION RULE AND CLASSIFICATION
In this section, we will introduce the definitions and nota-
tions for fuzzy association rule and fuzzy associative classifi-
cation.
LetX and Y are fuzzy sets, the two commonly used T -norm
are product: T (x, y) = xy and min: T (x, y) = min(x, y). For
fuzzy sets X and Y , µX(t) is the degree that a tuple t belongs
to a fuzzy set X , and then, the support and confidence of the
fuzzy association rule are:
Dsupp(X ? Y ) =
?
t?D T (µX(t), µY (t))
|D| ,
Dconf(X ? Y ) =
?
t?D T (µX(t), µY (t))?
t?D µX(t)
.
There are many generalizations of confidence measure pre-
sented in the literature [2], [3].
                                                                                                                                          1154
We apply fuzzy classification rules that each describe one
of the C classes in the data set. The rule antecedent is a fuzzy
description in feature space and the rule consequent is a crisp
(non-fuzzy) class label from the set {1, 2, . . . , c}.
Let C = {C1, C2, . . . , Cc} be a finite set of class labels. No-
tably, in associative classification, only those rules each with
one single class label as its consequent need to be considered.
Since Cc is a crisp set with value {0, 1}: µCc(tp) = 1 if
p ? classCc otherwise µCc(tp) = 0, the support of the fuzzy
association rule Ac ? Cc is calculated as follows:
Dsupp(Ac ? Cc) =
?
t?D T (µAc(t), µCc(t))
|D|
=
?
t?Cc T (µAc(t), µCc(t))
|D|
+
?
t/?Cc T (µAc(t), µCc(t))
|D|
=
?
t?Cc µAc(t)
|D|
=
?
t?Cc µAc(t)
|Cc| ×
|Cc|
|D| .
Let
Supp(Ac ? Cc) =
?
t?Cc(µAc(t))
|Cc| (3.7)
it only depends on the number of tuples in class c regardless of
the tuples of other classes. Compared with the Dsupp, it can
avoid the unbalance of the number of each class, which may
ignore the interesting rules of the class with a few samples
while the other classes have much more samples.
In this paper, we will employ it as the support. And the
confidence is calculated as follows:
Conf(X ? Y ) = Dconf(X ? Y )
=
?
t?D T (µAc(t), µCc(t))?
t?D µAc(t)
=
?
t?Ac T (µAc(t), µCc(t))?
t?D µAc(t)
+
?
t/?Ac T (µAc(t), µCc(t))?
t?D µAc(t)
=
?
t?Ac µAc(t)?
t?D µAc(t)
.
We will employ this two measures Supp and Conf to our
proposed classifier. The Supp is used to filter the concepts that
have litter effect on each class and Conf is used to measure
the strength of the fuzzy association rules of the class.
4. A NEW ASSOCIATIVE CLASSIFICATION ALGORITHM
BASED ON AFS
In this section, we will present a new associative classifica-
tion algorithm based on AFS theory. The pseudocode of the
main idea showed as following:
AFSFAC{class, attributes, ex., min Supp, ?1, ?2}
• Learned rules? {}
• Rule? Learned one rule{ex., ?1, ?2}
• while condition{ex.} > ?2
– Learned rules? Learned rules+Rule
– ex.? ex.? {ex. correctly classified by rule}
– Rule? Learned one rule{ex., ?1, ?2}
• Return Learned rules
A. Preprocess
Let F = {f1, f2, . . . , fs} is the feature set of the data set,
all tuples of the data set are D = {t1, t2, . . . , tn}. And the
data are labeled by c classes, which are C1, C2, . . . , Cc, i.
e. , D =
?
1?i?c Ci, Ci
?
Cj = ?, i 6= j. For any tuple
t = (?1, ?2, . . . , ?r), ?i = fi(t)(i = 1, . . . , r) is the value of
t on feature fi. Let M = {mij | 1 ? i ? s, 1 ? j ? si}
be the set of simple concepts associating to the features. For
example, if mj1,mj2,mj3 and mj4 are the fuzzy concepts
”small”, ”medium”, ”no-medium”, ”large” associating to the
feature fj respectively, the weight function of them can be
defined according to the training data and their semantics as
follows :
?mj1(ti) =
hj1 ? fj(ti)
hj1 ? hj2 (4.8)
?mj2(ti) =
hj4 ? |fj(ti)? hj3|
hj4 ? hj5 (4.9)
?mj3(ti) =
|fj(ti)? hj3| ? hj5
hj4 ? hj5 (4.10)
?mj4(ti) =
fj(ti)? hj2
hj1 ? hj2 (4.11)
where j = 1, 2, . . . , s,
hj1 = max
1?i?n
{fj(ti)}
hj2 = min
1?i?n
{fj(ti)}
hj3 =
?i=n
i=1 fj(ti)
n
hj4 = max
1?i?n
{|fj(ti)? hj3|}
hj4 = min
1?i?n
{|fj(ti)? hj3|}
B. The algorithm of proposed classifier
Since each class of the data set has the same method to
generate the association rules, we only need to give the method
of one class. Now, we have the following steps to design the
classifier for one class:
Step1: For each feature fi(i = 1, . . . , s), choose the corre-
sponding simple concepts(fuzzy partition) mij , j = 1, . . . , si,
where si is number of fuzzy set(fuzzy partition) of the feature
fi, and so the total of the fuzzy sets is
?r
i=1 si.
Step2: Let the form of association rule is Ac ? Cc, where
Ac represents the antecedent of the rule of class c and Cc
represents class c. Employing the Supp and Conf defined in
previous section, suppose inirule is the fuzzy sets pruned
                                                                                                                                          1155
by Supp, i. e. , inirulec = {mij | Supp(mij ? Cc) ?
min Supp, i = 1, . . . , r, j = 1, . . . , si, c = 1, . . . , C}, where
min Supp is the minimum threshold of Supp.
Step3: Generate the description of the class c from the fuzzy
sets produced in step 2. The rule selection is similar with
sequential forward selection. Let Dec is the description of
class c, ?1 and ?2 are parameters. Let Dec = ? and delk = ?.
3.1: Generate the first rule. First, choose the maximum
confidence of the fuzzy sets mi1j1 from inirule, i. e. ,
mi1j1 = arg max
mij?inirule
Conf(mij ? Cc)
Let A1 = mi1j1 , if max(DA1 ? Dc) ? ?1, then
Dec=Dec
?
A1. Otherwise, rule = A1
?
(inirulec ?
mi1j1), where
A1 = arg max
Ai?rule
Conf(Ai ? Cc)
denotes the rule with maximum implication in rule. Go
on until max(DA1 ?Dc) ? ?1.
3.2: After obtaining the first rule, let allk = {k | Dc(k) ?
DDec(k) > ?2, k ? D ? delk},
k? = arg max
k?allk
Dc(k)?DDec(k)
Mk? = {mij | Dc(k?) ? Dmij (k?) ? ?2, mij ?
inirulec}. Then, let inirule = Mk? , following the
strategy of step 3.1 to find a rule Ai with simple concepts
in Mk? .
3.2.1 choose the maximum implication of the fuzzy sets
mi1j1 from Mk? (inirule), i. e. ,
mi1j1 = arg max
mij?Mk?
Conf(mij ? Cc)
3.2.2 Let A1 = mi1j1
while max(DA1 ? Dc) ? ?1 or Dc(k?) ?
DA1(k
?) ? ?2
if Dc(k?)?DA1(k?) ? ?2
break
delk = delk
?
k?
elseif max(DA1 ?Dc) ? ?1
rule = A1
?
(inirulec ?mi1j1)
Msk? = {Ai | DAi(k?)?Dc(k?) ? ?2}
A1 = arg max
Ai?rule
Conf(Ai ? Cc)
endif
endif
endwhile
Dec=Dec
?
A1.
3.3: Go back to 3.2 until allk is ?. Finally, we obtain the
final description Dec of class c.
Step4: The classifier has been built, then we can test the
classifier. For each testing sample u, the following steps is
proceeded:
4. 1 Compute its AFS structure.
4. 2 Compute its membership degree of each class, i. e.
µDec(u), c = 1, . . . , C.
4. 3 Estimate the class of the sample. That is, the class label
of the sample is arg
C
max
c=1
µDec(u).
Determine the class label of each testing sample or new sample
by its membership degree belonging to each class. Suppose
y is a testing sample or new sample, by (4.8), (4.9), (4.10),
(4.11) one can compute the weigh ??(y) for all simple concept
? ? M . Let the characterization of class i, ?Xi =
?
???Xi
? =?
i?I(
?
m?Ai m) ? EM , i = 1, 2, ..., l. By (2.5), one has
A?k(y) = {x | ?(y, x) ? Ak}, k ? I . By formula (2.4), we
have the membership degree of y belonging to ?Xi as follows:
µ?Xi (y) = sup
i?I
???
??Ai
m?(A?i (y))
??
= sup
i?I
???
??Ai
?
x?A?i (y) ??(x)?
x?X?{y} ??(x)
??
the class label of y is arg max
1?k?l
{µ?Xk (y)}.
We traverse all the parameter ?1 and ?2 through 0.1 to 0.9
with step length 0.1 of training samples. Then, compute the
average accuracy average?1 with respect to ?1. Find the first
?1 that satisfies train?1+0.1 ? train?1 and train?1?0.1 ?
train?1 , and then ?1=?1.
The following method is to determine a suitable ?2.
Method 1 Choose ?2 with minimum error rate, that is
?2 = arg min
?2?[0.1,0.9]
train?1,?2 . If there are more than one ?2
satisfied the condition, choose the maximum. So it can be
wrote as ?2 = max arg min
?2?[0.1,0.9]
train?1,?2 .
Method 2 From 0.9 to 0.1, find the first ?2 that satisfies
train?1,?2+0.1 ? train?1,?2 and train?1,?2?0.1 ? train?1,?2 ,
and ?2 = ?2.
5. EXPERIMENT RESULTS
In this section, first, we give the experiment result with
our algorithm on five UCI data sets—-Iris, Wine, Breast-
Wisconsin, Sonar and Pima. Second, we evaluated our method
against other associative classifiers on the data sets. For all
of the data sets, ten-folds cross validation were applied. The
experiment results have showed that the good performance of
the AFS logic.
The experiment result with method 1 in table I and the result
with method 2 in table II.
To evaluate the performance of our proposed method, in
table III, we compared the accuracy of AFSFAC with the
well-known associative classifiers CBA, CMAR and CPAR (in
some literature is called hybrid between rule-based methods
and associative classifier), and with nonassociative classifiers
such as rule-based classifiers (Ripper) and decision trees
classifiers(C4. 5). The published results of their experiments
were collected from the literature.
6. CONCLUSION
In this paper, an associative classification algorithm based
on AFS is proposed, which can automatically generate the
                                                                                                                                          1156
TABLE I
THE RESULT OF EACH DATA SET WITH METHOD 1
Data set ? ?1 ?2 Train Test Num. R
Iris
0.2 0.2 0.8 0.0319 0.0333 4.9
0.3 0.2 0.8 0.0319 0.0333 4.9
0.4 0.2 0.8 0.0319 0.0333 4.4
0.5 0.2 0.9 0.0326 0.0333 4.0
Wine
0.2 0.4 0.6 0.0131 0.0393 23.6
0.3 0.4 0.6 0.0131 0.0393 23.6
0.4 0.4 0.6 0.0144 0.0393 23.3
0.5 0.4 0.6 0.0150 0.0281 18.7
Breast
0.2 0.3 0.4 0.0275 0.0315 28.6
0.3 0.3 0.4 0.0265 0.0315 27.6
0.4 0.3 0.4 0.0265 0.0315 27.1
0.5 0.3 0.4 0.0292 0.0315 16.8
Sonar
0.2 0.5 0.5 0.1106 0.2404 40.0
0.3 0.5 0.3 0.1122 0.2500 45.3
0.4 0.6 0.6 0.1368 0.2308 31.9
Pima
0.2 0.5 0.7 0.2227 0.2565 41.8
0.3 0.5 0.9 0.2271 0.2370 26.5
0.4 0.8 0.7 0.2571 0.2708 18.0
TABLE II
THE RESULT OF EACH DATA SET WITH METHOD 2
Data set ? ?1 ?2 Train Test Num. R
Iris
0.2 0.2 0.8 0.0319 0.0333 4.9
0.3 0.2 0.8 0.0319 0.0333 4.9
0.4 0.2 0.8 0.0319 0.0333 4.4
0.5 0.2 0.9 0.0326 0.0333 4.0
Wine
0.2 0.4 0.8 0.0225 0.0169 16.8
0.3 0.4 0.8 0.0225 0.0169 15.8
0.4 0.4 0.8 0.0194 0.0169 16.0
0.5 0.4 0.8 0.0218 0.0225 15.0
Breast
0.2 0.3 0.9 0.0320 0.0386 15.3
0.3 0.3 0.9 0.0302 0.0329 13.0
0.4 0.3 0.9 0.0304 0.0329 12.8
0.5 0.3 0.9 0.0310 0.0300 10.7
Sonar
0.2 0.5 0.5 0.1106 0.2404 40.0
0.3 0.5 0.5 0.1138 0.2404 38.9
0.4 0.6 0.6 0.1368 0.2308 31.9
Pima
0.2 0.5 0.7 0.2227 0.2565 41.8
0.3 0.5 0.9 0.2271 0.2370 26.5
0.4 0.8 0.9 0.2594 0.2656 16.8
TABLE III
THE COMPARISON OF DIFFERENT DATA SETS
Data set Iris Wine Breast-w sonar Pima
#attr 4 13 10 60 8
#cls 3 3 2 2 2
#rec 150 178 699 208 768
C4. 5 94.00 92.12 94.71 - 73.70
Ripper 94.00 92.12 95.28 - 73.19
CBA 94.67 94.96 96.28 77.50 72.90
CMAR 94.00 95.00 96.40 79.40 75.10
CPAR 94.70 95.50 96.00 79.30 73.80
AFSFAC(1) 96.67 97.19 96.85 76.92 76.30
AFSFAC(2) 96.67 98.31 97.00 76.92 76.30
membership degree. The results generated by this classifier
is interpretable and similar to human intuition. Performance
studies on the five data sets indicated that the proposed method
is highly competitive, compared with other classification ap-
proaches and fuzzy logics in term of accuracy.
REFERENCES
[1] R.Agrawal, T.Imielinski and A.Swami, Mining association rules between
sets of items in large databases, In Proc. of SIGMOD, pp.207,216,1993.
[2] D.Dubois, H.Prade, and T.Sudkamp, On the representation, measurement,
and discovery of fuzzy associations, IEEE transactions on fuzzy systems,
vol. 13, NO. 2, april 2005,
[3] Thomas Sudkamp, Examples, counterexamples, and measuring fuzzy
associations, Fuzzy sets and systems 149(2005)57-71.
[4] Xiaodong Liu, Tianyou Chai and Wei Wang, Approaches to the Repre-
sentations and Logic Operations for Fuzzy Concepts in the Framework
of Axiomatic Fuzzy Set Theory I, II, Information Sciences, vol.177,
pp.1007-1026, 1027-1045, 2007.
[5] Xiaodong Liu, Wei Wang and Tianyou Chai, The Fuzzy Clustering
Analysis Based on AFS Theory, IEEE Transactions on Systems, Man
and Cybernetics Part B, vol.35, no.5, pp. 1013-1027, 2005.
[6] P.R.Halmos, Measure theory, Springer-Verlag, New York Inc.1974.
[7] Xiaodong Liu, Witol Pedrycz, The Development of Fuzzy Decision
Trees in the Framework of Axiomatic Fuzzy Set Logic, Applied Soft
Computing, vol.7, pp.325-342, 2007.
[8] Srikant R, Agrawal R, Mining quantitative association rules in large rela-
tional tables, Proceedings of the ACM SIGMOD international conference
on management of data, pp 1C12, 1996.
[9] Chen G, Wei Q, Fuzzy association rules and the extended mining algo-
rithms. Information Sciences 147:201-228, 2002.
[10] Delgado M, Marin N, Sachez D, Vila M A, Fuzzy association rules:
general model and applications. IEEE Transactions on Fuzzy Systems
11(2):214-225, 2003.
[11] Au W, Keith C, C.Chan, Mining fuzzy association rules in a bank-
account database. IEEE Transactions On Fuzzy Systems 11(2):238-248,
2003.
[12] Duch W, Setiono R, Jack M, Zurada, Computational intelligence
methods for rule-based data understanding. Proceedings Of the IEEE
92(5):711- 805, 2004.
[13] Yan P, Chen G, Discovering a cover set of ARsi with hierarchy from
quantitative databases. Information Sciences 173(4):319-336, 2005.
[14] Berzal F, Blanco I, Sachez D, Serrano J M, Vila M A, A definition for
fuzzy approximate dependencies. Fuzzy Sets and Systems 149:105-129,
2005.
[15] Fu A, Wong MH, Sze SC, Wong WC, Wong WL, Yu WK, Finding fuzzy
sets for the mining of fuzzy association rules for numerical attributes.
In:IDEAL-98, 1st International symposium on intelligent data engineering
and learning. Hong Kong, pp 263C268, 1998.
[16] Kaya M, Alhajj R, Genetic algorithm based framework for mining fuzzy
association rules. Fuzzy Sets Syst 152(3):587C601, 2005.
[17] Zhang W, Mining fuzzy quantitative association rules. In: Proceedings of
the 11th IEEE international conference on tools with artificial intelligence,
Chicago, Illinois, 1999.
[18] Au W-H, Chan KCC, FARM: A data mining system for discovering
fuzzy association rules. In: Proceedings of the FUZZ-IEEE-99. Seoul,
Korea, pp 1217C1222, 1999.
[19] Liu X, A new mathematical axiomatic system of fuzzy sets and sys-
tems. Journal of Fuzzy Mathematics 3:559-560, 1995.
[20] Liu X, Two algebra structures of AFS structure. Journal of Fuzzy
Mathematics 3:561-562, 1995.
[21] Liu X, The fuzzy theory based on AFS algebras and AFS structure.
Journal of Mathematical Analysis and Applications 217:459-478, 1998.
[22] Liu X, The topology on AFS algebra and AFS structure, Journal of
Mathematical Analysis and Applications 217:479-489, 1998.
[23] Liu X, The fuzzy sets and systems based on AFS structure, EI algebra
and EII algebra, Fuzzy Sets and Systems 95:179-188, 1998.
[24] Liu X, Pedrycz W, Zhang Q, Axiomatics fuzzy sets logic, IEEE Inter-
national Conference on Fuzzy Systems 1:55-60, 2003.
[25] Zhang Y, Liang D, Tong S, On AFS algebra part I, Information Sciences
167:263-286, 2004.
[26] Zhang Y, Liang D, Tong S, On AFS algebra part II, Information Sciences
167:287-303, 2004.
