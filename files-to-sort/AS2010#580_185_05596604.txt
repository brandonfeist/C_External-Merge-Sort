Optimal synaptic learning in non-linear associative memory
Andreas Knoblauch
Abstract— Neural associative memories are single layer per-
ceptrons with fast synaptic learning typically storing discrete
associations between pairs of neural activity patterns. For
linear learning such as employed in Hopfield-type networks
it is well known that the so-called covariance rule is optimal
resulting in minimal output noise and maximal storage capacity.
On the other hand, numerical simulations suggest that non-
linear rules such as clipped Hebbian learning in Willshaw-type
networks perform better, at least for sparse neural activity
and finite network size. Here I show that the Willshaw and
Hopfield models are only limit cases of a general optimal model
where synaptic learning is determined by probabilistic Bayesian
considerations. Asymptotically, for large networks and very
sparse neuron activity the Bayesian model becomes identical to
an inhibitory implementation of the Willshaw model. Similarly,
for less sparse patterns, the Bayesian model becomes identical
to the Hopfield network employing the covariance rule. For
intermediate sparseness or finite networks the optimal Bayesian
rule differs from both the Willshaw and Hopfield models and
can significantly improve memory performance.
I. INTRODUCTION
A neural associative memory is a single layer perceptron
with fast, typically “one-shot” learning corresponding to the
storage of M associations between pairs of binary memory
vectors {(uµ ? vµ) : µ = 1, ...,M}. They exist both in
a hetero-associative feed-forward form storing the mapping
between the address memories uµ and the associated content
memories vµ, but also in an auto-associative recurrent form
where address and content memories are identical, where
typical tasks include denoising and pattern completion.
Neural associative memories have close relationships to
Hebbian cell assemblies [1] and are widely used in neuro-
science as models of neural computation for various brain
structures, for example, neocortex [2], [3], [4], [5], [6],
hippocampus [7], [8], perirhinal cortex [9], cerebellum [10],
[11], [12], and mushroom body [13]. In addition, neural asso-
ciative memories are potentially useful in technical applica-
tions such as cluster analysis, speech and object recognition,
or information retrieval in large databases [14], [15], [16],
[17], [18], [19], [20], [21].
Learning in neural associative memories is strongly con-
strained by the “one-shot” property. For example, gradient
descent methods (such as error-backpropagation) are not vi-
able because they require repeated training of the whole pat-
tern set. Instead it is straight-forward to use simple Hebbian-
like learning rules: If, during presentation of a single pattern
pair, both the presynaptic and postsynaptic neurons are active
then the synaptic weight must be increased. There have been
numerous previous attempts to develop optimized learning
Andreas Knoblauch is with the Honda Research Institute Europe, Carl-
Legien-Strasse 30, D-63073 Offenbach/Main, Germany (phone: +49 69
89011 750; email: andreas.knoblauch@honda-ri.de).
models that maximize “storage capacity” as defined, for
example, by the number of stored memories or the stored
Shannon information per synapse (e.g., see [22]).
One of the earliest and simplest one-shot learning model
is the so-called Steinbuch or Willshaw model with binary
synapses and clipped Hebbian learning [23], [24], [25], [26],
[27], [28], [29]. Here a single coincidence of presynaptic
and postsynaptic activity increases the synaptic weight from
0 to 1, while additional coincidences do not cause further
changes. In contrast, linear learning models of the Hopfield-
type [30], [31], [32], [33], [34], [35], [36], [37] add the
contributions of each pattern pair. For binary memory pat-
terns the general linear learning rule can be described by
four values ?, ?, ?, and ? specifying the weight increments
for the pre-/postsynaptic activations uµi /v
µ
j = 0/0, 0/1, 1/0,
and 1/1. A third model class is based on the “Bayesian
Confidence Propagation Neural Network” or BCPNN rule
of Lansner and Ekeberg [38], [39], [40], [41], [42], [43],
[6] and employs Bayesian maximum-likelihood heuristics for
synaptic learning and retrieval.
The first two model classes are theoretically well in-
vestigated, whereas the BCPNN-type models still lack a
comparable consideration. Surprisingly, the maximal network
storage capacity C in bits per synapse is almost identical
for the binary Willshaw and linear Hopfield type models:
The Willshaw model can achieve up to C = 0.69 bits
per binary synapse, whereas the linear learning model can
achieve only a slightly higher capacity of C = 0.72 bits per
real-valued synapse. However, closer investigations reveal
that the Willshaw model can achieve non-zero capacity only
for very sparse patterns where the number of active units
per pattern vector scales logarithmic with the vector size. In
contrast, the linear model achieves the maximum C = 0.72
for almost arbitrary sparseness. Only for linearly or non-
sparse patterns performance drops to the capacity of the
original Hopfield network (e.g., C = 0.14bps [30], [35] or
rather C = 0.33bps for “hetero-associative” feed-forward
networks considered here [44]). In any case, the linear
learning model achieves maximal storage capacity only for
the optimal covariance learning rule (e.g., [33], [35]) which
becomes equal to the Hebb rule for very sparse patterns, and
equal to the Hopfield rule for non-sparse patterns. Moreover,
simulation experiments show that the capacity of the optimal
linear model remains well below the capacity of the Willshaw
model for any reasonable finite network size (e.g., C=0.2bps
vs C=0.5bps for n = 105 neurons [44]). This suggests that
the linear covariance rule is not always optimal, in particular
not for finite networks and sparse memory representations
as found in real brains [45]. Similarly, the BCPNN model
has recently been shown to have a suboptimal performance
978-1-4244-8126-2/10/$26.00 ©2010 IEEE
[46], [47], basically equivalent to the linear homosynaptic
rule which is well known to have a factor 1 ? p lower
storage capacity than the optimal covariance rule (here p is
the fraction of active units in an address memory; see rule
R3 in [33]p259).
This paper develops the generally optimal associative
memory that minimizes output noise and maximizes storage
capacity by activating neurons based on Bayesian maximum-
likelihood decisions. The corresponding neural interpretation
of the Bayesian associative memory corresponds, in general,
to a novel non-linear learning rule. A signal-to-noise analysis
allows to compute the storage capacity and to compare
optimal Bayesian learning to the previous model types.
Specifically, it turns out that the previous models are only
special limit cases of the general optimal Bayesian model.
Asymptotically, for large networks and very sparse memory
patterns, the Bayesian model becomes essentially identical to
the binary Willshaw model (but implemented with inhibitory
rather than excitatory synapses [48]). Similarly, for less
sparse patterns, the Bayesian model becomes identical to
the linear model employing the optimal covariance rule.
For intermediate sparseness and finite networks the optimal
Bayesian learning rule performs significantly better than the
previous learning models.
II. MEMORY STORAGE
The task is to store M associations between address
patterns uµ and content patterns vµ where µ = 1 . . .M .
We assume that all memory patterns are binary vectors.
Address patterns uµ have dimension m and content patterns
v
µ dimension n. Then we assume that each address neuron
i and each content neuron j can memorize its unit usage
M1(j) := #{µ : vµj = 1} (1)
M0(j) := #{µ : vµj = 0} = M ?M1(j) (2)
M ?1(i) := #{µ : uµi = 1} (3)
M ?0(i) := #{µ : uµi = 0} = M ?M ?1(i) . (4)
Similarly, each synapse ij can memorize its synapse usage
M11(ij) := #{µ : uµi = 1, vµj = 1} (5)
M01(ij) := #{µ : uµi = 0, vµj = 1} =M1 ?M11 (6)
M00(ij) := #{µ : uµi = 0, vµj = 0} =M ?0 ?M01 (7)
M10(ij) := #{µ : uµi = 1, vµj = 0} =M0 ?M00 (8)
for i = 1, . . . ,m and j = 1, . . . , n. Note that it is sufficient to
memorize M , M1, M ?1, and M11. Thus, an implementation
on a digital computer requires only about (mn +m + n +
1)ldM memory bits. Note also that the synaptic weights of
the Willshaw network and the linear models can be expressed
in terms of the synapse usages. The weights of the Willshaw
network are wij = 1 if M11 ? 1 (and wij = 0 otherwise).
The weights of the linear models are wij = ?M00+?M01+
?M10 + ?M11.
III. OPTIMAL RETRIEVAL
Given a query pattern u˜ the memory task is to find the
“most similar” address pattern uµ and return a reconstruction
vˆ of the associated content vµ. In general the query u˜
is a noisy version of one of the address patterns uµ. For
clarity the following analysis considers only zero query noise
assuming u˜ = uµ. For the general case see a technical report
[44].
Now the content neurons j have to decide independently of
each other whether to be activated or to remain silent. Given
the query u˜, the optimal maximum-likelihood decision
vˆj =
{
1 ,
pr[vµ
j
=1|u˜]
pr[vµ
j
=0|u˜]
? 1
0 , otherwise
(9)
minimizes the expected Hamming distance dH(vµ, vˆ) :=?n
j=1 |vµj ? vˆj | between original and reconstructed content.
If the query pattern components are conditional independent
given the activity of content neuron j, e.g., assuming inde-
pendently generated address pattern components, we have for
a ? {0, 1}
pr[u˜|vµj = a] =
m?
i=1
pr[u˜i|vµj = a] =
m?
i=1
Mu˜ia(ij)
Ma(j)
. (10)
With pr[vµj = a|u˜] = pr[u˜|vµj = a]pr[vµj = a]/pr[u˜] (the
Bayes formula) we obtain
pr[vµj = 1|u˜]
pr[vµj = 0|u˜]
=
(
M0(j)
M1(j)
)m?1 m?
i=1
Mu˜i1(ij)
Mu˜i0(ij)
. (11)
For a neural formulation we can take logarithms of the
probabilities and obtain synaptic weights wij and dendritic
potentials xj := log(pr[vµj = 1|u˜]/pr[vµj = 0|u˜]),
wij = log
M11M00
M10M01
(12)
xj = (m? 1) log M0
M1
+
m?
i=1
log
M01
M00
+
m?
i=0
wij u˜i (13)
such that pr[vµj = 1|u˜] = 1/(1 + e?xj ) writes as a sigmoid
function of xj , and a content neuron fires, vˆj = 1, iff the
dendritic potential is non-negative, xj ? 0. Note that indices
of M0(j), M1(j), M00(ij), M01(ij), M10(ij), and M11(ij)
are skipped for brevity. Thus, the generally optimal Bayesian
learning rule is non-linear and differs from both the Willshaw
and linear models.
Evaluating eq. 13 is much cheaper than eq. 11, in particular
for sparse queries having only a small number of active
components with u˜i = 1. However, the synaptic weights
eq. 12 may not yet satisfy Dale’s law that a neuron is either
excitatory or inhibitory. To be more consistent with biology
we may add a sufficiently large constant c := ?minij wij
to each weight. Then all synapses have non-negative weights
w?ij := wij+c and the dendritic potentials remain unchanged
if we replace the last sum in eq. 13 by
m?
i=0
wij u˜i =
m?
i=0
w?ij u˜i ? c
m?
i=0
u˜i . (14)
Here the negative sum could be realized, for example, by
feedforward inhibition with a strength proportional to the
query pattern activity (e.g., [49]).
IV. ANALYSIS OF SIGNAL-TO-NOISE RATIO AND
STORAGE CAPACITY
To build a memory system with high retrieval quality we
have to minimize the expected Hamming distance dH(vµ, vˆ).
The expected Hamming distance E(dH) = nqp10 + n(1 ?
q)p01 can be computed from the component output error
probabilities
p01 := pr[vˆj = 1|vµj = 0] = pr[xj ? ?j |vµj = 0] (15)
p10 := pr[vˆj = 0|vµj = 1] = pr[xj < ?j |vµj = 1] (16)
where nq is the mean activity of a content pattern and ?j
is the firing threshold, e.g., ?j = 0 for dendritic potentials
xj as in eq. 13. Intuitively, retrieval quality will be high if
the “high potential distribution” pr[xj |vµj = 1] and the “low
potential distribution” pr[xj |vµj = 0] are well separated, i.e.,
if the signal-to-noise ratio (SNR)
r := SNR(µlo, ?lo, µhi, ?hi) :=
µhi ? µlo
max(?lo, ?hi)
(17)
is large (see [31], [33], [35], [37]). Here µlo := E(xj |vµj =
0) and ?2lo := Var(xj |vµj = 0) are expectation and variance
of the low-potential distribution, and µhi = E(xj |vµj = 1)
and ?2hi := Var(xj |vµj = 1) are expectation and variance of
the high-potential distribution.
In the following we compute the SNR assuming that each
address unit is one with the same probability p := pr[uµi = 1]
independently of other units. For the content patterns it is
sufficient to assume q := pr[vµj = 1]. Then let u˜ = uµ be a
noise-free query pattern having k one-entries. Without loss of
generality we assume further that u˜i is one for i = 1, . . . , k
and zero for i = k + 1, . . . ,m. Equivalently to eq. 13 (but
replacing m?1 by m for brevity), a content neuron j will be
activated if the dendritic potential xj exceeds the threshold
?j := log(M0/M1) (instead of ?j = 0), where
xj := m log
M0
M1
+
k?
i=1
log
M11
M10
+
m?
i=k+1
log
M01
M00
(18)
= m log
M0
M1
+
k?
i=1
log
M11
M0 ?M00
+
m?
i=k+1
log
M1 ?M11
M00
. (19)
For given M1,M0 the remaining variables are binomially
distributed, M00 ? BM0,1?p and M11 ? BM1,p, where
pr[BN,P = z] =
(
N
z
)
P z(1? P )N?z . For large NP (1? P )
the binomial BN,P can be approximated by a Gaussian Gµ,?
with mean µ = NP and variance ?2 = NP (1? P ). Given
uµi and v
µ
j we have then
M11 ?
??????
?????
G
M1p,
?
M1p(1?p)
,uµi /v
µ
j = 0/0
G
M1p,
?
M1p(1?p)
,uµi /v
µ
j = 1/0
G
(M1?1)p,
?
(M1?1)p(1?p)
,uµi /v
µ
j = 0/1
G
1+(M1?1)p,
?
(M1?1)p(1?p)
,uµi /v
µ
j = 1/1
(20)
M00 ?
??????
?????
G
1+(M0?1)(1?p),
?
(M0?1)p(1?p)
,uµi /v
µ
j = 0/0
G
(M0?1)(1?p),
?
(M0?1)p(1?p)
,uµi /v
µ
j = 1/0
G
M0(1?p),
?
M0p(1?p)
,uµi /v
µ
j = 0/1
G
M0(1?p),
?
M0p(1?p)
,uµi /v
µ
j = 1/1
(21)
From this we can approximate the distribution of the potential
xj for low units and high units, respectively. For large k and
m ? k the sums of logarithms are approximately Gaussian
distributed.
For Gaussian random variables X ? Gµ,? it is
E(logX) = logµ?
??
i=1
1 · 3 · · · (2i? 1)
2i
(?/µ)2i
? logµ (22)
Var(logX) ? (?
µ
)2 (23)
which can be proved using the Newton-Mercator series (for
a detailed proof see [44], app. C). Here approximations are
valid for ? ? µ and tight for ?/µ? 0 if µ 6? 1.
From this we can compute the exact mean dendritic
potentials µlo, µhi and variances ?2lo, ?2hi for low-units and
high-units. Fortunately, it turns out that the mean potential
difference ?µ := µhi ? µlo can be well approximated by
using only the first order term in eq. 22 (while all higher-
order terms become virtually identical for µhi and µlo;
for more details see [44], apps. D,F). Then the first-order
approximations µ?lo, µ?hi of µlo, µhi are
µ?lo = m log
M0
M1
+ k log
M1p
M0 ? (M0 ? 1)(1? p)
+(m? k) log M1 ?M1p
1 + (M0 ? 1)(1? p)
= ?k log(1 + 1? p
M0p
)
?(m? k) log(1 + p
M0(1? p) )
? ?k(1? p)
M0p
? (m? k)p
M0(1? p) (24)
µ?hi = m log
M0
M1
+ k log
1 + (M1 ? 1)p
M0 ?M0(1? p)
+(m? k) log M1 ? (M1 ? 1)p
M0(1? p)
= log
[(
(M1 ? 1)p+ 1
M1p
)k
×
(
M1(1? p) + p
M1(1 ? p)
)m?k]
? k(1? p)
M1p
+
(m? k)p
M1(1? p) (25)
where the approximations are valid for large M0p,M1p ?
?. Therefore the mean difference ?µ between the high- and
low-distributions is
?µ := µhi ? µlo ? µ?hi ? µ?lo
?
(
k(1? p)
p
+
(m? k)p
1? p
)(
1
M1
+
1
M0
)
(26)
In order to get the signal-to-noise ratio eq. 17 we still have
to compute the variances ?2lo and ?2hi for xj in eq. 19.
Given the unit usages M1, the random variables M00(i, j)
and M11(i, j) are independent and thus the variances sim-
ply add. Because each variance summand is positive, for
large M1p,M0p ? ? we can simply assume M11 ?
G
M1p,
?
M1p(1?p)
and M00 ? GM0(1?p),?M0p(1?p) in all
cases (cf., eqs. 20,21). With eq. 23 we get
Var(logM11) ? 1? p
M1p
(27)
Var(log(M0 ?M00)) ? 1? p
M0p
(28)
Var(log(M1 ?M11)) ? p
M1(1? p) (29)
Var(logM00) ? p
M0(1? p) . (30)
Thus, the variances ?2 := Var(xj) ? ?2lo ? ?2hi for the
potentials of both low-units and high units are approximately
?2 ? k(Var(logM11) + Var(log(M0 ?M00)))
+(m? k)(Var(log(M1 ?M11)) + Var(logM00))
? k(1? p)
p
(
1
M1
+
1
M0
)
+
(m? k)p
1? p
(
1
M1
+
1
M0
)
=
(
k(1? p)
p
+
(m? k)p
1? p
)(
1
M1
+
1
M0
)
? ?µ
(31)
Therefore, the signal-to-noise-ratio r := ?µ/? (eq. 17) is
given by
r2 ? ?µ ? ?2
?
(
k(1? p)
p
+
(m? k)p
1? p
)(
1
M1
+
1
M0
)
? m
Mq(1? q) (32)
where the last approximation is true because for large
M,M1,M0, k ?? the unit usage and pattern activity will
be close to the mean values, M1 ? Mq, M0 ? M(1 ? q),
and k ? pm.
In summary, for Mpq ?? the SNR r ? m/(Mq(1?q))
of optimal Bayesian learning is identical to the asymptotic
SNR of linear learning with the optimal covariance rule
(e.g., see ?Covariance3 in [33] p259; or eq. 3.28 in [35] p95).
Since the network storage capacity C can be written as a
function of the SNR r (e.g., see [35], [44]), for Mpq ??
the Bayesian learning model has also the same asymptotic
network storage capacity as the linear covariance rule, C =
0.72 bps.
V. RELATION TO LINEAR LEARNING MODELS AND THE
COVARIANCE RULE
In general, the synaptic weights of the Bayesian associative
network (eq. 12) are a non-linear function of presynaptic
and postsynaptic activity. In the following we show that
under some conditions the optimal Bayesian rule can be
approximated by a linear learning rule. For large networks
the synapse usages will be close to its expectations, i.e.,
M00 ? M0(1 ? p), M01 ? M1(1 ? p), M10 ? M0p,
and M11 ? M1p. In fact these approximations will make
only a negligible relative error if the standard deviations
are small compared to the expectations. The most critical
variable is M11 having expectation M1p and standard de-
viation M1p(1 ? p). Thus, the approximations are valid for
M1p ? ?, or Mpq ? ?, if we assume M1 ? Mq. Then
the argument of the logarithm in eq. 12 will be close to one.
A linear approximation of the logarithm around one yields
wij ? f(M00,M01,M10,M11) := M11M00
M10M01
? 1 . (33)
Similarly, the resulting function f can be linearized around
the expectations of the synapse usages. The partial derivatives
are
?f
?M00
|Mxy=E(Mxy) =
M11
M10M01
|Mxy=E(Mxy)
=
1
M0(1 ? p) (34)
?f
?M01
|Mxy=E(Mxy) = ?
M11M00
M10M201
|Mxy=E(Mxy)
= ? 1
M1(1? p) (35)
?f
?M10
|Mxy=E(Mxy) = ?
M11M00
M210M01
|Mxy=E(Mxy)
= ? 1
M0p
(36)
?f
?M11
|Mxy=E(Mxy) =
M00
M10M01
|Mxy=E(Mxy)
=
1
M1p
(37)
4 7 10 22 25 32 50
10?5
10?4
10?3
10?2
10?1
100
pattern activity k (=l)
o
u
tp
ut
 n
oi
se
 ?
m=n=100
4 12 71 100 292 595 1250 2500
10?7
10?6
10?5
10?4
10?3
10?2
10?1
100
m=n=5000
pattern activity k (=l)
o
u
tp
ut
 n
oi
se
 ?
L?Hebb
L?Cov
Willshaw
Bayes
L?Hebb
L?Cov
Willshaw
Bayes
Fig. 1. Experimental retrieval output noise ? := dH(vµ, vˆ)/l as function of pattern activity k := mp. for the optimal Bayesian rule (cyan solid), Willshaw
rule (black dashed), linear covariance rule (red solid), and the linear Hebb rule (blue dashed). Here ? is defined as the expected Hamming distance between
retrieval output vˆ and original content vµ normalized by the mean content pattern activity l := nq. Queries u˜ contained half the one-entries of the original
address patterns uµ. For each data point the number of stored memories, M , has been chosen maximal such that the Willshaw model does not exceed
? = 0.01 (data taken from [29]). Left panel: Small networks with m = n = 100 and M = 7, 26, 20, 11, 10, 7, 4 for k = l = 4, 7, 10, 22, 25, 32, 50.
Right panel: Larger networks with m = n = 5000 and M = 3985, 31481, 7082, 736, 202, 49, 12 for k = l = 4, 12, 71, 292, 595, 1250, 2500. Each
data point averages over 10000 retrievals in 100 different networks.
For q := M1/M the linear approximation writes finally
wij ? M00
M(1? p)(1? q) ?
M01
M(1? p)q
? M10
Mp(1? q) +
M11
Mpq
(38)
or for ? := 1/(Mpq(1? p)(1? q))
wij
?
= pqM00 ? p(1? q)M01
?(1? p)qM10 + (1? p)(1? q)M11 . (39)
This is essentially (up to factor ?) the covariance rule as
discussed in many previous works (e.g., see [50], [51],
[32], [52], [33], [53], [34], [35], [36], [37]). Thus, in the
asymptotic limit Mpq ? ? optimal Bayesian learning
becomes equivalent to the covariance rule of linear learning
models (see Fig. 1).
VI. RELATION TO THE WILLSHAW MODEL AND
INHIBITORY ASSOCIATIVE MEMORY
The Willshaw or Steinbuch model is one of the sim-
plest models for distributed associative memory employing
synapses with binary weights
wij = min(1,M11(ij)) ? {0, 1} . (40)
The dendritic potentials of the content neurons are simply
xj =
?m
i=0 wij u˜i. The Willshaw model works particular
good for “pattern part retrieval”, i.e., if the query pattern u˜
contains a subset of the one-entries of an address pattern
uµ. Then the optimal threshold is maximal, i.e., equal to the
query pattern activity ?j =
?m
i=0 u˜i. This implies that a
single “missing” input, i.e., u˜i = 1 but wij = 0, excludes
activation of content neuron j.
This observation suggests that the Willshaw model should
be interpreted as an essentially inhibitory network where
zero weights become negative, active weights zero, and the
optimal threshold zero. In particular for a diluted network
with low connectivity the inhibitory interpretation of the
Willshaw model with optimal threshold control seems to be
much more realistic than an excitatory interpretation (e.g.,
compare [48] vs. [54]).
Under some conditions the optimal Bayesian model be-
haves quite similar: Obviously, the synaptic weight eq. 12
becomes plus or minus infinity if one of the synapse usages
Mxy is zero. Since memory patterns are typically sparse it
is most likely that only the synapse usage M11 ? BM,pq
remains zero. Then the optimal synaptic weight is wij = ??
such that, similar as for the Willshaw model with maximal
threshold, a single inhibitory input, u˜i = 1 and wij = ??,
can silence the postsynaptic content neuron j.
In fact, for sufficiently sparse memory patterns with
Mpq ? ? but still Mp ? ? and Mq ? ? the synapse
usages M00 ? M(1 ? p)(1 ? q), M01 ? M(1 ? p)q,
and M10 ? Mp(1 ? q) will be close to their large mean
values, whereas the synapse usage M11 remains small and,
occasionally, even may assume zero. Thus, the synaptic
weights, up to an essentially constant offset, are dominated
by the infinitely negative weights due to the M11 = 0
events. Therefore, in such a regime, the Bayesian model
becomes equivalent to the Willshaw model (Fig. 1). Note
also that this regime offers novel functional interpretations
for strongly inhibitory circuits, for example, involving basket
or chandelier cells [55], [48].
VII. CONCLUSIONS
This paper develops the optimal neural associative mem-
ory based on Bayesian maximum-likelihood considerations
assuming local synaptic learning (section II) and indepen-
dent address attributes [56], [57]. In general, the resulting
optimal synaptic learning rule is non-linear and differs from
previously investigated linear learning models of the Hopfield
type [30], [58], [32], [33], [53], [34], [35], [36], [37], simple
non-linear learning models of the Willshaw type [24], [23],
[25], [59], [26], [28], [18], [29], and BCPNN-type models
employing suboptimal Bayesian heuristics [38], [39], [40],
[41], [42], [43], [6].
The previous models become optimal only in the asymp-
totic limit of many stored memories, M ??, depending on
the pattern activity parameters p and q: In the limit Mpq ?
? of moderately sparse and non-sparse memory patterns,
the optimal Bayesian model becomes equivalent to the linear
network employing the covariance rule and thus achieves
a maximal network storage capacity of C = 0.72 bits per
synapse. In the limit Mpq ?? of very sparse patterns, the
optimal Bayesian model becomes equivalent to the Willshaw
network and thus achieves C = 0.69 bits per synapse. For a
large range of intermediate sparseness and finite network size
the optimal Bayesian model can perform significantly better
than the previous models. These theoretical results have been
verified by numerical experiments illustrated by Fig. 1 (see
also a related technical report [44], [47]).
Note that this paper shows that even the best method of
Hebbian-type synaptic plasticity (modifying synaptic weights
based on presynaptic and postsynaptic activity) can store
significantly less than one bit per real-valued synapse. By
contrast, we have shown elsewhere that storing memories in
similar network models by structural plasticity (meaning the
elimination and generation of synapses [60]) yields a much
higher diverging synaptic capacity where the information that
a binary synapse can store scales with the logarithm of the
network size n [61], [62], [63], [64], [29], [65].
ACKNOWLEDGMENT
The author is grateful to Julian Eggert, Marc-Oliver
Gewaltig, and Edgar Ko¨rner for providing the opportunity
to do this work at the Honda Research Institute. He is also
grateful to them and to Ursula Ko¨rner, Anders Lansner,
Gu¨nther Palm, and Friedrich Sommer for helpful discussions
and comments.
REFERENCES
[1] D. Hebb, The organization of behavior. A neuropsychological theory.
New York: Wiley, 1949.
[2] V. Braitenberg, “Cell assemblies in the cerebral cortex.” in Lecture
notes in biomathematics (21). Theoretical approaches to complex
systems., R. Heim and G. Palm, Eds. Berlin Heidelberg New York:
Springer-Verlag, 1978, pp. 171–188.
[3] G. Palm, Neural Assemblies. An Alternative Approach to Artificial
Intelligence. Berlin: Springer, 1982.
[4] ——, “Cell assemblies as a guideline for brain research.” Concepts in
Neuroscience, vol. 1, pp. 133–148, 1990.
[5] E. Fransen and A. Lansner, “A model of cortical associative memory
based on a horizontal network of connected columns.” Network:
Computation in Neural Systems, vol. 9, pp. 235–264, 1998.
[6] A. Lansner, “Associative memory models: from the cell-assembly
theory to biophysically detailed cortex simulations.” Trends in Neuro-
sciences, vol. 32(3), pp. 178–186, 2009.
[7] D. Marr, “Simple memory: a theory for archicortex.” Philosophical
Transactions of the Royal Society of London, Series B, vol. 262, pp.
24–81, 1971.
[8] E. Rolls, “A theory of hippocampal function in memory.” Hippocam-
pus, vol. 6, pp. 601–620, 1996.
[9] R. Bogacz, M. Brown, and C. Giraud-Carrier, “Model of familiarity
discrimination in the perirhinal cortex.” Journal of Computational
Neuroscience, vol. 10, pp. 5–23, 2001.
[10] D. Marr, “A theory of cerebellar cortex.” Journal of Physiology, vol.
202(2), pp. 437–470, 1969.
[11] J. Albus, “A theory of cerebellar function.” Mathematical Biosciences,
vol. 10, pp. 25–61, 1971.
[12] P. Kanerva, Sparse Distributed Memory. Cambridge, MA: MIT Press,
1988.
[13] G. Laurent, “Olfactory network dynamics and the coding of multidi-
mensional signals.” Nature Reviews Neuroscience, vol. 3, pp. 884–895,
2002.
[14] T. Kohonen, Associative memory: a system theoretic approach. Berlin:
Springer, 1977.
[15] H. Bentz, M. Hagstroem, and G. Palm, “Information storage and
effective data retrieval in sparse matrices.” Neural Networks, vol. 2,
pp. 289–293, 1989.
[16] R. Prager and F. Fallside, “The modified Kanerva model for automatic
speech recognition.” Computer Speech and Language, vol. 3, pp. 61–
81, 1989.
[17] D. Greene, M. Parnas, and F. Yao, “Multi-index hashing for infor-
mation retrieval.” Proceedings of the 35th Annual Symposium on
Foundations of Computer Science, pp. 722–731, 1994.
[18] A. Knoblauch, “Neural associative memory for brain modeling and
information retrieval.” Information Processing Letters, vol. 95, pp.
537–544, 2005.
[19] X. Mu, M. Artiklar, P. Watta, and M. Hassoun, “An RCE-based
associative memory with application to human face recognition.”
Neural Processing Letters, vol. 23, pp. 257–271, 2006.
[20] A. Wichert, “Cell assemblies for diagnostic problem-solving.” Neuro-
computing, vol. 69, pp. 810–824, 2006.
[21] M. Rehn and F. Sommer, “Storing and restoring visual input with
collaborative rank coding and associative memory.” Neurocomputing,
vol. 69, pp. 1219–1223, 2006.
[22] G. Palm, “Memory capacities of local rules for synaptic modification.
A comparative review.” Concepts in Neuroscience, vol. 2, pp. 97–128,
1991.
[23] K. Steinbuch, “Die Lernmatrix.” Kybernetik, vol. 1, pp. 36–45, 1961.
[24] D. Willshaw, O. Buneman, and H. Longuet-Higgins, “Non-holographic
associative memory.” Nature, vol. 222, pp. 960–962, 1969.
[25] G. Palm, “On associative memories.” Biological Cybernetics, vol. 36,
pp. 19–31, 1980.
[26] J.-P. Nadal, “Associative memory: on the (puzzling) sparse coding
limit.” J.Phys. A: Math. Gen., vol. 24, pp. 1093–1101, 1991.
[27] F. Sommer and P. Dayan, “Bayesian retrieval in associative memories
with storage errors.” IEEE Transactions on Neural Networks, vol. 9,
pp. 705–713, 1998.
[28] F. Sommer and G. Palm, “Improved bidirectional retrieval of sparse
patterns stored by Hebbian learning.” Neural Networks, vol. 12, pp.
281–297, 1999.
[29] A. Knoblauch, G. Palm, and F. Sommer, “Memory capacities for
synaptic and structural plasticity.” Neural Computation, vol. 22(2), pp.
289–341, 2010.
[30] J. Hopfield, “Neural networks and physical systems with emergent col-
lective computational abilities.” Proceedings of the National Academy
of Science, USA, vol. 79, pp. 2554–2558, 1982.
[31] G. Palm, “On the asymptotic information storage capacity of neural
networks.” in Neural Computers, ser. NATO ASI Series F41, R. Eck-
miller and C. von der Malsburg, Eds. Berlin, Heidelberg, New York:
Springer Verlag, 1988, pp. 271–280.
[32] M. Tsodyks and M. Feigel’man, “The enhanced storage capacity in
neural networks with low activity level.” Europhysics Letters, vol. 6,
pp. 101–105, 1988.
[33] P. Dayan and D. Willshaw, “Optimising synaptic learning rules in
linear associative memory.” Biological Cybernetics, vol. 65, pp. 253–
265, 1991.
[34] P. Dayan and T. Sejnowski, “The variance of covariance rules for
associative matrix memories and reinforcement learning.” Neural
Computation, vol. 5, pp. 205–209, 1993.
[35] G. Palm and F. Sommer, “Associative data storage and retrieval in
neural nets.” in Models of Neural Networks III, E. Domany, J. van
Hemmen, and K. Schulten, Eds. New York: Springer-Verlag, 1996,
pp. 79–118.
[36] G. Chechik, I. Meilijson, and E. Ruppin, “Effective neuronal learning
with ineffective hebbian learning rules.” Neural Computation, vol. 13,
pp. 817–840, 2001.
[37] D. Sterratt and D. Willshaw, “Inhomogeneities in heteroassociative
memories with linear learning rules.” Neural Computation, vol. 20,
pp. 311–344, 2008.
[38] A. Lansner and O. Ekeberg, “An associative network solving the ”4-
bit adder problem”.” in Proceedings of the IEEE First International
Conference on Neural Networks, M. Caudill and C. Butler, Eds., San
Diego, CA, 1987, pp. II–549.
[39] ——, “A one-layer feedback artificial neural network with a Bayesian
learning rule.” International Journal of Neural Systems, vol. 1(1), pp.
77–87, 1989.
[40] I. Kononenko, “Bayesian neural networks.” Biological Cybernetics,
vol. 61(5), pp. 361–370, 1989.
[41] ——, “On Bayesian neural networks.” Informatica (Slovenia), vol.
18(2), pp. 183–195, 1994.
[42] A. Lansner and A. Holst, “A higher order Bayesian neural network
with spiking units.” International Journal of Neural Systems, vol. 7(2),
pp. 115–128, 1996.
[43] A. Sandberg, A. Lansner, K. Petersson, and O. Ekeberg, “A palimpsest
memory based on an incremental Bayesian learning rule.” Neurocom-
puting, vol. 32-33, pp. 987–994, 2000.
[44] A. Knoblauch, “Neural associative networks with optimal bayesian
learning.” Honda Research Institute Europe GmbH, D-63073 Offen-
bach/Main, Germany, HRI-EU Report 09-02, May 2009.
[45] S. Waydo, A. Kraskov, R. Quiroga, I. Fried, and C. Koch, “Sparse
representation in the human medial temporal lobe.” Journal of Neu-
roscience, vol. 26(40), pp. 10 232–10 234, 2006.
[46] A. Knoblauch, “Comparison of the lansner/ekeberg rule to optimal
bayesian learning in neural associative memory.” Honda Research
Institute Europe GmbH, D-63073 Offenbach/Main, Germany, HRI-EU
Report 10-06, April 2010.
[47] ——, “Neural associative memory with optimal bayesian learning.”
submitted, pp. –, 2010.
[48] ——, “On the computational benefits of inhibitory neural associative
networks.” Honda Research Institute Europe GmbH, D-63073 Offen-
bach/Main, Germany, HRI-EU Report 07-05, May 2007.
[49] A. Knoblauch and G. Palm, “Pattern separation and synchronization
in spiking associative memories and visual areas.” Neural Networks,
vol. 14, pp. 763–780, 2001.
[50] T. Sejnowski, “Storing covariance with nonlinearly interacting neu-
rons.” Journal of Mathematical Biology, vol. 4, pp. 303–321, 1977.
[51] ——, “Statistical constraints on synaptic plasticity.” Journal of Theo-
retical Biology, vol. 69, pp. 385–389, 1977.
[52] D. Willshaw and P. Dayan, “Optimal plasticity in matrix memories:
what goes up must come down.” Neural Computation, vol. 2, pp. 85–
93, 1990.
[53] G. Palm and F. Sommer, “Information capacity in recurrent
McCulloch-Pitts networks with sparsely coded memory states.” Net-
work, vol. 3, pp. 177–186, 1992.
[54] B. Graham and D. Willshaw, “Improving recall from an associative
memory.” Biological Cybernetics, vol. 72, pp. 337–346, 1995.
[55] H. Markram, M. Toledo-Rodriguez, Y. Wang, A. Gupta, G. Silberberg,
and C. Wu, “Interneurons of the neocortical inhibitory system.” Nature
Reviews Neuroscience, vol. 5, pp. 793–807, 2004.
[56] H. Zhang, “The optimality of naive bayes.” in Proceedings of the 17th
Florida Artificial Intelligence Research Society Conference, V. Barr
and Z. Markov, Eds. AAAI Press, 2004, pp. 562–567.
[57] P. Domingos and M. Pazzani, “On the optimality of the simple
Bayesian classifier under zero-one loss.” Machine Learning, vol. 29,
pp. 103–130, 1997.
[58] G. Palm, “Local synaptic rules with maximal information storage
capacity.” in Neural and synergetic computers, ser. Springer Series in
Synergetics, H. Haken, Ed. Berlin, Heidelberg, New York: Springer
Verlag, 1988, vol. 42, pp. 100–110.
[59] D. Golomb, N. Rubin, and H. Sompolinsky, “Willshaw model: Asso-
ciative memory with sparse coding and low firing rates.” Phys. Rev.
A, vol. 41, pp. 1843–1854, 1990.
[60] A. Holtmaat and K. Svoboda, “Experience-dependent structural synap-
tic plasticity in the mammalian brain.” Nature Reviews Neuroscience,
vol. 10, pp. 647–658, 2009.
[61] A. Knoblauch, “Synchronization and pattern separation in spiking
associative memory and visual cortical areas.” PhD thesis, Department
of Neural Information Processing, University of Ulm, Germany, 2003.
[62] ——, “On compressing the memory structures of binary neural asso-
ciative networks,” Honda Research Institute Europe GmbH, D-63073
Offenbach/Main, Germany, HRI-EU Report 06-02, April 2006.
[63] ——, “Neural associative memory and the Willshaw-Palm probability
distribution.” SIAM Journal on Applied Mathematics, vol. 69(1), pp.
169–196, 2008.
[64] ——, “The role of structural plasticity and synaptic consolidation for
memory and amnesia in a model of cortico-hippocampal interplay.”
in Connectionist Models of Behavior and Cognition II: Proceedings
of the 11th Neural Computation and Psychology Workshop, J. Mayor,
N. Ruh, and K. Plunkett, Eds. Singapore: World Scientific Publishing,
2009, pp. 79–90.
[65] ——, “Zip nets: Efficient associative computation with binary
synapses.” in Proceedings of the International Joint Conference on
Neural Networks (IJCNN 2010), 2010.
