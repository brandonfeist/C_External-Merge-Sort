A Conceptual Infrastructure for Knowledge Discovery Process with Granularity 
 
Ding Pan 
Center for Business Intelligence Research, School of Management  
Jinan University, Guangzhou, China 
tpanding@jnu.edu.cn 
 
 
Abstract—Knowledge discovery has received more and more 
attention from the business community for the last few years. 
One of the most important and challenging problems in it is 
the definition of discovery process model, which are well 
understood, efficiency, and quality of outcome. A conceptual 
infrastructure for knowledge discovery process is proposed 
with business understanding, model selection and domain 
knowledge integration in evolving database environment. The 
corresponding layered architecture creates an automatable 
process and communication mechanism to incorporate known 
knowledge and data granules into discovery process, through 
ontology service facility. A higher order mining method 
embedded in the process is proposed, to achieve monitoring 
and identifying changes statically and tracing trends 
dynamically. Finally, implementation techniques of basic data 
structure and simplified mining algorithms are discussed. 
Keywords-discovery process; conceptual infrastructure; 
higher order mining 
I.  INTRODUCTION 
The data mining (DM) and knowledge discovery in 
databases (KDD) movements have been based on the fact 
that the true value is not in storing the data, but rather in our 
ability to extract useful reports and to find interesting trends 
and correlations. The set of DM processes used to extract 
and verify patterns in data is the core of the knowledge 
discovery process. These processes comprise many steps, 
which involve data selection, data preprocessing, data 
transformation, DM, and interpretation and evaluation of 
patterns. During the last a few years, most research in DM 
focuses on the development of new algorithms or 
improvement in the speed or the accuracy of the existing 
ones [1]. Relatively little has been published about 
theoretical frameworks of knowledge discovery.  
In 1996, Fayyad et al. presented a process model that 
resulted from interactions between researchers and industrial 
data analysts [2]. The model did not address particular DM 
techniques, but rather provided support for the complicated 
and highly iterative process of knowledge generation. Since 
then, several different process models have been developed. 
These process models consist of multiple steps executed in a 
sequence, which often includes loops and iterations [3,4].  
The CRISP-DM (CRoss Industry Standard Process for 
Data Mining) process model includes six steps: business 
understanding, data understanding, data preparation, 
modeling, evaluation and deployment [3]. In general, the 
CRISP-DM is the most suitable for novice data miners 
working on industrial projects, due to the intuitive, industry-
applications-focused description, and has been already 
acknowledged and relatively widely used in both research 
and industrial communities. One of the main limitations of 
the CRISP-DM life-cycle representation is that it is 
essentially sequential and linear.  
Agrawal et al. showed an active data mining process, 
where the mining algorithm was applied to each of the 
partitioned data set and rules were induced [5]. Spiliopoulou 
et al. investigated the utility of inducting rules from results of 
other mining routines [6]. Several researches have 
concentrated on fusion of domain knowledge with DM 
system [7]. Wang et al. discussed some recent achievements 
in domain-oriented data-driven data mining (3DM) model 
[8]. However, little attention has been paid to establishment 
of a general infrastructure for supporting share of domain 
knowledge. 
In this paper we attempt to explore knowledge discovery 
process model to make data mining for evolving database 
environments, and to incorporate the domain knowledge and 
data granules with the KDD process. Our conceptual 
infrastructure for a data mining system is built upon three 
essential components: Ontology, KDD process model and 
Knowledge engineering. It also provides a higher order 
mining method to identify and trace changes in data or 
patterns. 
II. PROCESS MODEL AND INFRASTRUCTURE 
We present a model and an infrastructure for the KDD 
process, which is based on the concept of active, continuous 
mining and designed to integrate known knowledge and 
granules in order to support automatic discovery process.  
A. Mining ontologies 
Ontology provides a framework for sharing conceptual 
models about known knowledge. A model that allows agents 
to deal with explicit, declaratively represented ontologies 
was described in the FIPA Ontology Service [9]. The known 
knowledge may be is shared and reused by utilizing ontology 
service. 
It is necessary to set up conceptual frameworks into 
which data and datasets, databases and known knowledge 
might be classified. The families of available models need to 
be classified, and the relation of model-families to data-types 
needs to be made clear. We need abstract models of data and 
their relations, and of models and their relations. It is by 
knowing the ontological structure of the data available, and 
2010 International Conference on Artificial Intelligence and Computational Intelligence
978-0-7695-4225-6/10 $26.00 © 2010 IEEE
DOI 10.1109/AICI.2010.260
99
the models that may be applied to it, that we can reasonably 
make a choice of the model to use. 
A necessary high-level structure to business and data 
exploration process is the trio of interrelated processes of 
business-understanding, data-understanding and the 
availability of a model-ontology. The data ontology includes 
specifically knowledge of data, data-types and data-source; 
the model-ontology includes a clear understanding of the 
data-types associated with various classes of model. 
To make use of data granules mechanism (or groups, 
classes, clusters of a universe) to support discovery process, 
it need create various levels of data, as well as inherent data 
structure, by partitioning data attributes into intervals. The 
different levels of granularities form data granule ontologies. 
B. Process model 
The model, called C-KDD (Cohesive KDD) model, is 
shown in Fig. 1. It consists of four stages: planning, session 
mining, merge mining, and post-processing.  
During planning stage, the KDD process begins with 
business understanding, including business-aims and 
business-logic. Through interactive exploration and 
experimentation, discovery goals, business data, and 
subsequent processes are identified and the specification of 
discovery task schedule (TS) is generated. The ontological 
domain knowledge is used to eliminate irrelevant attributes, 
update the vague prior business factors, inferring other 
abstract attributes, etc. Moreover, the set of valid data 
attributes, process steps, and algorithms are composed in 
suggested order based on the user’s desiderata, by a data 
mining ontology.  
The session mining stage performs select-transfer-
premining and achieves partial data mining. It places 
emphasis on local and static rules induction, and executes 
induction on incremental data at regular intervals, e.g. 
month. As the functions are already specified in the TS, they 
are periodically repeated on incremental data as per the 
frequency or trigger condition, and whose outcome forms a 
rule bin (RB). Ontological knowledge is used to assist in 
determining the selected features, parameters, etc.  
The merge mining is initiated by mining queries or a 
trigger event. The query contents are listed in consultation 
with the TS; user can commit them, according to his 
requests. A trigger event is occurred as the causes of time or 
rule rising. It places emphasis on overall and dynamic rules 
discovery, in the interaction paradigm, the rules are merged 
and refined from several RBs. The parameters and constrains 
are supplemented with ontological knowledge. 
The post-processing stage begins with matching 
discovered rules and known knowledge, filters useless ones, 
then classified and ranked automatically interesting results 
according to interestingness. When a critical point threshold 
is reached, an alert will be triggered. Meanwhile, the user 
can review and confirm these findings. It would also 
integrate new interesting insights with the known 
knowledge, to perform knowledge evolution and 
presentation. Then, it forms a close-loop solution that helps 
to maintain the continuous knowledge discovery process. 
 
Figure 1.  C-KDD Process Model 
When unable to satisfy intelligence application or rule 
review, the process flow goes back to the planning stage to 
re-explore data.  
C. Higher order mining 
Higher order mining is a knowledge discovery process 
concerned with mining over previously induced patterns or 
models [10]. Its methods and techniques are applied in the C-
KDD process model.  
In planning stage, data attributes concerned with data 
mining are partitioned into intervals, called bags. Here, we 
consider only equi-width bags (the interval size of each bag 
is the same). For example, Age=20, Age=21, and Age=22 
form a bag (20 ? Age ? 22). When a quantitative attribute is 
partitioned into bags, the bags are mapped to consecutive 
integers, called BIDs; when attribute values are mapped to 
bags their value is replaced with the corresponding BID for 
that bag. For categorical attributes we also map the attribute 
values to a set of consecutive integers and use these BIDs in 
place of the categorical values. In the same way, these basis 
BID may be mapped to upper BIDs. So, a BID hierarchy 
defines a sequence of mappings from a set of low-level BIDs 
to higher, larger extent BIDs. It is useful to rule clustering 
and classification later stages. 
In the session mining stage, Induction can be separated 
into two steps, the first involves the generation of rules upon 
primary data, with the other performing rule clustering and 
classifying. Rule clustering is the combination of adjacent 
attributes values, or adjacent bags of attribute values in rule. 
For example, clustering (Age=20) and (Age=21) results in 
Session Mining 
Planning 
Schedule 
Generation 
Data 
Exploration
Business 
Understanding
Selection 
Induction 
Transform 
Merge Mining
Rule 
Extraction 
Mining 
Query 
Postprocessing 
Human-Machine 
Processing 
 Machine Processing 
Rule Review Filter & 
Alert 
Evolution & 
Deployment 
Grouping 
100
(20 ? Age ? 21). A clustered rule is an expression of the form 
Xc?Yc. Xc and Yc are items of the form (Attribute = value) or 
(bagi ? Attribute ? bagi+l), where bagi denotes the lower 
bound for values in the ith bag. Clustered rules will always 
have a support and confidence of at least that of the 
minimum threshold levels. 
Fig. 2 shows a unifying framework that uses heuristics 
for generating the clustered association rules, which we now 
describe. 
While the primary data is read, the minimum support is 
used along with the minimum confidence to generate a large 
number of association rules. Clustering rules engine forms 
clusters of adjacent association rules, supporting by the BID 
hierarchy, according to grouping criteria. These clustered 
association rules are then tested for their accuracy (by the 
verifier) against a sample of tuples from the primary data 
database. The accuracy is supplied to a heuristic optimizer 
that adjusts the minimum support threshold and/or the 
minimum confidence threshold, and restarts the mining 
process at the clustering rules engine. These heuristic 
adjustments continue until the verifier detects no significant 
improvement in the resulting clustered association rules, or 
the verifier determines that the budgeted time has expired. 
Once the clustered association rules are discovered for a 
particular level of BID, we then go up to try upper BID. 
Finally, the clustered rule structures are achieved. 
Associative classification uses association rules to build a 
classifier. The association rules must have the consequent 
bound to the class label, hence are considered associative 
classification rules (ACRs). Once a set of ACRs are 
gathered, they are ranked, with the highest ranked ACRs 
used to create classifiers. 
The framework of generating associative classification 
rules is similar to Fig. 2. Firstly, association rules must be 
pruned and ranked during the ACR generation process to 
reduce the volume of rules generated, and to allow for an 
informed selection procedure to build the classifiers from the 
most highly ranked rules. One procedure to evaluate ACRs is 
to rank them in the order of confidence, support, and 
precedence, respectively. Then, given a prioritized set of 
ACRs, a classifier is created based on the set of rules that 
cover the training set. The classifier is iteratively built where, 
if a rule covers some set of examples in the training set, it is 
added and those pertinent examples are removed from 
further rule evaluations. Each rule, therefore, only covers a 
particular set of examples, and hence only rules that increase 
the accuracy of the classifier are included. The default class 
is determined as the rule which covers the largest number of 
examples. 
In the merge mining stage, higher order mining focuses 
on frameworks that detect changes upon data that come from 
a non-stationary distribution, in pattern management suites 
and, obviously, in temporal mining methods. Not only the 
data change but their schema also changes as well. 
We can detect changes in patterns to identify and 
quantify the differences between patterns drawn from 
different snapshots of the dataset, namely RBs. We also trace 
the trends in association rules or clusters from a sequence of 
RBs. 
 
Figure 2.  Framework of Clustering Rules 
D. Knowledge engineering 
The processes of model interpretation, validation and 
evaluation have rightly been given places of importance in 
knowledge discovery through data exploration and mining, 
and in particular in the process model, to ensure the quality 
of the outcome of KDD. The problems about 
meaningfulness, significance and importance are a crucial 
part of trained DM-model understanding. These problems 
about what we know from DM, and indeed what cannot be 
known, are essentially part of an as developing ‘Knowledge 
engineering’ for KDD. 
Without doubt, a formal KDD theory of knowledge 
needs to be developed. A KDD epistemology will have close 
links with the data and model ontologies discussed above, 
and also with representations and storage concerning the 
prior knowledge and communication. 
For C-KDD process model, a hierarchy of processes 
operating on the constructs at each level of “Information 
Hierarchy” involving data, information, knowledge, and 
sometimes wisdom, may also be defined, and is illustrated in 
[11]. Corresponding to the data-level of the hierarchy, 
operations concerned with manipulations and storage of 
basic data resources we term “technical”. Information is 
derived by operations performed on data using the tools and 
techniques of DM. We refer to the processes at this level as 
“contextual”. Then, derived and discovered information 
becomes knowledge only when it is subjected to processes 
which put the information in the context of the business 
problems or opportunities. Hence, the processes which deal 
with such “understanding” of information/knowledge are 
termed “tactical”. Finally, the processes, which put the 
integrated knowledge in the context of the whole enterprise 
and its aims, are referred as “strategic”. 
A parameter estimation method, used to explain the 
interestingness and evolutional regularity of the rule, has 
been discussed in [12]. 
Association rule 
mining 
Primary data 
Min. Supp. 
Min. Conf. 
Association rules 
Higher order mining 
 
Clustering rules 
engine 
Grouping 
criteria 
Verifier 
 
Cluster analysis  
Test data 
Clustered association rules 
Heuristic 
Optimizer 
Generate clustered 
rule structure 
BID hierarchy 
101
III. LAYERED ARCHITECTURE 
In implementing the model, the machine processing steps 
require developing autonomous components. Each 
component is an agent, which obtains the known knowledge 
through ontology service. The discussion of six components 
is as below. 
1. Data selection (Csel) — to select data from data 
sources. It decides dynamically the target subset, specified in 
terms of the location and time span for heterogeneity.  
2. Data transformation (Ctra) — to manipulate data to 
satisfy Cind needs.  
3. Data induction (Cind) — to execute mining algorithm 
to induce partial and raw rules from incremental data. It 
consists of a set of functions for each algorithm. 
4. Rule grouping (Cgrp) — to execute clustering and 
classifying algorithm to group static and raw rules. It consists 
of a set of functions for clustering, classifying, verification, 
and  optimization algorithm. 
5. Rule extraction (Cext) — to achieve overall induction 
and refinement, only dealing with RBs. It is important to 
refine the premined results and perform higher order 
explanation, particularly those describing changes across the 
session.  
6. Rule filters (Cflt) — to verify mining results. The result 
that contradicts known knowledge may be considered being 
interesting, because they can occur due to new knowledge or 
un-codified simply general knowledge. 
7. Knowledge evolution (Cevl) — to incorporate newly 
discovered knowledge into the known knowledge. This 
derives the necessity to support the ontology evolution, 
involved identification, mapping, and merging etc. 
The motivation for the architecture is to synthesize the 
complete KDD process and to provide a continuous and 
convenient environment, as shown in Fig. 2. 
The top layer, the front-end user layer, composes the end 
user interface. It provides functionality of human machine 
processing, and implements the user interface for C-KDD 
model. The TSs, arranging each mining process and 
interrelated data, are stored in schedule pool (MSP) and 
provide monitoring information through ontology services. 
The Query Broker determines what TS should be contacted 
to meet a high-level user request. Rule Manager supports the 
review process for discovered results. Granularity Processor 
displays and controls the data granule transformation. 
The middle layer, the autonomous component layer, is 
implemental in machine processing. The autonomous 
components and the RBs are located in this layer. The 
components populated the processes, are managed by 
corresponding to each TS entry. The RBs should be 
restrictively shared in order to preserve privacy.  
Data mining in evolving domains presents several 
challenges from distributed, dynamic data sources. The 
storage management layer organizes the data sources by 
maintaining the ontology and metadata about the concept, 
term, and physical attribute of each one. It implements 
ontology service-based information integration. Granularity 
Manager organizes the data into various granularities 
according information induction requirement. Resource 
Broker determines what data set should be contacted to meet 
a request specified in TS, by searching through ontologies 
for concept and metadata for physical location about data 
that match the request criteria.  
All the three layers manipulate and share knowledge and 
metadata stored in repository segment. The segment 
maintains structural and semantic information about each 
data source, recording the relationship between attributes of 
the data sources with terms from business domain, 
computing contextual information gleaned from these 
linkages and other source-related information. Ontology 
Agent provides mediating knowledge base and ontology 
services, to support access to their services in a unique way 
that is well integrated with the FIPA Agent mechanism [9]. 
 
Figure 3.  Architecture of C-KDD 
IV. IMPLEMENTATION 
In order to experiment and evaluate the feasibility of the 
design, we constructed our architecture in a prototype. We 
concentrate on meta-model of RB and simplified mining 
algorithms invoking ontology services. 
In the C-KDD model, the mining components are mainly 
three parts: data induction, rule grouping, and rule extraction, 
which use the rule bin (RB) to preserve the former results. 
Accordingly, design of data structure for RB is a 
significantly important activity in the implementation.  
The meta-model of RB consists of four classes stored in 
metadata repository. A leading class identifies each RB class 
table. An attribute class defines the attributes’ properties of 
each RB. A method class stores its data operations and 
condition. A constraint class describes each event 
occurrence. They are used to provide continuous and active 
data mining. The primary attributes of the classes are as 
follows: 
Leading (ClassName, ParentName, Operation); 
Attribute (AttributesName, ClassName, MethodName, 
AttributesType, AssoAttribute); 
…
Granularity 
Manager
Resource Broker Partitioner 
TS 
Generator
User Manager 
Query 
Broker 
Query 
Processor
Front-end User 
Metadata 
Repository 
Storage Management DB1 DB2 DBn 
O
ntology A
gent 
Knowledge 
Base 
Autonomous Component 
Csel Ctra Cind Cext Cflt Cevl 
Function 
Library RB1 
… RBm 
Data 
Explorer
MSP 
Rule 
Manager 
Component Manager 
Repository 
Repository 
Manager Granularity 
Processor 
Cgrp 
102
Method (MethodName, ClassName, Parameters, 
MethodType, Condition, Action); 
Constraint (ConstraintName, ClassName, MethodName, 
Parameters, Event, Timing). 
Table I is an example of the RB table for association rule, 
whose attributes are defined by the above attribute class, 
given a data table Source (A, B, C, D). The record counts of 
attribute for computing support and confidence of 
association rule are stored in this RB table, particularly ID of 
known knowledge is stored in field KK, according to year 
session. The details of rule extraction algorithm using 
ontology services (OS) are shown as below. It is shown to 
have linear scale-up. 
TABLE I.  RB TABLE EXAMPLE 
Data RuleID Ant_count Rule_count Rule BID KK 
2005 R1 Count(B) Count(A, B) B ? A 91 R1_ID
2005 R2 Count(C) Count(A, C) C ? A 81 R2_ID
… … … … …  … 
2006 R1 Count(B) Count(A, B) B ? A 92 R1_ID
… … … … …  … 
RuleExtractionAR (TSid,Session,minsup,minconf,OutRule) 
Begin                //obtain possible interesting rules 
RB_table = OS_SelectRBTable (TSid);     
//obtain RB table using OS 
Total_Rec = OS_CountRec (TSid);     //total of records 
For each No_Session in Session do 
Begin 
No_Rule = Count_Rule (RB_table, No_Session);  
//count number of rules in the session 
For i = 1 to No_Rule do 
Begin 
Support (i) = RB_table.Rule_count (i) / Total_Rec; 
Confidence (i) = RB_table.Rule_count (i)  
// RB_table.Ant_count (i); 
If Support(i)>=minsup and Confidence(i)>= 
minconf  then 
If OS_IsInteresting (TSid,RB_table.KK(i)) 
then     //verify interesting using OS 
Output (OutRule, Support(i), Confidence(i)) 
  //output interesting rule 
End 
End 
End 
The change trend of rules (second order rule) may also be 
extracted from OutRule. The rule is a R = (ruleClause, 
statistics), ruleClause has the form: A?B, the statistics may 
be the support, confidence, and accuracy. A higher order rule 
is defined as a temporal sequence of R. A sequence of 
statistics for the same rule R in each session forms a time 
series; therefore time series analysis can be operated on it. 
We will not discuss here in details for tracing the trends in 
rules. 
V. CONCLUSION 
Implementation of a continuous KDD process has been 
attracting more and more interests from various industries. 
We have proposed a conceptual infrastructure for the KDD 
process model that endeavors to separate autonomous mining 
sub-processes from a full KDD process, and integrate known 
knowledge using ontology services. The architecture utilizes 
ontology service to provide an inherent mechanism for share 
known knowledge and data granules hierarchy, and 
constitute a knowledge discovery platform extending support 
for data independence and granularity transformation. We 
have also proposed a higher order mining method embedded 
in discovery process, to achieve monitoring and identifying 
changes statically and tracing trends dynamically. We have 
also discussed that data attributes concerned with data 
mining were partitioned into bags, and formed bags 
hierarchy to support clustering and classifying rules.  
ACKNOWLEDGMENT 
This work is supported by the National Natural Science 
Foundation of China under grant No. 70771044 and 
70872020. 
REFERENCES 
[1] X. Wu, P. S. Yu, and G. Piatetsky-Shapiro, “Data mining: how 
research meets practical development,” Knowledge and Information 
Systems, vol. 5, no.2, 2003, pp. 248-261. 
[2] U. Fayyad, G. Piatesky-Shapiro and P. Smyth, Advances in 
Knowledge Discovery and Data Mining, California: AAAI Press, 
1996. 
[3] CRISP-DM, CRoss Industry Standard Process for Data Mining, 
http://www.crisp-dm.org, 2003. 
[4] K. Cios and L. Kurgan, “Trends in data mining and knowledge 
discovery,” Advanced Techniques in Knowledge Discovery and Data 
Mining, London: Springer, 2005, pp. 1-26. 
[5] R. Agrawal and G. Psaila, “Active data mining,” Proc. of the 
KDD’95, AAAI Press, 1995, pp. 3-8.  
[6] M. Spiliopoulou and  J. Roddick, “Higher order mining,” Proc. of 2nd 
Data Mining Methods and Databases, WIT Press, 2000, pp. 309-320. 
[7] P. Domingos, “Toward knowledge-rich data mining,” Data Mining 
and  Knowledge Discovery, vol.15, no.1, 2007, pp.21-28. 
[8] G. Wang and Y. Wang, “3DM: domain-oriented data-driven data 
mining,” Fundamenta Informaticae, vol. 90, 2009, pp. 395-426. 
[9] FIPA, FIPA Ontology Service Specification. FIPA XC00086D, 2001. 
[10] J. F. Roddick, M. Spiliopoulou, D. Lister, and A. Ceglar, “Higher 
order mining,” SIGKDD, vol.10, no. 1, 2008, pp. 5-18. 
[11] D. Pan, “An integrative framework for continuous knowledge 
discovery,” Journal of Convergence Information Technology, vol. 5, 
no. 3, 2010, pp. 46-53.  
[12] D. Pan and Y. Pan, “The estimation of rule measure based on 
principle of information diffusion,” Proc. of the 5th ICMLC 2006, 
IEEE Press, 2006, pp. 1025-1029. 
 
103
