 Association Rule Mining Analysis on Concept 
Lattice and Rough Set 
 
Dexing Wang, Lixia Cui,Yanhua Wang 
College of Information Technology 
Shanghai Ocean University 
Shanghai, 201306, China 
e-mail: wangdexing198706@yahoo.com.cn 
Jian Zhang 
College of Information Technology 
Shanghai Ocean University 
Shanghai, 201306, China 
 
 
Abstract—Rough set is used extensively in the field of date mining, 
which is also an useful tool for mining rules from decision tables, 
then solves the bottleneck of knowledge acquisition effectively, 
while concept lattice represents knowledge shown with 
hierarchical structure on the Hasse diagram, thus it is properly 
applied to the description of mining association rules in databases. 
Association rules mining based on rough set and concept lattice 
can be shown that they perform the same function. However the 
latter represents the association rules more visual, vivid and 
concise than the former, therefore, association rule mining on 
concept lattice is applied to find common relations among 
different knowledge clusters in computer education. 
Keywords: association rule;concept lattice; rough set;data mining 
I. INTRODUCTION 
Association rule[1,2] mining is an important data mining 
method for determining consumer purchasing patterns in 
transaction databases. Nowadays, many applications have 
used to discover useful information. 
Rough set is used extensively in the field of date mining 
or knowledge discovery in databases, which is firstly proposed 
by Z. Pawlak[3], nowadays has become an useful tool for 
mining rules from decision tables, then solves the bottleneck 
of knowledge acquisition effectively, while Galois (Concept) 
Lattice was presented by R. Wille[4], reflects entity-attribute 
relationships between objects, represents knowledge shown 
with hierarchical structure on the Hasse diagram, thus it is 
properly applied to the description of mining association rules 
in databases, Literature[5,6,7] has testified that what rough set 
and concept lattice represent is isomorphic, despite their 
algebraic structures are not different. 
As we known, association rules mining on rough set is 
predominant on data mining, which should deal with multi-
step attribute reduction that deleting the reduplicative rows or 
columns. However, attribute reduction[8-14] is not just only 
one set, usually, finding out all attribute reductions or the least 
attribute reduction has been testified that it is NP-Hard 
problems, in the meantime, providing that all attribute 
reductions or the least attribute reduction have been found, 
association rule mining from the attribute reductions is a very 
difficult work because how to generate and express association 
rules are not very easy tasks.  
In this paper, the contribute is that association rule 
mining based on concept lattice can perform the same function 
as that of on rough set, but the latter represents the rules more 
visual, vivid and concise than the former. 
The paper is organized as follows: In section II, we review 
the preliminaries of association rules, basic notions of concept 
lattice, then discuss the generation algorithm of concept lattice 
in section III, association rules mining on concept lattice is 
described in section IV, in addition, preliminaries of mining 
rules from decision tables on rough set are reviewed in section 
V, association rule mining on concept lattice and rough set are 
illustrated and the comparison between them is made in section 
VI, finally, we reach the conclusion in section VII. 
II. BASIC NOTIONS OF ASSOCIATION RULE AND CONCEPT 
LATTICE 
A. Basic notions of association rule 
Mining association rule is an important branch of data 
mining, which describes potential relations among data items 
(attribute, variant) in databases, The well-known Apriori 
algorithm was proposed by R. Agrawal et.al., in 1993. Mining 
association rules can be stated as follows: 
   Let I={i1, i2, … im} be a set of items. Let D, the task-
relevant data, be a set of transactions, where each transaction 
T is a set of items such that T?I. The quantities of items 
bought are not considered. Each transaction is assigned an 
identifier, called TID. Let A be a set of items, A transaction T 
is said to contain A if and only if A?T. An association rule is 
an implication of the form A?B, where A?I, B?I, and 
A?B=?. The rule A?B holds in the transaction set D with 
support s, where s is the percentage of transactions in D that 
contain A?B (i.e., both A and B). This is taken to be the 
probability, P(A?B). The rule A?B has confidence c in the 
transaction set D if c is the percentage of transactions in D 
containing A that also contain B. This is taken to be the 
conditional probability, P(B|A).  
That is, Support (A?B)=P(A?B)=s; 
Confidence(A?B)=P(B|A)=Support(A?B)/Support (A)=c. 
Mining association rules should be mining strong 
association rules that satisfy the user-specified both minimum 
support threshold and confidence threshold. 
This paper is supported by innovation program of shanghai municipal 
education commission under grant No. 08YZ120 
2010 Second International Workshop on Education Technology and Computer Science
978-0-7695-3987-4/10 $26.00 © 2010 IEEE
DOI 10.1109/ETCS.2010.545
432
B. Basic notions of concept lattice  
A context is defined as a triple C=(O,D,R?, where O is a 
set of objects, D is a set of attributes, and R is the binary 
relation between O and D, according to the inclusion relations 
between extents and intents, there is an unique ordered set that 
describes the structure of inherent lattice, which defines natural 
groupings and relationship descriptions between the objects 
and their attributes in the context, this structure is also known 
as (Galois) Concept Lattice[15](for short, CL). 
Definition 2.1: Each pair such as (A,B) derived from the 
context is called a concept, where A?P(O), B?P(D), and P(O), 
P(D) are the power sets of O and D respectively, A and B 
create a connection in terms of the following properties. 
A’={m?D|g?A, gRm}, B’={g?O|g?B, gRm}. 
Where A’=B, B’=A. A is called the extent of concept, 
and B is called the intent, denoted as Extent(C) and Intent (C) 
respectively. 
Definition 2.2: In a CL, partial order relation “< ” between 
concept C1=(A1, B1) and C2=(A2, B2) is defined as 
C1<C2?B2?B1, then C1 is the sub-concept (son) of C2, and C2 
is the sup-concept (father) of C1, that is, the relation between 
C2 and C1 is the relation as of father and son.  
If C1<C2, there exists no a concept C=(A,B), such that 
satisfies C1<C<C2, C1<C2 is called a direct-sub-concept-direct-
sup-concept-relation between C1 and C2, the Hasse diagram of 
concept lattice is generated according to the partial order 
relation: If C1<C2 is a direct-sub-concept-direct-sup-concept, 
there exists an edge from C2 to C1. 
From the definitions, the hierarchical relations between the 
concepts are shown to us, which can be used as an efficient 
tool for data analysis and knowledge acquisition. 
III. GENERATION ALGORITHM OF CONCEPT LATTICE 
From above the discussion, we can obtain the algorithm 
to generate concept lattice, which should be inserted initial 
concepts (corresponding to initial concepts) one by one, then 
adjust it, the algorithm is described as following:  
Algorithm Insertion (CL, C):  insert a concept into CL. 
x A concept C=(A,B) is inserted in the CL, if there   is      
an equivalent concept of C in the CL such that C1i 
(C1i?C), we merge the intent of C into the intent of 
C1i then finish the operation of insertion, that is, 
Intent(C1i):=Intent (C1i)?Intent(C); 
x Else, on the assumption that C0 is a direct-sup-concept 
of C, firstly insert C as a sub-concept of C0 into the CL, 
finish the operation in turn as follows: 
Each C1i{C1i<C0,C1i<C} connects C as a direct-
sup-concept of C1i in the same time, then we remove 
the connections between C1i and C0. 
                Insert the concept Ck?(Ak,Ik) that is generated 
when C intersects concepts into the CL, by the way, 
the judgment of the equivalent relations of sup-concept 
and sub-concept  in the algorithm can be implemented. 
 Attribute set A={a1,a2,… ,an} in the context (O,D,R), 
partial relation generated by attribute ai can be represented by 
{Oi1/vi1, Oi2/vi2,…,Oik/vik}, where Oij/vij is an object set Oij of 
attribute ai,  whose value is vij, therefore, we can gain an array 
of initial concepts CL(ai)= {(Ai1,ai=vi1), (Ai2,ai= 
vi2), …(Aik,ai=vik)}  by attribute ai.   
The generation algorithm of the CL:  Create-In-Attr(CL) 
is below: 
x Initialization: Generate CL with the complete concept 
(O,{}) and the null concept ({},all) as only two 
concepts.  
x for i:=1 to n do for Ci?C(ai) do Insertion(CL, Ci). 
IV. MINING ASSOCIATION RULES BASED ON CONCEPT 
LATTICE 
As for concept C=(A, B) in a CL, we can introduce 
support(B)=|A|/M of intent B to concept C, M is the sum of 
transactions in D, |A| is the extent cardinal of concept C. 
Definition 4.1: If Support (B) of concept C in a concept lattice 
is big than the support threshold, the concept C is called a 
frequent concept, where intent B of concept C is frequent 
items. 
Theorem 4.1: Frequent items is corresponding to the intents 
of concepts lattice each other.  
Theorem 4.2: If C=(A, B) is a concept in a concept lattice, the 
support of B is |A|/M, where |A| is the extent cardinal of C, M 
is the sum of transactions in D. If B is frequent itemsets whose 
support is bigger than the support threshold, B must be a 
group of equivalent intents of the concepts. 
According to the theorems mentioned above, association 
rules can be mined in a given concept lattice[16-18]. 
Theorem 4.3: ? If concepts C1=(A1,B1), C2=(A2,B2) satisfy 
C2?sup(C1), we can min association the rule: B2 ? B1-B2, its 
confidence is |A1| / |A2|, otherwise, B1-B2 ? B2, its confidence 
is 100%. 
? If concepts C1=(A1,B1)?C2=(A2,B2) do not satisfy 
C2?sup(C1), and there exists nonempty common maximum-
sub-concept C=(A,B), namely, ?C’=(A’,B’), which makes 
C?sup(C’) ? C1?sup(C’) ? C2?sup(C’) true, there are 
association rules between B1 and B2: B1?B2, B2? B1, their 
confidences are |A|/|A1|, A|/|A2| respectively[16]. 
V. PRELIMINARIES OF MINING ASSOCIATION RULES FROM 
DECISION TABLES ON ROUGH SET 
Decision table can be stated as follows: 
DT=(U,AT?{d},f), where U is a non-empty, finite set 
called the universe, U={u1,u2,u3,…un}, elements of U are 
called objects, At is a non-empty, infinite set of attributes, 
AT={c1,c2,c3,…cm}, which is a set of conditions, Vci  is the 
value of ci , i=1,2,?m,  d is the decision attribute, its value is 
Vd, f is information function f: U ? Va  (a? AT?d).  
The key step of mining rules algorithm[8] from decision 
table is to find out the dependence relationship between 
condition attributes AT and decision attributes d. 
Definition 5.1:  X?AT, if u,?v?U,?there exist?o?d(u)=d(v). 
433
Where “X(u)=X(v)” denote the a?X, satisfying 
a(u)=a(v), that is u and v have the same value of attribute a, 
“o” is the implication function, then decision attribute “d” is 
dependent of condition attribute subset X, denoted as X?d.  
   From the definitions, we can reach the conclusion: If we 
find out those condition attribute sets X, whose the object of 
decision attribute values d(u)=d(v) is the same object of the 
condition attribute X(u)=X(v), the certain or uncertain rules 
can be mined[9-13]. 
VI. COMPARISON OF MINING ASSOCIATION RULES BETWEEN 
CONCEPT LATTICE AND ROUGH SET 
There is a decision table as shown in the decision table 1, 
attribute a, b, c are condition attributes, where the condition 
attribute a is divided in two classes {a1(low), a2(high)}, as like 
as the condition attribute b and c, the condition attribute b is 
divided in three  classes {b1(black), b2(red), b3(yellow)}, and 
the condition attribute c is divided in two classes {c1(blue), 
c2(brown)}, at last, attribute d is the decision attribute, and 
decision attribute d is divided into two classes {d1,d2}. 
We can mine rules from the decision table 1 [19] using 
rough set, taking the condition attribute b as an example: 
 
U/b={X1, X2 , X3 }={{1, 2, 3},{4},{5, 6}} 
U/d={Y1, Y2}={1, 2, 3, 6}, {4, 5}}, according to the 
attribute reduction ,then the certain rules can be mined:  
If hair=”b1(black)” then the class=”d1”, that is: 
  b1(black)? d1 ;  
If hair=”b2(red)” then the class=”d2”,  that is:  
 b2(red) ? d2;  
the confidence of the two rules is 100%. 
X3?Y1={6}, X3?Y2={5}, the uncertain rules can be 
mined: 
If hair=”b3 (yellow)” then the class=”d1”, or the class=”d2”, 
that is: b3 (yellow) ? d1 , whose confidence of the two rules is 
50%. 
If hair=” b3 (yellow)” then the class=”d1”, or the 
class=”d2”, that is: b3 (yellow) ? d1, whose confidence of the 
two rules is 50%. 
According to the data in the decision table 1, the concept 
lattice can be built against the algorithm in section 3, and the 
Hasse diagram corresponding to the concept lattice has been 
drawn as Fig 1, on the concept lattice we can mine the 
association rules, just as Apriori algorithm[22] does.  
As for the same example above, the rule b1 (black)? d1 
has mined based on rough set, In Fig.1, the support of 
frequent itemsets is not divided by M=6 in order to be 
convenient for explanation, we can easily obtain frequent 
itemsets, for example, such as the support of frequent itemsets 
{b1d1} is the extent cardinal of concept({1,2,3},{b1d1} is 3, 
but frequent itemsets {b1} does not exist, there is an common 
maximum-sub-concept {b1d1},  whose support is 3, then the 
confidence of the rule b1(black)? d1  is 3/3=100%. 
However, there exist some uncertain rules, the rules b3 
(yellow) ? d1 mined in the decision table 1 based on rough set, 
the support of itemsets {b3} is the extent cardinal of 
concept({5,6},{b3}) is 2, but the itemsets {b3d1} does not exist, 
there is a common maximum-sub-concept({5},{a2b3c2d1}), 
whose support is the extent cardinal is 1, then the confidence 
of the rules b3 (yellow) ? d1 is 1/2=50%, Finally, it is tested 
that the results are identified. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
VII. CONCLUSIONS 
In this paper, from the comparison of association rules 
mining based on between rough set and concept lattice, we can 
obtain the conclusion that they perform the same function, 
however, in the course of association rules mining, the latter 
represents the rules more visual, vivid and concise than the 
former. The approach of mining association rules based on 
concept lattice presents association rules more visual, vivid 
and concise than that of on rough set. 
In view of finding out all attribute reductions or the least 
attribute reduction has been testified that it is NP-Hard 
problems, in the meantime, providing that all attribute 
reductions or the least attribute reduction have been found, 
mining association rules from the attribute reductions is a very 
difficult work because how to generate and express association 
rules are not very easy tasks, in addition. Since frequent 
itemsets and their supports are shown on the Hasse diagram of 
Tab. 1  Decision Table 
U a(High) b(hair) c(eyes) d(class)
1 a1(low) b1(black) c1(blue) d1 
2 a2(high) b1(black) c1(blue) d1 
3 a2(high) b1(black) c2(brown) d1 
4 a2(high) b2(red) c1(blue) d2 
5 a1(low) b3(yellow) c1(blue) d2 
6 a2(high) b3(yellow) c2(brown) d1 
Fig 1 The  Hasse diagram of concept lattice 
according to Tab. 1 
434
concept lattices, it is more convenient for us to mine 
association rules, thus improves mining association rules 
efficiency. 
ACKNOWLEDGMENT 
This paper is supported by innovation program of shanghai 
municipal education commission under grant No. 08YZ120. 
REFERENCES 
[1] Agrawal R,. Imielinski T,. Swami A,.Mining association rules between 
sets of items in large databases. In: Proc of ACM SIGMOD Conference 
on Management of Data, Washington DC,1993, pp.207-216.      
[2] Jitender Deogun, Vijay V. Raghavan, Heayri Sever. : Association 
mining and formal concept analysis. In: Anita,W.,San,C Eds. Proc. of 
the RSDMGRC’98. Duke: Elsevier Science Publishers, 1998.  
[3] Pawlak, Z.: Rough set, International Journal of   Computer and 
Information Sciences, 11, 1982,pp.341-356.  
[4] Wille R., Restructuring lattice theory: An approach based on hierarchies 
on concepts, in Ordered Sets (I. Rival,ed.), Dordrecht-Boston: Reidel, 
1982,pp.445-470.  
[5] Lidong Wang, Xiaodong Liu.Concept analysis via rough set and AFS 
algebra,Information Sciences, Volume 178, Issue 21, November 2008, 
pp.4125-4137. 
[6] Xuegang Hu, Yuhong Zhang and Xinya Wang.The representation and 
resolution of rough sets based on the extended concept lattice,L. Wang 
and Y. Jin (Eds.): FSKD 2005, LNAI 3613, pp.1309-1312. 
[7] Changzhong Wang, Congxin Wu, Degang Chen. A systematic study on 
attribute reduction with rough sets based on general binary relations, 
Information Sciences, Volume 178, Issue 9, May 2008, pp2237-2261. 
[8] Yuguo He.An Efficient Attribute Reduction Algorithm,Lecture Notes in 
Computer Science, Intelligent Data engineering and automated learning, 
E.Corchado et al. (Eds.): IDEAL 2006, LNCS 4224, pp.859 – 868. 
[9] Min Liu, Mingwen Shao, Wenxiu Zhang, Cheng Wu.Reduction method 
for concept lattices based on rough set theory and its application. 
Computers & Mathematics with Applications, Volume 53, Issue 9, May 
2007,pp1390-1410. 
[10] Masahiro Inuiguchi, Yukihiro Yoshioka, Yoshifumi Kusunoki.Variable-
precision dominance-based rough set approach and attribute reduction, 
International Journal of Approximate Reasoning, Volume 50, Issue 8, 
September 2009, pp.1199-1214. 
[11] Zuqiang Meng, Zhongzhi Shi.A fast approach to attribute reduction in 
incomplete decision systems with tolerance relation-based rough sets, 
Information Sciences,Volume 179,Issue 16,July 2009, pp.2774-2793. 
[12] Q.H. Hu, Z.X. Xie, D.R. Yu, Hybrid attribute reduction based on a novel 
fuzzy-rough model and information granulation, Pattern Recognition 
Vol 40, No.12,2007,pp.3509–3521. 
[13] Smolinski, T.G., Milanova, M., Boratyn, G.M., Buchanan, R., Prinz, A. 
Multi-objective evolutionary algorithms and rough sets for 
decomposition and analysis of cortical evoked potentials. In: IEEE 
International Conference on Granular Computing, Atlanta, GA, 2006, 
pp.635-638. 
[14] Smolinski, T.G., Boratyn, G. M. Milanova, M., Buchanan, R., Prinz,A.  
Hybridization of independent component analysis, rough sets, and multi-
objective evolutionary algorithms for classificatory decomposi-tion of 
cortical evoked potentials, Lecture Notes in Computer Science, Volume 
4146, 2006,pp.174-183 
[15] Godin R.: Incremental concept formation algorithm based on Galois 
(concept) lattice. Computational Intelligence, ,Vol 11,No 2, 1995, 
pp.246 -267. 
[16] Dexing Wang, Xuegang Hu, Hao Wang.: The research on model of 
mining association rules based on quantitative extended concept lattice, 
IEEE proceeding of the First International Conference on Machine 
Learning and Cybernetics, Beijing, 2002,pp.134-138.   
[17] Dexing Wang, Xuegang Hu, Hao Wang, The research on the model of 
mining association rule by interest-weighted on concept lattice, Journal 
of Nanjing University (Natural Sciences), Vol. 38, Computer Issue II, 
2002,pp.89-95 (in Chinese) 
[18] Dexing Wang, Xuegang Hu, Xiaoping Liu, Hao Wang. Association rules 
mining on concept lattice using domain knowledge.In Proceedings of 
2005 International Conference on Machine Learning and Cybernetics, 
Volume 4, Issue 18-21 Aug. 2005, pp.2151-2154 
[19] Huanglin Ceng, Intellectual computation, Chongqing University 
Publishers, 2004, pp.50-69. (in Chinese)       
[20] Bin Liu, Rough analysis of rules acquisition, Computer Engineering and 
Application, No.17, 2001,pp.117-137. (in Chinese)            
[21] Xiaowei Yan, Chengqi Zhang, Shichao Zhang.Genetic algorithm-based 
strategy for identifying association rules without specifying actual 
minimum support. Expert Systems with Applications, Volume 36, Issue 
2, Part 2, March 2009, pp.3066-3076. 
[22] Jiawei Han,M. Kamber. Data Mining--Concepts and Techniques. Higher 
Education press, Morgan Kaufmann Publishers, 2001, pp.232-236. 
 
 
 
 
435
