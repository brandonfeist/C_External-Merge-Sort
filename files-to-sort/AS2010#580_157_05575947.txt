              A Probabilistic Approach to Apriori Algorithm 
                             Vaibhav Sharma                      M.M. Sufyan Beg      
                 Computer Science Department                Department of Computer Engineering 
       Institute of Technology and Management Jamia Millia (A Central University)  
                      Gurgaon, Haryana, India              New Delhi, India 
                          shvaibhav@yahoo.com                        mbeg@jmi.ac.in 
 
       Abstract 
We consider the problem of applying probability concepts 
to discover frequent itemsets in a transaction database. 
The paper presents a probabilistic algorithm to discover 
association rules. The proposed algorithm outperforms the 
apriori algorithm for larger databases without losing a 
single rule. It involves a single database scan and 
significantly reduces the number of unsuccessful candidate 
sets generated in apriori algorithm that later fails the 
minimum support test. It uses the concept of recursive 
medians to compute the dispersion in the transaction list 
for each itemset. The recursive medians are implemented 
in the algorithm as an Inverted V-Median Search Tree 
(IVMST). The recursive medians are used to compute the 
maximum number of common transactions for any two 
itemsets. We try to present a time efficient probabilistic 
mechanism to discover frequent itemsets. 
Keywords: Data mining, KDD, association rules, 
frequent itemsets, probability, statistics, apriori algorithm. 
1. Introduction  
The exponential increase in disk space and availability of 
cheap storage has contributed to the accumulation of large 
quantities of data. Knowledge Discovery in Databases 
(KDD) is the process of extraction of frequent patterns 
from database. KDD is an important tool to transform the 
vast amount of data into information. KDD is also referred 
to as Data Mining. In different contexts, data mining is 
referred as knowledge extraction, data analysis, data 
dredging, data fishing and data snooping. Data mining 
finds its application in marketing, surveillance, fraud 
detection, scientific discovery, pattern recognition and 
customer analytics. Data mining techniques were classified 
on the basis of databases to be mined by Chen et al [7]. 
Culotta et al. proposed an integrated supervised machine 
learning method that learns both contextual and relational 
patterns to extract relations [8]. A wide variety of data 
mining techniques have evolved but they all share the same 
basic features [11],[12],[15].  
Data Mining is a very challenging task. The most time 
consuming step in data mining is the discovery of frequent 
itemsets. Itemsets that satisfy the criterion of minimum 
support are called frequent itemsets. Minimum support 
gives a lower limit to the occurrences of a set of items in a 
given database. Agrawal et al. proposed a mathematical 
model to address the problem of mining association rules 
in a transaction database [2]. Association rule discovery is 
the most important aspect of data mining [13]. Various 
techniques have been developed to discover frequent 
itemsets in a database. But Apriori algorithm[1] was the 
first algorithm to discover the frequent itemsets in a time 
efficient manner. It was the first algorithm that 
significantly improved the data mining techniques to 
identify frequent itemsets. Apriori algorithm is explained 
in Section 2 of the paper. Different techniques have been 
used by different researchers to implement apriori 
algorithm. Monte Carlo simulation techniques using 
statistical methods have been successfully applied to 
apriori algorithm [10]. Olson and Wu also correlated 
Monte Carlo simulation techniques with data mining 
concepts [14]. Bodon proposed a very fast implementation 
technique for apriori algorithm [5]. Bodon also proposed a 
trie based apriori algorithm for mining frequent itemset 
sequences [6]. 
The paper proposes a probabilistic approach to apriori 
algorithm. Using concepts of probability and statistics, the 
paper presents an implementation faster than the apriori 
algorithm. The structure of the proposed algorithm is kept 
similar to the apriori algorithm for ease of understanding. 
Though the proposed algorithm inherits many concepts 
from apriori algorithm, but its mechanism for mining 
frequent itemsets is completely different. It is based on the 
fact that not all (k-1)-itemsets have equal probability of 
generating k-candidate sets with high support. This is 
opposite to the concept of apriori algorithm which gives 
equal priority to all the itemsets. The proposed algorithm 
uses concept of “bimodal distribution” to classify the 
itemsets into two groups. The first group contain itemsets 
that have very high support relative to others. The second 
group contain itemsets that have moderate support. The 
itemsets in both the groups are already above the minimum 
support. The itemsets in the second group are further 
processed to identify those combinations of itemsets that 
will not generate high support candidate sets. These low 
support combinations are later pruned from the possible 
candidate sets. The algorithm provides a time efficient 
method to discover frequent itemsets in a transaction 
database. 
2010 IEEE International Conference on Granular Computing
978-0-7695-4161-7/10 $26.00 © 2010 IEEE
DOI 10.1109/GrC.2010.69
402
The paper is divided into 6 sections. The Second Section 
gives only a brief account of apriori algorithm. The Third 
Section discusses the key concepts used in the proposed 
algorithm. The proposed algorithm is presented in detail in 
Section 4. A detailed explanation along with pseudocode is 
provided in this section. The Fifth Section analyse the 
performance results for the proposed algorithm and 
compares them with the apriori algorithm. The paper 
concludes with the Sixth Section giving directions for 
future research. 
 
2. Apriori Algorithm  
The general structure of the apriori algorithm is given 
below. The apriori algorithm involves multiple passes. For 
k-passes, it scans the database k times. In the first database 
scan large 1-itemsets with frequency at least equal to 
minimum support are generated. A subsequent pass 
through the algorithm is a two step process. In the first 
step, lexicographically similar large itemsets Lk-1 are 
combined to generate candidate sets Ck using the 
apriori_gen function. The second step counts the 
frequency of each candidate set in Ck. This step scans the 
entire database. This counting operation can be performed 
in a faster way by using a candidate hash tree. After the 
counting operation, all the candidate sets with support less 
than minimum support are pruned to give large k-itemset. 
These two steps are repeated until no more large itemsets 
are formed. The application of this counting technique was 
extended beyond transaction databases in [3]. 
        L1 = { large 1-itemsets }; 
        for ( k=2; Lk-1 ? ? ; k++) do begin 
            Ck = apriori_gen (Lk-1); 
            for all transactions t ? D do begin 
                 Ct = subset ( Ck, t ); 
                 for all candidates c ? Ct do 
         c.count ++; 
            End 
            Lk = { c ? Ck | c.count ? minsup} 
        End 
        Answer = ? L? k ; 
3. Key Concepts used in the proposed 
algorithm 
Probabilistic and statistical concepts applied in the 
proposed algorithm identify and eliminate candidate sets 
that will otherwise fail in the minimum support test. The 
single database scan and the data sets used to store 
information after the scan are also discussed. 
3.1. Bimodal Distribution 
A multimodal distribution is a mixture of two or more 
different unimodal distributions. A bimodal distribution is 
a specific case of multimodal distribution with only two 
modes depicted as two distinct peaks (local maxima) in the 
probability density function.  
Condition for bimodality in a probability density function 
states: “A mixture of two normal distributions with equal 
standard deviations is bimodal only if their means differ by 
at least twice the common standard deviation.” The 
concept of bimodality is used in the function classify of the 
algorithm. 
The apriori algorithm combines all the lexicographically 
suitable (k-1)-itemsets with equal probability to generate k-
candidate sets. During candidate generation, there is no 
differentiation among (k-1)-itemsets. But statistically, (k-1) 
-itemsets with large support value have relatively higher 
probability of generating k-candidate sets with larger 
support values. The (k-1)-itemsets with high support values 
are independently present in relatively large number of 
transactions, and this contributes towards higher 
probability for any two of them being present together in a 
transaction. Using bimodal distribution in the probability 
density function for support, we try to classify the (k-1)-
itemsets into two groups – HIGH and MEDIUM based on 
their probability to generate k-candidate sets with large 
support values. This is discussed in more detail in Section 
4.3. 
3.2. Recursive Medians as a measure of dispersion 
for a given distribution  
Median divides a distribution into two new distributions 
with equal number of elements. If we, recursively, find out 
all the possible medians for a list, then, we can say with 
conformity that only one element is present between any 
two successive medians. This fact becomes complicated 
when the distribution is large and many values in the 
distribution turn out to be medians. But still it holds true. 
The concept of recursive medians was first given in 
Median of Medians Algorithm [4]. This concept is widely 
used in Selection algorithms and Quick sort. Median of 
Medians Algorithm is not used as such in the proposed 
algorithm. It is used in a much simpler way and with little 
modification in the proposed algorithm.  
The recursive medians concept is used in the function 
low_probable_combine_itemsets. In this function, 
recursive medians are calculated for a list of transactions 
against an itemset. It is used to find the maximum number 
of exactly same transaction ids for any two transaction lists 
within well defined boundaries. Recursive medians are 
used to evaluate dispersion of elements in a given 
distribution. Any two successive medians indicate a range 
containing only one element. This concept when used 
repetitively with all the possible medians for a distribution 
deduces the maximum number of elements present within a 
given range. Though, it cannot predict the exact number of 
“same” elements for two distributions but it gives a 
maximum limit to the number of elements that can be 
“same”. This concept is explained in detail in Section 4.5. 
403
3.3. Inverted V-Median Search tree (IVMST) 
It is a V-shaped binary search tree where splitting nodes hold 
the value of recursive medians calculated for a distribution. 
The concept of recursive medians in the context of algorithm 
is implemented in the form of an Inverted V-Median Search 
Tree. The root node of the IVMST holds the first median 
value for the distribution. The root node is the only binary 
node with two children. Rest of the nodes are unary with 
only one child. IVMST follows the basic principle of binary 
search tree. The left subtree of a node contains nodes with 
keys less than the node’s key. The reverse is true for the right 
subtree of a node. Thus, leftmost and rightmost leaf nodes 
hold the smallest and largest median value respectively. The 
implementation details of IVMST are given in Section 4.5. 
3.4. Single Data Scan 
Unlike apriori algorithm which performs k data scans for k 
passes, the proposed algorithm performs only one data scan. 
The database is not used to count the support for the k-
candidate sets after the first pass. Rather, the information is 
collected in the data sets L1, C1 and C?1. The information 
stored in L1, C1 and C?1 is modified in further passes. For 
small values of k, the size of data sets Lk, Ck and C?k is larger 
than the database. But for large values of k, the size of data 
sets becomes significantly smaller than the database. This 
feature may appear similar to AprioriTid, but its 
implementation in the proposed algorithm is completely 
different. The format of the entries in L1, C1 and C?1 is 
different from that in AprioriTid. The format for these data 
sets is given in Table 1.  
Instead of storing information in form of transactions (done 
in AprioiTid), we store information in form of itemsets along 
with the corresponding transactions in which they are 
present. The first field in C1 represents the 1-itemset and the 
second field is a list of all the transaction ids which contain 
that itemset. C?1 is same as C1 except that it contains the count 
of all the transaction ids as ‘support’ in its second field. This 
way of storing information is inherent to the success of the 
algorithm as it proves to be time efficient in later passes. 
Also, this format is a key requirement, keeping in mind the 
probabilistic approach of the algorithm. 
       Table 1: Format of Data Sets 
Data Set Format 
L1 {1-itemset} 
C1 {1-itemset, list of transaction ids} 
C?1 {1-itemset, support} 
4. Algorithm 
This section discusses the proposed algorithm in detail. It is 
divided into 6 sub sections. The first sub section explains the 
notations used in the algorithm. The second subsection 
presents the general structure of the proposed algorithm. Sub 
sections 4.3 - 4.6 explain the various functions invoked in the 
algorithm. 
4.1. Notations used in the algorithm 
The algorithm uses various data sets to perform different 
operations. Also, values returned from the functions called 
from the main need to be stored in appropriate data sets. The 
notations used in the algorithm are given in Table 2. 
Table 2: Notations 
Data sets Format of each entry 
Lk-1 {Large (k-1)-itemset} 
Ck-1 {Large (k-1)-itemset, list} 
C?k-1 {Large (k-1)-itemset, support} 
MEDIUM???????????? {(k-1)-itemset, support} 
MEDIUM {(k-1)-itemset, list} 
CkNew {k-candidate set} 
LOW 
{k-candidate sets} 
(Created from the lexicographic 
association of two (k-1)-itemsets ? 
MEDIUM) 
CkList {k-candidate set, list} 
HIGH_SUPPORT { support1, support2,…, supportn } 
MEDIUM_SUPPORT { support1, support2,…, supportn } 
4.2 Proposed Algorithm 
L1 = ?Large 1 ? itemsets? 
C?1 = ?Large 1 ? itemsets, support? 
C1 = ?Large 1 ? itemsets, list? 
For ?k ? 2 ; Lk ? ? ; k++} do begin 
   MEDIUM???????????? = classify(C?k-1); //see Section 4.3 
        For all c ? Ck-1 do begin 
           For all m ? MEDIUM???????????? do begin 
  If ( m.itemset = c.itemset ) Then 
    Insert ? c. itemset, c. list ? into MEDIUM;  
           End 
        End 
        CkNew = apriori_gen( Lk-1 ); //see Section 4.4 
        LOW=low_probable_combine_itemsets(MEDIUM);   
//see Section 4.5 
        Delete all k-candidate sets present in LOW from CkNew; 
        CkList = gen_candidates_with_list ( Ck-1, CkNew ); 
    //see Section 4.6 
        For all c ? CkList do begin 
           If ( c.count (list) ? minsup ) Then 
               Lk = ? c. itemsets ? 
              Ck = ? c. itemsets, c. list ? 
  C?k = ? c. itemset, c. count ?list? ? 
           End if 
         End 
End 
Answer := ? L? k; 
 
404
L1, C1 and C?1 are the data sets obtained after the database 
scan. A subsequent pass through the algorithm, say pass k, 
consists of 4 phases. The first phase is the classification 
phase. In this phase, the large itemsets Lk-1 from previous 
pass are classified in two groups on the basis of bimodality 
in probability density function for support. These two groups 
are HIGH and MEDIUM. The second phase calls the 
apriori_gen function and generates k-candidate sets from 
lexicographic association of large itemsets Lk-1. These k-
candidate sets are stored in CkNew. The next phase is 
performed in two steps. First, the large k-itemsets present in 
MEDIUM are processed in the function 
low_probable_combine_itemsets to identify those k-
candidate sets that will fail in the minimum support test. 
These k-candidate sets are stored in data set LOW. In the 
second step, all the entries present in LOW are deleted from 
CkNew. In the last phase, the list of transactions corresponding 
to each k-candidate set in CkNew is generated using function 
gen_candidates_with_list by intersection of the list of all its 
immediate (k-1) subsets present in Ck-1. Pruning of all the k-
candidate sets with support below minimum support gives 
the large k-itemsets. This process is repeated until no new 
large k-itemsets are produced by the previous pass. 
4.3 Classification of Large (k-1)-itemsets 
In the function classify, C?k-1 is passed as an argument. The 
data set C?k-1 contains support for each large (k-1)-itemset. 
This function generates a probability density function (pdf) 
for support. We check the condition for bimodality in this 
probability density function. If such a condition exists, we 
divide the support distribution into two groups – 
HIGH_SUPPORT and MEDIUM_SUPPORT. The point of 
division is given by the condition of bimodality. 
HIGH_SUPPORT contains all the support values above the 
point of separation and MEDIUM_SUPPORT contains all 
support values below it. 
Select  
    max_val ? MAXIMUM (c.support ); 
    min_val ? MINIMUM ( c.support ); 
    mean ? AVG ( c.support ); 
where c ? C?k-1; 
For all c ? C?k-1 do begin 
    difference += ( c.support – mean ) ^ 2; 
    If ( c.support = max_val ) Then  
          Pop c from C?k-1; 
          Insert c.support into HIGH_SUPPORT; 
    Else If ( c.support = min_val ) Then  
          Pop c from C?k-1; 
               Insert c.support into MEDIUM_SUPPORT; 
End 
standard_deviation = ? (difference Total Transactions? ); 
standard_difference = 2*standard_deviation; 
 
lean = ? 1?c. support ? ????? - ? 1?c. support ? ?????; 
                         Total number of Transactions 
/* c ? C?k-1 and lean is a measure of Skewness*/ 
While ((AVG(HIGH_SUPPORT)-AVG(MEDIUM_SUPPORT)  
? standard_difference) AND MEDIUM_SUPPORT ? ? )  
/*Condition for Bimodality*/ 
    If ( C?k-1 ? ? ) Then 
          Pop c from C?k-1 ; 
               Insert c.support into HIGH_SUPPORT;  
    Else  
          Pop c from MEDIUM_SUPPORT;  
          Insert c.support into HIGH_SUPPORT; 
End while loop 
Pop the last support value inserted into HIGH_SUPPORT; 
If ( HIGH = NULL) Then 
  If ((lean < 0) OR  
       (lean = 0 AND (min-minsup ? max_val-mean))) Then 
               HIGH_SUPPORT = C?k-1; 
  ElseIf((lean > 0) OR  
           (lean = 0 AND (min-minsup < max_val-mean))) Then 
               HIGH_SUPPORT = NULL; 
End If 
MEDIUM_SUPPORT = C?k-1 – HIGH_SUPPORT; 
Update MEDIUM_SUPPORT to contain only distinct 
values; 
For c ? C?k-1 do begin 
      For m ? MEDIUM_SUPPORT do begin 
 If ( c.support = m.support ) Then  
    Insert c.itemset, c.support into MEDIUM????????????; 
      End 
End 
The data sets HIGH_SUPPORT and MEDIUM_SUPPORT 
may contain the same value multiple times. This is because 
more than one itemsets may have same support. This 
function also keeps track of skewness in the pdf.  
MEDIUM_SUPPORT contains all the support values present 
in C?k-1 but absent in HIGH_SUPPORT. MEDIUM???????????? is 
constructed from MEDIUM_SUPPORT. MEDIUM???????????? contains 
all those itemsets present in C?k-1 whose support is present in 
MEDIUM_SUPPORT. 
4.4 Candidate Generation 
The apriori_gen is exactly similar to the candidate 
generation function used in apriori algorithm. It involves two 
steps to generate k-candidate sets from lexicographic 
combination of (k-1)-itemsets. The first step is the join step. 
It is followed by prune step. The first step uses all possible 
lexicographic combinations of Lk-1 to generate new k-
candidate sets. The prune step removes a candidate set if any 
one of its immediate subset is not present in Lk-1. 
405
apriori_gen ( Lk-1) 
Join step 
Insert into CkNew  
select p.item1, p.item2,…, p.itemk-1, q.itemk-1   
from Lk-1 p, Lk-1 q  
where p.item1 = q.item1, p.item2 = q.item2,…,  
               p.itemk-1 < q.itemk-1; 
Prune step 
For all itemsets c ? CkNew do begin 
       For all (k-1) – subsets s of c do 
 If (s ? Lk-1) Then 
    delete c from CkNew 
       End 
End 
4.5 Generating low probable combinations of (k-1)-
itemsets 
In this function, all pairs of lexicographically similar (k-1)-
itemsets (? MEDIUM) are processed to identify those pairs 
which will generate k-candidate set with support lesser than 
minimum support. Recursive medians are used to identify 
such candidate sets. These low support k-candidate sets are 
stored in the data set LOW. Pruning these unsuccessful k-
candidate sets at an earlier stage results in increased 
performance of the algorithm. The average case and the 
worst case complexity for insertion as well as searching in 
IVMST is O(log n). Hence, IVMST presents a very efficient 
method to implement the recursive medians concept.  
The first field of data set MEDIUM is the (k-1)-itemset itself 
and the second is a list of transaction ids in which that 
itemset is present. We denote the end limits of the 
overlapped region for transactions lists of the two 
lexicographically similar (k-1)-itemsets as START and 
FINISH respectively. Thus, START and FINISH are the 
boundary limits which denotes the region where the two (k-
1)-itemsets may have “same” transaction ids. The “same” 
transaction ids refer to transactions that contain both the 
itemsets. 
The identification of the “same” transaction ids is performed 
in two stages. In the first stage, we check whether the range 
of the boundary limits (represented by FINISH-START) is at 
least as large as minimum support. If the two itemsets fail in 
this test, the resulting k-candidate set is immediately placed 
in LOW and we proceed with the next pair of (k-1)-itemsets. 
If the pair of itemsets passes this test, then we move to the 
second stage of testing. The second stage is implemented 
using the concept of recursive medians and IVMST. IVMST 
is created for each of the two itemsets. All possible medians 
for the list of transaction ids are represented as nodes in 
IVMST. Using the fact that successive medians have only 
one element in between, we predict the maximum number of 
“same” transaction ids that can be present in the range 
{START, FINISH}. We again check whether the number of 
“same” transaction ids will be at least equal to the minimum 
support. If they fail to fulfill this criterion, then the resulting 
candidate set is pushed into LOW. 
For all possible lexicographic k-candidate combinations 
among (k-1)-itemsets in MEDIUM do 
Let A and B be any two random lexicographically similar  
(k-1)-itemsets in MEDIUM  
Let [AB] be k-candidate set obtained from lexicographic 
association of A and B  
START ? MAXIMUM (Min_TidA, Min_TidB); 
FINISH ? MINIMUM (Max_TidA, Max_TidB); 
If (FINISH-START < minsup) Then 
    Insert [AB] to LOW; 
Else 
    Repeat for both A and B  
       IVMSTX ? Inverted V Median Search Tree (X); 
       /* Depth of nodes on left subtree are taken negative */  
       /* Depth of nodes on right subtree are taken positive */ 
       HLX ? { node_depth; node ? IVMSTX and 
                     node_value ? START }; 
       ValueLX? node_value (node_depth = HLX);      
       HRX ? { node_depth; node ? IVMSTX and 
                      node_value ?  FINISH };     
       ValueRX? node_value (node_depth = HRX);      
       For all nodes ? IVMSTX do begin 
           If (node_value ? list AND  
                node_value ? RANGE{START, FINISH} ) Then 
                   XX ? XX+1; 
       End 
       If (ValueLX ? START AND ValueLX ? list) Then 
                   XX ? XX-1; 
       If (ValueRX ? FINISH AND ValueRX ? list) Then 
      XX ? XX-1; 
       ElementsX ? ABSOLUTE(HRX-HLX) + XX; 
    End Repeat;  
    Maximum_same_list=MINIMUM (ElementsA,ElementsB); 
    If (Maximum_same_list < minsup) Then 
           Insert [AB] to LOW; 
End of If Else Statement 
End of for loop 
4.6 Generating list of transaction ids for newly 
generated candidates  
This function generates the list of transaction ids for each of 
the k-candidate set present in CkNew. We use the property that 
all possible (k-1) subsets for the k-candidate set are already 
present in Ck-1, as k-candidate sets with missing (k-1)-subsets 
were already pruned in the apriori_gen function. The list for 
406
each of the k-candidate set is generated by intersection of the 
list (of transaction ids) of all its (k-1) subsets. The k-
candidate set along with the list is pushed into the data set 
CkList. This is followed by pruning of all those k-candidate 
sets that do not have support at least equal to minimum 
support. Thus large k-itemsets are obtained. 
gen_candidates_with_list ( Ck-1, CkNew ) 
For all c ? CkNew do begin 
   For all possible (k-1) subsets s of c do  
      Insert { itemset,list } into CkList  
      where itemset = c.itemset AND list = ? s???? i.list   
// s ? Ck-1 
   End 
End 
5. Performance Study  
To confirm the time efficient approach of the algorithm, we 
implemented the proposed algorithm and the apriori 
algorithm on a 512 MB RAM Memory and 1.67 GHz Intel 
processor. We carried out a substantial performance 
evaluation and compared it with results obtained for apriori 
algorithm.  
Table 3 compares performance of the proposed algorithm 
with the apriori algorithm for different minimum supports 
against different number of transactions. As minimum 
support decreases, the performance of the proposed 
algorithm increases and after a certain point, it performs 
better than the apriori algorithm. The relative performance 
also increases with increase in number of transactions. These 
results are also demonstrated in Figure 1. Total number of 
data items and average number of items per transaction are 
30 and 15 respectively. 
Table 4 compares the number of items in MEDIUM and 
LOW corresponding to the data values shown in Table 3. 
The execution time is shown to compare the increase in 
performance with increase in number of candidate sets in 
LOW. 
Figure 2 compares the number of elements in MEDIUM and 
LOW against different number of initial data items for a 
given database and minimum support. For this experiment, 
number of transactions in the database is 2K and minimum 
support is 20%.The proposed algorithm performs better with 
increase in the number of candidate sets present in LOW. 
Figure 2 shows that the number of elements in MEDIUM 
and LOW increases with increase in number of data items. 
Thus, the proposed algorithm performs better for large 
number of data items in the database. 
5.1 Result Analysis 
The performance of the proposed algorithm is directly 
proportional to the number of passes over the algorithm, 
number of elements in LOW and data items in the database. 
During initial passes, size of Lk, Ck and ??k is more than the 
database. Hence, initial passes are time consuming. But 
during the latter passes, these data sets become significantly 
smaller than the database. Larger minimum support 
generates less number of candidate sets and involves lesser 
passes over the algorithm. Thus, Figure 1 shows a dip in 
performance of the proposed algorithm for larger minimum 
support values. But, it performs better than the apriori 
algorithm as minimum support decreases. 
                    Table 3: Execution time in seconds 
 
 Table 4: Execution time v/s size of MEDIUM and LOW 
 
6. Conclusion and future directions  
The proposed algorithm presents a probabilistic approach to 
apriori algorithm. The proposed algorithm creates exactly the 
same number of rules as the apriori algorithm but in lesser 
time. The results confirm that the discovery of association 
rules in larger databases is faster in the proposed algorithm 
than the apriori algorithm.  
Integration of probabilistic approach to data mining in 
distributed databases shall produce amazing results [9]. 
Hence, the effectiveness of the proposed algorithm needs to 
be checked on distributed databases. This algorithm was 
implemented on a synthetic database. Its application on real 
407
world databases presents a future direction for further 
research. 
 
Figure 1: Execution time v/s Minimum Support 
References 
[1] R. Agrawal and R. Srikant. “Fast Algorithms for Mining 
Association Rules,” In Proc. of the 20th VLDB Conference, 
Santiago, pp. 487-499, Chile, 1994. 
[2] R. Agrawal, T. Imielinski, and A. Swami. “Mining Association 
Rules between Sets of Items in Large Databases.” In Proceedings of 
ACM SIGMOD, pages 207-216, May 1993. 
[3] R. Agrawal, R. Srikant. “Mining sequential patterns.” In proc. of 
the 11th International Conference on Data Engineering (ICDE'95), 
pages 3-14, March 1995. 
[4] M. Blum, R.W. Floyd, V. Pratt, R.L. Rivest and R.E. Tarjan. 
“Time bounds for selection,” J. Comput. Syst. Sci. 7(1973), pp – 
448-461.   
 
Total number 
of data items 
Number of elements 
in MEDIUM 
Number of 
elements in LOW 
10 139 0 
20 3238 127 
30 21899 1923 
Figure 2: Size of MEDIUM and LOW v/s number of data items 
[5] Ferenc Bodon.  “A fast APRIORI implementation,” In proc. of 
IEEE ICDM Workshop on Frequent Itemset Mining 
Implementations (FIMI'03), Melbourne, Florida, USA, 2003. 
[6] Ferenc Bodon. “A Trie-based APRIORI Implementation for 
Mining Frequent Item Sequences.” In ACM SIGKDD Workshop on 
Open Source Data Mining Workshop (OSDM’05), pages 56-65, 
Chicago, IL,USA, 2005. 
[7] M. Chen, J. Han and P. S. Yu. “Data Mining : An overview 
from a Database Perspective,” IEEE Transactions on Knowledge 
and Data Engineering, vol. 8, no. 6, pp. 866-883, Dec. 1996. 
[8] Aron Culotta, Andrew McCallum, Jonathan Betz. “Integrating 
probabilistic extraction models and data mining to discover 
relations and patterns in text,” In Proceedings of the main 
conference on Human Language Technology Conference of the 
North American Chapter of the Association of Computational 
Linguistics, p.296-303, June 04-09, 2006, New York. 
[9] M M Sufyan Beg, “Parallel and Distributed Discovery of 
Association Rules.” In Artifical Intelligence Application Book 
(Fadzilah Siraj, Eds), University Utara Malaysia. 
[10] Xitao Fan, Ákos Felsövályi, Stephen A. Sivo (Monte Carlo), 
SAS® for Monte Carlo studies: a guide for quantitative researchers. 
[11] U. Fayyed, G. Piatetsky-Shapiro, P. Smyth and R. Uthurasamy 
(eds.). “Advances in Knowledge Discovery and Data Mining,”  
AAAI Press / The MIT Press, 1996. 
[12] W.J. Frawley, G. Piatetsky-Shapiro and C. Matheus. 
Knowledge “Discovery In Databases: An Overview. In Knowledge 
Discovery In Databases,” eds. G. Piatetsky-Shapiro, and W. J. 
Frawley, AAAI Press/MIT Press, Cambridge, MA., 1991, pp. 1-30. 
[13] V. Kumar and M. Joshi. “Tutorial on High Performance Data 
Mining,” In proc. of International Conference on High 
Performance Computing (HiPC-98), Dec. 1998.  
[14] David L.Olson and Desheng Wu. “Decision making 
with uncertainity and data mining.” In X. Li, S. Wang and Z.Y. 
Dong(Eds.), Lecture notes in Artificial Intelligence (pp. 1-9). 
Berlin: Springer(2005). 
[15] P. Wright. Knowledge Discovery In Databases: Tools and 
Techniques, ACM Crossroads, Winter 1998. 
 
408
