     Influence Of Discrete Granularity On Using    
 Association Rules To Complete Missing Values
Yue-an ZHU 
information science and technology school 
Jinan University 
Guangzhou 
China 
iwillgoon@126.com 
Jian-hua WU 
Zhuhai Campus 
Jinan University 
Zhuhai 
China 
tjhwu@jnu.edu.cn  
Abstract—this paper investigates the impacts of different discrete 
granularities of continuous attribute on the algorithms which 
employ association rules to complete missing value. The Chi2 
algorithm is used to discrete the continuous attributes. By using 
three different discrete granularities, the impacts of the 
algorithm on completing missing value are explored. 
Experimental results show that different discrete granularity has 
negligible impacts on using association rules to complete missing 
value. However, the completing accuracy has an inclination to 
decrease with the decrease of the discrete granularity. 
Keywords- missing value; association rule; discrete granularity; 
Chi2 algorithm 
I.  INTRODUCTION  
Missing values is very common in all kinds of science 
research. Incomplete data set brings great difficulty in data 
using and analysis and it is one of the main reasons for 
indefinite information system. Reference [1] summarizes some 
definition of missing value as follows: error occurring in 
transferring or collecting data, null value, exceeding the range, 
value that can’t meet the standard and so on. There are many 
reasons for missing value, but the main causes are probably as 
follows: some information is unavailable at the moment; some 
information is omitting; some attributes of the object can’t be 
used; some information is viewed as unimportant; the cost to 
obtain those information is high; the system requires high real-
time, i.e., decision must be made quickly before the 
information is available etc. Researcher propounded many 
methods to solve the missing value which base on different 
theory and are applied in different occasion. Generally, those 
methods are divided into such kinds: deleting the records 
where missing value exists and then obtaining complete data 
set, completing the missing value manually, treating Missing 
values as special values like [12], utilizing the statistics method, 
conducting data mining or inference in those data sets etc. In 
recent years, there comes up with the method using association 
rules to complete missing values(ARMV), see [4, 5, 9] and it 
achieves good performance compared with other methods used 
to complete missing values such as Robust Bayesian Estimator 
(RBE) [11]. So far, research of ARMV is mainly on discrete 
data set. This paper investigates how the discrete granularity of 
continuous attribute in non-discrete data set affects the 
performance of ARMV. We say some continuous attribute has 
discrete granularity 2, we mean using 2 intervals to represent 
the value of that attribute. The greater granularity is, the fewer 
intervals are used to represent the original data. 
The rest of this paper is organized as follows: Section 2 
introduces ARMV. Section 3 introduces the data discretization 
algorithm. Section 4 provides experimental results and 
evaluates the influence of discrete granularity of continuous 
attributes on ARMV. Finally, conclusions are drawn in section 
5 
II. ARMV 
A. association rules 
In 1993, R.Agrawal put forward the concept of association 
rules first time. Combined with the transaction database, 
R.Agrawal gave its formal description as follows [9, 10]. Let I 
= {i1, i2, ..., in} be a set of n distinct literals, called item set. Let 
DB be a transaction database, where each transaction Ti ? I is a 
set of items, which has a unique identifier associated with it, 
called TID. If a set of items X that fulfills the relationship “X 
? T”, we say T contains X. An association rule is an 
implication which has the form “X?Y”, where X ? I, Y ? I 
and X?Y =?. An association rule has two attributes: support 
and confidence whose values must be no less than the user-
specified minimum support (minSup) and minimum confidence 
(minConf) values respectively. The rule “ X?Y” that has a 
support value s% means the percentage of transactions 
containing X ? Y in the transaction database, denoted as 
X YSup(X Y)  | DB |/|DB |?? = , where X YDB ? is the 
transaction set containing X?Y. The rule “X?Y” that has a 
confidence value c% means the conditional probability, i.e., Pr 
(Y|X) = Pr (X?Y) / Pr (X).So, it means c% of transactions in 
database containing X also contains Y. In fact, association rule 
is an approximate dependency relationship in database, using 
support and confidence to evaluate this relationship.  
The task of mining association rule in transaction database is 
to find all association rules whose support and confidence are 
no less than minsup and minconf respectively. The primary 
concept behind most association rule algorithms is a two phase 
procedure [10]: In the first phase, all frequent item sets are 
found. An item set is said to be frequent if it satisfies a user-
978-1-4244-7941-2/10/$26.00 ©2010 IEEE
defined minimum support requirement. The second phase uses 
these frequent item sets to generate all the rules which satisfy 
the user-specified minimum confidence. The first phase 
discovering all frequent item sets dominates the performance of 
the mining association rules process. Algorithms to discover all 
frequent item set can be divided into two categories: one is 
producing the candidate set and the other is not producing the 
candidate set. The Apriori algorithm, a multiple-passes and 
producing candidate set algorithm, is a typical and famous 
technique to identify frequent item sets [10]. 
B. Applying ARMV on incomplete transaction database 
Association rule has the form of imputation “X?Y”, where 
X?Y =?, X is called antecedent and Y is called consequent. 
Let us see an example how to apply association rules to 
complete missing value, where the Zoo database is obtained 
from UCI machine learning repository. 
Given data: aquatic = y, tail = y, hairs = n, legs =?, fins = ?. 
Supposed minsup = 0.8 and minconf = 0.15, we used 
association rules r1 “aquatic = y, hairs = n ? legs = 0” to 
obtain the new data: aquatic = y, tail = y, hairs = n, legs = 0, 
fins = ?. Again, we used association rules r2  “tail = y, legs = 0 
? fins = y” to obtain the complete records: aquatic = y, tail = y, 
hairs = n, legs =0, fins = y. But how to select the association 
rules to complete missing values need more complicated 
procedure. In the following, we will introduce methods 
converting the relation database to transaction database and 
give our algorithm of selecting association rules to complete 
missing values. 
1) Transforming the relation database to transaction 
database 
We use the method like [7] to transform the relation 
database to transaction database. D is dataset defined over n 
attributes 1,..., nA A  and contains m cases 1,..., mc c . 
Let ic , 1 i m? ? , be transaction ID and cases of 
attributes 1,..., nA A , 1,... na a , be corresponding items, we can 
get transaction dataset TD. That is, let 
{ | 1,i ij ijIS A a i a= = ?   is a case of iA }  is an item set 
eliminating the missing values. After transforming, a 
transaction of TD looks like 1 1,{ ,..., }
i i
i j n njc A a A a= = . 
Then given a minsup, we utilize the Apriori algorithm to mine 
the large item sets of TD, denoted as 1,..., gL L , 1g ? . Then 
for each element uve  of uL ,1 , 1u g v? ?  ? , we compute the 
confidence “ \{ }uv uvw uvwe d d} ?{ ” for  each element 
, 1uvwd w ? , of uve . If the confidence is no less than the user-
specific threshold minconf,  the element of association rule is 
add to the set of association rules. Now, let’s see an example 
below. 
 
 
 
TABLE I.  (A) DATASET OF 5 CASES?B?TRANSACTION FORM OF 
DATASET 
Case A1 A2 A3  TID Items 
c1 2 1 ? t1 A1=2, A2=1 
c2 2 1 2 t2 A1=2,A2=1,A3=2, 
c3 2 1 2 t3 A1=2, A2=1,A3=2 
c4 ? 2 1 t4 A2=2,A3=1 
c5 1 ? 1 t5 A1=1, A3=1 
TABLE II.  THE  ASSOCIATION RULES PRODUCING BY TABLE 1, WHERE 
MINSUP = 0.3, MINCONF = 0.6 
ID Asscotiaon rules sup conf 
r1 {A1=2} —>{A2 =1} 60% 100% 
r2 {A2=1}—>{A1=2} 60% 100% 
r3 {A1=2}—>{A3=2} 40% 66.7% 
r4 {A3=2}—>{A1=2} 40% 100% 
r5 {A1=2, A2=1}—>{A3=2} 40% 66.7% 
r6 {A1=2, A3=2}—>{A2=1} 40% 100% 
r7 {A2=1, A3=2}—>{A1=2} 40% 100% 
 
There are 5 cases and 3 attributes in table 1(a), where “?” 
represents the missing value. Table 1(b) is the transaction form 
of Table 1(a) in which the missing values is eliminated. Then, 
given the specific threshold minsup = 0.6 and minconf = 0.3, 
we applied the Apriori algorithm on Table 1(b) to find large 
item set and obtained the association rules.  
2) Selecting the association rules to complete missing 
values 
Firstly, we score the each association rules based on the 
following function. 
S (length, support, confidence) = length + 0.5×confidence 
+ 0.25×support.                                                                 … (1) 
And sort them based on the score of rule by decreasing order. 
Reference [9] gave a good heuristic function to select the 
association rule to complete missing value. Here, the methods 
to select the rule will not affect our research. So, we adopt a 
simple method to select the rule. Here, length is equal to the 
number of element of antecedent in association rule. For 
example, the antecedent of the rule “aquatic = y, hairs = n ? 
legs = 0” has two components, so the “length” of the rule is 
two. Then, for each case of test data set, we scan the rules one 
by one and identify which rules’ antecedent is implicated in 
the case. These identified rules form the matching rules set 
called Rm. If Rm is not null, we select the first rule where the 
consequent of the rule is missing value. If Rm is null, we use 
the value that appears most frequent in that attribute as the 
missing value. The following is algorithmic description of 
missing data completing: 
 
 
 
 
 
 
 
 
 
 Figure 1.  algorithmic description of missing data completing 
III. DATA DISCRETE ALGORITHM INTRODUCTIONS 
Data discretization technique can be used to reduce the 
number of values for a given continuous attribute by dividing 
the range of the attribute into several intervals. Interval labels 
can then be used to replace the actual data values. Replacing 
numerous values of a continuous attribute by a small number of 
interval labels can thereby reduce and simply the original data 
[6]. Several methods have been proposed to discretize data as a 
preprocessing step for the data mining process. Roughly, we 
can divide the approaches into two classes [5]: 
• Unsupervised methods. "Blind" methods, where we 
have no classification information available for the 
object being considered. These methods rely on 
assumptions of the distribution of the attribute values. 
• Supervised methods. Classification information is 
available, and this information can be taken into 
consideration when discretizing the data. A common 
denominator can be used to minimize the number of 
objects from different decision classes into the same 
discretization class. 
In 1992, Kerber put forward heuristic algorithm 
ChiMerge to discrete the continuous attribute which based 
on 2?  statistics. Different from the conventional top-down 
splitting strategy, ChiMerge algorithm begins with 
initialization, adopts bottom-up strategy and consists of a 
series of merging step. It continues to merge the intervals 
until a stop condition is satisfied. The stop condition 
depends on an important parameter ?  which is decided 
artificially. The ChiMerge algorithm have been widely used 
and referred in machine learning papers, and adopting this 
method came as a natural choice. The algorithm has been 
refined into a modified Chi2 version [4]. In this paper, we 
use Chi2 algorithm to discrete data. The more details about 
Chi2 algorithm see [4]. 
IV. EXPERIMENT RESULTS 
Chi2 algorithm is implemented with C language and visual 
C++ is used as programming platform. The experiment is 
conducted in such environment: windows XP SP2 operating 
system, Intel(R) Pentium(R) ? 2.40GHz CPU, 768M memory. 
The three medical data sets used in this experiment are gotten 
from UCI machine learning repository. The first data set, 
named bupa.data about liver function disordered, has six 
attributes. The second data set, named diabetes.data about 
diabetes, has five attributes. The third data set, named new-
thyroid.data about thyroid, has eight attributes. 20% missing 
values are introduced in those data sets. The association rules is 
produced on those data sets under the condition minsup = 0.2 
and minconf = 0.5 repectively. We are interested to find that 
some discrete granularity of all attributes can’t be the same on 
the same granularity. For example, in Table 1, in D1, A1, A2, 
A3, A4, A5, A6 has granularities as following: 2, 2, 2, 2, 2, 1 
respectively. Why not the same? Because those two intervals 
used to represent the data of A6 are so “similar” and the Chi2 
algorithm will combines them. Fig.1 shows that different data 
discrete granularity has little effect on ARMV, but as the data 
discrete granularity decreases, accuracy of ARMV inclines to 
decrease.  
TABLE III.  THREE DATA DISCRETIZATION OF BUPA.DATA 
 A1 A2 A3 A4 A5 A6 CA 
DD1 2 2 2 2 2 1 ------- 
DD2 3 4 3 4 3 3 ------- 
DD3 7 6 6 8 6 7 ------- 
(Remark: A = attribute, DD = data discrete granularity, CA = categorical atrribute. the ab. in following 
is the same as this) 
TABLE IV.  THREE DATA DISCRETIZATION OF NEW-THYROID.DATA 
 A1 A 2 A 3 A 4 A 5 CA 
DD1 2 2 2 2 2 --------- 
DD2 4 4 4 3 4 --------- 
DD3 6 6 8 8 6 --------- 
 
TABLE V.  THREE DATA DISCRETIZATION OF DIABETES.DATA 
 A1 A2 A3 A4 A5 A6 A7 A8 CA 
DD1 2 2 1 1 1 2 1 2 --- 
DD2 4 4 4 4 3 3 4 4 --- 
DD3 6 6 8 6 7 6 7 7 --- 
 
 
 
1) Algorithm SelectARToCompleteMV 
2) Begin 
3) Rm = ? ; 
4) for each case Ti ?  the test data set do 
 // finding matching rules set Rm 
5)    for each rule ?  Rules set do 
6)         if antecedent(rule) is implicated Ti then 
  7)             Rm= Rm ? rule; //adding the rule to Rm 
  8)        endif 
  9)   endfor  
 // select the most priority rule 
10)   if Rm?null then 
11)       Ri=First(Rm);  
12)   endif 
13)   Imputation data=consequent(Ri); 
14) endfor 
15) end  
10%
30%
50%
70%
90%
DD1 DD2 DD3
DD = data discretization
ac
cu
ra
cy
 r
at
e
bupa New-thyroid diabetes
 
Figure 2.  accuracy change under different data discretization 
V. CONCLUSIONS 
Association rules are approximate functional dependency. It 
plays an important role in supermarket basket analysis. In 
recent years, the research on ARMV has been on the upgrade, 
seeing [2, 3, 7]. Compared with RBE (Robust Bayesian 
Estimator) [11], ARMV achieves a better performance in 
completing missing values and has a good applicative 
perspective. But most of researches are on discrete data sets. 
This paper investigates influence of the different data 
discretization of continuous attribute on using ARMV. 
Experimental results show that data discretization granularity 
of continuous attribute has no great effect on ARMV, but as the 
data discretization granularity decreases, the accuracy of 
ARMV has the trend to decrease. The result has some guide 
meaning for future research of ARMV on non-discrete data set. 
Future research is conducting more jobs on more data set.  
 
REFERENCES 
 
[1] Jukka Parviainen. Tik-61.181 Special Course in Information Technology 
[EB/OL], http://www.cis.hut.fi/Opinnot/T-
61.6010/s99/presentations/oct27_JP.ppt 
[2] A. Ragel and B. Cremilleux, MVC – a preprocessing method to deal 
with missing values, Knowledge-Based Systems, 12(5–6), 285–291 
(1999). 
[3] Jau-Ji Shen, Chin-Chen Chang, Yu-Chiang Li: Combined association 
rules for dealing with missing values.Journal of Information Science, 
2007, 1~13 
[4] H. Liu, R. Setiono. Discretization of Ordinal Attributes and Feature 
Selection. In Proceedings of the 7th International Conference on Tools 
with Artificial Intelligence, Washington D.C., Nov 1995. pp. 388-391. 
[5] Kerber R. ChiMerge: Discretization of numeric attributes. In 
Proceedings of Tenth Nat. conference on artificial Intelligence, MIT 
Press, 1992: 123~128, 
[6] J. Han, M. Kamber, Data Mining : Concepts and Techniques, Morgan 
Kaufmann, August 2000. 
[7] Wu, C., Wun, C., Chou, H.: Using association rules for completing 
missing data.In: Proceedings of 4th International Conference on Hybrid 
Intelligent Systems (HIS 2004), Kitakyushu, Japan, 5-8 December 2004, 
pp. 236–241. IEEE Computer Society Press, Los Alamitos (2004) 
[8] R. Agrawal, T. Imielinski and A. Swami, Mining association rules 
between sets of items in large database.In: P.Buneman and S. Jajodia 
(eds), Proceedings of the 1993 ACM SIGMOD International Conference 
on Management of Data, Washington, D.C., May 1993 (ACM Press, 
New York, 1993) 207–216. 
[9] R. Agrawal and R. Srikant, Fast algorithms for mining association rules 
in large databases. In: J.B. Bocca, M. Jarke and C. Zaniolo (eds), 
Proceedings of the 20th International Conference on Very Large Data 
Bases, Santiago, Chile, September 1994 (Morgan Kaufmann, San 
Francisco, CA, 1994) 487–499. 
[10] Ramesh C, etc. A Tree Projection Algorithm for Generation of Frequent 
Item Sets. Journal of Parallel and Distributed Computing, 61, 350–371 
(2001). 
[11] Ramoni, M., and Sebastiani, P. (2001), “Robust Learning with Missing 
Data,” Machine Learning, vol. 45, no. 2, pp. 147-170. 
[12] Frick, J.R., and Grabka, M.M. (2003), “Missing Income  Information in 
Panel Data: Incidence, Imputation and its Impact on the Income 
distribution,” in the Proceedings of the Workshop on Item-Non-response 
and Data Quality in Large Social Surveys, October 9-11. 
  
 
