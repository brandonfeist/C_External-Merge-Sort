Horizontal Format Data Mining with  
Extended Bitmaps 
Buddhika De Alwis1, Supun Malinga2, Kathiravelu Pradeeban3, Denis Weerasiri4, Shehan Perera5
Department of Computer Science and Engineering, University of Moratuwa, Sri Lanka 
1
chamibuddhika@acm.org 
2
supunm@users.sourceforge.net 
3060372t@uom.lk 
4060527b@uom.lk 
5
shehan@uom.lk
Abstract 
Analysing the data warehouses to foresee the patterns of 
the transactions often needs high computational power 
and memory space due to the huge set of past history of 
the data transactions. Apriori algorithm is a mostly 
learned and implemented algorithm that mines the data 
warehouses to find the associations. Frequent item set 
mining with vertical data format has been proposed as an 
improvement over the basic Apriori algorithm.  
In this paper we are proposing an algorithm as an 
alternative to Apriori algorithm, which will use bitmap 
indices in conjunction with a horizontal format data set 
converted to a vertical format data structure to mine 
frequent itemsets leveraging efficiencies of bitmap based 
operations and vertical format data orientation. 
Categories and Subject Descriptors 
[Data Mining] Association Rule, Apriori, Bitmap Indices. 
[Data Analysis] Data warehousing, Data Analysis. 
General Terms - Data Analysis and Mining 
Keywords - Data mining, Association Rule, Apriori, Vertical 
format mining, Bitmap Indices 
I. INTRODUCTION
Data Mining is an emerging concept or a tool in database 
concepts, which is young, yet powerful and promising [1, 2]. 
Knowledge discovery, prediction, clustering, and 
classifications through pattern recognition are some of the key 
design aspects of data mining. Most of the existing techniques 
on the associations are based on analysing the data 
warehouses - the existing bulk data of transaction history. 
Given the existence of a set of items, association rules enable 
the prediction of the existence of one or more other items 
based on the knowledge gathered by classifying the data 
warehouses. Association rules are commonly used in mining 
web usage [3], intrusion detection [4], and bioinformatics [5]. 
Statistical bias caused by suggesting a hypothesis by non-
representative data or a narrow sample to match the 
hypothesis is defined as data-snooping bias, which leads to a 
wrong decision in calculations, so these factors also should be 
taken in to consideration [6]. Apriori algorithm [1] is 
considered the fundamental algorithm for mining for 
associations. Hybrid algorithms converging the classification 
and association rule mining have also been suggested [7]. 
With the explosive growth of the data base size, scalability for 
the data mining algorithms becomes crucial to be able to work 
with very large data sets effectively. In this paper, a hybrid 
frequent item-set mining method extending Apriori algorithm 
is proposed. 
II. PRELIMINARIES
Finding frequent item sets plays an important role in data 
mining as the first step in determining association rules. 
A. MARKET BASKET ANALYSIS  
Association rule learning is mostly explained by market 
basket analysis [1] – retrieving the association between the 
items that customers purchase. Here products or sets of items 
(item-sets) which occur in many transactions are found.  
TABLE I - TRANSACTIONAL DATA
TID List of item_IDs 
T100 I1, I2, I5 
T200 I2, I4 
T300 I2, I3 
T400 I1, I2, I4 
T500 I1, I3 
T600 I2, I3 
T700 I1, I3 
T800 I1, I2, I3, I5 
T900 I1, I2, I3 
Each of the patterns of behaviour of buyers identified 
through this analysis can be used to place the goods in the 
shops or restructure pages in the catalogues. 
A set consists of i goods, called i-set-element (i-item-set).  
The percentage of transactions containing a given set, called 
the support (provision) set.  It is believed that in order for a set 
of interest, its support must be above a user-defined minimum, 
such sets are called 'frequent'. Table 1 [8] describes several 
transactions (T100, T200, etc), stored in a relational database. 
220978-1-4244-7896-5/10/$26.00 c©2010 IEEE
Corresponding column mentions the relevant list of item ids 
for the particular transaction. As an example “T200” 
transaction contains “I2” and “I4” item ids. 
In frequent item-set mining, we derive rules based on two 
measurements called minimum support and the minimum 
confidence that reflect the usefulness and certainty of the 
discovered rules. Typically, association rules are considered 
interesting if they satisfy both a minimum support threshold 
and a minimum confidence threshold [1]. 
B. ALGORITHM APRIORI 
 This algorithm defines several stages. At the i-th stage 
identifies all frequent i-element sets. Each stage consists of 
two steps: the formation of candidates (candidate generation) 
and the counting of candidates (candidate counting).  At the 
step of forming the candidates, algorithm creates a lot of 
candidates from the i-element sets.  At the step of counting 
candidates algorithm scans the database of transactions, 
computing support sets of candidates.  After scanning, the 
algorithm discards the candidates, ensuring that less than a 
user-defined minimum and saves only the common i-element 
sets.  
During the 1
st
phase of the selected set of candidates 
contains all 1-element sets.  Algorithm calculates their support 
during the step counting candidates. Thus, after the first phase 
all frequent 1-element sets are known. Reasoning in a straight 
line, a candidate can "burn" all pairs of goods. However, 
Apriori reduces the number of sets of candidate sifting - a 
priori - those candidates who may not be frequent, based on 
information received at previous stages of the information on 
which of the sets are the most abundant.  Screenings are based 
on the simple assumption that if the set is frequent, all its 
subsets must also be frequent. Thus, before the counting of 
candidates step algorithm can reject any candidate set, a 
subset of which is not frequent. This process is continued until 
the number of frequent n-item-sets becomes zero, where n 
determines the no. of children in the item-set.  
C. ALGORITHM OPTIMIZATION 
1)  PRUNING
Mannila et al. proposed pruning techniques to reduce the 
size of candidate set [9] as an optimization. Pruning strategy 
for the algorithm is based on a characteristic: a frequent item-
set is a set where all its subsets are frequent.  
2)  DYNAMIC HASHING AND PRUNING
Park et al. suggested a hash based algorithm (DHP), which 
generates candidate large itemsets efficiently, while reducing 
the size of the transaction data base [10]. Number of candidate 
itemsets generated by DHP is smaller than that generated 
using existing algorithms, making it efficient for large 
itemsets, specifically for the large 2-itemsets. 
3)  PARALLEL DATA MINING 
Park et al., extended DHP into a parallel data mining 
algorithm, facilitating parallel generation of the candidate 
itemsets and parallel determination of large itemsets [11]. 
4)  TRANSACTION REDUCTION 
Agrawal et al. proposed this algorithm to reduce the 
number of transactions scanned in the future iterations [12, 
13]. A transaction that does not contain any frequent k-
itemsets can not contain any frequent (K +1) itemsets, hence 
can be removed.  
5)  PARTITION 
Savasere et al. designed a Partition-based highly parallel 
algorithm [14], which logically divides the data base into 
several disjoint blocks. Each considered separately a sub-
block and it generates all the frequent sets. Sub-block size is 
chosen to allow each sub-block be placed in the main 
memory, each stage scanned only once.  
6)  SAMPLING 
A subset of the sample is taken for mining in the sampling 
based mining techniques. Toivonen further developed this 
[15], which significantly reduced the I /O costs, often with 
inaccurate results and distortions in the data. 
7)  DYNAMIC ITEMSET COUNTING
Dynamic itemset counting technology [16] proposed by 
Brin et al. marks the starting point of the database divided into 
blocks. The results of algorithm need fewer database scans 
than Apriori. ECLAT (Equivalence CLASS Transformation) 
algorithm developed by Zaki [17] is used to mine efficiently 
using vertical data formats. 
8)  VERTICAL FORMAT DATA MINING 
Apriori algorithm mines frequent patterns from a horizontal 
data format which represents the items categorized into 
particular transactions, where vertical data format represents 
data as transactions categorized into particular items. So for a 
particular item, there is a set of corresponding transaction ids. 
Instead of horizontal data format, Apriori can be extended to 
use vertical data format for efficient mining. 
Vertical format data mining only has to parse the dataset 
once to get the itemsets. For the itemset generation from the 
2
nd
 itemset it only needs to refer the previous itemset. So this 
eliminates the need to parse through the dataset each time to 
count the frequency of itemsets, for each round. Hence 
vertical format mining is more efficient than its horizontal 
from, which has motivated general purpose algorithms such as 
VIPER [18], based on the vertical format representation. 
III. HORIZONTAL FORMAT DATA MINING WITH EXTENDED 
BITMAPS
We propose the algorithm 'Horizontal Format Data Mining 
with Extended Bitmaps', for mining a data set in a horizontal 
format, by converting it to a data structure of vertical format 
along with bitmaps indices to store the itemset data.  
?. PARSING THE DATASET TO CREATE 1ST ITEMSET AND BITMAPS
Here, the first and only parsing of the dataset is done. 
1. Creating the master array. 
2. Creating bitmaps relevant to each item in master 
array 
2010 International Conference of Soft Computing and Pattern Recognition 221
a – masterArray; in – Item n; b – associatedItemArray; c – BitSet 
Fig. 1: Main data storage structure of the extended vertical data mining. 
As in Fig. 1, masterArray is an array of Item objects of all 
items in the dataset. Under each item, there is an array of other 
items occurring together with it in transactions which we call 
from, which we refer from now on as AssociatedItems. Under 
each AssociatedItem, we store a bit vector indicating the 
presence of the AssociatedItem for every transaction where 
the Item is found. Basically length of bit vectors give the 
number of transactions that its root Item in masterArray is 
found. If the AssociatedItem is in the transaction, bit vector 
value for that transaction will be 1 (true), otherwise 0 (false). 
B. REMOVE REDUNDANT ASSOCIATED ITEMS.
Here for better memory utilisation, the redundant duplicate 
mappings (AssociatedItems) are pruned from the main data 
structure such as, Item i1 mapped to AssociatedItem i2 and 
Item i2 mapped to AssociatedItem i1 in the masterArray. 
C. PRUNING ITEMSETS ACCORDING TO APRIORI PROPERTY.
Association mining is carried out solely in this step, in three 
major phases. The core logic of Apriori property is used, but 
implementation is done using the manipulation of bit vectors.   
1st frequency itemset extraction - The items not satisfying 
the minimum support are removed from the masterArray [1]. 
2nd frequency itemset extraction - Here we iterate through 
the associatedItemArrays for each Item in masterArray. For 
each AssociatedItem, the cardinality of its bit vector is 
compared with the minimum support value. AssociatedItems 
having less than the minimum support are removed. 
Extracting frequency itemsets above two - For mining 
frequency item sets of three or above, we start intersecting bit 
vectors of AssociatedItems, compare result bit vectors' 
cardinality with minimum support value, and remove the 
AssociatedItems as Apriori property suggests. This will take 
place for increasing frequency itemset values and stop when 
no more frequent item sets are present. In this recurring step, 
two main data structures [Fig. 2, Fig. 3] are used. 
Fig. 2: Pattern Structure 
Pattern structure Fig. 2, is used to store each resultant bit 
vector from intersection of bit vectors and the itemset ids for 
those intersected bit vectors' belonging items. As Pattern is 
also a representation of item sets, we can use it as a frequent 
itemset itself. 
Fig. 3: Candidates Structure 
Candidates structure Fig. 3, is used to store the frequent 
item sets of each level of itemset size. Here each candidate 
level is an array of patterns. Each level indicates the frequency 
itemset level, hence frequent itemsets of length one will be 
under level one, frequent itemsets of length two will be under 
level two and so on. Once the above steps are completed, the 
frequent item sets are retrieved by iterating through the 
candidate structure. 
IV. EXPERIMENTAL STUDY
The algorithm was implemented and benchmarked in a 
system with 2 GB memory and 2.5 Ghz core 2 Duo Processor, 
against an implementation of Apriori algorithm for chosen 
datasets [19], along with different values of minimum support, 
as both the algorithms mine the datasets in horizontal form.  
TABLE II: T= 10; I = 4; D = 10K 
Minimum 
Support (%) 
T10I4D10K 
Apriori (sec) HFDM-EB (sec) 
0.75 1934.4 10.6 
1 1365.4 10.5 
2 238.4 10.4 
5 1.6 9.6 
FIG. 4: T= 10; I = 4; D = 10K 
TABLE III: T = 10; I = 4; D = 100K 
Minimum 
Support (%) 
T10I4D100K 
Apriori (sec) HFDM-EB (sec) 
0.75 21039.1 134.2 
1 12600.5 132.1 
2 2365.1 131.7 
5 1.3 132.1 
222 2010 International Conference of Soft Computing and Pattern Recognition
FIG. 5: T= 10; I = 4; D = 100K 
TABLE IV: T = 40; I = 10; D = 100K 
Minimum 
Support (%) 
T40I10D100K 
Apriori (sec) HFDM-EB (sec) 
5 232 10.9 
10 19 10.8 
20 0.5 10.6 
40 0.3 10.4 
FIG. 6: T = 40; I = 10; D = 100K 
Table II, Table III, and Table IV compare the performance 
of  the algorithms, where the respective graphs are shown in 
Fig. 4, Fig. 5, and Fig. 6. In most of the cases the proposed 
algorithm showed a significant improvement over the Apriori, 
while the Apriori implementation was more efficient at high 
support levels. Here the results of our algorithm are dominated 
by the time to build the bitmap and not time taken to mine it. 
The time taken to build the bitmap is independent of the 
number of frequent item sets. But Apriori's time drops 
drastically when the number of frequent items is low. Thus 
Apriori was seen having a higher performance at high support 
levels where number of frequent item sets found is low. 
V. CONCLUSION AND FUTURE WORK
In this paper we have proposed an efficient algorithm for 
Association Rule Mining, which recovers the associations as 
the Apriori and the other existing algorithms do. We 
implemented it in Java, which may be more efficient, if 
implemented in C or a lower level language.  
For each data item, a bitmap is created for each associated 
item. If there are n associated items for a data item, then the 
number of candidate sets generated is n(n-1). So there can be 
redundant bitmaps created for the same data item pairs. 
Currently redundant pairs are pruned after creating the vertical 
format. But, if there is a dynamic pruning mechanism to prune 
redundant data item pairs while creating the vertical format 
memory can be well optimized.  
The algorithm lends well to Map Reduce like distributed 
data mining since mining of each data item is independent of 
others. So this algorithm can be enhanced to work in a 
distributed environment with or without a shared memory. 
For some larger data sets that are having many items per 
transaction, the algorithm fails to withstand due to utilizing 
prohibitive amount of memory. It can be mitigated by using 
compressed bitmap [20] implementation instead of plain 
bitmaps, so the memory is utilized better. 
REFERENCES
[1] Jiawei Han and Micheline Kamber. “Data Mining, Concepts and 
Techniques,” 2nd Edition, 2006.  
[2] Abraham Silberschatz, Henry F.Korth, and S.Sudarshan. “Database 
System Concepts,” 5th Edition. McGraw-Hill, Inc., New York, San Francisco, 
Washington, DC, 2005. 
[3] Jaideep Srivastava , Robert Cooley , Mukund Deshpande, Pang-Ning 
Tan. “Web Usage Mining: Discovery and Applications of Usage,” 
Department of Computer Science and Engineering, University of Minnesota.  
ACM SIGKDD Explorations Newsletter, v.1 n.2, January 2000.   
[4] Wenke Lee , Salvatore J. Stolfo, "Data mining approaches for intrusion 
detection," in Proceedings of the 7th conference on USENIX Security 
Symposium, p.6-6, January 26-29, 1998, San Antonio, Texas. 
[5] Elisabeth Georgii, Lothar Richter, Ulrich Rückert and Stefan Kramer. 
“Analyzing microarray data using quantitative association rules,” 
Bioinformatics 2005, 21(Suppl 2):ii123-ii129. 
[6] Ioannidis, John P. A. (August 30, 2005). "Why Most Published 
Research Findings Are False". PLoS Medicine (San Francisco: Public Library 
of Science) 2 (8). doi:10.1371/journal.pmed.0020124. ISSN 1549-1277.  
[7] Behrouz Minaei-Bidgoli1, William F. Punch. “Using Genetic 
Algorithms for Data Mining - Optimization in an Educational Web-based 
System,” Genetic Algorithms Research and Applications Group (GARAGe). 
Department of Computer Science & Engineering. Michigan State University. 
Online: www.lon-capa.org/papers/v90-gapaper.pdf  [Accessed: 25th 
December, 2009] 
[8] Wei-Qing Sun, Cheng-Min Wang, Tie-Yan Zhang, and Yan Zhang. 
2009. “Transaction-item association matrix-based frequent pattern network 
mining algorithm in large-scale transaction database.” W. Trans. on Comp. 8, 
8 (Aug. 2009), 1327-1336. 
[9] H. Mannila, H. Toivonen, and A. Verkamo. Efficient algorithm for 
discovering association rules. AAAI Workshop on Knowledge Discovery in 
Databases, pp 181-192, Jul. 1994. 
[10] J.S. Park, M.S. Chen, and PS Yu. “An effective hash-based algorithm 
for mining association rules”. In Proceedings of ACM SIGMOD International 
Conference on Management of Data, pp 175-186, May 1995. 
[11] J.S. Park, M.S. Chen, and P.S. Yu. “Efficient parallel data mining of 
association rules,” 4th International Conference on Information and 
Knowledge Management, Baltimore, Maryland, Nov. 1995. 
[12] R. Agrawal, and R. Srikant. “Fast algorithms for mining association 
rules,” in Proceedings of. 1994 Int. Conf. Very Large Databases (VLDB'94),
Sep. 1994. 
[13] J. Han and Y. Fu. “Discovery of multiple-level association rules from 
large databases,” in Proceedings of. Int. Conf. Very Large Databases 
(VLDB'95), pp 402-431, Sep. 1995. 
[14] A. Savasere, E. Omiecinski, and S. Navathe. “An efficient algorithm 
for mining association rules in large databases,” in Proceedings of the 21st 
International Conference on Very Large Database, pp 432-443, Sep. 1995. 
[15] H. Toivonen. “Sampling large databases for association rules,” in 
Proceedings of the 22nd International Conference on Very Large Database,
Bombay, India, pp 134-145, Sep. 1996. 
[16] S. Brin, R. Motwani, JD Ullman, and S. Tsur. “Dynamic itemset 
counting and implication rules for market basket data,” in Proceedings of 
ACM SIGMOD International Conference On the Management of Data, pp 
255-264, May 1997. 
[17] M. J. Zaki. “Scalable algorithms for association mining,” IEEE Trans. 
Knowledge and Data Engineering, 12:372–390, 2000. 
[18] Shenoy, P. and Haritsa, J. and Sudarshan, S. and Bhalotia, G. and 
Bawa, M. and Shah, D. (2000) “VIPER: A Vertical Approach to Mining 
Association Rules.” In: ACM SIGMOD International Conference on 
Management of Data (SIGMOD 2000), May 16-18, 2000 , Dallas, Texas. 
[19] Frequent Itemset Mining Dataset Repository. [Online]. Available: 
http://fimi.cs.helsinki.fi/data/ [Accessed: 25th December, 2009] 
[20] Kesheng Wu, Ekow Otoo and Arie Shoshani. “Optimizing Bitmap 
Indices with Efficient Compression.” ACM Transactions on Database 
Systems, v31, pages 1 - 38, March, 2006. 
2010 International Conference of Soft Computing and Pattern Recognition 223
