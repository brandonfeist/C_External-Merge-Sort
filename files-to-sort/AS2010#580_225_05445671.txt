   Automatic Topic(s) Identification from Learning material: An Ontological Approach 
 
Sonal Jain/ Assistant Professor 
GLS Institute of Computer Technology, 
Ahmedabad, India. 
sonalsethia@rediffmail.com   
 
Dr. Jyoti Pareek/ Reader 
Department of Computer Science,  
Rollwala computer center, 
Gujarat University 
 Ahmedabad, India. 
drjyotipareek@yahoo.com
 
Abstract— The ability to judge the relevance of topics 
and related sources in information-rich environments is 
a key to success when scanning online learning 
environments. A Learner may be looking for learning 
materials explaining given topic or exercises on the 
topic. Any learning material may cover multiple topics 
related to multiple subject domains. This paper presents 
ontological approach for identifying major topics, 
covered in the learning material. Along with the topics, 
the subject and discipline to which those topics belongs 
to and relevance of the topic in the learning material as 
compared to other topics present in the same document 
are also discovered. Domain ontology is developed to 
retrieve the topics covered in the document. We present 
an evaluation against a manually categorized topics as 
well as author’s judgment of relevance of the topics 
discovered by our system. Evaluation results show that 
the technique presented by us is effective in identifying 
topics and subtopics covered in a single learning 
document.  This work is part of research conducted on 
developing a web service for automatic semantic 
annotation of learning material.  
Keywords-Topic identification, ontology, semantic annotation 
I.  INTRODUCTION  
With energetic development of the Internet, especially the 
web page interaction technology, online learning object 
repositories have become more and more realistic and 
popular in the past ten years. The reach of Internet to 
student community has increased its acceptability. Learning 
object repositories are essentially storage and retrieval 
systems for learning objects where the learning objects are 
organized so as to be easily searchable. These repositories 
enable sharing and reuse of learning materials by different 
users. Different learners have different requirements and 
therefore require different learning contents. One of the key 
issues in using learning objects is their identification by 
search engines. This is usually facilitated by assigning 
descriptive metadata to the learning object. These learning 
object metadata is stored along with learning objects in 
learning object repository. This information background is 
needed for querying services to perform accurate queries for 
learning object retrieval. The work presented in this paper is 
part of Development of web service for automatic semantic 
annotation of learning material. With respect to 
personalization and adaptability in learning systems, we 
have identified certain essential metadata elements and 
topic(s) identification is one of them. A learning material 
may contain topics and terms related to multiple subjects 
and domains. Thus topic(s) identification along with subject 
and relevance of each topic will assist in effective search 
and recommendation for a learner for a personalized 
learning management system. A topic refers to a particular 
subject that is discussed about in a piece of text [1]. It is the 
common theme that semantically links together pieces of 
text that may be physically disconnected [2]. Identification 
of topics covered in the document and subject domain of 
important terms covered in the document will help in 
meaningful and need based retrieval of document by a 
learner. Researchers have indicated that automatic metadata 
generation is more efficient, less costly, and more consistent 
than human-oriented processing [3][4][5][6][7][8][9]. This 
paper showcases our work of automatic retrieval of topics, 
subtopics and relevance of each topic. We view learning 
material as explanation of concepts. In the first section, this 
paper discusses related work done by researchers in topic 
identification. In the later sections our approach is discussed, 
followed by evaluation, results and conclusion.  
II. RELATED WORK 
Researchers have tried to generate topics from different 
sources of information like research papers, news articles, 
multiparty dialogues etc. Majority of them are focused on 
collection of material from various sources and identifying 
topics from them to categorize them. Most of the approachs 
of topic identification are based on natural language 
processing for named entity extraction, association rule data 
mining [10], clustering of association rules [10], topic 
keyword clusters using the k-nearest neighbor graph and the 
keyword clustering function [11] and information retrieval 
techniques. Machine learning techniques have been used to 
combine linguistic, statistical and position information to 
identify topic levels for headline in the text [12].  Shuhua 
Liu has proposed generation of topics by partitioning the 
text [13]. Further partitions are made to form smaller groups 
of text segments that represent sub-topics, which may 
contain one or a few paragraphs.   Miura Nakayama has 
proposed Random Projection, one of the dimension 
reduction techniques. He has examined and verified a topic 
word model in which topic can be identified by means of  
2010 Second International Conference on Computer Engineering and Applications
978-0-7695-3982-9/10 $26.00 © 2010 IEEE
DOI 10.1109/ICCEA.2010.221
358
word distribution under same author [17].  Wordnet and 
Latent Semantic Indexing based model for dimension 
reduction has been proposed by Václav Snášel et. al [20]. 
Kino Coursey et. al. has presented an unsupervised system 
for automatic topic identification, which relies on a biased 
graph centrality algorithm applied on a graph built from 
Wikipedia [21]. 
III. OUR APPRAOCH 
Our observation of learning material states that it can 
cover concepts belonging to multiple subjects/topics/areas. 
For example, a document related to Computer Science 
subject may contain certain concepts which in turn are 
related with the Mathematics, Statistics or Management. 
Numerous learning objects thus may belong to multiple 
disciplines and in turn multiple subjects. The novelty of our 
tool is that we have tried to identify concepts belonging to 
different topic or subject areas in same document.  With 
each identified topic and concepts covered under it, we have 
reported its relevance as compared to other topics present in 
the same document with the help of subject domain 
ontology. This identification will prove helpful to intelligent 
learning systems while presenting learning documents to 
learner according to his needs. For example, learning 
material explaining six sigma concepts will be related to 
Total Quality Management,  as well as Statistics as Six 
Sigma concept is based on Standard Deviation. A  student 
will not able to comprehend Six Sigma concept till he 
understands Standard Deviation. Thus Intelligent learning 
management system should make learner alert of the 
relation between concepts related to multiple subject areas.  
Similarly learning object explaining Deadlock detection will 
discuss about cycle in a graph which actually in detail will 
be enclosed under data structures subject. An intelligent 
eLearning system can search learning materials 
corresponding to the topics and its relevance extracted by 
our tool, (supplemented as part of metadata) and can present 
its suggestion to a learner.  
 
 
Figure 1.  Topic Extraction 
Our approach of Topic extraction employs a multistage 
process, first it identifies key concepts within a document, 
and then these terms are compared with terms present in 
domain ontology. In the next stage, topics/subtopics from 
domain ontology containing one or more terms as siblings 
are listed out. Finally the subjects and disciplines, to which 
retrieved topic belongs to are identified and displayed.  
A. Domain Ontology 
 
Ontology [19] is a specification of an abstract, simplified 
view of the world that we wish to represent for some 
purpose. This view is called conceptualization. Therefore, 
ontology defines a set of representational terms that 
typically include concepts and relations.  The system stores 
the domain knowledge of various disciplines and their 
subjects in the form of ontology. The domain ontology is an 
ontological structure of the topics and terms in a particular 
subject domain together with the relationships between 
those topics and terms. Figure 2 gives an overview of 
created domain ontology with multiple layers.  
 
 
 
Figure 2.  Subject Domain Ontology 
The top layer contains disciplines like Computer Science. 
Separate ontologies for other disciplines like Mathematics, 
Statistics, Management, Electronics etc are created.  The 
second layer contains Subjects under that discipline. The 
third layer contains the broad topics covered under that 
subject. Topics again can contain sub-topics. These 
subtopics may be again represented by collection of various 
terms. We have represented domain ontology using 
Ontology web language. We developed ontologies in the 
Protégé Ontology Editor [14] and used the Web Ontology 
Language (OWL) to express the ontologies (Protégé OWL).  
Figure 3 shows the snapshot of protégé editor.  
 
359
 
Figure 3.  Subject terms as entities in Protégé editor 
 
The development of the domain ontology incurs cost in 
terms of both time and manual effort. But we have observed 
that once it is developed, its presence will help in effective 
and standardized automatic generation of metadata and 
achieving higher precision level in the retrieval process. The 
benefits of educational use of ontologies have been 
identified by many researchers [15][16]. Many researchers 
have used these ontologies in various forms and for various 
purposes. For this study, we have chosen to annotate the 
learning material belonging to Computer Science discipline. 
We have build domain ontology for four subjects, Operating 
systems, Database management systems, Data Structures 
and Statistics. To evaluate the system, ontology 
corresponding to certain portion of Operations management 
subject is also developed. The concept and terms required to 
create domain ontology are identified through three different 
techniques (1) manually, through back-of-the-book index of 
some of the major texts (2) through a semi-automatic 
technique by parsing the Wikipedia pages and (3) 
Semantically parsing ebooks.  
 
B. Topic Extraction process 
 
This phase skims a document for noun phrases to 
understand the semantics of the document; we have used 
Stanford Parser [13] to grammatically tag the content 
extracted from html document. Some noise like stop words 
is filtered from the phrases generated. Once list of Noun 
phrases is finalized, for each generated Noun phrase a 
vector of formatting features applied on it is created.  If 
formatting feature is applied on the phrase, then certain 
weight is assigned to the phrase [18]. In the next step 
frequency of word and position of word in document are 
given importance and each noun phrase generated is ranked. 
The lists of the generated keyphrases are displayed for user 
approval. Figure 4 and 5 shows User interface for the topic 
extraction tool and Figure 6 shows the list of system 
generated key phrases ready for user approval.  Input to the 
system can be either html or docx file.  
 
 
Figure 4.  User Interface 
 
Figure 5.  Keyphrase Approval from user 
 
 
Figure 6.  Keyphrase comparison with domain ontology entities 
360
These approved key phrases are compared to the terms 
present in the domain ontologies. Collection of these terms 
represents certain topic/subtopic of some subject/domain to 
which it belongs. For each subtopics, topics, subject and 
discipline are retrieved. OWL Parser is developed to retrieve 
parent, child and siblings. Input to parser is collection of 
keyphrases. Parser parses owl file and retrieves 
topics/subtopics which contains collection of all or some of 
those keyphrases as siblings. Figure 6 shows the output. It 
has been observed while testing that most of the learning 
materials cover or use concepts from various subjects and 
domains. Thus it is important at this stage to figure out the 
topics which actually characterize the document. The 
relevance of document with respect to topic(s) /subtopic(s) 
enclosed in the learning material is calculated as Total 
number of its key terms identified under the topic/subtopic 
divided by Total number of key terms identified in the entire 
document. Document can be characterized using the topic 
with higher relevance. Figure 7 shows topics/subtopics and 
its relevance in the document.  
 
Figure 7.  Relevance of topic with respect to document content 
IV. EVALUATION :RELEVANT VS. EXTRACTED TOPICS 
In this study, we have randomly collected sample of 200 
learning material belonging to different subjects like 
Operating systems, Database management systems, Data 
and File structures, Statistics and Total quality management. 
The documents were processed by our tool and 
topics/subtopics along with their corresponding subject 
domains and significance pertaining to the document were 
extracted.  Evaluation of this study was done in two phases. 
In first phase subject experts were asked to list down the 
topics/subtopic(s) covered in the document. These 
topics/subtopic(s) were then compared with system 
generated output. Topics and subtopics exactly matching, 
partially matching and topics/subtopics that are not found 
are then counted. The result of the counting is as shown in 
figure 8.  
 
 
Figure 8.  Evaluation Results 
The measure used to evaluate the results was the F-score, 
defined as  
 
 
 
In this study, the main concern is that, how many of the 
suggested topics/subtopics are correct (precision) and how 
many of the manually assigned topics/subtopics are 
retrieved (recall). As the proportion of correctly suggested 
topics is considered equally important as the amount of total 
topics extracted by the system, ? was assigned the value 1, 
thus giving precision and recall equal weights.  
 
Precision = Number of topic(s)/subtopic(s) identified 
correctly by the system / Total topic(s)/subtopic(s) 
generated by the system. 
 
Recall is calculated as Number of topic(s)/subtopic(s) 
identified correctly by the system / Number of 
topic(s)/subtopic(s) identified by the authors.  
 
The results are shown in the next section. In the second 
phase, agreement of authors was sought for the 
topic(s)/subtopic(s) that were found by the system but were 
not listed out by authors in the first phase of evaluation.  
Average topics/subtopics extracted by the system is 6.05, 
while the average topics/subtopics per document listed by 
subject experts is 3.76.  Figure 9 shows the agree/disagree 
listing of the topics/subtopics extra found by the tool.  
 
 
Figure 9.  Sample result of second phase of evalution 
V. RESULTS 
We have evaluated our tool in two ways viz., Strict and 
Lenient. In strict evaluation, while calculating total number 
of topics/subtopics identified correctly by the system, we are 
considering partially matched topics/subtopics as 
topics/subtopics not found. In lenient evaluation, we are 
considering partially matched topics/subtopics as fully 
matched topics/subtopics. Table 1 shows precision, recall 
and F-score in both the evaluations.   
TABLE 1: EVALUATION RESULTS 
Strict Evaluation 
Precision Recall F-score 
0.5057541 0.8015656 0.63 
 
361
Lenient Evaluation 
Precision Recall F-score 
0.6273393 0.9609625 0.76 
In the evaluation of second phase, it was found that in 
82.3% cases authors have agreed with the system generated 
topics/subtopics which they missed out while listing down 
the topics.   
VI. CONCLUSION 
Thus, this paper showcases that ontological approach to 
identify topics from the learning document is an effective 
strategy. Precision and Recall of our results proves the 
effectiveness of the tool. Second phase of the evaluation 
also signifies that at times author is not able to identify the 
topics or a subtopic from a document if its coverage or 
relevance is very less in the document. But the system can 
identify those topics. Identification of the topic relevance 
can be used as a metadata which can prove useful in 
presenting the learning documents according to the needs of 
the learner by intelligent learning management systems.  
REFERENCES 
 
[1]  E. Hovy and C. Lin, “Automated Text Summarization in 
SUMMARIST”, in Mani and Maybury (eds), Advances in Automatic Text 
Summarization, MIT Press, 1999. 
 
[2] G. Salton, J. A. C. Buckley, A. Singhal , “Automatic Analysis, theme 
generation and summarization of machine readable texts”, Science, 264, 
pp. 1421-1426, 1994 
 
[3] Ochoa, X., Cardinaels, K., Meire, M., & Duval, E.,  Frameworks for the 
Automatic Indexation of Learning Management Systems Content into 
Learning Object Repositories. World Conference on Educational 
Multimedia, Hypermedia & Telecommunications, EDMedia-2005, Montreal, 
Canada, pp. 1407-1414, 2005. 
 
[4] M. Meire, X. Ochoa, and E. Duval , SAmgI: Automatic Metadata 
Generation v2.0, Proceedings of World Conference on Educational 
Multimedia, Hypermedia and Telecommunications, pp. 1195-1204,2005. 
 
[5] Friesen, N. International OM survey: Report. ISO/IEC JTCI/SC36 
subcommittee, Available at  
http://mdlet.jtc1sc36.org/doc/SC36_WG4_N0109.pdf, 2004. 
 
[6] Downes, S. Resource Profiles. Journal of Interactive Media in 
Education, Special Issue on the Educational Semantic Web, vol. 5, 2004. 
 
[7] Duval, E., Hodgins, W. Metadata matters. Proceedings of International 
Conference on Metadata and Dublin Core Specification, DC-2004, 
Shanghai, China, 2004. 
 
[8] Simon, B., Dolog, P., Miklos, Z., Olmedilla, D., Michael, S. 
Conceptualizing Smart Spaces for Learning. Journal of Interactive Media 
in Education, Special Issue on the Educational Semantic Web, Vol. 3, pp. 
22-26, 2004. 
 
[9] E. Duval, N. Smith, and M. Van Coillie , Application profiles for 
learning, Proceedings of the IEEE International Conference on Advanced 
Learning Technologies (ICALT), pp. 242-246, 2006. 
 
[10] Chris Clifton , Robert Cooley , Jason Rennie, TopCat: Data Mining for 
Topic Identification in a Text Corpus, IEEE Transactions on Knowledge 
and Data Engineering, v.16 n.8, p.949-964, August 2004. 
  
[11] Hsi-Cheng Chang, Chiun-Chieh Hsu, "Using Topic Keyword Clusters 
for Automatic Document Clustering," icita, vol. 1, pp.419-424, Third 
International Conference on Information Technology and Applications 
(ICITA'05) Volume 1, 2005 
 
 
[12] Rui-Chao Wang, Nicola Stokes, William Doran, John Dunnion, Joe 
Carthy, A Hybrid Statistical/Linguistic Approach To Headline Generation, 
Intelligent Information Retrieval Group, Department of Computer Science, 
University College Dublin, Ireland , Proceedings of the Fourth 
International Conference on Machine Learning and Cybernetics, 
Guangzhou,2005.  
 
[13] Shuhua Liu, Enhancing E-Business-Intelligence-Service: A Topic-
Guided Text Summarization Framework, Proceedings of the Seventh IEEE 
International Conference on E-Commerce Technology, 2005. 
 
[14] Knublauch, H., Fergerson, R. W., Fridman Noy, N., & Musen, M. A.  
The Protégé OWL plugin: An open development environment for semantic 
web applications. Proceedings of the International Semantic Web 
Conference, pp.229-243, Hiroshima, Japan.,2004 
 
[15] A. Mitrovic and V. Devedzic , A Model of Multitutor Ontology-Based 
Learning Environments, In Proceedings of ICCE Workshop on Concepts 
and Ontologies in Web-based Educational Systems, ISSN: 0926- 4515, 
New Zealand,2002. 
 
[16] Aroyo, L., Dicheva, D. and Cristea, A., Ontological support for web 
courseware authoring , Proceedings of the 6th International Conference on 
Intelligent Tutoring Systems ITS'2002, Biarritz, France and San Sebastian, 
Spain, pp.270—280, 2002. 
 
[17] Nakayama, M.   Miura, T , Identifying Topics by using Word 
Distribution,  Communications, Computers and Signal Processing, PacRim 
2007. IEEE Pacific Rim Conference  pp. 245-248, ISBN: 978-1-4244-
1189-4, 2007 
 
[18] Sonal Jain, Jyoti Pareek, "KeyPhrase Extraction Tool (KET) for 
Semantic Metadata Annotation of Learning Materials," icsps, pp.625-628, 
IEEE digital library, International Conference on Signal Processing 
Systems, 2009 
 
[19] B. Chandrasekaran, J. R. Josephson, and V. R. Benjamins, What are 
ontologies, and why do we need them? IEEE Intelligent Systems, 
14(1):20–26, 1999. 
 
[20]  Václav Snášel, Pavel Moravec, Jaroslav Pokorný , "Using Semi-
discrete Decomposition for Topic Identification," isda, vol. 1, pp.415-420, 
2008 Eighth International Conference on Intelligent Systems Design and 
Applications, 2008 
 
[21] Kino Coursey, Rada Mihalcea, William Moen, International 
Conference On Computational Linguistics ,Proceedings of the Thirteenth 
Conference on Computational Natural Language Learning ,pp 210-218, 
ISBN:978-1-932432-29-9, published by Association for Computational 
Linguistics  ,2009 
 
 
 
 
362
