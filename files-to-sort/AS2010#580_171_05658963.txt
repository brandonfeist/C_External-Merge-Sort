Quantitative Association Rules Mining Method Based 
on Trapezium Cloud Model
Wang Zhao-hong 
Department of Computer Science and Technology 
Weifang University 
Weifang, China 
wangzhhwfxy@163.com 
 
 
Abstract—The quantitative association rules mining method is 
difficult for their values are too large. The usual means is 
dividing quantitative Data to discrete conception. The trapezium 
Cloud model combines ambiguity and randomness organically to 
fit the real world objectively, divide quantitative Data with 
trapezium Cloud model to create concepts, the concept cluster 
within one class, and separated with each other. So the 
quantitative Data can be transforms to Boolean data well, the 
Boolean data can be mined by the mature Boolean association 
rules mining method. 
Keywords- Quantitative Association Rules; Trapezium-Cloud 
Model; Conception Division; Frequent Item Sets 
I. INTRODUCTION 
Association rule mining refers mainly to get the 
knowledge such as "customers bought tea. also purchased the 
coffee," which meet the minimum support and the minimum 
confidence. At present association rules can be divided into 
two types: Boolean association rules and quantitative 
association rules, and most of the research are focused on the 
Boolean association rules research. 
First, give the formal description of association rules 
mining: Suppose I = (i1, i2... im) is a collection of m different 
items, T = (t1, t2... tn) is a transaction database, tj is a group of 
items of I set, tj?I. Each transaction with a unique identifier 
TID linked. If X is a subset of I with X?tj, we say that a 
transaction contains X. An association rule is a "X?Y" 
implication, in which X?I, Y?I, and X?Y=?.  
Definition 1: If the ratio of transaction T contains X?Y is 
Sup, the association rules X?Y in T has a support degree Sup. 
                      
%100
n
y)support(xSup ×= ?
                         (1) 
Support (X?Y) stands for the number which support X?
Y transactions, n stands for the total number of transactions.  
Definition 2: If the ratio transaction T contains X also contains 
Y is Conf, then the confidence level of association rules X?Y 
in T is Conf. 
 
            
%100
support(x)
y)support(xConf ×= ?
             (2) 
Association rules are called the strong association rules if 
they meet the minimum support and minimum confidence [1] 
[2]. 
Definition 3: Boolean association rules are got from the 
transaction database whose items values are 0 or 1.  
Mining process is as follows: 
(1) Identify the transaction database T in all the non-empty 
sets which meet user-defined minimum support degree, if the 
item sets meet the minimum support degree which are called 
frequent item sets. 
(2) Using frequent item sets generate the required 
association rules. For each frequent item sets A, identify all 
non-empty subset A?a, if 
confmin
support(a)
support(A) ?
, then 
generated association rules a ?(A-a) [1][2]. 
The above methods only consider whether the transaction 
contains the item, regardless of the number of item, for 
example, they only consider whether a particular transaction 
contains tea, regardless of a pound of tea or five pounds of tea; 
a lot of useful information are lost[3][4].  
In applications, quantitative attribute Data quantity 
gathered in a certain range may contain very useful knowledge. 
Upper and lower limits in the expression is too rigid, for 
example: for <age, 18, 38>, 16-17 years old, which certainly 
are not divided into this interval, but if we use "young" 
concept to describe the age of people, 16-17 years old, which 
can be classified into the concept or not depending on the 
Individuals, or they belong to the concept partly. The method 
mentioned in this paper can identify association rules from the 
quantitative Data. Here, we first look at the mathematical 
theory. 
II. CLOUD MODEL 
Cloud model is a qualitative and quantitative conversion 
model; it combines ambiguity and randomness organically. 
This work was supported by the Scientific and technological
development projects of Shandong Province (Grant Nos.
2008GG30001030), Natural Science Fund Project of Weifang 
University (Grant Nos. 2008Z08)
 
978-1-4244-6977-2/10/$26.00 ©2010 IEEE
Set U is a mathematical domain U={x}, T is the language 
value associated with the U. ?T (x) is a stable tendency 
random number which expressed the elements x subordination 
of T concept, subordination’s distribution in the domain is 
known as the cloud [5][6]. 
Cloud mathematical expected curve is its subordination 
curves from the view of fuzzy set theory. However, 
"thickness" of the curve is uneven, waist is the most scattered, 
the top and bottom of the curve are convergent, cloud's "thick" 
reflects the subordination degree randomness, near to or away 
from the concept center have smaller subordination 
randomness, while concept center have the largest 
subordination randomness, which is consistent with people's 
subjective feelings. 
The digital features of the normal cloud characterized by 
three values with the expectation Ex, entropy En, excess 
entropy He), the expected value Ex: the center value of the 
concept domain, is the most representative qualitative value of 
the concept, it should be 100% belongs to the concept; entropy 
En: is a qualitative measure of the concept’s ambiguity, 
reflecting the accepted range values of the concept domain; 
hyper entropy He: can be described as entropy En of entropy, 
reflecting the degree of dispersion of the cloud droplets. The 
normal cloud is the most important cloud model, because 
various branches of the social and natural sciences have proved 
the normal distribution’s universality. The equation of normal 
cloud curve: 
 
e 2En 2
Ex)(? 2
)(MEC A
?
?
=?
 (3) 
Expectation curve is a normal curve, for a qualitative 
knowledge, the elements outside the Ex ± 3En in its 
corresponding cloud model all can be ignored, because it has 
been proved that approximately 99.74% elements of model fall 
into the range of Ex ± 3En by the mathematical characteristics 
of normal distribution. Extending the normal cloud model to 
get trapezoidal cloud model, trapezoidal cloud model can be 
expressed by six values, they are the expected number: Ex1 and 
Ex2, entropy: En1 and En2, hyper entropy: He1 and He2. 
 
Figure 1.  The digital features of Trapezoidal cloud model  
Trapezoidal cloud curve equations are determined by the 
expectation and the Entropy are following: 
       
( )
( )
( )
A
A
A
2(x Ex1)
2 1 3 1 12En1
Ex1<=x<=Ex2
2(x Ex2)
2 2 2 3 22En2
MEC (x)
MEC (x) = 1
MEC (x)
e
e
Ex En x Ex
Ex x Ex En
?
?
? <= <=
?
?
<= <= +
=
=       (4) 
Clearly, the left and the right half-cloud expectation curve 
is a normal curve.  
It can complete the qualitative and quantitative transform 
more accurately, if there is a range belongs to the concept 
totally, then it can be expressed by the upper edge of 
Trapezoid, if only one value belongs to the concept totally, 
then the upper edge of Trapezoid degenerate to a point, 
trapezoidal cloud model also degenerated into the normal 
cloud model. He1 and He2 can have different values, and thus 
the concept of the border on behalf of different fuzzy situation, 
when the He1 and He2 all degenerate to 0, trapezoidal cloud 
model expressed a concept with accurate border subordination, 
when one of the He1 or He2 degenerate to 0, which expressed 
a concept with one r accurate border subordination and one 
vague border subordination, so trapezoidal cloud model has a 
better generality. 
III. DIVIDING QUANTITATIVE DATA TO CONCEPTS USING 
TRAPEZOIDAL CLOUD MODEL 
A. Cloud Transform 
Any function can be decomposed into cloud-based 
superposition with allowed error range, which is Cloud 
Transform. The equation is: 
)()(
1
xfxg j
m
j
jc?
=
?
 
))()(0(
1
?<?< ?
=
xfxg jj
m
j
c
     (5) 
g(x): data distribution function 
fj(x): cloud-based expectations function  
cj :coefficients 
m: the number of superimposed cloud,  
?: user-defined maximum error 
From the concept of clouds: in the domain the element’s 
subordination to the concept has statistical and random 
properties. In addition, the high-frequency elements’ 
contributions to the concept are higher than the low-frequency 
elements. That is the reason to use probability density function 
of data distribution to get the concept set, so the concept 
division algorithms can be done. 
According to the definition of cloud transform, the 
quantitative attribute’s domain dividing into m concepts can 
evolve to a problem to get answers from the formula:  
? +=
=
m
j
ijjjjjjij HeHeEnEnExExfxg c
1
)2,1,2,1,2,1()( ?
I.e. to 
get Ex1j, Ex2j, En1j, En2j, He1j, He2j and cj for each cloud 
concept, the quantitative attribute domain is divided into a 
number of concepts by using cloud model, the data in each 
concept aggregate, and the data between different concepts 
separated [7][8]. 
B. Concept division  algorithm 
Cloud transform recover the data distribution concepts 
from a large number of property values, the conversion is from 
quantitative Data to qualitative concept, is a clustering 
problem essentially. Local peak of the histogram is that the 
data aggregation part, taking it as a concept center is 
reasonable, the higher the peak, indicating more data 
convergence there, deal with it with priority. The concept 
division algorithm is:  
Algorithm 1: Concept division algorithm  
Input: the domain of quantitative attributes that need the 
concept division, the overall error threshold ?, and the peak 
height error threshold ?y, the length error ?x between 
trapezoidal top edge and the minimum value. 
Output: m concepts and the corresponding digital features 
of attribute i.  
{ 
(1) Count the each possible values x of attribute i and get  
the actual data distribution function g (x); 
(2)  j=0; 
(3) Clouds=??g’(x)=g(x); 
(4)  while max(g’(x))>? 
(5)     Exj=Find_Ex(g’(x)); 
(6)     Ex1j=search1(g’(x),?y,?x); 
(7)     Ex2j=search2(g’(x),?y,?x); 
(8)     En1j=Find_En(cj,Ex1j,?); 
(9)     En2j=Find_En(cj,Ex2j,?); 
(10)   gj(x)=cj*Cloud(Ex1,En1j,Ex2j,En2j); 
(11)   g’(x)=g’(x)-gj(x); 
(12)   j=j+1; 
(13) endwhile 
(14) for j=0 to m-1 do 
    (15) Clouds(Ex1j,=Ex2j,En1j,En2j,He1j,He2j)= 
Calculate_He(gj(x), g’(x),Cloud(Ex1j,Ex2j,En1j,En2j)); 
(16) endfor 
} 
In Step 1, using statistical methods to get the actual data 
distribution function g(x); Step 2, 3 does variable 
initialization; Step 4 the division of the process is ended, if the 
error limit less than a given error, Step 5 Search for the peak 
value of cj of property in the data distribution function g(x), 
and its corresponding value x is defined as the cloud model 
center (expectation); Step 6, 7 search approximate horizon line 
near the peak (within the error limit threshold ?y), if the width 
is greater than the minimum width of the threshold value ?x, 
where were identified as uniformly distributed, the two 
endpoints of the line are recorded as the trapezoidal top edge 
endpoints Ex1j, Ex2j; otherwise get the trapezoidal top edge 
endpoints are equal to the peak point value Ex1j=Ex2j=Exj, 
trapezoidal cloud degenerated into the normal cloud. 
Trapezoidal cloud height coefficient is the function value of 
the Ex1j or Ex2j. The step 8, 9 calculate cloud model entropy 
En1j and En2j to fit g (x) for the half-liter cloud with Ex1j, 
half-falling cloud with Ex2j. to get En1j searching left area of 
the cloud model with Ex1j, to get En2j searching right area of 
the cloud model with Ex2j, the entropy value increase from 0 
step from the smaller value until the threshold ? is greater than 
the difference between the half-normal cloud value and 
distribution histogram value; the step 10 calculate distribution 
function of the corresponding Trapezoidal; step 11 use the 
original data minus the known distribution function of 
trapezoidal cloud model data distribution to get the new data 
distribution function g'(x); Repeat step 4 to 12 until the peak 
value is less than the error threshold. Step 14, 15, 16 
determine half hyper entropy of all cloud model with the 
residuals of distribution histogram. 
IV. MINING ASSOCIATION RULES FROM CONCEPTS 
MAPPED FROM QUANTITATIVE DATA 
Dividing quantitative attributes into several concepts using 
concept division algorithm, mapping the attributes data to the 
corresponding concept, sometimes, one attribute value may be 
mapped to different concepts, then mapping it to the concept 
that the attribute data has the largest subordination. 
in the Boolean association rules I={i1,i2?…? im}, the 
original meaning of ii is to buy not to buy tea, through the 
concept division algorithm a quantitative attribute value is 
extended to buy a lot of tea, buy some tea, buy little tea. Of 
course, every transaction belong to one of these situations 
only, problems can be solved by the Boolean association rules 
mining, using the famous Apriori frequent item set algorithm 
and the non-frequent item sets of FP-tree algorithm can carry 
out association rules mining[9][10]. 
V. EXPERIMENTAL ANALYSIS 
Take two typical supermarket retail food rice and millet 
 as example, the majority data scattered between 0.5-5kg, 
Extract 20000 transaction records randomly as experiment 
data, use concept division algorithm with error threshold ?=0. 
5, and the peak height error threshold ?y=0.03, the length 
threshold error between trapezoidal top edge ?x =0.03. Then 
every transaction record data are divided into buy a lot of rice 
(millet), buy some rice (millet), buy little rice (millet), the 
number of the concepts can be more or less according to the 
threshold values.  
Using the famous Apriori algorithm to mine the database 
with the concepts mapped from the quantitative data, 
MinSup=30%,  MinConf=80%. We get the following results. 
< a lot of rice> ?<a little of millet >,<a little of bean > 
< some rice> ?<some millet >,<some bean > 
to improve its accuracy further, reduce error thresholds, and 
lower the minimum support for association rules and, but this 
will take more time, the minimum support degree and the 
minimum confidence degree shouldn’t too small, so do the 
error thresholds, if so, the association rules does not make 
sense, the minimum support degree and the minimum 
confidence degree are given by the experts in the field. 
VI. CONCLUSION 
In this paper, we propose a new method to mining 
association rules from quantitative data based on the 
trapezoidal cloud model, which first take the original data 
distribution in the database into account, and then use the 
trapezoidal cloud model which combines ambiguity, 
randomness and uncertainty organically, and transforms the 
quantitative concept and qualitative data each other, in the 
conversion take account of the basic characteristics of human 
behavior fully, because of the presence of randomness, the 
same quantity data may belongs to different concept. for the 
people’s age falls into the interval (18,35), we all think that he 
was young, but the 16-17 year-olds should be assigned to the 
concept “young” or not, that different people may have 
different views, even the same individuals may have different 
views at different times. concept division algorithm Based the 
trapezoidal cloud model take the above situation into account, 
not all the data have the above characteristics, only the 
elements on the conceptual border have this randomness, and 
the randomness can not affect the whole concept, the elements 
fall into the interval (Ex1, Ex2) fall into its own concept, which 
are their certainty. Therefore, this method can simulate the 
phenomena of human society better. The quantitative data are 
change into Boolean data in a reasonable manner, use 
sophisticated Boolean association rule mining algorithm, and 
we can achieve quantitative association rules. Using the 
quantitative data mining method can excavate more knowledge 
from database to support decision-making better. 
REFERENCES  
[1] J. Han, and M. Kamber, Data Mining Concepts and Techniques, 2nd ed., 
Morgan Kaufmann Publishers: 2006, pp.98-112? 
[2] Mehmed Kantardzic, Datamining Concepts, Models, Methods and 
Algorithms, Beijing: Tsinghua University Press, 2009? 
[3] Lu Qiu-gen, Fuzzy clustering algorithm and implementation, Computer 
Knowledge and Technology, 2008, vol.3(27) ,pp.1987-1990? 
[4] Liao Zhian, Data Fuzzy processing, Journal of Ezhou University, 2005, 
vol.12(6),  pp.49-51. 
[5] Li De-yi and Liu Chang-yu, Discussion of the universal nature on the 
normal cloud model. China Engineering Science, 2004,vol. 6,  pp. 28-33. 
[6] Fan Zhoutian, Fuzzy Matrix Theory and Applications, Beijing:Science 
Press, 2006. 
[7] LuJianjiang, Zhang Yanlin, Song Zilin, Fuzzy Association Rules and 
Application, Beijing:Science Press, 2008. 
[8] Wang Hu, Mao Wenting, telecom customer behavior study based on 
cloud model and association rules, Journal of Wuhan University of 
Technology (Information and Management Engineering), 2009, vol.31 
(5), pp.77-79? 
[9] Yan Yuejin, Li Zhoujun,Chen Huowang, Efficient maximal frequent 
itemsets Mining Based on FP-Tree, Journal of Software, 2005, 
vol.16(2), pp.215-222.   
[10] J Han, J Pei, Y Yin, Mining Frequent Patterns without Candidate 
Generation A Frequent Pattren Tree Approach, data Mining and 
knowledge discovery, 2004, vol. 8,  pp.53-87? 
