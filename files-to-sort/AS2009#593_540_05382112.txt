All-monotony: a generalization of the all-confidence antimonotony
Yannick Le Bras, Philippe Lenca, Sorin Moga
Institut Telecom - Telecom Bretagne
UMR CNRS 3192 Lab-STICC
Universite´ europe´enne de Bretagne
{firstname.lastname}@telecom-bretagne.eu
Ste´phane Lallich
Universite´ de Lyon
Laboratoire ERIC, Lyon 2, France
stephane.lallich@univ-lyon2.fr
Abstract—Many studies have shown the limits of sup-
port/confidence framework used in Apriori-like algorithms
to mine association rules. One solution to cope with this
limitation is to get rid of frequent itemset mining and to
focus as soon as possible on interesting rules. Many works
have focused on the algorithmic properties of the confidence.
In particular, the all-confidence which is a transformation
of the confidence, has the antimonotone property. In this
paper, we generalize the all-confidence by associating to any
measure its corresponding all-measure. We present a formal
framework which allows us to make the link between analytic
and algorithmic properties of the all-measure. We then propose
the notion of all-monotony which corresponds to the monotony
property of the all-measures. Our results show that although
being very interesting, all-monotony is a demanding property.
I. INTRODUCTION
Rules-based data mining methods are very important
approaches in machine learning and datamining. They may
provide a concise and potentially useful knowledge that
is understandable by the end-users. This last feature, that
allows to explain why a particular prediction is made for
a particular case or why two items are related, could be
of strong importance in various applications like in credit
scoring or medicine.
In unsupervised learning paradigm, association rule min-
ing is certainly the most popular method [1]. Our work
focuses on this method, so we first recall its fundamentals.
An association rule is a rule A ? B, where A and
B are two sets of items (also called itemsets) such that
A 6= ?, B 6= ? and A ? B = ?, meaning that given a
database DB of transactions (where each transaction is a
set of items) whenever a transaction T contains A, then T
probably contains B also [1]. Within the support/confidence
framework, the problem of mining association rules is to
generate all association rules that have support and con-
fidence greater than an user-specified minimum support
threshold and minimum confidence threshold respectively.
The support of A ? B, denoted supp(A ? B), is the
proportion of transactions containing A and B in DB, and
the confidence, denoted conf(A ? B), is the proportion of
transactions containing A and B inside the set of transactions
containing A in DB. Association rule discovery is a two-step
process. Firstly, the minimum support constraint is applied
to find all frequent itemsets in DB. Secondly, the frequent
itemsets and the minimum confidence constraint are used to
form rules. This strategy implies two major problems: the
first one concerns complexity issue while the second one
concerns the quality of extracted rules.
Finding all frequent itemsets in DB is computationally
expensive since there are O(2k) possible itemsets where k is
the number of items in DB. However the downward-closure
property of support (also called antimonotonicity [1]) allows
to implement very efficient algorithms to find all frequent
itemsets. Interesting surveys may be found in [2] and [3].
It is important to notice that support-based approaches give
an important role to the support constraint. Thus mining
interesting rules without the support has been identified as
an important challenge [4].
It is also well known that finding all and only all
interesting rules is a difficult problem. Indeed algorithms
produce a huge number of rules, especially within the
support/confidence framework, while the percentage of in-
teresting rules is often only a very small fraction of the all
rules. A first strategy to reduce the number of mined rules
consists of increasing the user-specified minimum support
threshold. Unfortunately this strategy will miss many inter-
esting rules, especially the nuggets (rules with low support
and very high confidence). A second strategy consists of
increasing the user-specified minimum confidence threshold.
This will favor rules with large consequent which may give
many uninteresting rules: for example, in a classical market
database, we would extract the rule washing liquid? bread,
which is due only to the fact that bread is bought by most of
the consumers. The use of the confidence only takes all its
sense only when the value of the confidence is compared to
supp(B). Thus one can use additional objective measures of
interest in order to rank the rules in a post-analysis phase.
Objective measures are said data driven as they could be
defined on the basis of the contingency table of A and B.
Interesting surveys and comparisons may be found in [5],
[6], [7].
Another efficient approach is to apply additional con-
straints [8], [9] on item appearance or to use additional
interestingness measures as soon as possible to reduce both
the time to mine databases and the number of founded
2009 International Conference on Machine Learning and Applications
978-0-7695-3926-3/09 $26.00 © 2009 IEEE
DOI 10.1109/ICMLA.2009.110
759
itemsets like in [10]. Indeed, as it was stressed in [11],
interestingness measures may play different roles. They can
be used to prune the search space (this is the case for the
support), to select the interesting rules (this is the case for
the confidence), or to quantify the utility of rules (this is the
case for a cost/benefit measure).
We are interested in objective measures that can be used
for all of these roles. To overcome the previous mentioned
problems with the dictatorship of the support constraint
different solutions were proposed at the algorithmic level
and mainly for the confidence measure. We focus here on
the all-confidence [12] that makes the confidence measure
antimonotone. This is thus a promising property.
The rest of the paper is organized as follows. In Section II,
we present recent works that focus on the algorithmic
properties of some measures. In Section III, we present
our formal framework which allows us to make the link
between analytic and algorithmic properties of the measures.
We apply in Section IV this framework and propose the all-
monotony that generalizes the antimonotone property of all-
confidence. We then demonstrate a sufficient condition of
non-existence of the proposed generalization all-monotony.
The practical application to 37 measures is discussed in
Section V and we conclude in Section VI.
II. RELATED WORKS
Many studies have shown the limits of support/confidence
framework used in Apriori-like algorithms to mine associ-
ation rules. In order to find nuggets, a lot of these works
have focused on the algorithmic properties of the confidence
measure. These works are mainly divided into two families:
the first one tries to reduce the number of considered items
and the second one exploits the analytical properties of
measures. We here focus on the latter one.
Inspired by the support example, [13] proposed the h-
confidence, a transformation of the confidence, that has the
antimonotony property. In addition, the h-confidence allows
to filter the uninteresting itemsets with the cross-support
property. These measures are used in HYPERCLIQUE-MINER
algorithm proposed by [13]. The h-confidence measure is
mathematically identical to all-confidence measure intro-
duced by [12]. So like the support measure they could be
used in Apriori-like algorithms. Our proposal, which is an
extension of [14], focusses on all-confidence. An itemset is
considered as interesting under the all-confidence constraint
if the confidence of all rules that can be generated from this
itemset is greater than a fixed threshold.
Based on one monotony property of the confidence,
[15] introduced the Universal Existential Upward Closure
property. This property allows to explore the itemsets lattice
with a top down approach for classification rules. It is thus
efficient in particular for nuggets discovery. A generalization
of this property is provided in [16].
For an associative classification task, following a work
by [17], [18] introduced the CORCLASS algorithm based on
direct branch-and-bound exploration of candidates and on
an antimonotonic property of convexity of the ?2.
In [19] the author introduced the notion of optimal rule
set for classification rules. A rule set is optimal if it contains
all rules, except those with no greater interestingness than
one of its more general rules. Optimal rule sets allow to
define an efficient pruning strategy that could be applied
with 13 measures as demonstrated case by case by [19].
This work is, from the best of our knowledge, one of the
first works that is applied to a large set of measures. In
addition, an optimal rule set has similar properties as closed
itemsets and nonredundant rule sets as proposed in [20].
A necessary and sufficient condition of applicability of the
underlying pruning strategy is proposed in [21].
III. ASSOCIATION RULE’S SPACE
In this section we adapt the formal framework presented in
[16], close to the formalism used by [22], which enables an
analytic study of objective interestingness measures related
to their algorithmic properties. We will propose the all-
monotony property that generalizes the monotony property
of the all-confidence and use our framework to study the
condition of existence of this property in Section IV.
If the size of the database is not taken into account
such measures can be expressed using the contingency
table of relative frequencies, and consequently with only
three variables: a measure can be considered as a function
R3 ? R ? {??,+?} (in fact the measures have various
restricted domains in R as shown in [6]). The study of their
variations, with respect to these variables, links the measures
and their algorithmic properties. It implies to describe a
domain of definition in order to study only real cases. The
final goal is then to derive, from the algorithmic properties
founded, some efficient pruning strategies.
A. Association rules & database
A boolean database DB is completely described by a
triplet (A, R, T ), where A is a set of items, T is a set
of attributes, and R is a binary relation on A × T . An
association rule is defined by a database DB, a non-empty
set A ? A (A is an itemset) called antecedent, and a non-
empty set B ? A called consequent such that A ? B = ?.
We denote a rule by A DB?? B, or simply A??B when there
is no possible confusion. The support of an itemset A is
the frequency of appearance of this itemset in the database.
We denote it by suppDB(A), or, if there is no ambiguity,
supp(A). The support of the rule A??B is the support of
itemset AB.
B. Contingency tables
Let A and B be two itemsets on the database DB. The
contingency table (left part of Figure 1) in relative joint
760
frequencies of A and B gives information about the simul-
taneous presence of these itemsets (right part of Figure 1).
B B¯
AB¯ A¯BAB
A
BA¯B¯
A supp(AB) supp(AB¯) supp(A)
A¯ supp(A¯B) supp(A¯B¯) supp(A¯)
supp(B) supp(B¯) 1
Figure 1. Contingency table of A and B.
The contingency table has 3 degrees of freedom: one
needs at least 3 of its values to describe it, and 3 values
are enough to find all the other values. For example, the
contingency table is fully described by the two marginal
frequencies supp(A) et supp(B), and the joint frequency
supp(AB). An association rule on a given database is de-
scribed by two itemsets and we naturally can speak about
the contingency table of an association rule. This leads us
to propose the notion of descriptor system.
Definition 1. We call descriptor system of the contingency
table a triplet of real functions (f, g, h) over the association
rules which fully describes the contingency table of associ-
ation rules.
Example 1. Let consider the functions ant(A ? B) =
supp(A), cons(A ? B) = supp(B), conf(A ? B),
ex(A ? B) = supp(AB) and c-ex(A ? B) = supp(AB¯).
Then the triplets (conf, ant, cons), (ex, ant, cons) and
(c-ex, ant, cons) are descriptor systems of the contingency
table.
In other words, an objective interestingness measure is a
function from the space of association rules to the space
of extended real numbers R ? {??,+?} and thus can be
expressed with a descriptor system (with 3 real functions).
C. Minimal joint domain
A descriptor system d of the contingency table is then a
triplet of variables over the space of association rules, and
an interestingness measure m can be written with the help
of a function ?m of this triplet. If we want to make an
analytic study of this function, we only need to restrict the
analysis to the joint variation domain of the triplet. Indeed,
the elements that are out of this domain do not correspond
to a real situation and therefore the analysis has no sense.
Definition 2. We call the couple (?m,Dd), made from
this function and the joint variation domain (associated
to a specific descriptor system), the d-adapted function of
measure of the measure m.
It is important to notice that the form of the functional
part of this function of measure depends on the descriptor
system. However, when this system is fixed, the adapted
function of measure is uniquely defined. In the following,
we voluntarily omit to mention the chosen descriptor system
if there is no possible ambiguity.
If d is a descriptor system of the contingency table, the
joint variation domain associated to this system is defined
by the constraints laid down by the values of d between
themselves.
Example 2. Let ex : (A DB?? B) 7? suppDB(AB) be a
function and let dex = (ex, ant, cons) be a descriptor
system.
For this system, we have the set of constraints:
??? 0 < ant < 10 <cons< 1max{0, ant+ cons? 1}? ex ?min{ant, cons} (1)
Let us define the following domain: D = (x, y, z) ? Q3
such that D satisfies the constraints (1), i. e.??? 0 < y < 10 < z < 1max{0, y + z ? 1} ? x ? min{y, z} (2)
The domain Ddex associated with dex matches this defi-
nition and thus we know that Ddex ? D (D is complete).
To prove the inverse inclusion (i.e. D is minimal), we
have to prove that each element of D has a corresponding
association rule (and then a database).
Proof: Considering an element (x, y, z) of D we need
to construct a database DB containing an association rule
A ? B, such that m(A ? B) equals to ?m(x, y, z). Since
x, y and z are rational numbers, we can define n ? N such
that (x× n, y× n, z × n) ? N3. Our database should verify
the following equalities:
supp(AB) = x, supp(A) = y, supp(B) = z (3)
The constraints (2) of the domain assure that 0 < x < y <
y + z ? x ? 1 holds. We can thus construct the database
of Table I, satisfying the equalities (3). Then, the second
inclusion is verified.
1 x · n y · n (y + z ? x) · n n
A 1 · · · 1 1 · · · 1 0 · · · 0 0 · · · 0
B 1 · · · 1 0 · · · 0 1 · · · 1 0 · · · 0? ?? ?
AB
? ?? ?
A¬B
? ?? ?
B¬A
Table I
DATABASE FOR THE DOMAIN Ddex
Finally, we have identified the joint variation domain of
the descriptor system dex: it is exactly D. The study of the
?m function can be restricted to domain D.
IV. TOWARDS all-measure AND all-monotony
We will now focus on the descriptor system dex, and write
it d. The variation of a function f on the first variable in
761
the domain D will refer to the study, for all (y, z), of the
variations of x 7? f(x, y, z) in {x|(x, y, z) ? D}.
Definition 3. (all-measure) Let m be an interestingness
measure for association rules. We define the all-m measure
on pattern I as:
all-m(I) = min
AB=I,A 6=?,B6=?
{m(A? B)}. (4)
The all-measure of pattern I is the smallest value of m
measure over all I generated rules.
Definition 4. (all-monotony) We say that the measure m is
all-monotone if the all-m measure is antimonotone.
We first show that all increasing monotone transforma-
tions of confidence are all-monotone.
Property 1. The confidence and all increasing functions of
confidence are all-monotone.
Proof: The case of confidence is easy since the all-
measure of confidence is the same as the all-confidence of
[12].
Let m be a such measure, and I ? I ? two itemsets. Since
m is an increasing function of confidence:
min
AB=I
m(conf(A? B)) = m
(
min
AB=I
conf(A? B)
)
Since the confidence has all-monotony property:
m
(
min
AB=I
conf(A? B)
)
? m
(
min
AB=I?
conf(A? B)
)
Finally, m has the all-monotony property:
min
AB=I
m(conf(A? B)) = m
(
min
AB=I
conf(A? B)
)
?
? m
(
min
AB=I?
conf(A? B)
)
= min
AB=I?
m(conf(A? B))
On the other hand, we propose Theorem 1 and Theorem 2
to prove that a large number of measures don’t verify this
property.
Theorem 1. Let m be an interestingness measure for
association rules and (?m,Dex) an adapted function of
measure of m. If ?m is strictly decreasing on the second (the
antecedent) and the third (the consequent) variables then the
measure m does not have the all-monotony property.
Proof: Suppose that ?m is strictly decreasing on the
second (the antecedent) and the third (the consequent) vari-
ables. We will prove that, for this measure, it is possible to
build a simple database which will be a counterexample for
the all-monotony property. We choose the minimal number
of items necessary to study the all-monotony property, i.e.
3. Let A, B and C be these items. For a database, we will
compare the association rules generated by AB and ABC
A 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0
B 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0
C 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0
Table II
EXAMPLE OF DATABASE THAT VERIFIES THE (11-16) CONSTRAINTS.
itemsets and for this database let be {x, y, z, t, y?, z?} ? Q
as
supp(AB) = x, supp(A) = y, supp(B) = z
supp(ABC) = x, supp(C) = t
supp(AC) = y?, supp(BC) = z?
Using the support definition, we have:
0 < supp(A) < 1 =? 0 < y < 1 (5)
0 < supp(B) < 1 =? 0 < z < 1 (6)
0 < supp(ABC) ? min(supp(AC), supp(BC))
=? 0 < x ? min(y?, z?) (7)
y? = supp(AC) < supp(A) = y (8)
z? = supp(BC) < supp(B) = z (9)
t = supp(C) ? supp(AC) + supp(BC)
?supp(ABC)
? y? + z? ? x (10)
Inequalities are strict in order to fully exploit the fact that
the function ?m is strictly decreasing. In addition, without
loss of generality, we suppose that the item C is less frequent
that B, i.e. t ? z. The constraints (5-10), that allow to
construct our rational numbers, can be summarized by:
(5) =? y ?]0, 1[ (11)
(6) =? z ?]0, 1[ (12)
(8) =? 0 < y? < y (13)
(9) =? 0 < z? < z (14)
(7) =? 0 < x ? min{z?, y?} (15)
(10) =? y? + z? ? x < t (16)
t ? z (17)
t+ (y ? y?) + (z ? z?) ? 1 (18)
Remark 1. The values y = 416 , z =
8
16 , y
? = 216 , z
? =
3
16 , x =
1
16 , t =
7
16 verify these constraints and we can
build the database of Table II.
Remark 2. Generally, for all n ? N, any database with
only 3 items {A,B,C} and n+ 3 transactions T1, · · · ,Tn+3
with T1 = {A},T2 = {B},T3 = {C},T4 = T5 = · · · =
Tn+3 = {A,B,C}, verify the (11)-(16) constraints.
Considering a data base verifying the (11-18) constraints,
we can write1, under the variations hypothesis:
1The constraint number used is on the top of associated inequality.
762
m(A? B) = ?m(x, y, z)
13
< ?m(x, y?, z) = m(AC? B)
14
< ?m(x, y, z?) = m(A? BC)
15+13
< ?m(x, x, z)
17? ?m(x, x, t)
17? ?m(x, x, t) = m(AB? C) (19)
m(B? A) = ?m(x, z, y)
14
< ?m(x, z?, y) = m(BC? A)
13
< ?m(x, z, y?) = m(B? AC)
15+14
< ?m(x, z, x)
17? ?m(x, t, x)
17? ?m(x, t, x) = m(C? AB) (20)
(19) and (20) =? all-m({A,B}) < all-m({A,B,C}),
which contradicts the anti-monotony property.
Example 3. The adapted function of Bayes factor is
?BF (x, y, z) =
x
y ? x ×
1? z
z
.
It is strictly decreasing on the second (the antecedent) and
the third (the consequent) variable and using Theorem 1,
this measure does not have the all-monotony property.
Let’s us compute BF on Table II: BF (A ? B) = 13 ,
BF (AC? B) = 1, BF (B? AC) = 1, BF (BC? A) = 32 ,
BF (B? A) = 37 , BF (A? BC) = 139 , BF (AB? C) =?
and BF (C? AB) =?.
It shows that BF does not have the all-monotony property
(all-BF ({A,B}) < all-BF ({A,B,C})).
Remark 3. The strict monotony is a strong hypothesis. Let
see the Loevinger measure:
L(A? B) = 1? supp(AB¯)
supp(A)supp(B¯)
Its adapted functions of measure on D is
?L(x, y, z) = 1? y ? x
y · (1? z)
On x ? y = 0 plane, its partial derivative with respect to
the third variable is zero so it is not strictly monotone with
respect to this variable. But if we compute the Loevinger
measure on Table II, we obtain: L(A? B) = ? 12 , L(AC?
B) = 0, L(B ? AC) = 0, L(BC ? A) = 19 , L(B ? A) =? 16 , L(A? BC) = 113 , L(AB? C) = 1, and L(C? AB) =
3
35 .
The Loevinger measure does not have the all-monotony
property. There are other measures that have the same
property on the second variable on the plane defined by
x? z = 0. We then propose the theorem 2
Theorem 2. Let m be an interestingness measure for
association rules and (?m,Dex) an adapted function of
measure of m. If ?m (x, y, z) is strictly decreasing on the
second and the third variable, elsewhere that on x? z = 0
and x ? y = 0 planes and if ?m (x, y, z) is constant on
x? z = 0 and x? y = 0 planes, then the m measure does
not have the all-monotony property.
Proof: The proof is the same as for Theorem 1 where
the inequality (17) is substituted by an equality.
Remark 4. The converse of Theorem 2 is not true and
then we don’t have a necessary and sufficient condition. The
contramin measure is a counter-example. Using the database
introduced on Remark 2, we obtain:
contr(A? B) = supp(AB)? supp(AB)
supp(B)
all-contr({A,B}) = n? 1
n+ 1
all-contr({A,B,C}) = n? 1
n
i.e. ?n > 1, all-contr({A,B}) < all-contr({A,B,C})
and thus the contramin is not all-monotone. Its adapted
function of measure is 2x?yz and the sign of its variation
with respect of z depends on 2x ? y and contramin is not
decreasing with respect of its third variable.
V. PRACTICAL APPLICATIONS AND DISCUSSIONS
The main purpose of our work is to provide a common
framework to answer if a measure is or is not all-monotone
and thus if it can be used or not in an Apriori-like algorithm
instead of support to efficiently prune the search space.
The first positive result of our study is that a set of
classical interestingness measures are all-monotone. This
is the case of increasing functions of the confidence like
confidence, Sebag-Shoenauer, Ganascia and example and
counter-example rate. It should be noticed that they rank
the rules in the same manner but depending on the user
goals one may prefer to use one or the other [6].
The Theorem 2 and specific cases (see some counterex-
amples of the remarks 1 and 2) lead to less optimistic
results. A large set of measures are not all-monotone: speci-
ficity, precision, lift, leverage, centered-confidence, Jaccard,
positive confidence, odds ratio, Klosgen, contramin, added
value, conviction, one way support, J1-mesure, Piatetsky-
Shapiro, cosine, Loevinger, information gain, Bayesian fac-
tor, Zhang, implication index, Kappa, Yule’s Q, Yule’s Y,
interestingness weighting dependency, collective strength,
and Pearson. While being very interesting, the all-monotony
is a demanding property.
In [14], we also show that the measure of recall is all-
monotone, since all-recall(I) = all-conf(I).
Finally, we observe that Theorem 1 and 2 eliminate every
measure verifying the Piatetsky-Shapiro third condition for
objective interestingness measures [23]: m monotonically
763
decreases with supp(A) (or supp(B)) when other param-
eters remain the same. Moreover, the search for interesting
itemsets using the measure all-m introduces a learning
bias towards the itemsets for which each item is strongly
associated with all the others according to the measure m.
VI. CONCLUSION
Objective measures do have some interesting algorithmic
properties. However, these properties have not been fully
exploited, except for the confidence measure. We have pro-
posed a formal framework for the analytic study of measures
in relation with their algorithmic properties. We here propose
all-monotony, an extension of the all-confidence property.
We then apply this framework to the all-monotony and
show that although being very interesting, all-monotony is a
demanding property. Indeed few measures are all-monotone
and thus could be used with the underlying pruning strategy.
However, increasing functions of the confidence are all-
monotone.
REFERENCES
[1] R. Agrawal, T. Imieliski, and A. Swami, “Mining association
rules between sets of items in large databases,” in ICMD,
1993, pp. 207–216.
[2] B. Goethals, “Frequent set mining,” in The Data Mining and
Knowledge Discovery Handbook. Springer, 2005, pp. 377–
397.
[3] J. Han, H. Cheng, D. Xin, and X. Yan, “Frequent pattern
mining: current status and future directions,” DMKD, no. 1,
pp. 55–86, 2007.
[4] J. Li, X. Zhang, G. Dong, K. Ramamohanarao, and Q. Sun,
“Efficient mining of high confidience association rules with-
out support thresholds,” in PKDD, 1999, pp. 406–411.
[5] L. Geng and H. J. Hamilton, “Interestingness measures for
data mining: A survey,” ACM, no. 3, Article 9, 2006.
[6] P. Lenca, P. Meyer, B. Vaillant, and S. Lallich, “On selecting
interestingness measures for association rules: user oriented
description and multiple criteria decision aid,” EJOR, no. 2,
pp. 610–626, 2008.
[7] E. Suzuki, “Pitfalls for categorizations of objective interest-
ingness measures for rule discovery,” in Statistical Implicative
Analysis, Theory and Applications, 2008, pp. 383–395.
[8] J. Pei and J. Han, “Can we push more constraints into frequent
pattern mining?” in KDD, 2000, pp. 350–354.
[9] M. Hahsler, C. Buchta, and K. Hornik, “Selective association
rule generation,” Computational Statistics, no. 2, pp. 303–315,
2008.
[10] R. J. Bayardo, Jr, R. Agrawal, and D. Gunopulos, “Constraint-
based rule mining in large, dense databases,” in ICDE, 1999,
pp. 188–197.
[11] Y. Yao, Y. Chen, and X. Yang, “A measurement-theoretic
foundation for rule interestingness evaluation,” in ICDM.
IEEE Computer Society, 2003, pp. 221–227.
[12] E. Omiecinski, “Alternative interest measures for mining
associations in databases,” TKDE, no. 1, pp. 57–69, 2003.
[13] H. Xiong, P.-N. Tan, and V. Kumar, “Mining strong affinity
association patterns in data sets with skewed support distri-
bution,” in ICDM, 2003, pp. 387–394.
[14] Y. Le Bras, P. Lenca, S. Lallich, and S. Moga, “Ge´ne´ralisation
d’une proprie´te´ d’antimonotonie de la confiance pour
l’extraction de motifs inte´ressants non fre´quents,” in Atelier
QDC/EGC, 2009, pp. 17–24.
[15] K. Wang, Y. He, and D. W. Cheung, “Mining confident rules
without support requirement,” in IKM, 2001, pp. 89–96.
[16] Y. Le Bras, P. Lenca, and S. Lallich, “Mining interesting rules
without support requirement: A general universal existential
upward closure property,” Annals of Information Systems,
2010.
[17] S. Morishita and J. Sese, “Transversing itemset lattices with
statistical metric pruning,” in PODS, 2000, pp. 226–236.
[18] A. Zimmermann and L. D. Raedt, “Corclass: Correlated
association rule mining for classification,” in DS, 2004, pp.
60–72.
[19] J. Li, “On optimal rule discovery,” TKDE, no. 4, pp. 460–471,
2006.
[20] M. J. Zaki, “Mining non-redundant association rules,”
DMKD, no. 3, pp. 223–248, 2004.
[21] Y. Le Bras, P. Lenca, and S. Lallich, “On optimal rules
mining: a framework and a necessary and sufficient condition
for optimality,” in PAKDD, 2009, pp. 705–712.
[22] C. He´bert and B. Cre´milleux, “A unified view of objective
interestingness measures,” in MLDM, 2007, pp. 533–547.
[23] G. Piatetsky-Shapiro, “Discovery, analysis, and presentation
of strong rules,” in KDD. AAAI/MIT Press, 1991, pp. 229–
248.
764
