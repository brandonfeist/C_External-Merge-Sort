Hierarchical Clustering Ensemble Algorithm based 
Association Rules 
 
Taoying Li and Yan Chen  
Transportation Management Collage 
Dalian Maritime University 
Dalian, China 
ytaoli@126.com 
 
Abstract—Present, there is more research on supervised 
clustering ensemble algorithm, but the research on unsupervised 
clustering ensemble is studied less. In order to partition data 
points under fully unsupervised conditions, the hierarchical 
clustering ensemble algorithm based on association rules 
(HCEAR) is proposed in this paper. The optimal number of 
clusters is determined by average degree of clustering using 
distribution of all clustering memberships and support degree of 
association rules. Then variation of the hierarchical clustering 
algorithm was adopted for best partition. Related theories ware 
proved detail in this paper. Finally, the HCEAR is applied in 
instance and results show it is effective. 
Keywords- clustering; hierarchical clustering; association rules; 
clustering ensemble 
I.  INTRODUCTION 
Clustering is to group data points into several clusters that 
the intra-cluster similarity is maximized while the inter-cluster 
similarity is minimized [1-3]. With the development of data 
mining, some ensemble methods integrating some clustering 
methods were advanced recently, Topchy proofed that the 
ensemble method can be better than any single clustering 
algorithm in [4-5]. 
Clustering ensemble is to integrate some results from some 
existing clustering partitions for higher quality and better 
robust. In recent year, ensemble learning technology has been 
widely used in many areas, such as biology feature 
recognition, Computer-aided medical diagnosis, text 
recognition, web information filtering, analysis of seismic 
waves, and so on. Early research on ensemble learning 
technology was focus on supervised learning, and the research 
on unsupervised learning, equals to clustering ensemble, is in 
the initial stage [6-7]. The clustering ensemble is to search a 
mutual partition from results of different clustering algorithms 
and has been proved to be a full NP problem. 
In the process of clustering ensemble, above all, we should 
generate m clustering memberships of dataset X, and then 
integrate the results of these m clustering memberships 
according to the mutual function. The research on the design 
of mutual function is a hot topic in clustering ensemble. 
Almost all of existing clustering ensemble methods needs to 
pre-design thresholds that are difficult to choose, such as co-
association matrix advanced by A. L. Fred [8], the method 
based on three hyper graphs advanced by A. Strehl and J. 
Ghosh [2], the EA (Evidence Accumulation) method advanced 
by A. Fred [9], WSnnG (Weighed Shared nearest neighbors 
Graph) advanced by H. Ayad [10], and so on. Although the 
CEFS (clustering ensembles guided feature selection) 
advanced by Yi Hong doesn’t need to pre-design the 
threshold, large complexity and complicated Calculation make 
the algorithm difficult to use widely. 
The HCEAR (Hierarchical clustering ensemble based 
association rules) algorithm is proposed in this paper. HCEAR 
makes use of the distribution of clustering memberships, 
combines m subset of Descartes with support degree of 
association rules, and then achieves the partition by variation 
of hierarchical clustering. 
This paper is arranged as follows: In section 2, theory of 
HCEAR algorithm is presented and proved, and then give the 
steps and analysis the complexity. The instance analysis is 
given in section 3. The conclusions are deduced in section 4. 
II. HCEAR ALGORITHM 
Assume there are n data points X={x1,x2,…,xn} and m 
clustering memberships with ki clusters that are derived from 
different methods or the same methods with different 
parameters. These m clustering memberships are 
R={R1,R2,…,Rm}, which can be denoted as formula (1). 
 },,{ ,2,1, mkiiii CCCR "= , mi ??1  (1) 
Here, Ri is the ith clustering membership that denotes 
results of the ith clustering, ki is number of clusters in the ith 
clustering membership, Ci,j is the jth cluster of the ith clustering 
membership. 
A. Definitions and Theorems 
Definition1: m subset of Descartes ),,,( 21 m
m jjjC "  is an 
intersection among all m clusters, which come from m 
clustering memberships and any two do not come from the 
same, denotes as formula (2). 
 
  
?"??"
m
jmjjm
m
m
CCCjjjC ,,2121 21,),,,( =  (2) 
This work was supported by a grant from the Ph.D. Programs Foundation 
of Ministry of Education of China (No.200801510001) 
978-1-4244-3693-4/09/$25.00 ©2009 IEEE
Here, mikj ii ,,2,1,1, "=?? , ijiC ,  is the jith cluster of ith 
clustering membership. 
Definition2: m Descartes mC is the set among all m subset 
of Descartes, denotes as formula (3). 
 { }),,,( 21 mmm jjjCC "=  (3) 
According to the definition2, the number of m subset of 
Descartes in m Descartes is up to km. 
Definition3: full m Descartes ),,,()( 21 m
mm jjjC "?  is 
the union among all m subset of Descartes, denotes as formula 
(4). 
 ),,,(),,,()( 21
111
21
1
1
2
2
m
m
m
k
j
k
j
k
j
m
mm jjjCjjjC
m
m
"
  
""? ???
===
=
 (4) 
Theorem1: the intersection between any two different m 
subsets of Descartes is empty set. 
Proof: Because the intersection between empty set and any 
one set is empty set, it is only to prove that the intersection 
between any two sets is empty set. 
Now we assume that there be two non-empty m subsets of 
Descartes ),,( 21 m
m jjjC " and )',,','( 21 mm jjjC " , and the 
intersection between them are non-empty set, that is 
??)',,','(),,( 2121 m
m
m
m jjjCjjjC "?" . 
Because they are different sets, different clusters were 
chosen in at least one clustering membership. Assume that the 
sth cluster and tth cluster were chosen in the ith clustering 
membership, and they can be revised as 
),,,,,,,( 1121 mii
m jjsjjjC "" +? and 
)',,',,',,','( 1121 mii
m jjtjjjC "" +? . From the definition of m 
subsets of Descartes, we can 
see simii
m CjjsjjjC ,1121 ),,,,,,,( ?+? "" , 
timii
m CjjtjjjC ,1121 )',,',,',,','( ?+? "" . 
According to the clustering analysis, we know that the 
intersection between any two clusters in a clustering is empty 
set, that is ?=tisi CC ,, ? , so ),,,,,,,( 1121 miim jjsjjjC "" +?  
?=+? ),,,,,,,( 1121 mii
m jjsjjjC ""? . This conflicts with 
assumption, so the theorem is proved to be correct. 
Theorem2: full m Descartes contains all points to be 
clustering. 
Proof: We assume full m Descartes do not contain all 
points, that is to say that there are points not in full m 
Descartes. Let ),,,()( 21 m
mm
p jjjCx "?? . 
According to clustering analysis, all points have their own 
clusters, which equals to that if and only if one cluster in any 
clustering membership contains xp. Let these clusters are 
kqmqq
CCC ,,2,1 ,, 21 " , and they meet iqip Cx ,? (i=1,2,…,m), 
so 
kqmqqp
CCCx ,,2,1 21 ?"??? and xp is a element of a m 
subsets of Descartes. On the base of Definition3, 
),,,()( 21,,2,1 21 m
mm
qmqq jjjCCCC k "??"?? ? , so 
),,,()( 21 m
mm
p jjjCx "?? , and it conflicts with 
assumption. Thereby the assumption is wrong and the theorem 
is proved to be correct. 
Theorem3: The number of non-empty sets in all m subsets 
of Descartes is greater than or equal to ii kmax
. 
Proof: Let
i
i
kk maxmax = . We assume that the number of 
non-empty sets in all m subsets of Descartes is r, and meet r 
< maxk , and the nun-empty sets are Q1,Q2,…,Qr. In any 
clustering memberships with the number of clusters greater 
than r, we can find clusters that contain the r sets. Assume the 
r sets be Ci,j1,Ci,j2,…, Ci,jr in the ith clustering membership Ri, 
which equal to 
ljil
CQ ,? ,( rl ,,2,1 "= ). So 
rjir
QQQ ,21 ?"?? CCC jiji ?"?? 21 ,,? . According to 
Theorem 2, rjijiji CCC ,,, 21 ?"?? contains all points. 
Consequently, each Ci,j( maxkjr ?< ) is a subset of 
rjijiji
CCC ,,, 21 ?"?? , which conflicts with the nature of 
clustering that the intersection between any two clusters are 
empty. Thereby the assumption is wrong and the theorem is 
proved to be correct. 
Then we introduce the support degree of association rules. 
Definition4: support degree of set A is the number that all 
clusters of l clustering memberships contain A, which can be 
denoted as formula (5). 
 ??
= =
?=
l
i
k
j
ji
i
CAASup
1 1
,)(
 (5) 
Here,
??
? ?
=?
else
CA
CA jiji ,0
,1 ,
,
, ml ??1 . 
By the Definition3 we can see that the maximum of 
support degree is m and the only support degree of non-empty 
m subsets of Descartes and their subsets is m. If Sup (A) is l, 
then sup (A) can be (l-1) ( ml ??2 ). 
According to Theorem2 and Theorem3, we know that 
nCk mii ?? (max
, and let mCl (= . 
Definition5: average degree of clustering Ri is a ratio 
between the sums of maximum support degree of each cluster 
multiplied with the number of points in the cluster and the 
number of clusters multiplied with the number of all points. It 
can be shown as formula (6). 
 [ ]?
=
×=
i
ii
k
j
kikj
i
i CSupCnk
RAverage
1
),(max,1)(  (6) 
Here, ikj ??1 , mi ??1 . 
B. Optimal Number of Clusters of HCEAR 
Let )]([ ii RAverageinversrF = , mi ??1  
The optimal number k of clusters of HCEAR is iR  that 
meets formula (7). 
 )]}({inverse[minmin
1 i
m
iii
RAverageF
=
=  (7) 
In order to calculate easily, we can add X as a clustering 
membership while the number of clustering in all clustering 
memberships, which ensure the support degree of any set is 
greater than 1. According to [12], we know that the optimal 
number k of clusters meets nk ? , so we only consider 
clustering memberships whose number of clusters be equal or 
less than n . 
C. Processes and Complexity of HCEAR 
The methods searching for optimal partition can be divided 
into two types, one is to learn clustering memberships that 
meet the number of clusters equals k and the number of them 
at least equals to 3, Otherwise, it don’t not possess statistical 
characteristics and cannot achieve the purpose, the other is to 
learn all clustering memberships, which should prune the X if 
X is a clustering membership. 
The Steps of HCEAR searching for optimal partition can 
be shown as follows: 
Step1: Init the number i of times of operation, go to Step2. 
Step2: Search the nun-empty subsets whose support 
degrees are (m-i), if there are two sets A and B, and they 
meet BA ? , then A can be pruned from existing sets and let 
the number of remaining sets be l. go to Step3. 
Step3: If l=k, then go to Step7, else, let i=i+1, if 
( 2?? mi ), then go to Step2, else, go to Step4. 
Step4: Calculate the number of elements of intersection 
between any two different set from l non-empty sets, and we 
can gain a under the half-angle matrix A, which is 
qppq QQaA ?== )( , and meets lqp ?<?1 , then go to Step5. 
Step5: Search the max pqa , merger Qp and Qq as Q’p, add 
row and column of Q’p to matrix A, and remove rows and 
columns of Qp and Qq, and other subsets of Q’p. Then we 
should re-calculate the value of l, go to Step6. 
Step6: if l=k, then go to Step7, else, go to Step5. 
Step7: if the intersection between any two different sets is 
empty set, the clustering ensemble is end, and we can stop the 
operation, else, go to Step8. 
Step8: Search set A and set B, which meet the number of 
elements of intersection between them is maximum, then find 
the set C whose support degree is maximum among sets that 
contains BA? . If CBCA ?? > , then prune BA? from B, 
else, prune BA? from A. go to Step7. 
The time complexity degree of CEFS in [11] is )( 2mknO , 
and that of HCEAR in this paper is )( 3mkO . Because the 
number k of clusters meets k<<n, thereby the time complexity 
degree of HCEAR is far less than that of CEFS. 
III. INSTANCE ANALYSES 
There are 30 points need to clustering, and their proper 
partition are {{1-10},{11-20},{21-30}}, where j denotes the 
jth points. Table I shows 9 different clustering memberships. 
TABLE I.  15 DIFFERENT CLUSTERING MEMBERSHIPS  
Serial 
Number Clustering Memberships 
1 {1-30} 
2 {{1-17,20,25},{18-19,21-24,26-30}} 
3 {{1-6,8-9,13,21},{7,11-12,14,16-18,20,25},{10,15,19,22-24,26-30}} 
4 {{1-9,12,},{10-11,13-17,23-24},{18-22,25-30}} 
5 {{1-7,9-11},{8,12-18,20,25},{19,21-24,26-30}} 
6 {{1-8,10-11},{9,12-18,25},{19-24,26-30}} 
7 {{1-7,9-11},{8,12-17,20},{18-19,21-30}} 
8 {{1-11},{12-17},{19,21-24,26-30},{18,20,25}} 
9 {{1-7,9-11},{12-17},{19,21-24,26-30}{8},{18,20,25}} 
A. Optimal Number of Clusters 
From Table I, we known n=30, m=5, k=3, and we can get 
the maximal support degree of clusters of all clustering 
memberships and the inverse Fi as shown in Table II. 
TABLE II.  MAXIMAL SUPPORT DEGREE AND FI  
Ri 
Sup(Rij) Fi j=1 j=2 j=3 j=4 j=5 
i=1 1     1 
i=2 2 3    0.85 
i=3 2 2 2   1.5 
i=4 3 2 2   1.29 
i=5 5 3 7   0.6 
i=6 4 3 2   1.01 
i=7 5 4 2   0.85 
i=8 3 7 7 3  0.74 
i=9 5 7 7 9 6 0.79 
From Table II, we know k=3. Then we will search the 
optimal partition with k=3. In order to present detail, we will 
learn all clustering memberships and memberships with k=3 
respectively. 
B. Search for Optimal Partition from Clustering 
Memberships with k=3 
According to the Steps of HCEAR, we can gain the non-
empty sets while support degree changes from m to 2. 
TABLE III.  SUPPORT DEGREE AND THEIR CORRESPONDING NON-
EMPTY SETS  
Support 
Degree Non-empty Sets 
5 {1-6},{7},{8},{9},{10},{11},{12},{13},{14,16-17},{15},{18,25},{19,22,26-30},{20},{21} 
4 {1-7},{1-6,9},{13-17},{19,22-24,26-30},{19,21,22,26-30},{7,11},{10,11},{12},{18,25},{20} 
3 {1-7,9},{1-7,11},{1-6,8},{8,12},{8,13},{19,21-24,26-30},{7,10,11},{12-17},{18,20,25},{12,14,16-18,20} 
2 {1-7,9-11},{18,20,25},{19,21-24,26-30},{1-6,8-9},{8,12-17},{12-18,25} 
From Table III, we known that the number l of sets meets 
l>k while support degree equals to 2, and we can merger some 
sets for partition points to k clusters. According to Step4, we 
get the under half-angle matrix A as in shown in Table IV, 
A1={1-7,9-11},A2={18,20,25},A3={19,21-24,26-30},A4={1-
6,8-9},A5={8,12-17}, A6={12-18,25}. 
TABLE IV.  HALF-ANGLE MATRIX A  
 A1 A2 A3 A4 A5 A6 
A1       
A2 0      
A3 0 0     
A4 7 0 0    
A5 0 0 0 1   
A6 0 1 0 0 6  
According to the process of HCEAR, we get {1-11} by 
uniting {1-7,9-11} and {1-6,8-9}, and then add row and 
column of {1-11} to matrix A, at the same time, we should 
remove rows and columns of subsets of {1-11}. We should 
continue the process of HCEAR until l=k=3, and we get {1-
11},{8,12-18,20,25},{19,21-24,26-30}. According to Step8, 
we gain A={1-11}, B={8,12-18,20,25}, BA? ={8}, C={1-6,8}, 
and because of CBCA ?? > , we should remove {8} from B. 
Thereby, we get the partition that {1-11},{12-
18,20,25},{19,21-24,26-30}, which meets the condition of 
clustering, and we can stop the clustering. 
C. Search for Optional Partition from All Clustering 
Memberships 
Take the clustering with k>2 as memberships, according to 
the steps of HCEAR, we can gain the non-empty sets while 
support degree changes from m to 2, and non-empty sets for 
support degree being 2 are {1-11},{8,12-18,25},{9,12-
18,25},{19,21-24,26-30}. 
Because the number of non-empty sets is greater than k, 
we can merger {8,12-18,25} and {9,12-18,25} to {8-9,12-
18,25} firstly, then prune {8-9} from {8-9,12-18,25} on the 
base of Step8, finally, we get the results of clustering, that is 
{{1-11},{12-18,20,25},{19,21-24,26-30}}. 
Either learns clustering memberships whose number of 
clusters equals to optimal k or all clustering memberships, the 
results we have gained are both {{1-11},{12-
18,20,25},{19,21-24,26-30}}, and the number of points 
divided into wrong clusters equals to 3, which is superior to 
any clustering membership. 
IV. CONCLUSIONS 
At present, the clustering ensemble is relatively mature and 
widely used, however, it is applied less under unsupervised 
situation. Many existing clustering ensemble algorithms need 
to pre-design initial thresholds, which influence the results of 
clustering ensemble directly. The HCEAR (hierarchical 
clustering ensemble algorithm based on association rules) is 
proposed in this paper. The algorithm implements clustering 
ensemble under full unsupervised condition. The algorithm 
adopts hierarchical clustering to partition points by making use 
of the distribution of results of all clustering memberships and 
the support degree of association rules. The theorems and 
definitions were detail proved. Finally, the algorithm was 
applied in practice data and results show that the HCEAR is 
superior to anyone of the clustering memberships and it’s 
effective. 
REFERENCES 
[1] H.Y. Wang, H.B. Lu, Z.P. Li. “Study of high dimensional data clustering 
based on cluster ensembles algorithm”, EL ECTRONIC 
MEASUREMENT TECHNOLOGY, Chengdu: South West Jiaotong 
University Press, vol.31, No.4, 2008, pp.41-45. (In Chinese) 
[2] A. STREHL, J.GHOSH. “Cluster ensembles a knowledge reuse 
framework for combining multiple partitions”. Journal of Machine 
Learning Research, Cambridge: MIT. Press, Vol.3, No.3, 2003 , pp.583-
617. 
[3] Z.Y. He, X. F. Xu, S.C. Deng, “A cluster ensemble method for clustering 
categorical data”, Information Fusion, 6, 2005, pp. 143–151. 
[4] A. TOPCHY, A K JAIN, W.PUNCH. “A mixture model for clustering 
ensembles”. Proceedings of the 4th SIAM International Conference on 
Data Mining, Lake Buena Vista, Florida, 2004, pp. 379-390. 
[5] A. Topchy, B. Minaei-Bidgol, A. K. Jain, et al. “Adaptive Clustering 
Ensembles”. Proceedings of the 17th International Conference on 
Pattern Recognition (ICPR2004), England, Volume 1, 2004, pp. 272-
275. 
[6] L.Y.Yang, W.Y. Wang. “Clustering Ensemble Approaches: An 
Overview”, Chengdu: Application Research of Computers Press, Vol.22, 
No.12, 2005, pp. 8-10. (In Chinese) 
[7] Y. Yang, F. Jin, M. KAMEL. “Latest development of clustering 
ensemble”, Computer Engineering and Applications, Beijing: Publishing 
House of Journal of Computer Engineering and Applications, Vol.44, 
No.11, 2008, pp. 142- 144. (In Chinese). 
[8] A. L. Fred. “Finding Consistent Clusters in Data Partitions” Proceedings 
of the 2nd International Workshop on Multiple Classifier Systems, 
Volume 2096 of Lecture Notes in Computer Science, Springer, 2001, pp. 
309-318. 
[9] Mizuki Oka, Yoshihiro Oyama, Hirotake Abe. “Anomaly Detection Using 
Layered Networks Based on Eigen Co-occurrence Matrix”.The 7th 
International Symposium on Recent Advances in Intrusion Detection, 
Berlin Heidelberg: Springer, 2004, pp. 223-237. 
[10] N. Athanasiades, R. Abler, JLevine, et al. “Intrusion Detection Testing 
and Benchmarking Methodologies”. Proceedings of the 1st IEEE 
International Workshop on Information Assurance, Darmstadt, 
Germany: IEEE Computer Society, 2003, pp. 63-72. 
[11] Y.Hong ,S. Kwong ,Y.C. Chang ,Q.S. Ren. “Unsupervised feature 
selection using clustering ensembles and population based incremental 
learning algorithm”. Pattern Recognition, Vol.41, 2008, pp.2742-2756. 
[12] S.L. Yang, Y.S. Li, X.X. Hu, R. Y. Pan, “Optimization Study on k-Value 
of K-means Algorithm”, Systems Engineering-theory & Practice, 
Beijing: Systems Engineering Society of China, Vol.2, 2006, pp. 97-101. 
(In Chinese). 
 
 
