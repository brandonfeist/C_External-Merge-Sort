2010 International Conference on Educational and Network Technology (ICENT 2010) 
Naive Bayes Associative Classification of Mammographic Data 
Benaki Lairenjam 
Department of Mathematics, Jamia Millia Islamia 
New Delhi, India 
e-mail: benakUai@yahoo.co.in 
Abstract- In this paper we focus on a new model, named ANB 
(Associative Naive Bayes) model. ANB model extend the 
modeling flexibility of well known Naive Bayes (NB) models by 
introducing rules generated by associative classifier. The model 
consists of two layers: an input layer and an internal layer. We 
propose an associative classifier algorithm (AAC), relaxing the 
condition of independence of attributes in NB, for generating 
rules and learning network parameter and a simple algorithm 
for training ANB models in the context of classification. 
Experimental results show that the learned models can 
significantly improve classification accuracy as compared to 
NB. 
Keywords-Naive Bayes classifier; association rule; 
associative classification; Bayes theorem 
I. INTRODUCTION 
Worldwide, breast cancer is the most common type of 
cancer among women and the leading cause of cancer deaths 
in women today. In United States current breast cancer 
lifetime risk is estimated at 12.7% for all women [9]. Among 
Asian and Pacific Islander women incident rates continued to 
increase at 1.5% per year (89 out of every 100,000) but are 
still significantly lower than white women [9]. Early 
detection and treatment of breast cancer while it is still small 
improves a woman's chances of survival before they can be 
felt. Currently, mammography remains one of the best 
screening methods used by radiologists for early detection of 
cancerous tumors. It plays a major role in early detection, 
detecting about 75% of cancers at least a year before they 
can be felt [13]. 
Several computer aided diagnosis (CAD) have been 
successfully developed to build breast cancer classifiers that 
can help in early detection of malignancy. CAD can help 
reduce the number of false positives and therefore reduce 
the number of unnecessary biopsies. The most common 
tasks performed by a CAD system are the classification task. 
Classifier such as Bayesian network [3], [8], artificial neural 
network [11], NB classifier [3], [14], associative classifier 
[12], [6] have been successfully used to predict breast 
cancer by several researchers. One of the simplest, and yet 
most consistently well performing set of classifiers is the 
NB models [5]. 
NB classifier is a simple probabilistic classification 
scheme based on Bayes rule of conditional probability. 
Classification with NB is the task of predicting the class of 
an instance from a set of attributes describing that instance 
and assumes that all attributes are conditionally independent 
978-1-4244-7662-6/$26.00 CO 2010 IEEE 276 
Siri krishan Wasan 
Department of Mathematics, Jamia Millia Islamia 
New Delhi, India 
e-mail: skwasan@yahoo.com 
given the class label. However, this assumption is clearly 
violated in many real world problems. To resolve this 
problem, methods for handling the conditional dependence 
between the attributes have become a lively research area. 
Effort has been made by introducing NB variants [7], [14]. 
Baye's theorem gives a mathematical representation of 
how the conditional probability of events 
HI' H 2, ........ H m (called hypothesis) that are pair wise 
disjoint is related to the event E (called evidence) that 
gives information about which hypothesis are correct. 
Associative classification is a classification process, 
where data mining techniques association rule mining and 
classification are used. In this process association rules are 
generated and analyzed for use in classification. 
Let D be a dataset. Each record in D is described by n 
attributes A = {AI' A2 , .......... An } where n denote the 
number of the attributes. The dataset also has a target 
attribute C = {CPC2, .......... Cn } , which is called the 
class attribute. The class attribute C is considered 
separately from A . An item P is an attribute value pair of 
the form (Ai' v) where Ai is an attribute taking a value v. 
A data record X= (xPx2, ........ xn) satisfies an 
item P = (Ap v) , if and only if Xi = Vi where Xi is the 
value of the ilh attribute of X . In associative classification, 
a rule is of the form P I /\ P 2 ........... PI ? C where the 
rule antecedent is a conjunction of items 
PPP2 .............. Pn(l?n) and consequent is the 
associated class label C . 
For a given rule R , the percentage of the records in D 
satisfying the rule antecedent that also have the class label 
C is called the confidence of R . The percentage of tuples in 
D satisfying the rule antecedent and having class label C is 
called the support of R . 
Receiver operating characteristic (ROC), is a graphical 
plot of the sensitivity, or true positives, vs. (1 - specificity), 
or false positives, for a binary classifier system [5]. The 
formula for sensitivity and specificity are: 
Sensitivity = TP/(TP + FN), Specificity = TN/(TN + FP) 
Here TP stands for true positive, FP for false positives, 
FN for false negatives. In a ROC curve the true positive rate 
(Sensitivity) is plotted in function of the false positive rate 
2010 International Conference on Educational and Network Technology (ICENT 2010) 
(I-Specificity) for different cut-off points of a parameter. 
Each point on the ROC plot represents a 
sensitivity/specificity pair corresponding to a particular 
decision threshold. The area under the ROC curve is a 
measure of how well a parameter can distinguish between 
two diagnostic groups (diseased/normal). 
In this paper, associative classification is used with NB 
classifier to develop a model ANB (Associative Naive 
Bayes) classifier for classifying breast cancer 
mammographic data. In our paper we assume that the 
attributes are not conditionally independent. All correlated 
itemsets with the class label are generated in the form of 
rules using AAC algorithm. Rules are introduced to relax 
some of the independence condition of the NB classifier. It 
is observed that application of these rules in creating the 
structure of NB and allowing only those features present in 
the rules as input can improve the development and 
performance of a classifier. For evaluation of the 
performance of ANB model and NB model we created 
receiver operating characteristic (ROC) curves and 
calculated the area under the ROC curve for comparison. 
NB is a simple form of Bayesian network that is widely 
used for classification and clustering [5]. Bayesian network 
is a directed acyclic graph with joint probability distribution, 
where there is a 1-1 correspondence between nodes in the 
graph and random variables in the probability distribution 
that is factorized according to the structure of the graph [4]. 
Bayesian network classifiers have been evaluated as 
potential tools for the diagnosis of breast cancer [3]. In [8] 
bayesian network computer model is used to accurately 
predict the probability of breast cancer on the basis of risk 
factors and mammographic appearance of 
microcalcifications, by using probabilistic relationships 
between breast disease and mammography findings to 
estimate the risk of malignancy. Breast cancer models which 
use BN theory such as [3], design a multi-view causal model 
which incorporates image analysis knowledge and domain 
knowledge of mammographic screening, aiming to obtain 
an understandable domain description, in terms of the 
variables represented and the mechanisms under which 
those variables are related to one another. This model 
established a clear relation between features extracted from 
the application of the single view CAD system on 
mammograms. 
Our model is created and tested using the data from UCI 
repository [10], and name of the dataset is mammographic 
mass data. We have used the same dataset in our earlier 
paper [17] for classifying the data using neural network and 
associative classifier. The dataset consists of 961 instances 
with 516 classified as benign and 445 as malignant. This 
database contains BI-RADS assessment, patient's age and 
three BI-RADS attributes together with the ground truth i.e., 
benign and malignant those have been identified on full 
field digital mammograms collected at the Institute of 
Radiology of the University Erlangen-Nuremberg between 
2003 and 2006. In our approach AAC algorithm is used to 
generate rules to create NB classifier model called ANB 
classifier model to classify breast cancer mammographic 
data. The ANB model is a tree shape two layer Bayesian 
277 
network. The class variable C is the root of the tree, the 
attributes are at leaves and the rules are all internal. The use 
of rules allows conditional dependencies to be encoded in 
the model. Rules are introduced as parents of attribute 
values. Several network models are created setting a starting 
minimum support of 5% and confidence of 50%. Using ten­
fold cross validation the network model in trained and tested. 
II. NAiVE BAYES CLASSIFIER 
Baye's theorem gives a mathematical representation of 
how the conditional probability of events 
HI,H2, ........ Hm (called hypothesis) that are pair wise 
disjoint is related to the event E (called evidence) that 
gives information about which hypothesis are correct. That 
is to fmd the conditional probabilities P( Hi I E) for 
i = 1,2, . ·  · ,m [18]. 
We have, P(Hi I E) = 
P(Hi n E) (1) 
P(E) 
P(Hi n E) = P(HJP(E I HJ (2) 
Since only one of the events HI' H2 , ........ H m can 
occur, the probability of E can be written as 
P(E) = P(HI nE)+ ..... +P(Hm nE) 
using (1) the above equation can be written as 
P(E) = P(HI)P(E I HI) + ..... + P(Hm)P(E I Hm) (3) 
Using equation (1), (2) and (3), Bayes theorem is, 
P(Hi I E) = m
P(HJP(E I HJ 
(4) 
'LP(HJP(E I HJ 
i=I 
NB classifier is a simple probabilistic classification 
scheme based on Bayes rule of conditional probability. It is 
an independent feature model [4]. It works on the 
assumption that its attribute are conditionally independent 
given the class label. It learns from the training data, from 
the conditional probability of each attribute values given the 
class label. 
The conditional independence assumption of Naive 
Bayes between the various attribute values A; , 
i = 1,2, · .. , n; n ;::: 1 ; determines that each attributes Ai' is 
conditionally independent of every attribute A j , for i '* j , 
given the class variable. In other words: 
P(Ai I C,AJ= P(Ai I C) (5) 
The expression for the probability that class 
C = <  CI, ...... Cm > will take on its ,di possible value, 
according to Bayes rule, is 
2010 International Conforence on Educational and Network Technology (ICENT 2010) 
p(Ci I AI, ... An) = m
P(CJP(AI' .... An I Ci) 
(6) 
Ip(Cj)P(AI,····An I Cj) 
j=1 
Using equation (1), 
n 
P(Ci)I1 peAk I Ci) 
p(Ci I AI'···? )= -m-----:::..;k=""-
I
n--­
Ip(cj)I1 peAk I Cj) 
m n 
j=1 k=1 
Where,Z = Ip(Cj)I1 peAk I C) 
j=1 k=1 
P( Ci) = prior probability and, 
P( Ak I Ci) = feature probability distribution 
(7) 
Learning task of this model is to estimate two sets of 
parameter class prior probability and feature probability 
distribution. Classification using this NB probability model 
is done by picking the most probable hypothesis which is 
also known as the maximization criterion. We have the 
classifier function as follows: 
n 
NB(AI,···AJ=argmaxP(CJI1P(Ak I CJ (9) cjeC k=1 
The structure of the NB is graphically shown in Fig. 1, 
where C is the class variable and AI' ... An are the attributes. 
Each attributes AI' ... An is the child of the parent node C . 
Figure I. Structure of NaIve Bayes. 
III. ASSOCIATIVE NAIVE BAYES CLASSIFIER 
In this section, we create a model ANB using AAC 
algorithm and NB classifier. The complete set of class 
association rules is determined from the training data set 
using associative classifier algorithm. The training data set 
is divided into IO-parts and for each part class association 
rules are generated setting a starting minimum support of 
5% and minimum confidence of 50%. For each rules 
generated ANB model is created. 
278 
A. Association Rule Generation 
Given a training dataset D with k classes and 
minimum support, the dataset is scan and frequent I-item set 
are generated. The frequent I-item set are sorted in support 
descending order. And items not satisfying the minimum 
support threshold are removed from the data sets. The 
remaining frequent items are arranged in the form of vertical 
TABLE I. TRAINING DATA SET 
SI.no Rl R2 R3 C 
I Al BI D3 CI 
2 A2 B3 04 CI 
3 A2 B3 04 C2 
4 Al B3 02 C2 
5 Al B4 D2 C2 
6 Al B2 03 C2 
7 A2 BI 03 CI 
8 A2 B4 D1 CI 
9 Al B4 D3 CI 
10 A2 BI 02 C2 
11 A2 B2 D2 C2 
12 A2 BI 01 CI 
13 A2 BI D1 CI 
14 Al B2 D2 C2 
15 Al B2 02 C2 
data representation [1]. 
Example, let D be data sets containing 12 elements as 
shown in TABLE I, here D is described by three attributes 
Rl, R2, R3 and a class label C . There are 15 records in 
this datasets i.e., IDI = 15 . The attributes Rl has 
values AI, A2 ; R2 has values Bl, B2, B3, B4 and R3 
has values Cl, C2, C3 and C 4 . The class label C has 
values Cl and C2. The support threshold is taken as 20% 
and confidence as 80%. 
First AAC algorithm scan the data set D once, fmds the 
items satisfying the minimum support value in D. The set is 
F = {AI, A2, Bl, B2, Dl, D2, D3} called the frequent 
item set. Then AAC Classifier sorts the items in F in 
support descending order, i.e., F-Sorted 
= {A2,Dl,Al,Bl,B2,D3,D2} . All other attribute 
values, which fail the support threshold, are removed from 
the data set D i.e., {B3, D2} . The remaining attribute 
values are arranged in the form of vertical data format w.r.t. 
F-Sorted as shown in TABLE II. From TABLE II intersect 
column A2 = {2,3,7,8,1O,11,12,13} and 
Cl = {1,3,4 ,5,8,12,13} and we get {3,8,12,13} [1]. 
Since I A2 n Cll= 4 satisfies the minimum support 
threshold Al is inserted in Cl set. Each column in TABLE 
II intersect with the class column and generated two sets Cl 
and C2 i.e., Cl = {A2,Dl,Al,Bl} and 
C2 = {B2, D3, D2} . For each intersection of the attribute 
value column with the class column prior probability and 
posterior probability of the attribute value given the class is 
2010 International Conference on Educational and Network Technology (ICENT 2010) 
calculated and stored in a probability table. Then for each 
class sets possible combination of the item sets are found and 
itemsets satisfying the minimum support threshold are 
inserted in CR-tree [2] for rule generation and at the same 
time posterior probability of the itemsets given the class label 
are calculated and stored in the probability table. 
TABLE II. REFINED DATA 
A2 DI Al BI B2 D3 D2 CI C2 
2 I I I 2 6 2 I 2 
3 3 4 3 4 9 II 3 6 
7 4 5 5 9 10 14 4 7 
8 5 6 7 10 15 5 9 
10 7 9 8 II 8 10 
II 8 14 12 14 12 II 
12 12 15 13 15 13 14 
13 13 15 
Rule pruning is done similar to CMAR algorithm [2] and 
a set of class association rules are generated. Once, a set of 
class association rules are generated, conditional probability 
of each rule from the probability table are stored in a 
conditional probability table. From the example in Table I 
the rules generated are: 
(Rl) {D3} ? {C2} confidence = 100.0%, sup(R) = 
33.33%, sup(A) = 33.33%, sup(C) = 53.33% 
(R2) {AI, D3} ? {C2} confidence = 100.0%, sup(R) = 
26.67%, sup(A) = 26.67%, sup(C) = 53.33% 
(R3) {B2, D3} ? {C2} confidence = 100.0%, sup(R) = 
26.67%, sup(A) = 26.67%, sup(C) = 53.33% 
(R4) {AI, Dl} ? {Cl} confidence = 100.0%, sup(R) = 
20.00%, sup(A) = 20.00%, sup(C) = 46.67% 
Where: sup(R) = support for rule, sup(A) = support for 
antecedent and sup(C) = support for consequent. 
The conditional probability table of the rules generated is 
shown in TABLE III. Thus, with the associative 
classification rule AAC algorithm generates conditional 
probability of each rule from the training datasets. 
TABLE III. CONDITIONAL PROBABILITY TABLE 
Rule CI C2 
RI 0 1.00 
R2 0 1.00 
R3 0 1.00 
R4 1.00 0 
B. ANBModel 
ANB network model consists of two layers an input 
layer and an internal layer and has a central node, called the 
class node C. Nodes in the input layer are represented by 
one characteristic from each rule. Each rule consists of 
attribute values and each attribute value in the rule is called 
a characteristic. Thus number of input nodes is equal to the 
number of characteristics in the rules; the number of internal 
279 
nodes is equal to the number of rules. The network model is 
shown in Fig. 2. 
Structure of ANB model: 
(1) Input node: Last layer of the network. In this layer each 
input nodes is input by one characteristic from each of 
the rules, i.e. each node represents one characteristic. 
(2) Internal node: Second layer of the network which is the 
neighborhood of the input layer. In this layer each 
internal node is connected with the characteristic of 
each rule. The number of nodes in this layer is equal to 
the number of rules. 
(3) Root node: The class variable is the root node it is 
connected with the internal node. 
Figure 2. Structure of ANB model. 
In an ANB model as shown in Fig. 2 the class variable 
C is the central node and the attribute values are at the input 
layer; the rules RI, R2, R3 and R 4 are all internal. The 
use of rules allows conditional dependencies to be encoded 
in the model. For instance by introducing a rule R3 as 
parent of the attribute value Al and D3 as shown in Fig. 2, 
we can represent the dependence statement Al is not 
conditionally independent to D3 given C . An ANB 
model the correlation among the attribute variables given 
the class by using the rules generated by AAC algorithm. 
C. Learning ANB model 
The model is learned using 10- fold cross validation. In 
the learning process for each instance ( in training data sets, 
possible combination of items in ( = {(p .. 1m} are found 
and each item combinations are compared with items in the 
rules and if a rule is found matching the item combination 
then the conditional probability of the rule is assigned to the 
rule node. For example given an instance 
( = {AI, B2, D3} , possible combination of the instance ( 
are {AI} , {B2} , {D3} , {AI,B2} , {AI,D3} , 
{B2, D3} , {AI, B2, D3} . From the model in Fig. 3 item 
combination {D3} , {AI,D3} and {B2,D3} matches 
with the item combination in rule RI, R2, and R3, then 
the posterior probability of RI, R2, and R3 given Care: 
2010 International Coriference on Educational and Network Technology (ICENT 2010) 
P(Rll C)= P(D31 C), 
P(R21 C) = P(Al,D31 C), 
andP(R31 C) = P(B2,D31 C) 
Since rule R 4 has no matching item combination 
posterior probability of R 4 is calculated as, 
P(R4 I C) = K ' where k is number of attribute values. 
Then the output class probability is computed by, 
P(C I t) = P(C)P(Rll C)P(R21 C)P(R31 C)P(R41 C) 
For each rule Ri, i = 1,2, ... n; n ? 1 , where n is the 
number of rules, the output class probability is computed by, 
n 
P(C I t) = p(C)I1 P(Ri Ie) (10) 
i=1 
When the accuracy on the training set is higher than a 
given accuracy threshold, the mining process is stopped. 
Accuracy threshold is set at 95%. 
IV. EV ALVA TION OF THE CLASSIFIER 
We experiment on mammographic data taken from UCI 
repository [10], and name of the dataset is mammographic 
mass data. The dataset consists of 961 instances with 516 
classified as benign and 445 as malignant. This database 
contains BI-RADS assessment, patient's age and three BI­
RADS attributes together with the ground truth i.e. benign 
and malignant those have been identified on full field digital 
mammograms collected at the Institute of Radiology of the 
University Erlangen-Nuremberg between 2003 and 2006. 
BI-RADS (Breast Imaging Reporting and Data system), 
developed by the American College of Radiology provides a 
standardized classification for mammographic studies. The 
system demonstrates good correlation with the likelihood of 
breast malignancy. The BI-RADS system can inform family 
physicians about key fmdings, identify appropriate follow­
up and management [15]. The BI-RADS attributes are mass 
shape, mass margin and mass density [10]. Depiction of fme 
micro calcification and subtle soft-tissue masses at high­
quality mammography is the key to detection of early breast 
cancer [16]. It has its own characteristics and can be used as 
a clue to classify masses. Masses can be circumscribed, 
micro lobulated, round, oval, lobular, irregular etc and 
radiologist will need to take a good look to fmd the 
suspicious lesions. Therefore in order to classify a mass, the 
BI-RADS attributes recorded were used as an input feature. 
There are three BI-RADS attributes- they are mass 
shape, mass margin and mass density. The masses are mass 
shape: round, oval, lobular and irregular, mass margin: 
circumscribed, microlobulated, obscured, ill-defmed, 
speculated and mass density: high, iso, low, fat-containing 
with the class attributes benign (non--cancerous) and 
malignant (cancerous). 
Data quality and evaluation can be found out easily by 
knowing the relationships between the attributes. Discovery 
of relationships between the attributes leads to data cleaning 
280 
rules and improved its constraints. These are carried out by 
analyzing rules between the attributes. 
Therefore cleaning process is carried out on the dataset. 
The dataset undergo normal statistical cleaning process 
where all the attributes are distributed normally to check 
outliers, extreme, noisy or missing values. These values are 
replaced with the attribute's mean or average depending on 
its suitability. All the continuous variables are converted 
into discrete variables. Histogram and normal curve graph 
were projected to ensure that all data are normally 
distributed. 
Out of the 961 instances two third of the data items are 
taken for training and one third of the data items for testing 
Le., 640 data items for training and 321 data items for 
testing. The model is learned using 10- fold cross validation. 
When the accuracy on the training set is higher than a given 
accuracy threshold, the mining process is stopped. Accuracy 
threshold is set at 95%. 
Our experimental result shows that the accuracy of 
classifying the test instance is 85.312% with root mean 
squared error rate of 0.35719. As compared to NB classifier 
in weka the accuracy percentage is 82.19 with root mean 
squared error rate of 0.3741. 
We also present ROC curves for visualizing classifier 
performance. It describes the relations between two indices, 
sensitivity and specificity. 
The ROC plot as shown in Fig. 3 shows False Positive 
rate (X axis), the probability of incorrectly diagnosing a 
case as positive when its true state is negative against True 
Positive rate (Yaxis) and the probability of correctly 
diagnosing a positive case across all decision levels for the 
0.9 
? O.B '> ., 
? 0.7 
? 0.6 
? 0.5 , 
? I 
? 0.4 
: 
? 0.3 
? 
2 0.2 
I-
0.1 
Figure 3. 
0.2 0.4 0.6 O.B 
False positive rate (1 - Specificity) 
No disaimination 
-O--ANB 
-..... . NAIVEBAYES 
ROC curve of ANB and Naive Bayes. 
diagnostic test. Ideally the curve climb quickly toward the 
top-left meaning the test correctly identifies cases. The 
diagonal grey line is a guideline for a test that is unable to 
correctly identifying cases. 
ROC curve for our model ANB and NB are shown in 
Fig. 3. Area under the ROC curve is an important criterion 
about classifier performance. AUC (Area Under the Curve) 
for our model is 0.92 and for Naive Bayes is 0.90. 
The experiment is made on a computer with a single 
1.73 GHZ Core (TM)2-CPU and 1280 MB memory and 
AAC algorithm is written in java and ANB classifier model 
were designed trained and tested in weka. 
2010 International Conference on Educational and Network Technology (ICENT 2010) 
V. CONCLUSION 
We have presented a new model ANB for classifying 
mammography mass data from UCI repository dataset and 
our experimental result have shown that accuracy 
performance of the model ANB reaches 85.312% then 
82.19% for NB. Our model also improves on error rate. We 
find that in ROC curve AUC for ANB is higher than NB. 
Thus our model increases classification performance when 
compared to NB model. 
REFERENCES 
[I] Z. Tangl and Q. Liao, "A New Class Based Associative Classification 
Algorithm," IAENG International Journal of Applied Mathematics, 
May. 2007. 
[2] W. Li, J. Han and J. Pei, "Accurate and efficient Classification Based 
on Multiple Class-Association Rules," IEEE ICDM, 2001,pp. 283-
299. 
[3] N. Ferreira, M. Velikova and P. Luca, "Bayesian Modelling of Multi­
View Mammography," Proc. ICMLIUAI/COLT 2008 Workshop on 
Machine Learning for Health-Care Applications, Helsinki, Finland, 
2008. 
[4] O. Maimon and Lior Rokach, Data Mining and Knowledge Discovery 
Handbook, Springer, 2005, pp.193-230. 
[5] I.H. Witten and E. Frank, Data Mining Practical Machine Learning 
Tools and Techniques, 2nd ed., Elsevier: Morgan Kaufmann 
Publishers, 2005, pp. 83-112. 
[6] M.L. Antonie, O.R Zaiane, and A. Coman, "Associative Classifiers 
for Medical Images," LNCS, Mining multimedia and Complex Data, 
vol. 2797/2003, pp. 68-83. 
[7] N.L. Zhang, T.D. Nielsen and F.V. Jensen, "Latent Variable 
Discovery in Classification Models," Artifical Intelligence in 
Medicine, vol. 30, Issue 3, pp. 283-299. 
281 
[8] E.S. Burnside, D.L. Rubin, J.P. Fine, RD. Shachter, GA Sisney and 
W.K. Leung, "Bayesian Network to Predict Breast Cancer Risk of 
Mammographic Microcalcifications and Reduce Number of Benign 
Biopsy," Radiology, vol. 240: Number 3, pp. 666-673, September 
2006. 
[9] http://www.medscape.comlcdc-commentry 
[10] UCI Repository, http://www.archive.ics.uci.edulml/datasets/ 
Mammographic + Mass 
[11] A. Adam, K. Omar, "Computerized Breast Cancer Diagnosis with 
Genetic Algorithms and Neural Network," 
[12] O.R Zaiane, M.L. Antonie, A. Coman, "Mammography 
Classification by an Association Rule-based Classifier," In: 
International Workshop on Multimedia Data Mining (MDMlKDD 
'2002) in conjunction with ACM SIGKDD, pp 62--69, (2002). 
[13] http://as.webmd.comlevent.ngl 
[14] H. Langseth and T.D. Nielsen, "Classification using Hierachical 
Naive Bayes models," Machine Learning, vol. 63, Issue 2, May 2006, 
pp. 135-159. 
[15] M. Margaret, Eberl, C.H. Fox, MD, S.B. Edge, CA Carter, and M.C. 
Mahoney, "BI-RAOS Classification for Management of Abnormal 
Mammograms," The Journal of the American Board of Family 
Medicine 
[16] P. Skaane, C. Balleyguier, F. Diekmann, S. Diekmann, P. Jean­
Charles, K. Young, T. Loren, Niklason, "Breast Lesion Detection 
and Classification: Comparison of Screen-Film Mammography and 
Full-Field Digital Mammography with Soft-copy Reading-Observer 
Performance Study," Published online before print August II, 2005, 
1O.1l48/radioI.2371041605, (2005). 
[17] B. Lairenjam and S.K. Wasan, "Neural Network with Classification 
Based on Multiple Association Rule for ClassifYing Mammographic 
Data," Proc. Intelligent Data Engineering and Automated Leaming­
IDEAL 2009, LNCS, pp. 465-476. 
[18] M.H. Duunham and S. Sridhar, Data Mining Introductory and 
Advanced topics, 1 st Impression, pp. 48-52. 
