EVALUATING TECHNOLOGIES BASED ROUGH SET THEORY   
VU THANH NGUYEN1, NGUYEN THI LY SA2 
1University of Information Technology, 34 Truong Dinh Str., 3 Dist, HoChiMinh City, VietNam 
2Colleague of Information Technology 
EMAIL: nguyenvt@uit.edu.vn, lysant80@gmail.com 
Abstract: 
Most data mining algorithms usually generate combining 
large rules, finding useful rules from rules set necessary and 
important. There have been several techniques proposed to 
assess the rule as useful measure which is based on rough set 
theory as the RIM, ERIM measure. On rough set theory has 
been studied, the article proposed AWERIM measure which is 
improved from  ERIM measure and applied this specific 
measure to an application of the data mining problem about 
bank loans.  
Keywords: 
RIM measure; ERIM measure; AWERIM measure 
1. Introduction.  
Discovered the combining rule is key areas in data 
mining and is used widely in many fields. However, these 
techniques often generate a large combined rules, how to 
choose the the rules which are really important? One 
solution to this problem is to use the interesting measures 
([3]) to organize and evaluate rules by these measures. The 
useful measures are divided into two main directions: 
objective measure and the subjective measure. The 
subjective measure depends on the support of the people to 
evaluate the rule, the objective measure bases on statistics or 
the structure of the patterns. The Jiye Li proposed a measure 
based on rough set theory: 
- The rule importance measure - RIM ([2]), the objective 
measure, is defined similarly useful measure, is used to 
evaluate the importance of rule; 
- The enhanced rule importance measure - ERIM) ([3]) is 
a combination of two measure subjective and objective, 
is defined based on the weight of condition attributes. 
In this article, the authors evaluate the AWERIM 
(Average of Weight based Rule Importance Measure) 
proposed measure and measure ERIM, comparing the 
advantages of the suggested measure to the Jiye Li’s 
measure by proposed a practical application of data on bank 
loans. 
2. Introduction about rough set theory. 
The rough set theory was developed by Zdzisrule 
Pawlak ([5]) in the early 1980s to be seen as a new approach 
to discover knowledge, and it forms a solid foundation for 
data mining applications. The highlights of issue of the 
rough set theory are making the idea to solve ambiguity and 
uncertainty of information systems. 
The decision Table. 
In rough set theory, data set is presented as a decision 
table with rows in the table called objects and the columns 
in a table called attributes. Attributes in decision table is 
divided into two categories: condition attributes and 
decision attributes; the condition attributes are used to 
describe the conditions and decision attributes are used to 
describe the results when there are some certain satisfied 
conditions. A decision table T = (U,C,D) with the symbol U 
is the set of objects in decision table, C is the set of 
condition attributes and D is the set of decision attributes. 
The reduct and core. 
The reduct and the core are two important concepts in 
rough set theory, the reduct set of attributes typical 
conditions likely to classify the entire board decision. A 
decision table may have reducts and their intersection is said 
the core, but it is a set of attributes needed for the whole 
data. Because reduct don’t contains redundant attributes 
therefore it is often used in the attributes selection process. 
A decision table may have more than one reduct, the 
determination of all reducts is this problem which has 
complexity to be NP-hard ([6]) and are interested by many 
researchers. 
 
 
1951
2010 IEEE978-1-4244-6527-9/10/$26.00 ©
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
 3. Association rule. 
Mining association rules to help find the relationship 
between the components in the data. Mining field of 
association rule so far has been studied and developed in 
several different directions, in general problems of mining 
association rule are divided into two main stages: (1) find all 
frequent itemsets, (2) mining association rules from these 
frequent itemsets 
Definition.  
With the transaction data set D, the association rule is 
implied operator X  Y, in which X, Y are the item sets. 
The X  Y Rule has the s support if s% of transactions in D 
which contains YX ? (
D
YX
s
?
= ), and the c reliability if 
c% of transactions containing Y in transactions containing X 
(
X
YX
c
?
= ). Mining association rule is to find 
combinations of all rules X  Y which have s support 
greater than the threshold support of minSup and the c 
reliability greater than the threshold of minConf. 
The conversion decision table to bitmap table. 
The methods of mining association rules have been 
proposed to apply to transaction data with the value domain 
of the properties of 0 and 1 and these properties are 
considered as the item. To apply this method to data defined 
in rough set theory, data in the decision table with multiple 
values attributes to need to convert decision table to bitmap 
table with attributes to be the name of the attribute in 
decision table associated with every its possible value. 
When the objects in decision table is converted to bitmap 
table which has transactions with values associated for each 
attribute to be 0 or 1. 
Mining association rules from bitmap table.  
There are several methods for mining association rules. 
In this article, the authors use the IT-tree search ([4]) to 
mine the frequent item sets which have the threshold of 
minSup support after converting decision table to bitmap 
table. From these frequent item sets, the association rules set 
X  Y is arisen with minConf threshold of reliability and X 
is the subset of C condition attributes set and Y is a subset of 
the D set of decision attributes.  
Eliminate redundant rules. 
Results of the process of mining association rules 
usually contain a lot of redundant rules; need to remove 
redundant rules from the association rules set. A rule is 
considered redundant rule if it satisfies one of the following 
conditions: 
(1) If there exists two rules of the form as 
ZYrZXr ?=?= 21 , . I say the rule r2 is the 
redundant rule if X subset of Y (X?Y).  
(2) If there exists two rules of the form 
as ZXrYXr ?=?= 21 , . I say the rule r2 is the 
redundant rule if Z subset of Y (Z?Y). 
4. Technical for evaluated rules. 
Discovered the association rule is one of the main 
approaches of data mining, but the number of rules 
discovered usually large while a small number of rules is 
actually useful from the user view. There are many proposed 
methods to choose the useful rules by using the utility 
measure as the interesting measure ([3]), the measure which 
is based on rough set theory is proposed by the Jiye Li 
author  to include the rule important measure - RIM ([2]), 
the enhance rule important measure - ERIM ([1]). 
The interesting measure. 
The interesting measure of a rule is a technique for 
evaluating the benefits, the usefulness of this rule. There are 
many useful measures as Support, Confidence, Lift, Laplace, 
Conviction, Interest, Cosine and Jacquard ... However, no 
one of measures can give the best results in all applications. 
The authors V. Kumar ([3]) compared and evaluated 21 
measures based on experimental tests and proposed some 
suitable areas. 
RIM and ERIM measures. 
Two these measures applied from rough set theory, 
particularly the reducts, for evaluating the arisen association 
rules. 
RIM measure. 
The reduct is a subset of the typical conditions 
attributes can describe the full meaning of the data set, so 
the association rules which are arisen from a reduct are 
typical knowledge. A decision table often have more than 
one reduct, the rule arising from the various reduct may 
contain different typical information, if we only used a 
reduct to generate rule may to miss the other important 
information. Therefore, we should use all reducts to 
generate rules, then some rules will appear more frequently 
in the files of rule rules; and we can say that the rule often 
appears to be viewed is more important than the rule appear 
regularly. Based on this idea, the Li Jiye author ([2]) 
1952
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
 proposed RIM measure to assess the importance of the rule. 
This measure is defined as follows:  
n
rulesetruleRuleSetsruleset
RIM jiji
??
=
|
 
Where n is the number of reducts, RIMi is the 
importance of rule rulei, rulesetj is the j-th rule set j arising 
from a j-th reduct and Rule Sets is the rule sets arising from 
reducts  
Can say that RIM measure is quite simple and easily 
calculated, providing a clear and direct view of the 
importance of a association rule. The measure is objective 
measure. However, the disadvantage of RIM is that when 
only one reduct from the decision table is found, then the 
value of RIM of all rules are found to be 100% (that all rules 
are important as together) so should not be able to evaluate 
the importance of rules based on the RIM measure.  
ERIM measure. 
The ERIM measure is proposed to efficiently measure 
of RIM, and this is a subjective measure which is defined 
based on the weight of condition attribute in decision table, 
and the weight is evaluated by experts of this field. 
According to experts, the attributes have the higher 
weighted more necessary so the rule which has greater 
weight is considered more important. The ERIM measure is 
defined as follows: 

=
=
in
k
kii wERIM
1
,
 
Where ERIMi is the ERIM measure of the i-th rule (rulei), ni 
the number of condition attributes in the rulei  rules and wi,k 
is  weight of k-th attribute of rulei rule. 
The ERIM of rule is the total value of the weights of 
condition attribute in the rule; these rules which have higher 
ERIM measure are considered important. For convenience 
to compare the rules by the ERIM measure, instead of using 
the ERIM value the percentage of ERIM value is used to 
compare the maximal ERIM value in rule set. 
The approach of ERIM measure includes three steps: 
The first step is generated set of rules which use the RIM; 
the second step is calculating the ERIM measure value for 
each rule in the obtained rule set in first step. Finally a 
combination of both RIM and ERIM measure to evaluate the 
rule. Advantages of this measure is combined subjective 
measure with  objective measure in evaluated rule process 
so its result can be better more than using other measures. 
However, this measure depends on the key element of the 
evaluation which is received by experts. This process is 
spent many times on statistics process and sometime 
difficultly to implement.  
The AWERIM measure. 
As described above, the ERIM measure has 
disadvantages as follows: if two rules have equal RIM 
measure and weights of the condition attributes in the 
decision table are the same, then by definition of ERIM 
measure, then the rule that has more number of condition 
attributes on the left side than the others will be  more 
important; this means that ERIM measure depends on the 
number of attributes on the left sides of the rule, the higher 
number of conditions attributes on the left sides is, the more 
important it will be. This is quite non reasonable because the 
condition attributes in both rules have equally weighted. 
To solve this problem, the article proposed AWERIM 
measure (Average of Weight based Enhanced Rule 
Importance Measure) as alternative measure to overcome 
the disadvantages of ERIM measure. The AWERIM 
measure is defined as follows: 
i
n
k
ki
i n
w
AWERIM
i

=
=
1
,
 
Where the AWERIM measure is AWERIM of i-th rule 
(rulei), in  the number of attributes in the conditions 
attributes in rulei and kiw ,  weight of k-th attribute of rulei. 
The AWERIM measure of rule is the average value of 
weight of the condition attributes in the rule; these rules 
have higher AWERIM measure to be considered important. 
For convenience to compare the AWERIM measure of rules, 
instead of using the AWERIM value, we use a percentage of 
AWERIM value compared with the value AWERIM 
maximal value in rule set. ERIM similar measure, approach 
of AWERIM measure also includes three steps: The first 
step generated rule set using the RIM measure; the second 
step calculated the value of AWERIM measure for each rule 
in the rule set obtained in the first step. Finally a 
combination of both RIM and AWERIM measure to 
evaluate the rule. 
5. Practical application. 
The authors use data mining for bank loans to evaluate 
the advantages and the rule measure AWERIM that article 
suggested to RIM measure and ERIM measure. 
 
 
1953
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
 Description of data set. 
The application on the mining data set for bank loans to 
predict customers can be approved for a bank loan or not 
based on some information from customers. Data source 
which is mined has the 14 attributes in which 13 conditional 
attributes (2 continuous valuable attributes, 11 category 
valuable attributes) and 1 decision attribute are included. 
List of attributes is presented in Table 1. 
TABLE 1. DESCRIPTION OF BANK LOAN DATA 
Attribute 
Type 
Attribute 
Name Interpretation 
Value 
Domain 
Name Amount 
customers want to 
borrow 
{0…N} Continuous V
alue  
Age Customer Age {0…N} 
Interest rate Interest rate of 
loan (unit %) 
{0.9, 0.98, 
1, 1.05, 1.1, 
1.12, 1.15,  
1.18, 1.2, 
1.3} 
Duration Period of Loan 
(unit: month) 
{12, 24, 36, 
48, 60, 72, 
84, 96} 
Repayment Repayment 
Schedule 
{Month, 
end of 
repayment}
Pay Interest Schedule for 
interest payment  
{Every 
year, Every 
month} 
Prestige Prestige of 
customer in the 
previous loans  
{Yet, Yes , 
No} 
Number of 
previous 
loan 
Number of 
previous loans 
{0, 1, 2, 3}
Marriage Marital status of 
Customer 
{Single, 
married, 
Divorce} 
Number of 
dependents 
 Number of 
dependents of 
customer  
{0, 1, 2, 3}
C
onditional  attribute  
C
ategories  
Ensured 
debt 
Guaranteed loan 
ratio (value of 
loans against 
collateral)  
{>=1, <1} 
Income Stable level of 
income of 
customers 
{Stable, 
Relatively 
Stable, Non 
Stable} 
Afford Repayment ability 
of customer, 
calculated = loan 
period*monthly 
income net + 
other assets) / 
(interest + loans a 
month * term 
loan). Values of 
attributes:> = 1, 
<1 
{>=1, <1} 
D
ecision 
A
ttribute
 To loan 
with 
interest 
Customer 
information on 
loan or not 
{Yes, No}
Building applications Model. 
Application is built by model in Pic.1. The first step, in 
the first phase inconsistent and data will be processed, after 
that data is divided into two parts by the random method, 
70% data for Training and 30% data for to test. Because the 
data source of Bank Loan has two attributes whose values 
are continuous values so training data needs to carry out to 
discrete values by algorithm which is proposed by HungSon 
authors ([8]). The results of phase of arising rule is 
association rule set after eliminating the redundant rules 
(applied algorithm to exploit the rules by IT-tree search for 
with minSup = 3%, minConf = 70%). From obtained rule set, 
calculating the RIM, ERIM, AWERIM measures for each 
rule and in build subclass for each measure. In the last step 
test data is used data to test the effectiveness of the 
subclasses that was built based on test accurate results. In 
this article, the authors use algorithm for building subclasses 
([9]), the input of this algorithm is rule set to be ordered by 
using measure, the output is the layer with this measure.  
1954
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
  
Figure 1. Building applications Model 
Weights of the condition attributes of Bank Loan are 
presented in Table 2. ERIM measure is defined ([2]) based 
on the weight of attributes, and these weights were 
determined to depend on the concept hierarchy, the 
attributes of the same hierarchy are same weight and equal 
weight of that hierarchy. But in this article the author is not 
set up the concept hierarchy because data source of Bank 
Loan has a few attributes. 
TABLE 2. WEIGHTS OF CONDITION ATTRIBUTES OF THE BANK LOAN 
Number  Condition 
attribute 
Weight 
1 Money 7 
2 Interest 3 
3 Period 3 
4 Original payment 
1 
5 Interest payment 
1 
6 Prestige 7 
7 Number of previous loans 
4 
8 Age 7 
9 Marital Status 4 
10 Number of dependent 
4 
11 Ensuring debit 8 
12 Income 8 
13 Afford to debit 8 
Experimental result.  
Table 3 is presented the results of 15 times for the 
experiment with three RIM, ERIM and AWERIM measures. 
Fig.2 is represented by drawings about accuracy in cases for 
each measure from the data in Table 3. 
Discussion. 
Although the number of rules using three RIM, ERIM 
and AWERIM measures lower than entire set of rules if the 
rule set does not use any these measures but subclasses that 
are built by three measures give test results which are the 
more correct than when subclasses are not built by three 
these measures, especially when the AWERIM measure is 
used (see table 3). ERIM measure is a advanced RIM 
measure but result of the ERIM measure is not the better 
result of RIM measure. Compared with the 2 measures 
proposed by Jiye Li (RIM, ERIM), the AWERIM measure 
of this article almost gives the better results. 
TABLE 3. RESULTS OF 15 TIMES FOR EXPERIMENTS WITH RIM, ERIM AND 
AWERIM MEASURES ON DATA SOURCE OF BANK LOAN  
All rules RIM  ERIM AWERIM 
Number  of 
rules Accuracy
 of 
rules Accuracy 
 of 
rules Accuracy
 of 
rules Accuracy
1 185 71.32 % 131 87.84% 131 87.84% 131 93.24%
2 152 68.70%  144 84.21% 144 77.63% 144 88.16%
3 163 70.67 % 86 86.67% 86 81.33% 86 89.33%
4 196 70.37 % 131 87.65% 131 87.65% 131 91.36%
5 174 65.85%  164 75.61% 164 78.05% 164 82.93%
6 176 76.62%  128 85.71% 128 87.01% 128 87.01%
7 183 62.82% 162 78.21% 162 78.21% 162 78.21%
8 198 64.79% 123 80.28% 123 80.28% 123 81.69%
9 167 70.51% 98 83.33% 98 83.33% 98 88.60%
10 177 69.62% 138 80.52% 138 80.52% 138 80.52%
11 198 72.37% 152 77.63% 152 80.26% 152 85.53%
12 191 67.90% 140 82.72% 140 82.72% 140 87.65%
13 178 64.38% 118 82.19% 118 82.19% 118 90.41%
14 200 63.85%  164  79.49% 164  79.49% 164  82.19%
15 183 70.51%  150 93.59% 150 91.03% 150 93.59%
1955
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
  
Figure  2. The Chart compares the accuracy of the subclasses that are used 
RIM, ERIM, and AWERIM measures 
6. Conclusion.  
In the article, the authors present technical evaluation 
based on the rough set theory of Li Jiye, RIM and ERIM 
measures, propose a AWERIM measure. Experimental 
results on data source of Bank Loan prove effectiveness of 
rule set used AWERIM measure and compare them with 
ERIM and RIM measures. 
At the present the authors intend to build applications 
with multiple attributes of data source to test the AWERIM 
measure performance used concept hierarchy ([2]), and 
develop other measure by combining two kind of subjective 
and objective measures to evaluate the rule. 
Reference. 
[1] Jiye Li, Nick Cercone, W. H . Wong, Lisa Jing Yan 
(2009). Enhancing Rule Importance Measure Using 
Concept Hierarchy. Faculty of Computer Science and 
Engineering, York University. 
[2] Jiye Li, Nick Cercone (2005). Discovering and 
Ranking Important Rules. Granular Computing, IEEE 
International Conference on Volume 2. 
[3] P. Tan, V. Kumar, J. Sivastava (2002). Selecting the 
Right Interestingness Measure for Association 
Patterns, in SIGKDD’02 ACM 
[4] M. J. Zaki, C. J. Hsiao (2005). Efficient Algorithms 
for Mining Closed Itemsets and Their Lattice 
Structure. IEEE Transactions on Knowlegde and 
Data Engineering. 
[5] Z. Pawlak (1991). Rough Sets – Theoretical Aspects 
of Reasoning about Data. Kluwer Academic 
Publishers, Dordrecht. 
[6] A. Skowron, C. Rauszer (1991). The Discernibility 
Matrices and Functions in  Information Systems. 
[7] Aleksander Ohrn (1999). Discernibility and Rough 
Sets in Medicine: Tools and Applications. PhD thesis, 
Department of Computer and Information and 
Science, Norwegian University of Science and 
Technology, Trondheim Norway. The ROSETTA 
Homepage, http://www.idi.ntnu.no/~aleks/rosetta 
[8] N. H. Son (1997), Discretization of Real Value 
attributes. A Boolean Reasoning approach. Thesis for 
Doctor of Philosophy. 
[9] S. Mutter (2004). Classification using Association 
Rules. A thesis of Diploma of Computer Science at 
the University of Freiburg 
 
1956
Proceedings of the Ninth International Conference on Machine Learning and Cybernetics, Qingdao, 11-14 July 2010
