On Classification Compatibility Laws of Zhongjing Prescriptions 
 
 
Xuemei Yang, Xinmei Lai, Candong Li 
Fujian University of Traditional Chinese Medicine 
Fuzhou, China 
 e-mail: yxm_wj@sina.com, xm9709@163.com, fjzylcd@126.com 
 
 
Abstract—Zhongjing prescriptions are selected as data set in 
this paper. First, a classification association rules mining 
algorithm, which selected features based on information gain, 
short for CARM-IG, is achieved. The algorithm is used to mine 
compatibility laws of Zhang Zhongjing from cold-heat, 
deficiency-excess in eight principals of TCM (Traditional 
Chinese Medicine). Second, 21 rules which are applied to the 
treatment of cold, heat or cold-heat mixing syndromes are 
mined. And 29 rules which are applied to the treatment of 
deficiency, excess or deficiency-excess mixing syndromes are 
discovered too. The reduction of  run time and memory cost of 
algorithm CARM-IG had been proved, at the same time, the 
concision and precision of classification compatibility laws are 
guaranteed because of the deletion of irrelevant Chinese 
medicines with classification. Finally, the compatibility laws of 
Zhang Zhongjing are listed in this paper, which would be used 
to direct clinical practice sometimes.  
Keywords-prescription compatibility laws of Zhang 
Zhongjing;  classification association rules;  features selection 
I. INTRODUCTION 
Zhongjing prescriptions, which are created by Zhang 
Zhongjing, have experienced approximately two thousand 
years of Clinical verification. The prescriptions are 
commended by the later generation as classical prescriptions 
because of strict compatibility and remarkable treatment 
result. Over the years, Zhongjing prescription research is 
more confined to personal summary about compatibility 
principle, clinical research, experimental study, classification 
of prescription etc. Zhongjing prescriptions have not been 
collected completely to establish a database, and 
mathematical, statistical and data mining methods have not 
been used to sum up compatibility principle of Zhongjing 
prescriptions objectively so far. 
Then, 268 prescriptions from “Shang Han Lun” and “Jin 
Kui Yao Lue” which are written by Zhang Zhongjing are 
selected from the prescription database of TCM information 
system through excluding duplicate prescriptions and 
deleting prescriptions without medicine in order to establish 
data set of Zhongjing prescription in this research firstly. 
Then Zhongjing prescriptions are classified by experts in the 
TCM field from two dimensions of cold-heat and deficiency-
excess in eight principles. Finally, a classification association 
rules mining algorithm based on information gain feature 
selection CARM-IG is applied to mining latent compatibility 
principles of Zhang Zhongjing, so that the compatibility 
principles of Zhang Zhongjing for the treatment of cold 
syndrome, heat syndrome, deficiency syndrome and excess 
syndrome etc. can be summed up objectively. 
 
II. RELATED WORKS 
Currently, association rules mining [1], cluster [2], 
correspondence analysis [3] etc. are data mining methods 
commonly used to discover prescription compatibility 
principle. Furthermore, several improved algorithms based 
on association rules mining have emerged [4]. A 
prescriptions data mining system integrated data 
preprocessing, high frequent item sets and association rules 
mining, cluster, classification etc. is developed in project 
DartGrid [5, 6]. New research way is explored because of the 
application of methods above for prescriptions compatibility 
principle mining, which is becoming one of the most 
important research directions for the interdisciplinary 
research of TCM in union with computer technology 
gradually. 
Algorithm CMAR (Classification based on Multiple 
Association Rules) based on FP-growth method is proposed 
in 2001 [7]. The objective of algorithm CMAR is to solve 
the problem of classification through mining classification 
association rules. But it is assumed that decision attributes 
(or category attribute) are closely related any conditional 
attributes (the attributes except decision attributes) in 
algorithm CMAR. Higher memory cost and running time 
will be spend when super high dimension data set such as 
prescriptions data sets of TCM with hundreds dimensions is 
processed. The assumption above causes that irrelative 
attributes exist in mined classification association rules. So 
the index of IF * ICF (the inverse class frequency) is 
proposed in paper [8], which is used to delete the attributes 
with higher frequent but lower contribution for classification 
from VCFP-Tree in order to improved the performance of 
algorithm. The index of IF * ICF usually is useless When the 
classification number is lower so that a Chinese medicine has 
appeared in each type of prescription. The method of feature 
selection based on information gain is introduced in 
algorithm CMAR. A classification association rules mining 
algorithm CARM-IG for TCM prescription high dimension 
data set is established, which can reduce the time and space 
cost, and mine classification association rules with higher 
quality through the deletion of irrelevant Chinese medicines 
2010 IEEE International Conference on Bioinformatics and Biomedicine Workshops
978-1-4244-8302-0/10/$26.00 ©2010 IEEE 712
with classification. Finally, the compatibility principles of 
Zhongjing prescriptions are summed up from different 
dimensions of classification. 
III. FEATURE SELECTION BASED ON INFORMATION GAIN 
The basic idea behind feature selection is to compute 
some measure that is used to quantify the relevance between 
an attribute and a given class label. Information gain is one 
of effective measures for feature selection. Such a measure is 
referred to as an attribute selection measure or a measure of 
the goodness of split. Information gain is defined as the 
difference of the expect information needed to classify a give 
sample 1( ,..., )mI s s  and the expect information needed to 
classify a give sample based on conditional attributes ( )E C . 
Specific definition is given by 
1( ) ( ,..., ) ( )mIG C I s s E C= ?                    (1) 
Let S be a set consisting of s data samples. Suppose the 
class label attribute has m distinct values defining m distinct 
classes Di (for i=1,…, m ). Let si be the number of samples 
of S in class Di. The expect information needed to classify a 
give sample is given by 
1 2
1
( ,..., ) log ( )
m
m i i
i
I s s p p
=
= ?

                 (2) 
Where pi is the probability that an arbitrary sample 
belongs to class Di, and is estimated by si/s. 
Let attribute C have v distinct values{ }1,..., vc c . Attribute 
C can be used to partition S into v subsets{ }1,..., vS S , where 
Sj contains those samples in S that have value cj of C. Let sij 
be the number of sample of class Di in a subset Sj. The 
entropy, or expected information based on the partitioning 
into subsets by C, is given by 
1
1
1
...
( ) ( , ..., )
v
j m j
j m j
i
s s
E C I s s
s
=
+ +
=

             (3) 
Where 
1 ...j mjs s
s
+ +
 acts as the weight of the jth subset, 
and is the number of samples in the subset divided by the 
total number of samples in S. 1( ,..., )j mjI s s  is defined 
as
2
1
lo g ( )
m
ij ij
i
p p
=
?

, and | |
i j
i j
j
s
p
S
=
 is the probability that a 
sample in Sj belongs to class Di. 
Anyway, the smaller the entropy value ( )E C , the greater 
the information gain value ( )IG C , the greater the purity of 
the subset partitions, and the higher the correlation between 
conditional attributes and decision attributes. The lowest 
threshold of information gain is usually been set, and the 
attributes whose information gain value is higher than the 
threshold will be chosen as the distinguish attributes for 
classification. The threshold of information gain is set to 0.1 
in this research refer book [11]. 
IV. CLASSIFICATION ASSOCIATION RULES MINING BASED 
ON INFORMATION GAIN FEATURE SELECTION 
A. Variant of Classification Frequent Pattern Tree 
A variant of classification frequent pattern tree (VCFP-
Tree) is a compress storage structure of data set in order to 
mine classification association rules without candidate 
frequent item sets generation. A VCFP-Tree must satisfy 
condition below [9]:  
1. A VCFP-Tree consists of one root labeled as "null", a 
set of item prefix subtrees as the children of the root, and a 
frequent-item header table F-list. 
2. Each node in the item prefix subtree consists of five 
fields: item, child-link, parent-link, node-link, and class-list, 
where item registers which item this node represents, child-
link links to child nodes of each node, parent-link links to 
parent node of each node, node-link links to the next node in 
the VCFP-Tree carrying the same item, or null if there is 
none, and class-list is a classification list which records every 
class label and its corresponding frequency.  
3. Each entry in the frequent-item header list consists of 
three fields, (1) item-name; (2) start of node-link, which 
points to the first node in the VCFP-Tree carrying the same 
item; (3) information gain value of the attribute. 
As shown in table? below, Chinese medicines usually 
is designated as attributes of prescriptions data set such as a 
and b etc. Whether a Chinese medicine appears in a 
prescription is been indicated by 0 or 1. Classification 
attributes often list in the end, such as classification tag. A 
VCFP-Tree (i.e., figure 1) is constructed based on the data 
set T. The order of items in the frequent-item header table F-
list is a: 4-b: 3-c: 3-d: 3 with 3 as the threshold of support 
count. The information stored in the node indicated by deep 
color arrow is that there is one prescription containing 
Chinese medicines abd and belonging to class C. 
TABLE I.  PRESCRIPTION DATA SET T. 
No. a b c d E classification tag 
1 1 0 1 0 1 A 
2 1 1 1 0 0 B 
3 0 0 0 1 1 A 
4 1 1 0 1 0 C 
5 1 1 1 1 0 C 
 
      
Figure 1.  VCFP-Tree based on data set T. 
713
B. Classification Association Rule 
Class association rule is an implication of the form R: C
?D, where C?D=?, sup(R)?min_sup, and conf (R)?
min_conf. C indicates conditional attribute (or item). D 
indicates decision attribute (or classification attribute). Sup 
(R) is the support of rule R, min_sup is the lowest threshold 
of support; Conf (R) is the confidence of rule R, min_conf is 
the lowest threshold of confidence. The support and 
confidence of rule R is defined below: 
Sup (R) = P (CD)                            (4) 
 Conf (R) = P (D|C)                            (5) 
P (CD) indicates the joint probability of C and D. P 
(D|C) indicates the conditional probability that belongs to 
class D under the condition of Chinese medicine C 
appearance. Rule support and confidence are two important 
measures of rule interestingness. They respectively reflect 
the usefulness and certainty of discovered rules. The rules 
with higher support and confidence are usually chosen as 
interesting pattern. 
For example: 
{Ramulus Cinnamomi *  Rhizoma Zingiberis Recens *  
Radix Paeoniae Alba}?{cold syndrome} 
(Support count=15, frequency of rule former piece=25) 
Where {Ramulus Cinnamomi * Rhizoma Zingiberis 
Recens * Radix Paeoniae Alba} before ? is the rule former 
piece; {cold syndrome } after ? is the rule rear piece. The 
support count indicates the frequency of three Chinese 
medicines above appearing in a same prescription and 
treating cold syndrome, and is also called the frequency of 
rule; the frequency of rule former piece indicates the 
frequency of three Chinese medicines above appearing in a 
same prescription. There are 268 prescriptions in Zhongjing 
prescriptions. So the rule support is 0.056, which is 
computed through support count 15 divided by 268. The rule 
confidence is 60%, which is computed through support count 
15 divided by the frequency of rule former piece 25. 
C. Efficient Mining of Classification Association Rules 
based on Information gain Feature Selection 
The concrete steps of algorithm CARM-IG which are 
used to mine classification association rule effectively are as 
following. 
Algorithm: CARM-IG 
Input: min_sup: the lowest threshold of support ?
min_conf: the lowest threshold of confidence?min_IG: the 
lowest threshold of information gain.  
Output: CAR?the set of classification association rules. 
Begin 
(1) Scans Zhongjing prescription database once. The 
information of relevant frequencies is recorded, so that the 
information gain value of every conditional attribute can 
been computed completely.  
(2) Conditional attributes which are higher than min_sup 
and min_IG are selected. The frequent-item header table F-
list is constructed in attributes support descending order. 
(3) Scans Zhongjing prescription database second, and a 
VCFP-Tree T is constructed according to section?$. 
(4) CAR=CFP-growth (T, min_sup, min_conf); 
(5) Return (CAR); 
End 
Algorithm CARM-IG scans Zhongjing prescription 
database firstly. The frequency ix
f
for every Chinese 
medicine xi, the frequency jy
f
 for each type of prescription, 
and the frequency ( | )j if y x  for each type of prescription 
containing Chinese medicine xi are recorded. jy
f
 is used to 
compute 1( ,..., )mI s s , and ( | )j if y x  is used to computer 
( )E C , so that information gain value IG of every 
conditional attribute can be computed according to (1). The 
conditional attributes (or Chinese medicines) with 
ix
f
>min_sup and IG>min_IG are chosen. The header table 
F-list is constructed in accordance with ix
f
 descending. The 
objective of the second scan of prescription data set is to 
create a compress storage structure for Zhongjing 
prescription, namely a VCFP-Tree T. Finally, the algorithm 
of CFP-growth (Class-Frequent Pattern growth) is achieved 
as following [7]. 
1. Based on F-list in Figure 1, the set of class-association 
rules can be divided into subsets without overlap: (1) the 
ones having d; (2) the ones having c, but no d; (3) the ones 
having b, but no d  nor c ; and (4) the ones having only a. So 
algorithm CARM-IG finds these subsets of class-association 
rules one by one.  
2. To find the subset of rules having d, algorithm CARM-
IG traverses nodes having attribute value d and look 
“upward” to collect a d-projected database, which contains 
{(a,b,c,d):C; (a,b,d):C; d:A} three tuples. It contains all the 
tuples having d. The problem of finding all frequent patterns 
having d in the whole training set can be reduced to mine 
frequent patterns in d-projected database.  
3. Recursively, in d-projected database, a and b are the 
frequent attribute values, i.e., they pass support threshold. (In 
d-projected database, d happens in every tuple and thus is 
trivially frequent. We do not count d as a local frequent 
attribute value.) We can mine the projected database 
recursively by constructing VCFP-trees and projected 
databases. Please see [10] for more details. 
4. It happens that, in d-projected database, a and b always 
happen together and thus ab is a frequent pattern. a and b are 
two subpatterns of ab and have same support count as ab. To 
avoid triviality, we only adopt frequent pattern abd. Based on 
the class label distribution information, we generate rule abd
?C with support count 2 and confidence 100%. 
5. After search for rules having d, all nodes of d are 
merged into their parent nodes, respectively. That is, the 
class label information registered in a d node is registered in 
its parent node. The VCFP-tree is shrunk as shown in figure 
2. Please note that this tree-shrinking operation is done at the 
same scan of collecting the d-projected. 
714
 
Figure 2.  VCFP-Tree after merging nodes d 
The remaining subsets of rules can be mined similarly. 
Anyway, the information used to compute information gain 
value for every conditional attribute is collected completely 
through algorithm CARM-IG without increase in computing 
time basically. The scale of VCFP-Tree created in memory 
has been reduced moderately after dimension reduction 
based on information gain filter. Then the mining of 
classification association rule becomes more quickly and 
efficient, and experiments show that the run time and 
memory cost of algorithm CARM-IG has been reduced 
respectively[12]. 
D.  Experiment Result 
Experimental platform configured to Intel 2G / 1G, 
Windows XP. Algorithm CARM-IG using java 
programming is achieved. The 268 Zhongjing prescriptions 
are chosen as data set, which are classified by experts in the 
TCM field from two dimensions of cold-heat and deficiency-
excess in eight principles. The lowest support, confidence 
and information gain threshold set 2%, 50% and 0.1 
respectively.  
Experiment results are summarized in table?and ? 
below. The two classification association rules sets on the 
treatments of cold-heat (21 rules) and deficiency-excess (29 
rules) etc. syndromes of Zhongjing prescription are listed 
respectively. 
TABLE II.  CLASSIFICATION ASSOCIATION RULES SETS ON COLD-HEAT 
Former piece Rear piece Conf  Sup 
radix bupleuri heat 100 3.36 
talcum heat 100 2.24 
bulbus lili heat 100 2.24 
rhizoma zingiberis * rhizoma 
coptidis 
cold-heat 100 2.24 
rhizoma zingiberis recens * 
radix aconiti lateralis praeparata 
cold  75 2.24 
radix aconiti lateralis praeparata 
* rhizoma atractylodis 
macrocephalae 
cold  75 2.24 
rhizoma zingiberis * radix 
aconiti lateralis praeparata 
cold  72.72 2.99 
radix aconiti lateralis praeparata cold  72.22 9.70 
radix et rhizoma asari cold  70.58 4.48 
fructus schisandrae chinensis cold  66.66 2.24 
ramulus cinnamomi * herba 
ephedrae 
cold  64.28 3.36 
ramulus cinnamomi * rhizoma 
zingiberis recens * radix 
paeoniae alba 
cold  60.00 5.60 
rhizoma coptidis heat  57.14 2.99 
rhizoma zingiberis recens * 
radix paeoniae alba 
cold 56.66 6.34 
gypsum fibrosum heat  55.55 3.73 
fructus aurantii immaturus other 55.55 3.73 
ramulus cinnamomi * rhizoma 
zingiberis recens 
cold  55.00 8.21 
ramulus cinnamomi * rhizoma 
zingiberis 
cold-heat 53.84 2.61 
rhizoma zingiberis cold  53.65 8.21 
ramulus cinnamomi * poria other  53.33 2.99 
ramulus cinnamomi * radix 
paeoniae alba 
cold  51.51 6.34 
TABLE III.  CLASSIFICATION ASSOCIATION RULES SETS ON 
DEFICIENCY-EXCESS 
Former piece Rear piece Conf Sup 
radix et rhizoma rhei * fructus 
aurantii immaturus 
excess  100 2.99 
radix et rhizoma rhei * natrii sulfas excess  100 2.99 
radix et rhizoma rhei * cortex 
magnoliae officinalis 
deficiency  100 2.61 
bulbus lili deficiency 100 2.24 
semen lepidii semen descurainiae excess  100 2.24 
radix et rhizoma rhei * fructus 
aurantii immaturus * cortex 
magnoliae officinalis 
excess  100 2.24 
natrii sulfas excess  90.90 3.73 
semen persicae excess  88.88 2.99 
fructus aurantii immaturus * cortex 
magnoliae officinalis 
excess  87.50 2.61 
radix et rhizoma rhei * semen 
persicae 
excess  85.71 2.24 
poria * rhizoma atractylodis 
macrocephalae 
deficiency 80.00 4.48 
radix rehmanniae deficiency 77.77 2.61 
radix aconiti lateralis praeparata deficiency 75.00 10.07 
rhizoma atractylodis 
macrocephalae 
deficiency 73.33 8.21 
rhizoma zingiberis * radix aconiti 
lateralis praeparata 
deficiency 72.72 2.99 
herba ephedrae * gypsum fibrosum deficiency-
excess 
72.72 2.99 
fructus aurantii immaturus excess  72.22 4.85 
radix et rhizoma rhei excess  71.87 8.58 
cortex magnoliae officinalis excess  71.42 3.73 
gypsum fibrosum deficiency-
excess 
66.66 4.48 
fructus jujubae * radix scutellariae deficiency-
excess 
66.66 2.99 
fructus trichosanthis deficiency-
excess 
66.66 2.24 
fructus jujubae * radix paeoniae 
alba 
deficiency 65.51 7.09 
rhizoma coptidis deficiency-
excess 
64.28 3.36 
715
radix paeoniae alba * herba 
ephedrae 
deficiency-
excess 
60.00 2.24 
rhizoma zingiberis * radix 
scutellariae 
deficiency-
excess 
60.00 2.24 
radix scutellariae deficiency-
excess 
58.62 6.34 
fructus jujubae * herba ephedrae deficiency-
excess 
53.84 2.61 
herba ephedrae deficiency-
excess 
50.00 5.97 
 
To sum up, the run time and memory cost of algorithm is 
reduced[12]. At the same time, the concision and precision 
of classification compatibility principles is guaranteed 
because of the deletion of irrelevant Chinese medicines with 
classification. The mined rules more follow with the 
prescription law directed by basic theories of traditional 
Chinese medicine by experts’ confirmation preliminary. 
Follow-up study focuses on the careful and thoroughgoing 
explanation for classification compatibility principle of 
Zhongjing prescriptions through literature research and 
expert advice. Further study and explanation for these rules 
are in progress. 
ACKNOWLEDGMENTS 
This work was supported in part by Foundation of 
Science & Technology Project of Fujian (2009Y0030 and 
2009J05074), by Fujian Provincial Health Department 
Special Project (YA-204), by Fujian Province Department of 
Education Technology Project (JA09130). 
REFERENCES 
[1] M.C. Yao, L. Ai. Association Rule Analysis for Compatibility 
Principle of Diabetes Prescriptions. Journal of Beijing 
University of Traditional Chinese Medicine, Beijing, China,  
2002, 25(6), pp. 48-50. 
[2] Q.F. He, X.Z. Zhou, and Z.M. Zhou. The Cluster Analysis 
based on the Function of Chinese Medicine. Chinese Journal 
of Information on Traditional Chinese Medicine. Beijing, 
China, 2004, 11(6), pp. 561-562. 
[3] J.S. Shang, L.S. Hu, X. Niu, and Y.C. Yin. The Data Mining 
Experiment for the Compatibility Principle of Banxia Xiexin 
Tang. Journal of China-Japan Friendship Hospital, Beijing, 
China, 2005, 19(4), pp. 227-229. 
[4] D.D. Zhao. The Improvement Algorithm of Apriori and Its 
Application in Chinese Medicine Knowledge Discovery. 
Computer and Modernization, 2007 (8). 
[5] Z.H. Wu, Y. Feng. A number of explorations on Knowledge 
Discovery in Database in Traditional Chinese Medicine field 
(?) [J]. Chinese Journal of Information on Traditional 
Chinese Medicine. Beijing, China, 2005, 12(10), pp. 93-94. 
[6] Z.H. Wu, Y. Feng. A number of explorations on Knowledge 
Discovery in Database in Traditional Chinese Medicine field 
(?) [J]. Chinese Journal of Information on Traditional 
Chinese Medicine. Beijing, China, 2005, 12(11), pp. 92-94. 
[7] W.M. Li, J.W. Han, and P. Jian. CMAR: Accurate & Efficient 
Classification based on Multiple Class-Association Rules [C]. 
In ICDM’01, San Jose, CA, 2001, pp. 369-376 
[8] B. Li. Research on Classification based on Multiple 
Association Rules of Herbal Formula Database [D]: [Master's 
Degree Thesis].Nanjing: Southeast University, Department of 
Computer Science and Engineering, Nanjing, China, 2005.  
[9] X.M. Yang, D.Y. Lin, X.H. Weng, L.R. Xiao. The 
Classification Association Rule Mining for the Prescriptions 
of Spleen-stomach Damp-heat Symptom in Ming and Qing 
Dynasties [J]. Chinese Journal of Information on Traditional 
Chinese Medicine, Beijing, China, 2006, 13(10), pp. 106-107. 
[10] J.W. Han, P. Jian, Y.W. Yin. Mining Frequent Patterns 
without Candidate Generation [C]. In SIGMOD’00, Dallas, 
TX, May 2000. 
[11] J.W. Han, K. Micheline. Data Mining: Concepts and 
Techniques.2001 by Morgan Kaufmann Publishers, Inc., pp.  
200. 
[12] X.M. Yang, D.Y. Lin, C.E. Zhou, X.M. Lai. Classification 
Association Rules Mining for Zhongjing Prescriptions. 
Proceedings 2008 IEEE International Symposium on IT in 
Medicine and Education, Xiamen, China, 2008, pp. 768-772. 
716
