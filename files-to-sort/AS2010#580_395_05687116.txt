Associative Prediction Model and Clustering for Product Forecast Data 
Ruhaizan Ismail, Zalinda Othman and Azuraliza Abu Bakar 
Centre for Artificial Intelligence Technology 
Faculty of Information Science and Technology, 
Universiti Kebangsaan Malaysia, Bangi, Selangor, 
Malaysia. 
ruhaizanismail@yahoo.com, {zalinda & aab}@ftsm.ukm.my  
Abstract—Association rules are adopted to discover the 
interesting relationship and knowledge in a large dataset. 
Knowledge may appear in terms of a frequent pattern 
discovered in a large number of production data. This 
knowledge can improve or solve production problems to 
achieve low cost production. To obtain knowledge and quality 
information, data mining can be applied to the manufacturing 
industry. In this study, we used one of the association rule
approach, i.e. Apriori algorithm to build an associative 
prediction model for product forecast data. Also, we adopt the 
simplest method in clustering, k-means algorithm to attain the 
link between patterns. The real industrial product forecast 
data for one year duration is used in the experiment. This data 
consists of 42 products with two important attributes, i.e. time 
in the week and required quantity. Since the data mining 
processes need a large amount of data, we simulated these data 
by using the Monte Carlo technique to obtain another 15 years 
of simulated forecast data.  There are two main experiments 
for the association rules mining and clustering. As a result, we 
obtain an associative prediction model and clustering for the 
forecasting data. The extracted model provides the prediction 
knowledge about the range of production in a certain period. 
Keywords-associative; association rules; prediction; 
clustering; manufacturing 
I. INTRODUCTION
Currently, the data in manufacturing industry is really 
huge and consists of various stages. However, because of 
the insufficient information many problems especially in the
operation management could not be solved efficiently. 
Company needs reliable knowledge to make the right 
decision particularly in forecasting or planning a system. It 
is necessary for a company to get an accurate demand 
forecasting for it to produce the required quantity at the 
right time. One way to get useful information is by using the 
data mining approach. Data mining is an analytic process 
designed to explore data in search for consistent pattern and 
relationship [1]. Typically, data mining techniques are 
already used in business, medical and various domains.
In production operation, there are some important 
decisions to make during the early planning stage such as 
the production planning, customer demands and selection of 
an appropriate manufacturing process [2]. In the 
manufacturing domain, data mining can be used to provide 
information for preventive maintenance, machine failure 
prediction or quality control [3]. 
This research employed an association rule that can 
identify interesting relationship, knowledge and frequent 
pattern. The discovered knowledge or frequent pattern can 
improve or assist in solving production problems in order to 
achieve low cost production. On the other hand, the decision 
quality can be improved to encourage a positive impact to 
the company. In order to obtain good knowledge and predict 
the outcomes, we proposed an associative prediction model. 
After that, we applied the clustering techniques to group the 
similarity and dissimilarity of the relationship and pattern.  
This paper is organized as follows: Section II is about 
the related work and previous research of associative 
prediction model, clustering and manufacturing 
productivity. In Section III, we present the methodology that 
comprises of the step of data collection and how we conduct 
the pre-processing, experiments and in section IV, the
results. In that section, results obtained from the two 
techniques will be compared and discussed. Meanwhile the 
Section V discloses the conclusions. 
II. RELATED WORK
Traditionally, the statistical techniques are used in order 
to discover the patterns in manufacturing data such as linear, 
quadratic and logistic discriminate analyses [3,4]. However, 
these statistical techniques face a problem when dealing 
with massive data from a huge database or multiple 
databases. According to [5], there are two main weaknesses 
of the local methods when dealing with the huge data. First, 
it is difficult to scale the high dimension data and second is 
the lack of interpretability of the model data. Therefore, data 
mining techniques can help to improve and solve the 
problems especially in the manufacturing domain. This is 
due to the fact that the manufacturing data consists of large 
records and many attributes.  
Associative prediction is a combination of two data 
mining function that are the association rules and prediction 
[6]. Association rule mining (ARM) is used to identify any 
interesting relationships between a set of items in a database 
especially for market basket analysis [6, 7]. ARM can also 
be defined as the discovery of association rules showing the 
attribute-value conditions that occur frequently together in a 
given set of data [10]. In ARM, Apriori algorithm is known 
1459978-1-4244-8136-1/10/$26.00 c©2010 IEEE
to be the standard algorithm to the mine association rules 
and it has been chosen for this study. The Apriori algorithm 
uses prior knowledge to generate frequent patterns from the 
dataset containing transactions [7, 14]. From [8], there are a 
few works done using the association rules in the 
manufacturing domain. They focused on the product design, 
manufacturing process and decision support. The 
association rule mining is used in past sales and product 
records to discover the associations among the customer 
needs, marketing employees and designers [9]. Production 
data can be categorized as a time series data because the 
sequences of values change with time. Association rules can 
discover the temporal relationships that are hidden in these 
data [20]. 
In this study, the proposed model is built using the 
association rules to associate attribute values in order to 
generate predictions or to estimate the expected outcomes. 
Meanwhile, the prediction is a technique to predict a future 
model of continuous-valued function. In predictive 
techniques, some examples of the predictive data mining are 
bagging, boosting, stacking and meta-learning. Usually, 
prediction techniques are used for numerical and continuous 
data. There are models generated from prediction techniques 
that are linear, nonlinear and generalized linear model of 
regression [6].  
However to obtain good results, data mining should 
provide multiple and/or integrated functionalities and 
techniques [5]. Because of that, we proposed an associative 
prediction model for product forecast data. In this research, 
we used the Apriori algorithm. Apriori is a technique for 
mining frequent itemsets for the Boolean association rules 
[6]. The algorithm is suitable for the mining manufacturing 
data due to the continuous change of the customers’ 
demands. Apriori algorithms can be used to provide 
sufficient critical information and improve the 
manufacturing productivity [11]. 
The third technique that we also applied is clustering. 
Clustering is a method used to group similar items and 
dissimilar items in clusters. It can also be used in certain 
fields in manufacturing to group a set of items into classes 
of similar objects. Generally, clustering is an unsupervised 
learning [6]. In [8], there are already a few past researches 
about clustering in manufacturing. It is a review based on 
the application area in manufacturing like yield, 
manufacturing process, design, defect detection, fault 
diagnosis and supply chain. In clustering techniques, there 
are five categories that can be classified; partitioning 
methods, hierarchical methods, density-based methods, 
grid-based methods and model-based methods. 
In this study, we are applying the k-means method to 
attain the link between the frequent patterns obtained from 
the previous associative model. The k-means algorithm is 
one of the simplest clustering techniques in partitioning 
categories. This algorithm aims to partition n objects into k 
clusters in which each observation belongs to the cluster 
with the nearest mean. Mean also represents the cluster’s 
centre of gravity [6]. The k-means algorithm can handle a 
mixture of categorical and numerical attributes. This 
algorithm does a distance computation by normalizing the 
numerical attributes. It uses the Euclidean distance measure 
to compute distances between the instances and clusters. 
The method is relatively scalable and efficient in processing 
large data sets. Thus, we are able to obtain the relevant link 
between patterns in the associative prediction model. 
Usually, it is computed using a fast, heuristic method that 
generally produces good solutions [17]. 
III. METHODOLOGY
A. Data Collection and Pre-processing 
There are four stages in data pre-processing phase; data 
cleaning, data integration, data transformation and data 
reduction [6]. The data is cleaned in such a way that the 
missing values are handled and the noises are smoothened in 
order to obtain a more consistent dataset for mining. Data 
integration merges data from multiple sources of data. 
Several attributes are transformed to categorical values and 
reduced to limit categories to ensure the efficiency of the 
mining process. Data reduction is the main challenge in pre-
processing because it needs to reduce the data size. It 
involves aggregating the data to identify several attributes to 
be removed and eliminate the redundant features.  
The data were collected from a plastic component 
manufacturer in Malaysia. The data consists of 42 product 
parts which is represented by 42 files. After various
considerations, two important attributes were taken into 
account in predicting the pattern for this forecast data. The 
attributes were A1; days and A2; required quantity. Each 
file contains 362 records that represent the required quantity 
in days. After that, the records are reduced to 49 that 
represent the required quantity in weeks. All 42 files were 
combined to produce one large dataset. This step is 
necessary in order to get the forecast pattern of one year for 
all components. 
Because the data mining needs a huge data to produce a 
more accurate pattern we have to simulate the data of 15
years. In this study the Monte-Carlo simulation approach 
have been used to simulate the 15 years forecast data. The 
Monte-Carlo approach is based on the generation of 
multiple trials to determine the expected value of a random 
variable. This method using the RAND() function to 
generate random numbers in interval (0, 1) and multiply 
these by the range of each variable. The range is the 
difference between the maximum value and the minimum 
value [12, 13]. Equation (1) has been used in this simulation 
for generating the required quantity in weeks. The 
simulation produced 735 records for each product (i.e. 49 
records in 15 years).  
Random values = RAND ()*(max – min) + min       (1) 
1460 2010 10th International Conference on Intelligent Systems Design and Applications
In data transformation process, the market basket 
analysis is applied to the dataset. The value of attribute A2 
was converted into range and combined with other parts of 
the product to produce p1r0, p1r1 and etc. We transform the 
data into a range in order to get the dependencies that will 
give the information on the frequency of the required 
quantity [20].The p1r1 notation can be defined as product 1 
(product latch noise eliminator) and r1 is range 1 that is 
within the required quantity of 500 – 1499. Attribute A1 
only exists between attribute week1 until week49. The data 
were mined to get the rules.  
We conducted two main experiments that are the ARM 
and Clustering. In ARM, there are two experiments using 
the Apriori algorithms. First, we used four datasets because 
of the limitations of the association approach. It is because 
there is a great possibility to find many spurious 
associations when involving a massive number of possible 
associations in large dataset [18, 19]. This is also done to 
select the important attributes in the 49 attributes. Then the 
second experiment was conducted by using the best 
attributes obtained from the first experiment. It is shown in 
Table II. In clustering, we conducted the second stage of 
experiment by applying the k-means method. 
IV. RESULT
A. ARM 
In ARM, two measurable parameters are used to measure 
the importance of a rule; confidence measure and support 
measure [15]. The minimum confidence (min_conf) and 
minimum support (min_supp) are predetermined to control 
the quality and the amount of generated rules [6, 7]. 
In the first experiment, the first dataset consisted data 
from the attribute week01 to week10. This dataset was 
measured by using min_supp of 0.25 and min_conf of 0.9, 
0.8, 0.7 and 0.60 respectively. A thousand association rules 
were generated from that experiment. The second dataset 
was formed from the attribute week11 to week20, the third 
dataset from the attribute week21 to week35 and the fourth 
dataset contained the attribute of week36 to week49.  
The best model from each datasets were chosen and 
compared to each other. The rightmost column is the 
number of rules generated after applying different min_supp
and min_conf. From the result in Table I, it shown that the 
generated number of rules was constant at the confidence 
value of 0.6 because the number of rules does not increase 
even when min_conf was changed to 0.5 to 0.1. Therefore at 
the first dataset, we get 1000 association rules and after that, 
4 rules, 10 rules and lastly 8 rules. So overall, we get 1022 
association rules with the min_conf value at 0.60. 
TABLE I.        RESULT FIRST EXPERIMENTS
Attributes  Apriori Number of 
Rules min_supp min_conf 
week01- 0.25 0.90 1000
week10 0.25 0.80 1000 
0.25 0.70 1000 
0.25 0.60 1000 
week11-
week20 
0.10 0.90 0 
0.10 0.80 2 
0.10 0.70 2 
0.10 0.60 4 
week21-
week35 
0.10 0.90 0 
0.10 0.80 3 
0.10 0.70 10 
0.10 0.60 10 
week36-
week49 
0.10 0.90 1 
0.10 0.80 2 
0.10 0.70 2 
0.10 0.60 8 
From the four datasets, we get 14 important attributes 
and 1022 association rules. The attributes shall be applied in 
the second experiment that also uses the Apriori algorithm. 
Table II shows the number of rules generated for the values 
of min_conf from 0.9 to 0.1. Based on these results, the 
same observation can be conducted, as at the confidence 
value of 0.6 the generated number of rules has stopped 
increasing. The results shown in Table II involved all 
attributes from week01 to week49.  
These results were obtained after we used the generated 
rules from the earlier four datasets. Table II also shows that 
the decreasing min_conf does not improve the number of 
rules generated. We evaluated the rules and chose the rules 
that have min_conf value higher from 0.6. In addition, the 
value of min_supp was set to 0.1 only. This is because if we 
use another value of min_supp, no rules will be generated.  
TABLE II.        RESULT SECOND EXPERIMENTS
Attributes  Apriori Number of 
Rules min_supp min_conf 
week01-
week49 
0.10 0.90 2 
0.10 0.80 10 
0.10 0.70 18 
0.10 0.60  30 
0.10 0.50 30 
0.10 0.40 30 
0.10 0.30 30 
0.10 0.20 30 
0.10 0.10 30 
TABLE III.     EXAMPLE OF THE RULES FROM PRODUCT DATASET
Rules Confidence 
1.week33=p4r1  ==> week19=p4r1      1.00 
2.week42=p3r3  ==> week45=p3r2     1.00 
3.week31=p3r2  ==> week28=p3r2      0.88 
4.week14=p4r4  ==> week19=p4r1      0.86 
5.week17=p3r2  ==> week28=p3r2  0.86 
6.week17=p4r2  ==> week19=p4r1     0.86 
7.week19=p3r1  ==> week46=p3r2      0.86 
8.week34=p3r1  ==> week21=p3r2    0.86 
9.week34=p3r1  ==> week33=p3r1      0.86 
10.week47=p4r1  ==> week49=p4r1    0.86 
11.week21=p3r2  ==> week34=p3r1 0.75 
2010 10th International Conference on Intelligent Systems Design and Applications 1461
12.week33=p3r1  ==> week34=p3r1 0.75 
13.week19=p4r1  ==> week33=p4r1     0.70 
14.week28=p3r2  ==> week17=p3r2      0.67 
15.week46=p3r2  ==> week19=p3r1  0.67 
Table III shows only 15 association rules using 
min_supp 0.1 and min_conf, 0.2 after 18 numbers of cycles. 
It is sorted by min_conf. From Table III, the first rule has 
min_conf 1.00. The first rule was represented in week33 that 
is within the period of 06
th
 September to 12
th
 September, 
product 4 (retaining ring) have been produced in the range 
of r1 (0 - 21999) and frequently associated with week19 (in 
31
st
 May to 06
th
 June) and in the same production range of 
product 4 (retaining ring), r1 (i.e. 0 – 21999). However, the 
13th rule showed otherwise that week19 is associated with 
week33 but with an unequal min_conf, i.e. 0.70.  
B. Clustering 
Based on the result obtained from the associative 
modelling as shown in Table II, we used k-means method 
for the second experiments. We used 6 for the number of 
clusters and 10 for value of seed with 30 association rules 
from Table II. The seed value is used for generating a 
random number. This number will be used in making the 
initial assignment of instances to clusters. In general, the k-
means is quite sensitive on how clusters are initially 
assigned. Thus, it is often necessary to try different values 
and evaluate the results. We used 6 as the number of cluster 
because it depends on the choice of distance measures with 
only 14 attributes involved.  
After the experiment was completed, the results show that 
there were overlapping clusters with the same attributes. 
Table IV shows all the attributes involved in the six clusters. 
C is represented to cluster and it started with 0 until 5, i.e. 
C0, C1, C2, C3, C4 and C5. Object in each cluster, such as 
w14:p4r4 was represented in week 14, product 4 and range 
production 4. In six clusters, we can see that there are same 
objects in different clusters. For example, in C1, C2 and C5, 
there are w21:p3r2 and w34:p3r1.  
Other than that, we can also conclude for each cluster 
that possesses the same characteristics in the objects. 
Criteria in C0 has only object related with product 4; 
retaining ring, C3 related with product 2; insulator AB, C4 
related with product 1; latch noise eliminator. Then, C1, C2 
and C5 are related with only product 3; housing. 
C. Knowledge Analysis 
Fig. 1 denotes the comparison between the association 
and the clustering that occurred in 14 important attributes 
(out of 49 attributes). These are the 30 final rules obtained 
from Table 3. From the result, the knowledge obtained is the 
frequent products discovered to be p3 and p4 (i.e. housing 
and retaining ring). Housing component frequently appeared 
in two ranges, r2 that represents the required quantity within 
0 to 999, and r3, the required quantity range of 1000 to 
1999. Meanwhile, retaining the ring frequently occurred in 
three ranges (i.e. r1, r2 and r4). These ranges represent 
values between 0 to 21999, 22000 to 41999 and 62000 to 
81999. 
There are relationships between the particular association 
rules as some rules imply to others (Witten and Frank, 
2005). Within the 14 discovered attributes, all attributes 
have association with each other. For example, attribute 
week33 is associated with week19, and otherwise. 
TABLE IV.        EXAMPLE OF THE RULES FROM PRODUCT DATASET
C0 C1 C2 C3 C4 C5 
w14 : p4r4 
w17 : p4r2 
w19 : p4r1 
w21 : p4r3 
w22 : p4r2 
w28 : p4r3 
w31 : p4r3 
w33 : p4r1 
w34 : p4r3 
w38 : p4r1 
w42 : p4r1 
w45 : p4r1 
w46 : p4r1 
w47 : p4r2 
w49 : p4r1 
w14 : p3r1 
w17 : p3r3 
w19 : p3r3 
w21 : p3r2 
w22 : p3r1 
w28 : p3r3 
w31 : p3r1 
w33 : p3r1 
w34 : p3r1 
w38 : p3r3 
w42 : p3r1 
w45 : p3r2 
w46 : p3r3 
w47 : p3r2 
w49 : p3r3 
w14 : p3r2 
w17 : p3r2 
w19 : p3r3 
w21 : p3r2 
w22 : p3r2 
w28 : p3r2 
w31 : p3r2 
w33 : p3r2 
w34 : p3r1 
w38 : p3r3 
w42 : p3r2 
w45 : p3r1 
w46 : p3r2 
w47 : p3r1 
w49 : p3r3 
w14 : p2r1 
w17 : p2r1 
w19 : p2r4 
w21 : p2r1 
w22 : p2r2 
w28 : p2r3 
w31 : p2r1 
w33 : p2r4 
w34 : p2r5 
w38 : p2r3 
w42 : p2r4 
w45 : p2r3 
w46 : p2r1 
w47 : p2r2 
w49 : p2r4 
w14 : p1r1 
w17 : p1r3 
w19 : p1r1 
w21 : p1r1 
w22 : p1r5 
w28 : p1r6 
w31 : p1r1 
w33 : p1r1 
w34 : p1r2 
w38 : p1r4 
w42 : p1r2 
w45 : p1r5 
w46 : p1r6 
w47 : p1r1 
w49 : p1r2 
w14 : p3r3 
w17 : p3r2 
w19 : p3r2 
w21 : p3r2 
w22 : p3r3 
w28 : p3r2 
w31 : p3r2 
w33 : p3r1 
w34 : p3r1 
w38 : p3r1 
w42 : p3r3 
w45 : p3r2 
w46 : p3r3 
w47 : p3r3 
w49 : p3r3 
However the min_conf between these two associations 
are different. If week33 is associated with week19, the 
product retaining ring produced is in the range of 21999, it 
has min_conf of 1.0 and for another association rule; if 
week19 is associated with week33 (with product retaining 
ring with the same range), it only has min_conf 0.7. 
Attribute week33 is also associated with week34 and 
week21. In this dataset, at least one attribute is associated 
with another attribute once and the most association that 
may possessed by one attribute is four associations.  
1462 2010 10th International Conference on Intelligent Systems Design and Applications
From Table IV, we attempt to relate the links between 
the association rules in Table III. From two tables, we gain 
knowledge that is displayed in Fig. 1. From that figure, it 
showed only four clusters. This is because in the 30 
association rules, association only occurred in C0, C1, C2 
and C5. In C3 and C4, association does not happen between 
the objects. 
Figure 1. Association and clustering between attributes . 
V. CONCLUSIONS
In manufacturing the environments, numerous factors 
contribute to the productivity. These factors such as the 
personnel, machine capacity or material are conditionally 
dependent on one another. Due to this, the data mining 
techniques can be applied on manufacturing data to assist 
the manufacturer in getting an interesting and valuable 
knowledge. In this paper, we used the Apriori algorithm to 
obtain association rules and predict when the most frequent 
production occurs in the plastic company. Also, we use the 
k-means algorithm to uncover the link between the observed 
frequent patterns. From the experiments, we obtained the 
association prediction model and found two most produced 
products, i.e. housing and retaining ring. Also, week 19 and 
week 33 are the frequent weeks, so it can be concluded that 
the production is massive in week 19 and week 33.  
Further analysis can be done using other techniques in 
association rules or clustering. Data in manufacturing 
possess the characteristics of imbalance and fluctuate. This 
will be an interesting challenge to the data mining 
researchers to obtain interesting and valuable information. 
This study can also be viewed from other angle to solve 
another issue that might be raised in the plastic industry 
such as the cycle time production, capacity, maintenance or 
quality control. 
REFERENCES
[1] Hill, T. & Lewicki, P. 2010. Introduction to data mining:  
STATISTICA Data Mining. http://www.statsoft.com/textbook/data-
mining-techniques/ [06 June 2010]. 
[2] Raviwongse, R., Allada, V. & Sandidge Jr., T. 2000. Plastic 
Manufacturing Process Selection Methodology Using Self-
Organising Map (SOM)/Fuzzy Analysis. Int. Journal Adv. 
Manufacturing Technology. 16 155-161. 
[3] Bergmann, A. 2010. Data mining for manufacturing: Preventive 
Maintenance, Failure Prediction, Quality Control. 
[4] Wang, K. 2006. Knowledge Enterprise: Intelligent Strategies in 
Product Design,  Manufacturing and Management. Kovacs, G., 
Wozny, M. &Fang, M. Data mining in manufacturing: The nature 
and implications. 207. Springer Boston. 
[5] Liao, T. W. & Triantaphyllou, E. 2007. Recent Advances in Data 
Mining of Enterprise Data: Algorithms and Applications. 6. 
Singapore. World Scientific Publishing Co. Pte. Ltd. 
C0 C0,C1,C2 C0, C1, C2, C5 C1, C2, C5 C1, C5 
w14:p4r4 
w47:p4r1 
w49:p4r1 
w19:p4r1 w42:p3r3 
w38:p3r3 
w31:p3r2 
w46:p3r3 
w46:p3r2 
w17:p4r2 
w17:p3r2 
w33:p3r1 
w33:p4r1 
w45:p3r2 
w28:p3r2 
w34:p3r1 w21:p3r2 
2010 10th International Conference on Intelligent Systems Design and Applications 1463
[6]     Han, J. & Kamber, M. 2001. Data mining Concepts and techniques.
Data mining on what kind of data. Academic press. Morgan 
Kaufmann. 
[7] Agrawal, R., Imielinski, T. & Swami, A. N. 1993. Mining 
association rules between set of items in large databases. 
Proceedings of the 1993 ACM SIGMOD International Conference 
on Management of Data, pp. 207-216. 
[8] Choudhary, A. K., Harding, J. A. & Tiwari, M. K. 2008. Data 
mining in manufacturing: a review based on the kind of knowledge. 
Journal Intell Manufacturing.
[9] Jiao, J. & Zhang, Y. 2005. Product portfolio identification based on 
association rule mining. Computer Aided Design. 37 149-172. 
[10] Pawar, P. A. & Aggarwal, A. K. 2004. Associative rule mining of 
mobile data           services usage for preference analysis, 
personalization & promotion. Proceeding of WSEAS International 
Conference on Simulation, Modeling and Optimization.
[11] Ajoku, P. N. & Nnaji, B. 2006. Improving productivity in 
manufacturing environments using data mining. Artificial 
Intelligence and data mining workshop.
[12] Jeges, R. Monte Carlo simulation in MS Excel. 
http://www.projectsmart.co.uk/docs/monte-carlo-simulation.pdf [12 
March 2010]. 
[13] Metropolis, N. 1987. The beginning of the monte carlo method. Los 
Alamos Science. (1987 Special issue dedicated to Stanislaw Ulam): 
125-130. 
[14] Agrawal, R. & Srikant, R. 1994. Fast algorithms for mining 
association rules in large databases. Proceedings of the 20th 
International Conference on Very Large Databases, pp. 487 - 499. 
[15] Witten, I. H. & Frank, E. 2005. Data mining: Practical machine 
learning tools and techniques. 2nd Ed. New York. Elsevier. 
[16] Nedunchezhian, R. & Anbumani, K. 2007. Post mining- Discovering 
valid rules from different sized data sources. International Journal 
of Information Technology. 3 (1): 47-53. 
[17] Khattak, A. M., Khan, A. M., Tahir Rasheed, Lee, Y-K., and Lee, S. 
2009. Comparative Analysis of XLMiner and Weka for Association 
Rule Mining and Clustering, The International Conference on 
Database Theory and Application, pp. 82-89.  
[18] Webb, G.I. 2007. Discovering Significant Patterns. Machine 
Learning 68(1). Netherlands: Springer, pp. 1-33. 
[19] A. Gionis, H. Mannila, T. Mielikainen, and P. Tsaparas, 2007.  
Assessing Data Mining Results via Swap Randomization, ACM 
Transactions on Knowledge Discovery from Data (TKDD), 1 (3): 
Article No. 14. 
[20] Gaurav, N. P. & Prabhakaran, B. 2009. Association rule mining in 
multiple, multidimensional time series medical data. Proceedings of 
the 2009 IEEE International conference multimedia and expo, pp. 
1716-1719. 
1464 2010 10th International Conference on Intelligent Systems Design and Applications
