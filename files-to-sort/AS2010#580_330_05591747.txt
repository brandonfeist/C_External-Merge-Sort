APT-Structure : Efficient Mining of Frequent
Patterns
Shiwei Zhu
School of Economics and Management
Beihang University
Beijing 100191, China
Email: sherwinzhu@buaa.edu.cn
Renqian Zhang
School of Economics and Management
Beihang University
Beijing 100191, China
Email: zhangrenqian@buaa.edu.cn
Guoping Xia
School of Economics and Management
Beihang University
Beijing 100191, China
Email: gxia@buaa.edu.cn
Abstract—Frequent pattern mining is a key step in many
data mining applications. In this paper, we propose a simple
and novel pattern growth algorithm, which uses a compact data
structure named Array-based Prefix Tree(APT). The APT has a
distinct feature that the space requirement can be predictable in
advance. The memory usage of APT is less than FP-Tree that
uses pointer to maintain the link between parent and child nodes,
and the traversal cost is lower. The mining algorithm based on
APT uses top-down traversal strategy, and unfiltered pseudo-
construct conditional database, which can improve computational
performance. Further computational experiments show that APT
algorithm is more efficient, and performs better than FPGrowth*
and AFOPT.
Index Terms—Frequent pattern mining, association mining,
Array-based Prefix Tree
I. INTRODUCTION
Since frequent pattern mining was first proposed by Agrawal
et al.(1993) for market basket analysis in the form of associa-
tion rule mining, a large number of concerned algorithms have
been proposed. The frequent pattern mining can be depicted
as follows.
Let ? be a set of items. A set ? = {?1, ..., ??} ? ? is called
an itemset, or a k-itemset if it contains k items.
A transaction over ? is a couple ? = (???, ?) where tid
is the transaction identifier and ? is an itemset. A transaction
? = (???, ?) is said to support an itemset ? ? ?, if ? ? ? .
A transaction database ? over ? is a set of transactions
over ?. We omit ? whenever it is clear from the context.
The support of an itemset ? in ? is the number of
transactions that support itemset ? in ?, denoted as ???(?).
Given a transaction database ? and a support threshold
??? ????, a itemset is called frequent itemset or frequent
pattern, if and only if ???(?) ? ??? ????.
Definition 1: Let ? be a transaction database over a set
of items ?, and ??? ???? a minimal support threshold. The
collection of frequent itemsets in ? with respect to ??? ????
is denoted by
?(?,??? ????) = {? ? ??????(?,?) ? ??? ????},
or simply ? , if ? and ??? ???? are clear from context.
The problem of frequent pattern mining is to find the
?(?,??? ????), given a set of items ?, a transaction
database ? over ? and minimal support threshold ??? ????.
Existing algorithms can be classed into three categories:
Apriori, FP-growth and Eclat. The Aprior is the first algorithm
proposed by Agrawal et al. [1], which employs a candidate
generate-and-test approach. Apriori is a breadth-first algorithm
and is based on the downward closure property:A ? itemset is
frequent only if all of its sub-itemsets are frequent. Park et al.
[2] proposed DHP algorithm, which use hashing technique to
speed up the item set counting. Savasere et al. [3] proposed
Partition algorithm, which partition the transaction database
into multi small blocks, in which local frequent item sets are
firstly found and then the global frequent item sets are got.
TreeProjection [4] algorithm constructs a lexicographical tree
and projects a large database transaction into a set of reduced,
item-based sub-databases. Toivonen [5] proposed Sampling
algorithm, which only scans the transaction database once.
Brin et al. [6] proposed DIC algorithm, which needs scanning
the transaction database 1.5 times.
FP-growth is a pattern growth algorithm proposed by Han et
al. [7], which works in a divide-and-conquer way. FP-growth
is a depth-first algorithm and does not generate the candidate.
The pattern growth is achieved by the concatenation of the
suffix pattern with the frequent patterns generated from a
conditional FP-tree. Pei et al. [8] proposed H-Mine algorithm,
which explores a hyper-structure. Liu et al. [9] proposed
OP algorithm, which opportunistically chooses array-based or
tree-based structure to represent the conditional database. Liu
et al. [10] proposed AFOPT algorithm, which use ascend-
ing frequency ordered prefix-tree to organize the conditional
database. Grahnea and Zhu [11] proposed FPGrowth* algo-
rithm, which use array technique for efficient mining.
Both Aprior and FP-growth mine frequent patterns form
a horizontal data format transaction database, Zaki [12] pro-
posed Equivalence CLASS Transfromation(Eclat) algorithm
by exploring the vertical data format, with a depth-first search
order like FP-growth. The computation is done by intersection
of tid sets. For dense dataset, dEclat [13] is proposed, which
uses a novel vertical data representation called Diffset, that
only keeps track of difference in the tid sets.
This pater proposes a new method to improve the mining
efficiency, which is based on Array-based Prefix Tree (APT).
The rest of paper is organized as follows. The next section, the
data structure of APT and the mining algorithm are depicted.
2010 International Conference on E-Business and E-Government
978-0-7695-3997-3/10 $26.00 © 2010 IEEE
DOI 10.1109/ICEE.2010.354
1395
Section 3 presents computational experiments that show the
new algorithm is more efficient than others. In the last section,
the paper is concluded.
II. MINING FREQUENT PATTERNS USING APT
In this section, we introduce the APT structure, and then
a novel algorithm named APT algorithm is developed to find
the frequent item set.
A. APT Structure
The APT Structure is used to compactly store the transaction
database in computer’s memory. By using array structure, the
traversal cost can be diminished. Our general idea of APT
Structure can be illustrated in the following example. Fig. 1 is
an example transaction database, which has 5 transactions. Let
the minimum support threshold is ??? ???? = 2. Following
the Apriori property, only frequent items will be in frequent
item set. By scanning the transaction database, the frequent
items {f:4,c:4,a:3,b:3,m:3,p:3} can be found.
7,' ,WHPV 2UGHUHGIUHTXHQWLWHPV
 IDFGJLPS IFDPS
 EFNVS FES
 EIKMR IE
 DEFIOPR IFDEP
 DIFHOSPQ IFDPS
Fig. 1. A transaction database.
Then the transaction database is scanned again to build
APT structure by removing the infrequent items and merging
the same transactions. As shown in Fig. 2, the left is FP-
Tree representation, and the right is APT structure. In APT
structure, each row is corresponding to a transaction, but
without duplicated items with previous row. In a row, each
entry includes three field: an item, a count and a link to the
next different row.
B. The APT Algorithm
Fig. 3 shows the pseudocode of APT algorithm. Given
a transaction database ? and a minimum support threshold
??? ????, APT algorithm scans the original database twice
to mine all frequent item sets. In lines 2-3 of the pseudocode,
the transaction database is first scanned, all the items are
counted, and frequent items are sorted in ascending order of
their supports, denoted as ? = {?1, ?2, . . . , ??}. Then, in line
4, we perform the second scanning of the transaction database
to construct the APT structure. In lines 5-9, the conditional
database for each ? ? ? is constructed, denoted as ??. For
each ? ? ?, the depth traversal is performed in lines 10-15.
Lines 17-31 are the pseudocode of procedure DepthTravese.
When mining conditional database ?? is finished, because it
contains the other items, the branch needs to be inserted into
the remaining conditional database. Provided the first frequent
item of a branch is item ?, then the branch will be inserted
into ?? .
URRW
I
D
F
P
S
E
E
P
F
E
S
(a) FP-Tree Structure
I F D P S
E P
E
F E S
(b) APT Structure
Fig. 2. The FP-Tree and APT structure.
1: procedure APT(?, ??? ????)
2: First Scan ?, Count and ? = {?1, ?2, . . . , ??};
3: Sort items in ? in ascending order of their support;
4: Second Scan ?, Construct the APT structure;
5: for all branch ? ? APT do
6: if ? is the first item then
7: Insert ? into conditional database ??;
8: end if
9: end for
10: for all item ? ? ? do
11: frequent itemset ? = {?};
12: ? = ?
?
?;
13: DepthTravese(??, ?, ?);
14: PushRight(??);
15: end for
16: end procedure
17: procedure DEPTHTRAVESE(??, ?, ?)
18: Scan the ??, Count and ?? = {?1, ?2, . . . , ??};
19: for all branch ? ? ?? do
20: if ? is the first item then
21: Insert ? into conditional database ???{?}
22: end if
23: end for
24: for all item ? ? ?? do
25: frequent itemset ? = ?
?
{?};
26: ? = ?
?
?;
27: DepthTravese(???{?}, ?, ??);
28: PushRight(???{?});
29: end for
30: return ? ;
31: end procedure
Fig. 3. The APT Algorithm.
C. The Conditional Database and Memory Usage
Because to build the physical construction of the conditional
database is expensive in memory and time, the APT algorithm
adopts the pesudo-construct strategy. In APT, only the branch
link are saved in the conditional database, so the construction
cost is relatively low. In the worst case, the total memory used
1396
by the APT algorithm is 4?, where ? is the number of entry in
APT structure. Of the 4? memory space, 3? is used to store the
APT structure, ? is used to store the conditional database. The
exact number of entry is unknown before the APT structure
is constructed, but it is less than the total transaction database
item count.
Comparing with other frequent item set mining method, the
efficiency of APT algorithm comes from the following as-
pects. First, APT algorithm adopts the frequent pattern growth
method and depth-first traversal strategy. Second, APT algo-
rithm uses pesudo-construct strategy to build the conditional
database, and the total memory consumption is determined
by the APT structure, which is unlike other frequent pattern
growth algorithms. Third, APT algorithm uses array to build
APT structure, so the transaction traversal is faster than other
methods, such as link list.
III. COMPUTATION EXPERIMENTALS
In this section, we study the performances of APT, FP-
Growth* and AFOPT algorithms on various real-world and
artificial data sets.
A. Experimental Setup
In our experiments, to make it easier to bridge our bench-
marks with previously published experimental results, we use
three real-world data sets and one artifical data set whose
general characteristics are summarized in Table. I.
TABLE I
SOME CHARACTERISTICS OF EXPERIMENTAL DATA SETS.
Data set #Item #Record #Avg. Length Source
Retail 16470 88162 10.3 Belgian Retail store
T10I4D100K 870 100000 10.1 IBM Almaden
Wap 8460 1560 141.3 WebACE
La1 29714 3204 151.1 LA Times (TREC)
These data sets were obtained from various application do-
mains. Retail contains the retail market basket data from an
anonymous Belgian retail store1. T10I4D100K was generated
using a data generator obtained from IBM Almaden, which is
often used in the association rule research community2. Wap
was from the WebACE project. Each document corresponds to
a web page listed in the subject hierarchy of Yahoo!. Finally,
La1 was obtained from articles of the Los Angeles Times that
was used in TREC-53.
The FPGrowth* and AFOPT algorithm are different imple-
mentations of FP-growth algorithm. The former uses a novel
array-based technique to reduces the need to traverse FP-trees,
and the later uses ascending frequency ordered prefix-tree
and a top-down strategy. The source code can found from
http://fimi.cs.helsinki.fi/src/.
All the algorithms were coded in C and C++, build with
Microsoft Visual C++ 2008, and run on a Microsoft Windows
7 Ultimate platform. The experimental PC is installed a Intel
1Available at http://fimi.cs.helsinki.fi/data/.
2Available at http://fimi.cs.helsinki.fi/data/.
3Available at http://www.daviddlewis.com/.
2 4 6 8 10 12
x 10?3
102
103
104
105
support(%) 
Ex
ec
ut
io
n 
tim
e(m
illis
ec
on
d)
 
 
APT
FPGrowth*
AFOPT
(a) Retail
2 4 6 8 10 12
x 10?3
102
103
104
support(%) 
Ex
ec
ut
io
n 
tim
e(m
illis
ec
on
d)
 
 
APT
FPGrowth*
AFOPT
(b) T10I4D100K
5 10 15 20 25 30
100
105
support(%) 
Ex
ec
ut
io
n 
tim
e(m
illis
ec
on
d)
 
 
APT
FPGrowth*
AFOPT
(c) Wap
0.5 1 1.5 2 2.5 3
103
104
105
106
107
support(%) 
Ex
ec
ut
io
n 
tim
e(m
illis
ec
on
d)
 
 
APT
FPGrowth*
AFOPT
(d) La1
Fig. 4. Computational times of APT, FPGrowth* and AFOPT.
Q9550 CPU, 4 GB DDRIII 1066 MHz RAM, and Seagate
Barracuda 7200.11 500GB Hard Disk.
B. Experimental Result
First, We would like to compare the performance of the
APT, FPGrowth* and AFOPT. Fig. 4 shows the results of
data sets Retail, T10I4D100K, Wap and La12. We can
see that the APT has the best performance.
For Retail, we set the minimum support from 0.002%
1397
2 4 6 8 10 12
x 10?3
102
103
104
support(%) 
Ex
ec
ut
io
n 
tim
e(m
illis
ec
on
d)
 
 
Ascending
Descending
(a) Retail
2 4 6 8 10 12
x 10?3
102
103
104
support(%) 
Ex
ec
ut
io
n 
tim
e(m
illis
ec
on
d)
 
 
Ascending
Descending
(b) T10I4D100K
Fig. 5. Computational times of APT with ascending and descending order.
to 0.0012%, with corresponding absolute support from 2 to
13. When the minimum support is 0.002%, there are 14246
frequent items and 1839084261 frequent item sets. When the
minimum support is 0.012%, there are 8241 frequent items
and 155111 frequent item sets. When the minimum support
is lower, the APT is much faster than the others, and when
the minimum support gets higher, the executing time is closed
to others. For T10I4D100K, the minimum support is from
0.002% to 0.0012% and the absolute support is from 2 to 12.
When the minimum support is 0.002%, there are 869 frequent
items and 19561714 frequent item sets. When the min support
is 0.012%, there are 866 frequent items and 286023 frequent
item sets. For Wap, because the number of transactions is
small, we set the minimum support from 2% to 12%, with
corresponding absolute support from 31 to 187. For La1, the
minimum support is from 0.5% to 3%, with corresponding
absolute support from 16 to 96. When the minimum support
is higher, the executing times are closed, because there are
little frequent item sets. However, when the minimum support
decreases, the frequent item sets increase substantially, and
computational times go up to 4661s and 3298s for FPGrowth*
and AFOPT, respectively. Second, We compare the impact of
item order for the performance of the APT. Generally, the
descending order means more sharing entries and less memory
usage. But, for traversal process, the ascending means less
depth and more performance. Fig. 5 shows the results of data
sets Retail, T10I4D100K for APT with ascending and
descending order. Generally, the execution time with ascending
order is less than with descending order, but the former needs
more space than the later. For Retail, there is obvious
difference between ascending order and descending order. But,
for T10I4D100K, there is little difference. This is related to
the property of transaction dataset.
IV. CONCLUSION
In this paper, we have proposed a new Array-based Prefix
Tree data structure named APT structure, which is a compact
representation of transaction database and need less storage
space than that of FP-Tree. APT structure is used in our novel
algorithm named APT algorithm for mining all frequent item
sets. The APT algorithm traverses the search space from top
to down, and store the conditional database using pseudo-
construct in the mining process. Performance comparisons
of APT algorithm against other well-known algorithms in-
cluding FPGrowth* and AFOPT have been done. The results
shows that the APT algorithm outperforms the others at all
support levels in computational times. The APT algorithm is
simple and has a distinct feature that the space requirement
is predictable in advance, which has a linear relationship
with the number of entries. In the further, we will study the
performance on very large databases.
ACKNOWLEDGMENT
This research is supported by the National Natural Science
Foundation of China (NSFC) (No.70871005, 70671007).
REFERENCES
[1] R. Agrawal, T. Imielinski, and A. Swami, “Mining association rules
between sets of items in large databases,” in Proceedings of the 1993
ACM SIGMOD International Conference on Management of Data,
Washington, DC, 1993, pp. 207–216.
[2] J. Park, M. Chen, and P. Yu, “An effective hash-based algorithm for
mining association rules,” SIGMOD Rec., vol. 24, no. 2, pp. 175–186,
1995.
[3] A. Savasere, E. Omiecinski, and S. Navathe, “An efficient algorithm for
mining association rules in large databases,” in VLDB ’95: Proceedings
of the 21th International Conference on Very Large Data Bases. San
Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1995, pp. 432–
444.
[4] R. Agarwal, C. Aggarwal, and V. Prasad, “A tree projection algorithm
for generation of frequent itemsets,” Journal of Parallel and Distributed
Computing, vol. 61, pp. 350–371, 2000.
[5] H. Toivonen, “Sampling large databases for association rules.” Morgan
Kaufmann, 1996, pp. 134–145.
[6] S. Brin, R. Motwani, J. Ullman, and S. Tsur, “Dynamic itemset counting
and implication rules for market basket data.” ACM Press, 1997, pp.
255–264.
[7] J. Han, J. Pei, and Y. Yin, “Mining frequent patterns without candidate
generation,” in Proceedings of the 2000 ACM SIGMOD International
Conference on Management of Data, Dallas, TX, 2000, pp. 1–12.
[8] J. Pei, J. Han, H. Lu, S. Nishio, S. Tang, and D. Yang, “H-mine: Hyper-
structure mining of frequent patterns in large databases,” 2001, pp. 441–
448.
[9] J. Liu, Y. Pan, K. Wang, and J. Han, “Mining frequent item sets by
opportunistic projection,” in KDD ’02: Proceedings of the eighth ACM
SIGKDD international conference on Knowledge discovery and data
mining. New York, NY, USA: ACM, 2002, pp. 229–238.
[10] G. Liu, H. Lu, and J. Yu, “Afopt: An efficient implementation of pattern
growth approach,” in In Proceedings of the ICDM workshop, 2003.
[11] G. Grahne and G. Zhu, “Efficiently using prefix-trees in mining frequent
itemsets,” 2003.
[12] M. Zaki, S. Parthasarathy, M. Ogihara, and W. Li, “New algorithms for
fast discovery of association rules,” in In 3rd Intl. Conf. on Knowledge
Discovery and Data Mining. AAAI Press, 1997, pp. 283–286.
[13] M. Zaki and K. Gouda, “Fast vertical mining using diffsets,” 2001, pp.
326–335.
1398
