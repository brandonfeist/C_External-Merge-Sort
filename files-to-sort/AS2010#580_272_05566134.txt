Hypothesis Testing Based Knowledge Discovery in  
Distributed Multiple Data Sources  
Shilei Bai,  Hui Ren,  Wei Jiang,  Yujian Jiang 
School of Information Engineering, Communication University of China 
Beijing, P. R. China  
E-mail: bsl@cuc.edu.cn 
 
Abstract—In the past several years, most data mining researchers 
focus on data mining from single data source. Nowadays, data 
mining from multiple data sources is a new problem in Web 
environment and is also an efficient technique for solving 
knowledge discovery in distributed databases. A new method for 
mining multi-data sources is presented in this paper. By sharing 
knowledge patterns discovered in other similar data sources, 
hypothesis testing is employed for verifying whether the patterns 
are also suitable for local data source or not. So that can improve 
the efficiency of KDD greatly. Finally the effectiveness of this 
method is analyzed and experimental result is given. This method 
can be extended as an efficient data mining algorithm in case of 
apriori hypothesizes are provided. And it can be also used for 
incremental data mining. 
Keywords- multiple data sources, hypothesis testing, knowledge 
sharing, knowledge discovery 
I.  INTRODUCTION  
With the availability of inexpensive storage and the 
progress in data capture technology, many organizations haves 
created ultra-large databases, and this trend is expected to 
grow. As an important way to obtain knowledge from mass 
data, data mining (also called knowledge discovery in 
databases, KDD) has been brought into focus. Increasing 
number of scholars have engaged in this area since 1990s, and 
achieved considerable fruits. However, the traditional research 
of KDD mostly aimed at centralized data source. With the 
rapid development of network technology, it is possible to 
realize the remote accessing of data and large-scale information 
sharing. Large amount of distributed and multiple data source 
information systems appeared. In practice, KDD systems need 
more and more deal with distributed multiple data source and 
incremental data [1,2,3]. In the environment of distributed 
multiple data sources, KDD faces some new problems as 
follows: 
i) Usually, there are many similar data sources in 
distributed database system. Take a big chain-store enterprise 
for example, affiliated agencies in different regions usually 
build local database, which is similar to each other. Does the 
knowledge discovered in one database equally applicable to the 
others?  
ii) In distributed data source environment, different data 
sources may be used by many users. Different users often have 
customizing requirements about the knowledge discovered in 
database, such as support, confidence, etc. If one user had run 
data mining algorithm and got some rules, does these rules 
meet another user’s demand?  
Above problems will be discussed in this paper, and an 
effective solution is given. 
II. RELATED WORK 
In order to discover knowledge in multiple data sources or 
distributed databases, the following methods are often used: 
A. Centralized Mining 
To collect data from various data sources, and establish a 
data warehouse or data mart, then run the data mining 
algorithms in local synthesized database [4,5]. This method can 
realize exhaustive mining through full search in all data 
records, but its cost is too high. Because KDD faces mass data, 
the spatio-temporal complexity of mining algorithm is usually 
very high and is sensitive to the amount of records. At the same 
time, the cost of data communication, data conversion, data 
pre-processing, etc. used to establish centralized database are 
also huge. In addition, information security, data 
confidentiality and other issues may also be involved in. 
Therefore, centralized mining is not an idea solution. 
B. Partition Method 
To break the data mining task of multiple data sources into 
three subtasks: i) classify the multiple data sources; ii) mining 
each database separately; iii) synthesis mining knowledge from 
similar databases [6,7]. Meanwhile, multi-agent technology has 
also been brought into distributed KDD systems. The system 
sends data mining agent to each site for data mining, then sends 
back the mining results to local for synthesis [8,9,10]. All the 
data records have to be processed through these methods, the 
complexity can not be significantly reduced, and the synthesis 
of pattern is also lack of effective algorithm at present [11]. 
C. Mining on Sample Space 
The research on sample detection method for data mining 
has been proposed since several years ago. Tovienven[12] 
proposed a random sampling approach to mining association  
rules. Firstly, mining the sample space, then check the results 
of the remaining data. This algorithm has higher efficiency. 
978-1-4244-5143-2/10/$26.00 ©2010 IEEE
However, the method is applicable to the mining of single, 
homogeneous database. Jong[13] and other researchers have 
proposed a variable precision association rule mining method. 
Firstly, obtain necessary knowledge from some samples by 
sampling techniques, and then use this information to mine the 
entire database in a heuristic way. The methods mentioned 
above are all faced to the single data sets which can not be used 
for distributed situation. Reference [14] proposed a distributed 
variable precision association rule mining algorithms SMDM. 
Firstly, randomly sample a certain capacity of data at each site, 
mine the sample data, and then re-use meta-learning strategies 
to obtain the final results. This method has higher efficiency 
but the accuracy of results decreases sharply. Meanwhile, it is 
considerable difficult for meta-learning and verification. 
III. KNOWLEDGE DISCOVERY METHOD BASED ON 
HYPOTHESIS TESTING 
In fact, the multiple data sources of the same enterprise or 
the same field are so similar that the implicated knowledge in 
these data may be shared to some extent. As a result, KDD 
system could improve the efficiency of knowledge discovery 
by knowledge-sharing mechanisms. We can use the knowledge 
discovered from database D to guide the knowledge discovery 
process in a similar database D’. We just need to test the 
knowledge and determine the applicability in D’, then reduce 
the repetitive work. It is more effective than traditional data 
mining. If testing all the related records in database is 
avoidable, the efficiency of knowledge discovery can be 
further improved. 
Based on this idea, knowledge from another similar data 
sources can be seen as a hypothesis in the local database, and 
the records in local database are used to evaluate the validity of 
the knowledge. So we can transform the knowledge judging 
problem into hypothesis testing problem. It is similar to 
product acceptance: rules (hypothesis) can be regarded as 
acceptance criteria, each record in local database looks as a 
product. The products consistent with the criteria are qualified, 
otherwise are rejected. If the rejection rate is too high, it means 
that the hypothesis does not conform to the database, which 
can not commendably describe the data. So it should be 
refused. If the rejection rate is low, the rule is adopted. In order 
to efficiently determine the hypothesis, we can sample from the 
database reasonably to reduce the capacity of subsample. Then 
we just need test the subsample space to verify the hypothesis. 
The traditional methods of sampling inspection mainly 
include three modes: unitary, duplex and sequential. Though 
the requirement of sub-sample capacity for unitary mode is 
larger than the other two, the process is simple. Since the 
testing process discussed in this paper is not complicated and 
not very sensitive to subsample capacity, unitary mode is a 
feasible method.  
Unitary sampling inspection means to sample only once, 
then makes a decision to accept or reject according to the 
observations results. It can be divided into two types, quality 
based inspection and the amount based inspection. The 
problem discussed in this paper is to check whether the data 
record is consistent with a given rule, so quality based 
inspection is adopted. 
In the quality based inspection, two criteria of rejection rate 
is usually set: p0 and p1, and p0?p1. p0 is called acceptable 
quality level (AQL), and p1 is called limit quality level 
(LQL).When the rejection rate p?p0, receive this batch of 
product; When p?p1, refuse this batch of products. Suppose 
the subsample capacity is n, and k stands for the number of 
inferior product in subsample. We need to choose suitable 
integers n and c to make the error rate as small as possible 
while making the following judgment: "When k ?c, this batch 
of product is eligible; When k>c, this batch of product is 
ineligible." None sampling inspection methods can guarantee 
the conclusion of testing to be entirely correct. But we can pre-
set a upper limit probability of getting wrong conclusion, and 
determine n and c by the pre-set limit. Then we can assure that 
the conclusion gotten from the sampling inspection to be 
correct in a high probability. In mathematical statistics, there 
are two kinds of mistake[15]. We call “refusing a correct 
hypothesis mistakenly” as the first mistake, and call “accept a 
wrong hypothesis mistakenly” as the second mistake. Suppose 
the upper limit probability that we make the first mistake is ?, 
and the upper limit probability that we make the second 
mistake is ?, this is a hypothesis testing problem. For a 
hypothesis H0 : p ? 1-Cr, Cr is a pre-set lower limit of 
reliability. Now we test H0 and the requirements are as follows: 
when p?1-Cr, the probability of committing  the first mistake 
does not exceed ?; when p>1-Cr, the probability of committing 
the second mistake does not exceed ?. 
For the population whose rejection rate is p, the number of 
population is N, the number of reject is M. We randomly select 
n products, suppose D is the number of reject. Then when N is 
a finite, D obeys hypergeometric h(n, M, N): 
( ; , , )
M N M
k n k
P D k N n p
N
n
Np Nq
k n k
N
n
?
?
= =
?
=
? ?? ?? ?? ?? ?? ?
? ?? ?? ?
? ?? ?? ?? ?? ?? ?
? ?? ?? ?
                              (1) 
When N is infinite, D obeys binomial distribution, since the 
amount of data which KDD dealt with is very large, binomial 
distribution can be used to approximate. In this case, the 
probability of that the reject in the sub-sample is not more than 
c is as follows: 
0
1 1
( , , ) (1 )
1
(1 )
c
k n k
k
c n c
p
n
L p n c p p
k
n
n x x dx
c
?
=
? ?
= ?
?
= ?
? ?? ?? ?
? ?? ?? ?
?
?                       (2) 
The requirements of the sampling plan can be expressed as 
follow: 
        0
1
( , , ) 1
( , , )
L p n c p p
L p n c p p
?
?
? ? ?
? >
???                               (3) 
Equation (2) showed that L is a monotonic decreasing 
function of p, then n and c which satisfies (3) can be obtained 
by solving the following equations: 
        0
1
( , , ) 1
( , , )
L p n c
L p n c
?
?
= ?
=
???                                                   (4) 
The solution of function (4) is complex. During the 
practical application, n and c are solved through a special 
mathematics table, which is more convenient for computer 
processing. 
 The above sampling method depends entirely on the 
parameters n and c, it is also known as (n, c) sampling plan. 
And the function L (p, n, c) is called operating characteristic 
function of (n, c) sampling plan, as shown in Figure 1: 
 
 
 
 
 
 
 
Figure1. Sketch map of operating characteristic function. 
Take association rules as example, assume that we have 
obtained a rule from the database D', R: yxxx n ?? …21 , 
now we verify whether it is equally effective in database D. 
Suppose the data set which contains items x1?x2?…?xn is S, 
and the data records don’t comply with the rule in S is SR. 
Suppose Rp S S= , then the confidence of the association 
rule R is equal to 1-p. Take S as the population, randomly 
sample n data records from S as subsample to test rule R. 
Suppose the number of records which don’t complied with R in 
subsample is k. It is reasonable to accept R with the smaller k, 
and it is reasonable to refuse R with the bigger k. Suppose the 
lower limit confidence of accepting R is Cr, then if 1p Cr? ? , 
accept R; if 1p Cr> ? , refuse R. The most convenient 
approach is to suppose 0 1 1p p Cr= = ? , 1? ?= << , then 
determine the integers n and c according to the above equation 
(4), and make sure the sampling plan. But unfortunately this 
kind of sampling plan does not exist. Because 
take 0 1 1p p Cr= = ? into equation (4), then we get 1? ?= ? .It 
is unable to meet the accuracy requirements: , 1? ? << . As can 
be seen from figure 1, when n and c are unchanged, 
if , 1? ? << , then 1 0p p is rather big. While p0 is closer to p1, 
then the curve in the diagram between p0 and p1 may be 
steeper. Which indicated the larger subsample n, high-
efficiency test can not be achieved. 
In practice we can pre-set Cr0 and Cr, and Cr0>Cr. Cr is 
defined as the lower limit confidence of rule R. Suppose the 
confidence of rule R is CR. If CR?Cr0, it means that R has 
higher reliability and is acceptable; if CR<Cr, it indicates that 
the credibility of R is low and should be denied. Suppose 
AQL=1-Cr0, LQL=1-Cr, Cr0>Cr? 1? ?= << , (n, c) is 
determined by (4) for testing rule R. If the testing result is 
accepting R, then the probability of CR<Cr is smaller than ?, 
re-testing is not needed; if the result is rejecting R, the 
probability of CR>Cr0 is smaller than ?, but it is still possible 
that the value of CR is between Cr0 and Cr. Thus R may still be 
an accepted rule. In this case, whether completely testing in the 
whole population space is needed, is decided according to our 
actual necessaries. For the high confidence rules, this approach 
only needs a small amount of samples to test; completely 
testing is needed only in case of very lower confidence rules. 
Because the testing approach in this paper is carried out 
through data comparison by computer, it is not very sensitive 
to the subsample capacity. So Cr0 can be made adequately 
close to Cr, thereby reducing the risk that mistakenly refuse 
such rules whose CR is between Cr0 and Cr. For the data of 
random distribution, the inspection process does not require a 
specific sampling plan and data pre-processing. It only need 
compare such related data records with the rule one by one. If it 
is found the rule can be accepted or rejected, then stopped. 
IV. EXPERIMENTAL RESULTS AND ANALYSIS 
In order to demonstrate the correctness of the algorithm, we 
have done several experiments with association rules. The 
experiment data set is generated by Assocgen[16]. The number 
of transactions in database D and D’ is 10,000 respectively. 
The association rules are generated by algorithm mentioned in 
[16] and the algorithm is implemented with C++. Set 
min_sup=2%, and min_conf=60%, we get 19 association rules 
from database D’. Then we verify these 19 rules in database D. 
Set ?=?=0.05. While hypothesis testing, we set Cr0=0.85, 
Cr=0.7, and get the sampling plan (n, c) ?(506, 122). The 
testing result shows that: in eleven rules whose confidence are 
more than 70%, there are only two rules are refused; and the 
other eight rules whose confidence are less than 70% are all 
refused. In this testing, there are only 506 records to be 
checked. It means the method proposed in this paper has higher 
efficiency. 
Because the hypothesis is made by the requirement of 
users, the parameters Cr0 and Cr are set by the user. Then 
different users may set the confidence of rules flexibly in need 
of their tasks.  As for the support of rules, hypothesis testing 
methods can also be used to determine whether it is satisfied 
the user’s requirements (the specific approach shall not be 
repeated here). Thus, the hypothesis testing method can solve 
the second problem proposed in section I. 
This method is also applicable to other types knowledge 
discovery tasks such as sequential patterns, classification rules, 
etc. It only needs to pre-process the data correspondingly with 
the knowledge type. Furthermore, the method can also be used 
to verify conjectural knowledge. Hypothesis derived from 
background knowledge or prior knowledge, or given by human 
experts, can also be verified by this method. 
p 
 ? 
0 
1-? 
L 
p0 p1 
1 
1 
What’s more, the method we proposed is valid on the 
multiple data sources which are similar, but not always 
effective to those multiple databases of different quality. 
V. CONCLUSIONS AND FUTURE WORK  
Data mining from multiple data sources is a new problem in 
Web environment and is also an efficient technique for solving 
knowledge discovery in distributed databases. A new method 
of knowledge discovery from multiple data sources is proposed 
in this paper. It shares knowledge found in other similar data 
source to determine the validity of knowledge in the local 
database through sampling test. The experimental results show 
that the method is effective and can significantly improve the 
efficiency of knowledge discovery.  
The hypothesis testing based knowledge discovery method 
proposed in this paper is also an effective solution to 
incremental data mining. In the real world, new data records 
added into an application database (such as a sales database), to 
some extent, is generally similar to original data. Under this 
circumstance, it is necessary to determine the impact of new 
added samples, and to make sure how to re-arrange subsample 
size and the sampling strategy. Specific research findings will 
be given in future. 
ACKNOWLEDGMENT 
This work was partially supported by “211” Project 
(No.21103040122) and “382” Talents Scheme (No. 
G2009382309), Communication University of China.   
REFERENCES 
 
[1] H. Kargupta, B. Park, D. Hershberger, and E. Johnson, “Collective data 
mining: A new perspective toward distributed data mining,” Advances 
in distributed and parallel knowledge discovery, chapter 5, pp.131–178, 
AAAI/MIT Press, 2000.  
[2] E. Cesario and D.Talia, “Distributed data mining models as services on 
the grid,” IEEE International Conference on Data Mining Workshops, 
2008, pp.486–495. 
[3] P. Kadel and H. Choi, “Incremental algorithm for distributed data 
mining,” The Sixth IEEE International Conference on Computer and 
Information Technology, 2006, pp.72–75 
[4] G. Goulbourne, F. Coenen and P. Leng, “Algorithms for computing 
association rules using a partial-support tree,” Knowledge-Based 
Systems, vol.13, 2000,  pp.141–149. 
[5] Y. Chen and L. Qu, “The research of universal data mining model 
system based on logistics data warehouse and application,” International 
Conference on Management Science and Engineering 2007, pp.280–285. 
[6] S. Zhang, X. Wu and C. Zhang, “Multi-database Mining,” IEEE 
Computational Intelligence Bulletin, vol.2, no.1, 2003,  pp.5–13. 
[7] N. Zhong and Y. Yao. “Interestingness, peculiarity, and multi-database 
mining,” IEEE International Conference on Data Mining 2001,  pp.1–8. 
[8] J. Rajan and V. Saravanan, “A framework of an automated data mining 
system using autonomous intelligent agents,” International Conference 
on Computer Science and Information Technology 2008, pp.700-704. 
[9] L. Cao, V. Gorodetsky and P. A. Mitkas, “Agent mining: the synergy of 
agents and data mining,” Intelligent Systems, IEEE, vol.24, Issue 
3,  May-June 2009, pp.64–72. 
[10] Danish Khan, “CAKE – classifying, associating & knowledge discovery 
an approach for distributed data mining (DDM) using parallel data 
mining agents (PADMAs),” IEEE/WIC/ACM International Conference 
on Web Intelligence and Intelligent Agent Technology 2008, pp.596–
601. 
[11] X. Wu and S. Zhang, “Synthesizing high-frequency rules from different 
data sources,”  IEEE Transactions on Knowledge and Data Engineering, 
vol. 15, No. 2, March/April 2003,  pp.353–367. 
[12] H. Toivonen, “Sampling large databases for association rules,” 
International Conference on Very Large Data Bases 1996, pp.134–145. 
[13] J. S. Park, P. S. Yu and M. Chen, “Mining association rules with 
adjustable accuracy,” International Conference on Information and 
Knowledge Management 1997, pp.151–160. 
[14] C. Wang and H. Huang, “Distributed Mining Adjustable Accuracy 
Association Rules Using Sampling,” Journal of Computer Research and 
Development, China, vol.37, no.9, 2000, pp.1101–1106. 
[15] Wang Fubao, Probability Theory and Mathematical Statistics (third 
edition), Shanghai: Tongji University Press, 1998, pp.308–319. 
[16] R Agrawal and R. Srikant, “Fast algorithms for mining association 
rules,”  International Conference on Very Large Data Bases 1994, pp. 
487–499. 
 
