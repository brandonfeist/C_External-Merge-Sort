Application Research of Cluster Analysis and 
Association Analysis 
Hai-Dong Meng, Yu-Chen Song, Fei-Yan Song, Hai-Tao Shen 
School of Mining Engineering 
Inner Mongolia University of Science and Technology 
Baotou, China 
haidongm@imust.edu.cn 
 
 
Abstract—For applications of data mining techniques in 
geosciences, through mining spatial databases which are 
constructed with geophysical and geochemical data measured in 
fields, the knowledge, such as the spatial distribution of 
geological targets, the geophysical and geochemical 
characteristics of geological targets, the differentiation among the 
geological targets, and the relationship among geophysical and 
geochemical data, can be discovered. Due to the complexity of 
geophysical and geochemical data, traditional mining methods of 
cluster analysis and association analysis have limitations in 
processing complex data. In this paper, a clustering algorithm 
based on density and adaptive density-reachable is presented 
which has the ability to handle clusters of arbitrary shapes, sizes 
and densities. For association analysis, mining the continuous 
attributes may reveal useful and interesting insights about the 
data objects in geoscientific applications. Quantitative association 
rules aims to deal with the relationships among continuous 
attributes of geoscientific data objects. An association analysis 
algorithm based on the distances among clusters projected on 
attributes is presented in this paper. Experiments and 
applications indicate that the algorithms are effective in real 
world applications. 
Keywords- Cluster analysis; Association analysis; Geo-spatial 
database; Geochemical data; Data processing 
I.  INTRODUCTION 
Cluster analysis has long played an important role in a wide 
variety of fields: psychology and other social sciences, biology, 
pattern recognition, machine learning, image processing, 
information retrieval, geosciences, and climate research. 
Cluster analysis aims to identify groups of objects (clusters) 
that satisfy some specific criteria, or share some common 
characteristics. A large number of clustering algorithms have 
been developed in a variety of domains for different types of 
applications. None of these algorithms is suitable for all types 
of data, clusters, and applications. In fact, it seems that there 
are always rooms for new clustering algorithms that are more 
efficient or better suited to particular types of data, clusters, or 
applications [1]. 
In many application domains, such as geoscientific data 
processing, clusters of data objects are of arbitrary shapes, sizes 
and densities, and the number of clusters is unknown. In such 
scenarios, traditional clustering algorithms, including 
partitioning methods, hierarchical methods, density-based 
methods and grid-based methods, cannot identify clusters 
efficiently or accurately. Obviously, these are critical 
limitations or weaknesses of clustering methods. Numerous 
algorithms have been proposed to solve these problems. For 
example, Ertöz, Steinbach, and Kumar proposed a clustering 
technique that addresses the problem of finding clusters in data 
when clusters are of widely differing sizes, densities and 
shapes, and when the data contains large amounts of noise and 
outliers [2]. Ayad and Kamel [3] presented a multiple data 
clusterings combiner, based on a proposed Weighted Shared 
nearest neighbors Graph (WSnnG). The problem addressed in 
the paper is that of generating a reliable clustering to represent 
the natural cluster structure in a set of patterns, when a number 
of different clusterings of the data is available or can be 
generated. A clustering method called CLARANS which aims 
to identify spatial structures that may be present in the data was 
proposed in [4]. DBSCAN [5, 6] is a simple and effective 
density-based clustering algorithm which is relatively resistant 
to noise and can handle clusters of arbitrary shapes and sizes, 
but it has trouble when the clusters have widely varying 
densities. Zhang, Ramakrishnan, and Livny [7] proposed an 
efficient data clustering method for very large databases, which 
is based on the notion of clustering feature and effective for 
incremental clustering. A clustering algorithm based on density 
and adaptive density-reachable is presented in [8, 9], which 
aims to handle complex data. 
Much work in data mining revolves around the discovery of 
rules within large quantities of data. The rules over relations 
are of the form X?Y where X and Y are conditions on tuples of 
the relation. The term association rule has been used to 
describe a specific form of such rules.  
Association rule analysis aims to find frequent patterns in 
datasets. Frequent patterns are patterns (such as itemsets, 
subsequences, or substructures) that appear in a data set 
frequently. Finding such frequent patterns plays an essential 
role in mining associations, correlations, and many other 
interesting relationships among data objects. Frequent pattern 
mining has become an important data mining task and a 
focused theme in data mining research. Association rule 
analysis is applicable to many application domains such as 
bioinformatics, medical diagnosis, Web mining, and scientific 
data analysis [1].  
Algorithms for discovering classical association rules make 
different assumptions about the type of datasets to be mined. In 
general, the dataset is a relational table. The domains of the 
597
attributes are restricted to boolean domains. The databases in 
most business and scientific domains have many types of 
attributes, including quantitative, categorical or boolean 
attributes. The classical association analysis algorithms for 
boolean attributes can not mine quantitative association rules 
from databases with continuous attributes, and may yield very 
unintuitive results when applied to continuous attributes. R. 
Srikant and R. Agrawal [10] proposed an algorithm for mining 
quantitative association rules in large relational tables, defined 
the problem of mining association rules over quantitative and 
categorical attributes in large relational tables, presented 
techniques for discovering quantitative rules, and referred to 
this mining problem as the quantitative association rules 
problem. For the problem of mining association rules over 
interval data, R. J. Miller and Y. Yang [11] proposed the 
definition of interest for association rules that takes into 
account the semantics of interval data and developed an 
algorithm for mining association rules under the definition. A 
definition of quantitative association rules based on statistical 
inference theory was presented by Y. Aumann and Y. Lindell 
[12], which reflects the intuition that the goal of association 
rules is to find extraordinary and therefore interesting 
phenomena in databases. The definition is not based on a 
discretization paradigm that “converts” quantitative data into 
categorical items, but based on the consideration of the 
distribution of the continuous data, via standard statistical 
measures such as mean and variance.  
In this paper, on the basis of cluster analysis and the 
definition proposed by R. J. Miller and Y. Yang [11], a new 
algorithm for mining distance-based quantitative rules is 
presented, which is a new approach to handling continuous 
attributes for association analysis. The purpose is to improve 
the validity and efficiency of association analysis for handling 
continuous attributes, and to make it more suitable for real 
world applications.  
The rest of this paper is organized as follows. We present a 
novel clustering algorithm based on density and adaptive 
density-reachable in section 2. In section 3, the creation of 
quantitative association rules based on distance is proposed. 
The experiments and applications of the algorithm are 
presented in section 4. Section 5 concludes with a summary 
and some directions for future research. 
II. CLUSTERING ALGORITHM BASED ON DENSITY AND 
ADAPTIVE DENSITY-REACHABLE 
Clustering aims to find useful groups of objects (clusters). 
There are many concepts about clusters. Based on the spatial 
distribution of data objects, the type of clusters can be 
described as follows [1]: 
Well-separated cluster. A cluster is a set of objects in which 
each object is closer (or more similar) to every other object in 
the cluster than to any object not in the cluster. 
Center-based clusters. Each object in a cluster is closer to 
center of the cluster than to centers of any other clusters. 
Contiguity-based clusters. Each object in a contiguity-based 
cluster is closer to some other object in the cluster than to any 
point in a different cluster. 
Density-based clusters. A cluster is a dense region of 
objects that is surrounded by a region of low density. 
In order to handle the different types of clusters and meet 
the needs of different applications, many clustering algorithms 
have been developed. These can be categorized into 
partitioning methods, hierarchical methods, density-based 
methods, grid-based methods, and model-based methods. 
In many application domains, clusters of data objects are of 
arbitrary shapes, sizes and densities, and the number of clusters 
is unknown. In such scenarios, traditional clustering algorithms, 
including partitioning methods, hierarchical methods, density-
based methods and grid-based methods, cannot identify 
clusters effectively or accurately. Obviously, these are critical 
limitations or weaknesses of clustering methods. In order to 
solve the problem, we proposed a clustering algorithm which 
has the abilities to find clusters of arbitrary shapes and sizes, to 
handle clusters and noise of varying densities, and to identify 
outliers effectively. The algorithm is called CADD (Clustering 
Algorithm based on Density and adaptive Density-reachable, 
CADD) [8, 9]. 
A. Relevant Definitions of the Clustering Algorithm 
(1)The density of data points: Given N d-dimensional data 
points in dataset D: {Xi} where i = 1, 2,…, N. The density of 
data points models the overall density of a set of points in 
dataset D as the sum of influence function associated with each 
point: 
 
 




N
j
XXd
i
ji
eXDensity
1
2
,
2
2

   	
Here, Gaussian influence function  
 
2
2
2
,
X, 
ji XXd
jiGuass eXf

  
indicates the density influence of each data points to the 
density of point Xi, and  is density adjustment parameter 
which is analogous to the standard deviation, and governs how 
quickly the influence of a point drops off [15]. 
(2)Local density attractors: Local density attractors are the 
data points at which the values of density function Density(X) 
are local maximum. 
(3)Density-reachable distance: Density-reachable distance 
is used to determine a circular area of data points x, labeled as 
={X| 0<d(Xi, Xj)R}, and the data points in the circular area 
are belong a same cluster. The definition formula is: 
CoefRDmeanR 
 )( 													
Here, 2/121
1 1
))(
1
1()( i
N
i i
XX
N
Dmean 

   
 is the mean distance 
between all data points in dataset D, and CoefR (0<CoefR<1)    
is named as the initial adjustment coefficient of density-
reachable distance. 
(4)Density-reachable: Density-reachable means that if there 
is a chain of objects p1, p2,?,pn=q, q is a local density attractor, 
and pn-1 is density-reachable from q, then for pi?D,(1?i<n-1) 
598
and d(pi,pi+1)R, we define that object pi,(1?i<n-1) is density-
reachable from q.  
(5)Adaptive density-reachable distance: In handling the 
clusters of varying densities, it is important to adjust the 
density-reachable distance R step by step during clustering. The 
adaptive adjustment is carried out through multiplying the 
original density-reachable distance R with an adjustment 
coefficient : 
RRAdap  																																
Here, RAdap is adaptive density-reachable distance, and  is 
defined as: 
)(
)( 1
i
i
AttractorDensity
AttractorDensity 
										
This is because that when the density value of local density 
attractor of a cluster is greater , the distance between objects in 
the cluster is smaller, and on the contrary, when the density 
value of local density attractor of a cluster is smaller, the 
distance between objects in the cluster is larger. When i=1, let 
. Because 
, so 1. It is necessary 
to point out that adaptive adjustment coefficient  may also be 
other value of function which can adjust the density-reachable 
distance effectively. 
)()( 10 AttractorDensityAttractorDensity 
)()( 1 ii AttractorDensityAttractorDensity 
B. The Clustering Algorithm 
The operation of the clustering algorithm consists of the 
following steps: 
 To calculate the density values of data points in dataset D 
using Equation (1), determine the Local density attractors 
Density(Attractori). 
 To compute initial density-reachable distance R using 
Equation (2) which can be adjusted by the initial 
adjustment coefficient of density-reachable distance 
CoefR. 
 According to the initial density-reachable distance R and 
the adaptive density-reachable distance RAdap, search data 
points which belong to local density attractors 
respectively. In this step, if the number of data points in 
the area of density-reachable distance is less than MinPts, 
such as MinPts 5 or 7, the data points are identified as 
noise or outlier and assigned to noise dataset.  
The process of the clustering algorithm is illustrated in 
Figure 1. 
 
 
Figure 1. The process of CADD 
According to the definitions we design and implement the 
clustering algorithm as below:  
 
Algorithm Clustering Algorithm based on Density and 
adaptive Density-reachable, CADD 
Input: Adjustment coefficient of density- reachable 
distance CoefR, density adjustment parameter . 
Output: Number of clusters, the members of each cluster, 
outliers or noise points. 
1: Compute the densities of each data points and initial 
density-reachable distance. 
2:  i1 
3:  Repeat  
4: Seek the maximum density attractor ODensityMaxi in the 
original dataset of clustering objects as the cluster 
centroid of Ci. 
5: Assign the data objects which are density reachable 
within adaptive density-reachable distance from 
ODensityMaxi to cluster Ci, and delete the clustered objects 
from original dataset. 
6:  ii+1 
7:  Until Original dataset is empty. 
8:  Make the clusters which have few objects (such as less 5 
or 7) into outlier or noise group. 
III. DISTANCE-BASED QUANTITATIVE ASSOCIATION RULES 
Association rules that contain continuous attributes are 
commonly known as quantitative rules. There are various 
methodologies for applying association analysis to continuous 
data. The types of methods mainly fall into three broad 
categories [1]: (1)discretization-based methods, (2)statistics-
based methods, and (3)non-discretization methods. The 
quantitative association rules derived using these methods are 
quite different in nature. 
The main concept for creating distance-based association 
rules is to cluster data objects and mine association rules 
among these clusters according to the distances between the 
clusters.  
A. Clustering 
It is necessary to point out that if there are a number of 
clusters in a data space, the strong association rules among 
attributes just exist in each cluster. Therefore, the creation of 
association rules just focuses in each cluster. The notion 
proposed in this section can make the process of quantitative 
association analysis more simple, efficient and effective. 
We use the clustering algorithm CADD to group the data 
points in a dataset. For a specific set of attributes, restrictions 
are placed on the properties of these data points when projected 
on X. For this reason, when a cluster is projected on X, the 
cluster is denoted as CX. Given N d-dimensional data points 
{ iX

}, (i=1,…,N) in CX, the Clustering Feature [7] is defined 
as: 
                								 ),,( 0 RXNCF 																								
Here, N is the number of data points in CX, 
 
N
i i
X
N
X
10
1 is the centroid of CX, and 
599
2/12
01
))(1( XX
N
R N
i i
    is the average distance from 
member points to the centroid or the radius of CX. According 
the clustering feature CF, the relationships among clusters 
projected on attributes are determined as presented later. 
B. The Association Rules 
For a cluster, if association rules exist among the attributes: 
X={x1,…,xx} and Y={y1,…,yy}, the definition of association 
rules is x1?Cx1[x1],…,xx?Cxx[xx]? y1 ?Cy1[y1], …,yy?
Cyy[yy]. The clusters are labeled with the attributes on which 
they are projected. For abbreviation, the rule is presented as 
Cx1,…,Cxx?Cy1,…,Cyy, where all the attributes X={x1,…,xx} 
and Y={y1,…,yy} are disjoint. The rule Cx1,…,Cxx? Cy1,…,Cyy 
holds with confidence c if |(?iCxi) ?(?jCyj)|/|?Cxi|c. The 
rule holds with support s, if |(?iCxi) ?(?jCyj)|s.  
For distance-based association rules, the confidence c is 
determined by the distances among the images of clusters 
Cx1,…,Cxx and Cy1,…,Cyy, and the support s is dominated by 
the numbers or densities of clusters. In this paper, we use the 
Euclidean distance metric to measure the distance between data 
points.  
According to the notion proposed by R. J. Miller and Y. 
Yang [11], the definitions of distance-based association rules 
are as follows:  
Let x1,x2,…,xx and y be pairwise disjoint sets of attributes. A 
N:1 distance-based association rule (DAR) is a rule of form R: 
Cx1,…,Cxx?Cy, where each Cxi is a cluster projected on xi and 
Cy a cluster on Y. R holds with degree of association D0 if: 
xiDYCxYCyD i  1])[],[( 0 						
jidXCxXCxD ixjjii  0])[],[( 													
Here, we emphasize that Cy[Y] indicates the CyCxi, Cxi[Y] is 
the image of all data points in the range of Cxi projected on Y, 
and  is the distance between centroids of 
Cy[Y] and Cxi[Y]. In real applications, the value of D0 is 
determined by the radius of the cluster. Because the creation of 
association rules is in each cluster, it is unnecessary to calculate 
Equation (7). We define the support S of the association rule as: 
])[],[( YCxYCyD i
||/|][| DsYCyS  																																			
Here, |Cy[Y]| is the number N of data points in the cluster, and 
|Ds| indicates the number of all data points in the data space. 
The degree of association replaces the traditional notion of 
confidence and the density threshold (or the number of data 
points in a cluster) replaces the notion of support. 
For 2-dementional data space, as shown in Figure 2, we 
discuss the creation of distance-based association rules. When 
there is one cluster in the data space, as shown in Figure 2(a), 
we can obtain the association rule: Cx ?  Cy, 
0])[],[( YCxYCyD
[( XCxD
])[1],[(
, S=100%, i.e., when the value of X is 
in the interval of [x1, x2], the value of Y is bound to [y1, y2], 
and Cy? Cx, , S=100%, i.e., when the 
value of Y is in the interval of [y1, y2], the value of X is bound 
to [x1, x2]. The situation is different in Figure 2(b), there are 
two clusters with same number of data points in the data space, 
and the rules derived from the data space are C1x ? 
Cy,
0])[], XCy
0YxCYCyD
0])[2],[(
, S=50%, and C2x ? Cy, 
YxCYCyD
CyXCxD ],[(
2],[( yCXCxD
(D
[2],[2(
, S=50%. If let the degree of association 
D0=R (radius of the cluster), we can not derive the rule Cy? 
Cx, because the centroid Ox of Cy[X] is located between C1 
and C2, and . As for Figure 2(c), we can 
obtain the rules: C1y?Cx, , S=50%, and 
C2y?Cx, , S=50%, but not Cx?Cy. If 
the distribution of data points in the data space is as shown in 
Figure 2(d), the association rules derived from the clustering 
result are C1x?C1y, , S=50% and C2x?
C2y, 
RX ])[
],[( XCxD
0])[ X
[1],[1 XyCXxC
0])
0])[1 XyC
0]) 
XyCXxCD
0])[1],[1(
, S=50%; C1y ? C1x, 
XxCXyCD
0])[2],[2(
, S=50% and C2y ? C2x, 
XxCXyCD , S=50%.  
The analysis indicates that the strong association rules only 
exist among the attributes in a cluster, and other clusters 
existed in the data space may weaken or break the association 
among the attributes in the cluster.  
        
(a) One cluster in data space.        (b)Two clusters parallel with X. 
         
(c) Two clusters parallel with Y.      (d)Two clusters oblique crossing axis. 
Figure 2. Association rules in 2-dimentinal data space. 
In this paper, we focus our research to the N:1 distance-
based association rules. N:1 distance-based association rules 
have a number of important applications. Often in data mining, 
we are interested in discovering which attributes determine one 
of a specific set of target attributes. For example, in 
geochemical applications, there are many chemical elements 
which are measured in the field, and we have to determine the 
relationship between a target element and a specific set of 
elements so as to solve some geochemical problems. Another 
example is that in medical domain we also have to research the 
relationships between a specific symptom and other symptoms, 
and so on.  
600
C. The Association Algorithm 
The algorithm presented in this section is a modification of 
the algorithm proposed in [11], and it includes three phases:  
 In first phase, clusters in the data space are identified 
using normal clustering algorithms, such as K-means or 
CADD [8, 9], and the clustering features are calculated.  
 In second phase, the distances  are 
calculated in a cluster which holds support S, here, Y is 
the target attribute chosen by domain experts interactively 
and X={x1,…,x
])[],[( YCxYCyD i
x} are the other attributes of data points in 
the data space. 
 In third phase, the rule is formed by combining the 
clusters projected on attribute Y which holds 
 and . 0])[],[( DYCxYCyD i  ||/|][| DsYCyS 
IV. APPLICATIONS 
In the real world application of data mining techniques, we 
use cluster analysis and association analysis to research the 
regional geochemical characteristics in a region of western 
China. The sampling area is 80*72 Km2, the distance between 
sampling points and lines is 2 Km, and the number of sampling 
points is 1517. The lithological distribution pattern in the 
region is shown in Figure 3: (1) the pale yellow area indicates 
humus, anemoarenyte and diluvium in Quaternary; (2) the 
yellow area is sandy and carboniferous dark gray-colored slate 
and silty mudstone with siltstone in Dyas; (3) the dark yellow 
area represents clastic rock, dark gray-colored conglomerate, 
graywacke, and silty slate with silty mudstone in Dyas; (4) 
yellow and yellowish-brown-colored granite porphyry, 
hornblende granite porphyry, and masanophyre in Yanshanian 
magmatism distribute in the pale pink area; (5) the pink area is 
flesh and red-colored granitello and medium-grain granite in 
Yanshanian magmatism; (6) the blue area indicates gray and 
sage green-colored rhyolite, rhyolitic tuff with breccia in 
Jurassic; (7) the purple area is dimgray-colored medium and 
coarse-grain quartz diorite and gneissic quartz diorite in 
Variscan magmatism; (8) the green area is sage green and pale-
gray-yellow-colored biotite plagioclase gneiss with amphibole 
plagioclase gneiss, quartz schist, shallow particle rock, and 
griotte.  
 
Figure 3. Lithological distribution pattern in the sampling region. 
The purposes of clustering analysis and association analysis 
are to research the characteristics of regional geochemical 
anomalies, to determine the distribution of anomaly source, and 
to analyze the type of mineral resources according to the 
relationship among the geochemical elements. 
A. Cluster Analysis 
There are 1443 samples actually in the geochemical 
measurement region, and for each sample 17 kinds of chemical 
elements are measured which are Ag, Al, As, Au, Cd, Co, Cu, 
Fe, Hg, K, Mn, Mo, Pb, Sb, Sn, W, Zn. The sampling points 
constitute a spatial dataset in which each data point has 17 
attributes, i.e., Ag, Al, As, Au, Cd, Co, Cu, Fe, Hg, K, Mn, Mo, 
Pb, Sb, Sn, W, Zn. The clustering result of CADD is shown in 
Figure 4. There are four clusters: Cluster1, Cluster2, Cluster3, 
Cluster4 and Outliers found in the spatial dataset. The 
distribution pattern of the clusters is consistent with the 
lithological distribution pattern in the region, which can 
indicate the characteristics of geochemical anomaly. The 
distribution area of Cluster1 reflects the geochemical 
characteristics of the dimgray-colored medium and coarse-
grain quartz diorite and gneissic quartz diorite in Variscan 
magmatism (the purple area). 
 
 
Figure 4. Clustering result of geochemical samples. 
B. Association Analysis 
Geological survey in the field indicates that mineral 
occurrences of W and Pb or Cu exist in the area of Cluster1. 
According to the geochemical theory [21], the geochemical 
characteristics of a geological unit can be evaluated using the 
association rule of the chemical elements. In this section, we 
use association analysis to analyze the relationship among the 
chemical elements so as to determine the characteristics of a 
geological unit, and provide useful knowledge for mineral 
resource survey.  
Because the chemical elements Cd and Sb are important 
indicator elements for the evaluation of mineral resources, we 
research the relationships Cd and Sb with other chemical 
elements using the distance-based association analysis. For 
Cluster1, when D0=0.005, the quantitative rules are as follows:  
(1)The association rule of Cd with other elements: 
As?[1.4,45], Ag?[0.01,0.29], Au?[0.3,1.1],  
Hg?[1.0,120], Mo?[0.02,4.7], Pb?[5.0,56],  
Sb?[0.03,2.78], Sn?[0.1,14], W?[0,8.7] 
? Cd?[0.01,0.22], D0=0.005, S=33.6%. 
(2)The association rule of Sb with other elements: 
Ag?[0.01,0.29], As?[1.4,45], Au?[0.3,1.1], 
601
Hg?[1.0,120], Mo?[0.02,4.7], Pb?[5.0,56],  
Sn?[0.1,14], W?[0,8.7.0] ? Sb?[0.03,2.78], 
 D0=0.005, S=33.6%. 
The association rules indicate that Cd and Sb are related 
with Ag, Au, Hg, Mo, Pb, Sn and W under certain degree of 
association and support. According to geochemical theory [21] 
we can conclude that the mineral resources in the region are 
related with Ag, Au, Hg, Mo, Pb, Sn and W. The conclusion is 
consistent with the real characteristics of mineral resources in 
the region. 
V. CONCLUSIONS 
For real world applications of data mining, the key 
technique is to handle complex data effectively and efficiently. 
The research results indicate that CADD can not only 
overcome the weaknesses of traditional partitioning and 
hierarchical methods to handle arbitrary clusters and outliers, 
but also improve the capability of density-based methods to 
deal with clusters of varying densities. At the same time, the 
algorithm can evidently reduce time and space complexity as 
compared with other density-based algorithms. In association 
analysis, mining the continuous attributes may reveal useful 
and interesting insights about the data objects which are of 
continuous attributes. Quantitative association rules aims to 
deal with the relationships among continuous attributes of data 
objects. The association analysis algorithm presented in this 
paper uses a clustering algorithm to identify the intervals of 
attributes in clusters and combines the clusters projected on 
attributes to form distance-based association rules held with 
degree of association and support. Experiments and 
applications indicate that the algorithms of clustering and 
association are effective in real world applications. In future 
research, we will do more work in the creation of N:N 
distance-based association rules, and make the algorithm more 
effective for processing geoscientific data. 
The research results may provide new methods and 
techniques for comprehensive interpretation of geophysical and 
geochemical data, and minimize the uncertainness and 
ambiguity of geoscientific data interpretation. Along with the 
development of geosciences, more and more data are collected, 
and data mining techniques have evolved important methods to 
process huge amounts of geoscientific data and to discover 
useful geoscientific knowledge efficiently and effectively. 
ACKNOWLEDGMENT 
The materials are based on work supported by National 
Natural Science Foundation of China under Grant No. 
40762003, and by Natural Science Foundation of Inner 
Mongolia under Grant No. 200711020814. 
REFERENCES 
[1] P.-N. Tan, M. Steinbach and V. Kumar. Introduction to Data Mining, 
Publish by Pearson Education Asian LTM. and Post & Telecom Press, 
2006. 
[2] L. Ertöz, M. Steinbach, and V. Kumar. Finding Clusters of Different 
Size, Shapes and Densities in Noisy High Dimensional Data. In Proc. of 
the 3rd SIAM International Conference on Data Mining: pp.47-58, 2003. 
[3] H. Ayad, and M. Kamel. Finding Natural Clusters Using Multi-cluster 
Combiner Based on Shared Nearest Neighbors. In Proceedings of the 4th 
International Workshop on Multiple Classifier Systems (MCS 2003), 
Springer Berlin, LNCS 2709: pp.159-175, 2003.  
[4] R. T. Ng and J. Han. CLARANS: A Method for Clustering Objects for 
Spatial Data Mining. IEEE Transactions on Knowledge and Data 
Engineering, 14(5): pp.1003-1016, 2002. 
[5] M. Ester, H.-P. Kriegel, and J. Sander, et al.. A Density-Based 
Algorithm for Discovering Clusters in Large Spatial Databases with 
Noise. In Proceedings-The 2nd Intl. Conf. on Knowledge Discovery and 
Data Mining, Portland, Oregon, AAAI Press: pp.226-231, August 1996. 
[6] J.-O. Sander, M. Ester, and H-P. Kriegel, et al.. Density-Based 
Clustering in Spatial Data sets: The Algorithm GDBSCAN and Its 
Applications. Data Mining and Knowledge Discovery 2: pp.169–194, 
1998.  
[7] T. Zhang, R. Ramakrishnan, and M. Livny. An Efficient Data Clustering 
Method for Very Large Databases. In Proc. ACM SIGMOD Conference 
on Management of Data, Montreal, Canada: pp.103-114, 1996. 
[8] H.-D. Meng, Y.-C. Song, and F.-Y. Song. Research and implementation 
of clustering algorithm for arbitrary clusters, In Proceedings - 
International Conference on Computer Science and Software 
Engineering, v 4: pp. 255-258, 2008. 
[9] Y.-C. Song, H.-D. Meng, M. J. O'Grady, and G. M. P. O'Hare. Research 
and Application of Clustering Algorithm for Arbitrary Data Set, In 
Proceedings - International Conference on Computer Science and 
Software Engineering, v 4: pp. 251-254, 2008. 
[10] R. Srikant and R. Agrawal. Mining Quantitative Association Rules in 
Large Relational Tables, ACM SIGMOD, Volume 25, Issue 2: pp.1-12, 
1996. 
[11] R. J. Miller and Y. Yang. Association rules over interval data, ACM 
SIGMOD, Volume 26, Issue 2: pp. 452-462, June 1997.  
[12] Y. Aumann and Y. Lindell. A Statistical Theory for Quantitative 
Association Rules, Journal of Intelligent Information Systems ,Volume 
20 ,  Issue 3: pp.255 – 283, May 2003.  
[13] S. Guha, R. Rastogi, and K. Shim. CURE: An Efficient Clustering 
Algorithm for Large Databases. In Proc. Of 1998 ACM-SIGMOD Intl. 
Conf. on Management of Data, ACM Press: pp.73-84, June 1998. 
[14] G. Karypis, E.-H. Han, and V. Kumar. CHAMELEON: A Hierarchical 
Clustering Algorithm Using Dynamic Modeling, IEEE Computer, 32(8): 
pp.68-75, August 1999. 
[15] A. Hinneberg and D. A. Keim. An Efficient Approach to Clustering in 
Large Multimedia Databases with Noise. In Proc. Of the 4th Intl. Conf. 
on Knowledge Discovery and Data Mining, New York City, AAAI press: 
pp.58-65, August 1998. 
[16] W. Wang, J. Yang, and R. Muntz. STING: A Statistical Information 
Grid Approach to Spatial Data Mining. In Proc. 1997 Intl. Conf. Very 
Large Data Bases (VLDB’97), Athens, Greece: pp.186-195, August 
1997. 
[17] R. Agrawal, J. Gehrke, D. Gunopulos, and P. Raghavan. Automatic 
Subspace Clustering of High Dimensional Data for Data Mining 
Applications. In proc. 1998 ACM-SIGMOD Intl. Conf. Management of 
Data (SIGMOD’98), Seattle, WA: pp.94-105, June 1998. 
[18] D. Fisher. Improving Inference through Conceptual Clustering. In Proc. 
1987 AAAI Conf. Seattle, WA: pp.461-465, July 1987. 
[19] J. Gennari, P. Langley, and D. Fisher. Models of Incremental Concept 
for Mation. Artificial Intelligence, 40: pp.11-61, 1989. 
[20] P. Cheeseman and J. Stutz. Bayesian Classification (AutoClass): Theory 
and Results. Advances in Knowledge Discovery and Data Mining, 
Cambridge, MA: AAAI/MIT Press: pp.153-180, 1996. 
[21] P.-G. Dai, X.-P. Gu, and J.-L. Yu. Applied Geochemistry, Published by 
Central South University Press, China, 2008. 
 
602
