Discovery of Association Rules from Data including
Missing Values
Shigeaki Sakurai, Kouichirou Mori, and Ryohei Orihara
Corporate Research & Development Center, Toshiba Corporation
1, Komukai-Toshiba-cho, Saiwai-ku, Kawasaki 212-8582, Japan
Email: {shigeaki.sakurai, kouichirou1.mori, ryohei.orihara}@toshiba.co.jp
Abstract—This paper proposes a method that deals with
missing values in the discovery of association rules. The method
deals with items composed of attributes and attribute values. The
method calculates two kinds of support. One is characteristic
support and the other is possible support. The former is based
on the number of examples that do not include missing values
in attributes composing target items. The latter is based on the
number of examples that do not include missing values in all
attributes. The method extracts all item sets whose characteristic
supports are larger than or equal to the predefined threshold. The
paper evaluates the proposed method by comparing it with the
previous method and verifies the effect of the proposed method.
I. INTRODUCTION
The discovery method of frequent patterns starts from basket
analysis of receipts collected from retail businesses. It is
possible for the method to efficiently generate candidates of
frequent patterns by using the monotonic property of the
patterns [1] [2]. The property is called the Apriori property.
Also, it is possible for the method to speedily discover the
patterns by devising a storage method for the data [3] [11].
However, the method deals with transactions composed of
different items. Here, each transaction corresponds to a receipt.
Each item has two values. One value shows that the item
is included in the receipt and the other value shows that
the item is not included. On the other hand, in the case of
examples composed of some attributes, each attribute can be
larger than or equal to three values. If the discovery method
discovers frequent patterns from the data, the method requires
transfer of the original values to the values composed of two
values. That is, the preprocessing generates items composed of
attributes and their attribute values. The number of generated
items corresponds to the number of attribute values. The
preprocessing tends to generate many items. As the number
of items is large, the discovery method requires generation
of huge candidate patterns. The number of candidate patterns
increases exponentially because the method requires checking
the combination of items. Therefore, if the discovery method
deals with the examples, it is important to decrease the number
of candidates.
Also, in the case of the examples, the examples do not
always have all attribute values. That is, some attribute values
are missing. The values are called missing values. The values
are usually preprocessed by the three kinds of methods. The
first one gets rid of examples including the missing values.
The second one completes missing values by referring to
distribution of attribute values related to the missing values
[5]. The third one regards missing values as specific attribute
values. However, the first method cannot use the information
of remaining attribute values in examples including the miss-
ing values. The second method cannot always infer appropriate
attribute values and assigns wrong attribute values to the
missing values. The wrong attribute values may discover
wrong patterns. The third method may discover patterns whose
meaning cannot be interpreted. This is because the missing
values are equally dealt with even if the missing values can
originally have different meanings. Therefore, it is necessary
to efficiently deal with the missing values.
These methods are common preprocessing methods in the
field of machine learning and do not always aim at the
discovery of frequent patterns. Some methods have been
proposed for the discovery of association rules. Ragel [6]
completes missing values by using association rules discovered
from the data including missing values. Ragel [7] divides
a database composed of examples into the valid databases.
Here, the database that does not include missing values is
valid database. In the valid database, Ragel [7] redefines the
support and the confidence, and discovers association rules
based on the support and the confidence. Shen and Chen [9]
propose a method that composes association rules and reuses
them in order to improve the validity of both association
rules and completed missing values. Shintani [10] divides a
database including missing values into some databases not
including missing values and discovers association rules from
the divided databases. Here, the number of examples including
the rules, the support of the rules, and the confidence of
the rules are larger than or equal to use-defined thresholds.
Othman and Yahia [4] introduce the concept of robustness for
association rules and aim to select robust association rules for
the completion of the missing values. However, these methods
based on the completion have the problem corresponding to
the second common preprocessing method. Also, the other
methods do not always sufficiently use the property between
the attributes and their attribute values in order to efficiently
discover the association rules.
Thus, this paper proposes a new method that decreases
the number of candidates by using the property between
attributes and attribute values. Also, the method sufficiently
uses the information of examples including missing values. We
can anticipate that the method discovers more valid frequent
International Conference on Complex, Intelligent and Software Intensive Systems
978-0-7695-3575-3/09 $25.00 © 2009 IEEE
DOI 10.1109/CISIS.2009.92
67
patterns. We note that we discover the frequent patterns and do
not discover association rules in this paper, but we can easily
discover the association rules from the frequent patterns by
evaluating the confidence of the frequent patterns. Lastly, this
paper verifies the effectiveness of the proposed method by
comparing it with the previous preprocessing.
II. DISCOVERY OF FREQUENT PATTERNS
A. Expression of items
The original discovery method of frequent patterns deals
with transactions such as receipts of the retail fields. The
transactions are composed of items that have two values. Here,
one value shows that the item is included in a transaction
and the other value shows that the item is not included in it.
Therefore, the method cannot deal directly with data composed
of some attributes and their attribute values. This is because
some attributes are larger than or equal to three values. The
data is called tabular structured data hereafter. If we try to
discover frequent patterns from the data, it is necessary to
divide each attribute into items composed of two values in
the preprocessing. For example, an attribute is given “blood
pressure” and it has three attribute values: “high”, “normal”,
and “low”. The combinations of the attribute and its attribute
values: “blood pressure: high”, “blood pressure: normal”, and
“blood pressure: low” are regarded as items composed of two
values. In general, if each example of the data t is given by
Formula (1), the example is interpreted as items defined by
Formula (2). Here, Ai, (i = 1, 2, · · · ,m) is an attribute, aixi
is an attribute value of the attribute Ai, m is the number of
attribute values composing examples, and the combination of
an attribute and an attribute value Ai : aixi is an item.
(a1x1 , a2x2 , · · · , amxm) (1)
(A1 : a1x1 , A2 : a2x2 , · · · , Am : amxm) (2)
B. Missing values
First, we note the data as shown in Table I. Here, a specific
attribute of each example has at most an attribute value. Also,
in this table, “-” shows a missing value. This table includes
missing values corresponding to respective attributes A1 and
A2.
TABLE I
EXAMPLES INCLUDING MISSING VALUES
A1 A2
t1 a11 a21
t2 a11 -
t3 a12 a21
t4 - a22
t5 a12 a22
t6 a13 a23
If two examples t2 and t4 including a missing value are
deleted from the data, the frequencies of attribute values a11,
a12 and a13 of the attribute A1 are 1, 2, and 1, and the
frequencies of attribute values a21, a22, and a23 of the attribute
A2 are 2, 1, and 1. Also, the number of examples where t2 and
t4 are deleted is 4. In the case of the attribute values a11, a12,
a13, a21, a22, and a23, the supports defined by Formula (3)
are 0.25, 0.5, 0.25, 0.5, 0.25, and 0.25, respectively. Therefore,
when the minimum support is 0.3, the patterns A1 : a12 and
A2 : a21 are extracted as frequent patterns.
supp(p) =
f(p)
N
(3)
Here, N is the number of examples and f(p) is the number
of examples including the pattern p.
On the other hand, we note individual attributes. In the case
of the attribute A1, the data includes only a missing value
in the example t4. In the case of the attribute A2, the data
includes only a missing value in the example t2. In addition,
in each case, the number of examples where the missing value
is deleted is 5. Therefore, the supports for the attribute values
a11, a12, and a13 are 0.4, 0.4, and 0.2. The supports for the
attribute values a21, a22, and a23 are 0.4, 0.4, and 0.2. When
the minimum support is 0.3, the patterns A1 : a11 and A1 : a12
are extracted as frequent patterns for the attribute A1, and
A2 : a21 and A2 : a22 are extracted as frequent patterns for
the attribute A2.
The aforementioned examples show that the complete dele-
tion of examples including missing values and the partial
deletion of examples based on each attribute lead to different
results. The latter one uses the information discarded by the
former one. We can anticipate that the latter one discovers
more valid patterns than the former one does.
Frequent patterns are usually composed of some items.
It is necessary to decide which examples are deleted from
the attribute set corresponding to the items. The attribute set
is called the attribute pattern hereafter. In the case that the
number of attributes is small, it is possible to decide which
examples are deleted for all combinations of attributes in
advance. However, the number of the combinations increases
exponentially as the number of attributes increases. In the case
that the number of attributes is large, it is not possible to decide
the example subsets corresponding to all the combinations in
advance. Therefore, it is necessary to decide which examples
are deleted when the attribute pattern is given.
We note the case that an attribute pattern includes all the
attributes. The number of the remaining examples in the case
is smaller than or equal to the numbers in the case of other at-
tribute patterns. This is because the attribute patterns based on
all the attributes are a super subset of other attribute patterns.
Therefore, if two supports are defined by Formula (4) and
Formula (5), the relationship suppchar(p, P ) ? supppos(p)
is satisfied for a pattern p. In the following, suppchar and
supppos are called characteristic support and possible support,
respectively. Here, P is an attribute pattern, Pall is the attribute
pattern composed of all attributes, and n() is a function
that calculates the number of examples not including missing
values in the attribute pattern P .
suppchar(p, P ) =
f(p)
n(P )
(4)
68
supppos(p) =
f(p)
n(Pall)
(5)
The characteristic support uses more examples than the
possible support does in order to evaluate the support. The
characteristic support is a more valid criterion. Thus, this paper
tries to discover patterns whose characteristic supports are
larger than or equal to the minimum characteristic support.
The patterns are a kind of frequent patterns and are called
characteristic patterns hereafter. Also, this paper calls the
patterns whose possible supports are larger than or equal to
the minimum characteristic support possible patterns.
If an attribute is added to an attribute pattern, examples
including new missing values may be added. That is, the value
of the denominator in the characteristic support may decrease.
The monotonic property is not always satisfied in the case of
the characteristic support. For example, we note the following
example. In the case of the attribute pattern (A1, A2), 100
examples are remaining and 27 examples include the attribute
value set (a11, a21). Also, the former examples include 10
missing values in the attribute A3 and the latter examples have
the attribute value a31 in the attribute A3. Then, in the case
of the attribute pattern (A1, A2), the characteristic support in
the pattern (A1 : a11, A2 : a21) is 0.27 (= 27100 ). In the case of
the attribute pattern (A1, A2, A3), the characteristic support in
the pattern (A1 : a11, A2 : a21, A3 : a31) is 0.3 (= 2790 ). The
monotonic property is unsatisfied in this example.
On the other hand, the possible support satisfies the mono-
tonic property, because the denominator in the possible support
is fixed. The possible support of a pattern gives the upper
bound of characteristic supports given by super patterns of the
pattern. Therefore, if the possible support is smaller than the
minimum characteristic support, the characteristic supports of
the super patterns are smaller than the minimum characteristic
support. That is, the super patterns do not become character-
istic patterns. It is not necessary to expand the pattern whose
possible support is smaller than the minimum characteristic
support. To the contrary, if the possible support of the pattern
is larger than or equal to the minimum characteristic support,
its supper patterns may become characteristic patterns. It is
necessary to keep the pattern even if the pattern is not extracted
as a characteristic pattern. In the following, the pattern whose
possible support is larger than or equal to the minimum
characteristic support is called a possible pattern.
According to these discussions, in the evaluation of the
patterns, it is necessary to evaluate the patterns by both the
characteristic support and possible support. That is, if the
characteristic support of a pattern is larger than or equal to
the minimum characteristic support, the pattern is extracted as
a characteristic pattern. Otherwise, it is necessary to evaluate
the possible support of the pattern. If its possible support is
larger than or equal to the minimum characteristic support,
the pattern is kept as the seed of the super patterns. We can
efficiently discover all characteristic patterns by the two-step
evaluations.
C. Generation of candidate patterns
The discovery method of frequent patterns can discover
frequent patterns from the tabular structured data by using
the expression of the items as shown in Section II-B. The
expression requires the generation of items corresponding to
each attribute value. The number of items tends to be huge
and the amount of calculation tends to be huge. On the other
hand, the discovery method repeats the generation of candidate
patterns and the calculation of their frequencies. It is important
to decrease the number of the candidate patterns. We note
the property of the tabular structured data. In the data, each
attribute has at most an attribute value. Each item set does not
include items whose attributes are equal to each other. It is
not necessary for the discovery method to evaluate a candidate
pattern such that attributes of items composing the pattern are
equal to each other. The discovery method can judge that the
pattern is not a characteristic pattern without calculating its
frequency.
Sakurai et al. [8] proposed the attribute constraint based on
the property of the tabular structured data in the sequential
pattern discovery and proposed the discovery method incor-
porating the constraint. Also, Sakurai et al. [8] verified the
effectiveness of the method by using medical examination
data. However, the method judges whether items composing a
candidate pattern have the same attribute when the candidate
pattern is generated, and it deletes the candidate pattern
including the same attribute. The method requires judging the
attribute constraint, even if the judgment can be performed
much more speedily than the calculation of the frequency
can. Thus, this paper proposes a method without judging the
attribute constraint.
First, this section explains the outline of the method. The
method generates attribute patterns by combining with at-
tributes corresponding to items. Also, the method generates
attribute value sets by combining with attribute values corre-
sponding to attributes. Here, the combinations are performed
for different attributes. This two-step generation method can
generate all candidate patterns composed of different attributes
without judging the attribute constraint. The attribute value set
is called the attribute value pattern hereafter.
Next, a more concrete method is explained in the following.
We can assume that the attribute and the attribute values are
arranged in a criterion such as the alphabetic order without
losing generality. Thus, we assume Ai < Aj , aik1 < aik2 ,
and aix1 < ajx2 for i < j and k1 < k2. The method
can compose all attribute patterns including two attributes by
combining the attribute Ai with the other attribute Aj . Figure
1 shows an example of the combinations. In this figure, the
method generates the attribute patterns (A1, A2), (A1, A3),
· · ·, (A1, Am), (A2, A3) · · ·, and (Am?1, Am) in order. Also,
the method can compose all attribute patterns including three
attributes by combining the attribute pattern (Ai, Ap) with the
other attribute pattern (Ai, Aq), where i < p < q. The attribute
patterns (Ai, Ap) and (Ai, Aq) have the common attribute
Ai. For example, the method generates an attribute pattern
69
(A1, A2, A3) by combining (A1, A2) with (A1, A3). Similarly,
the method generates the attribute patterns (A1, A2, A4), · · ·,
(A1, A2, Am), (A2, A3, A4), · · ·, and (Am?2, Am?1, Am) in
order.
Fig. 1. Generation of candidate patterns
In general, the method generates all attribute patterns includ-
ing r attributes by combining two attribute patterns including
(r ? 1) attributes. These attribute patterns including (r ? 1)
attributes include the common (r ? 2) attributes and the
common attributes are arranged before remaining attributes.
Figure 2 shows an outline of the generation of an attribute
pattern. In this figure, each circle shows an attribute and
designs on the circle identify the kind of attributes.
Fig. 2. Generation of an attribute pattern
We note the generation of attribute value patterns. The
generation is based on processes similar to the aforemen-
tioned generation of attribute patterns. That is, the method
generates all attribute value patterns including r attribute
values by combining two attribute value patterns including
(r?1) attribute values. These attribute value patterns including
(r ? 1) attribute values have the (r ? 2) common attribute
values and the common attribute values are arranged before
remaining attribute values. For example, the method gener-
ates the attribute value pattern (a11, a21) by combining the
attribute value a11 with the other attribute value a21. Similarly,
the method generates the attribute value patterns (a11, a22),
· · ·, (a11, a2m2), (a12, a21), · · · in order as shown in Figure
1. Also, the method generates (a11, a21, a31) by combining
(a11, a21) with (a11, a31). It generates the attribute value
patterns (a11, a21, a31), (a11, a21, a32), · · · (a11, a21, a3m3),
(a11, a22, a31) in order.
Lastly, we note the method that increases the number of
items. The method has two kinds of strategy. The first strategy
is the width-based method and generates candidate patterns
whose numbers of items are small in order. The other strategy
is the depth-based method and generates candidate patterns
related to specific items or item sets in order. In order to decide
the strategy, we note the attribute value patterns included in the
same attribute pattern. If the tabular structured data is scanned
for the attribute pattern, the frequencies of the attribute value
patterns are calculated and the frequency of examples not
including missing values for the attribute pattern is calculated
simultaneously. Also, if possible patterns are given for all
attribute patterns composed of the common attributes and a
different attribute, the candidate patterns are generated by
combining two possible patterns without generating redundant
candidate patterns. Here, the number of items composing the
candidate patterns is equal to the number that adds 1 to the
number of the possible patterns. We can anticipate that the
width-based method easily discovers all characteristic patterns.
On the other hand, in the depth-based strategy, if all
characteristic patterns related to specific items or item sets are
discovered, the patterns can be discarded. The strategy tends
to require smaller memory space than the width-based method.
In the case of the tabular structured data, the number of items
tends to be huge and the number of candidate patterns tends
to be huge. In order to save the memory space, it is reasonable
to use the depth-based strategy to some extent. Therefore,
the strategy used in this paper is a mixture of the width-
based strategy and the depth-based strategy. First, the strategy
checks first candidate patterns including attribute patterns A1,
A2, · · ·, and Am in order based on the width-based strategy.
Next, the strategy checks higher candidate patterns related
to the attribute patterns in order based on the depth-based
strategy. Lastly, in the higher candidate patterns, candidate
patterns related to a specific attribute pattern are checked in
order based on the width-based strategy. That is, the strategy
checks candidate patterns including attribute patterns A1, A2,
· · ·, Am, (A1, A2), (A1, A3), · · ·, (A1, Am), (A1, A2, A3), · · ·,
(A1, A2, Am), · · · in order.
In this section, we do not pay attention to the division of
the data. Each attribute value can keep a corresponding subset
of the data. It is possible for the division to decrease the
amount of calculation. On the other hand, the division requires
additional memory space to store the subsets. The division
does not always lead to the decrease of the calculation time
because of the swapping process of the space. In future work,
we will try to verify the effectiveness of the division.
D. Discovery method
According to the discussion in subsection II-B and subsec-
tion II-C, the discovery method of characteristic patterns is
described as shown in Figure 3. The method inputs both the
tabular structured data (DB) composed of N examples and
the minimum characteristic support (MinChSp). It outputs all
70
characteristic patterns. First, the method calculates the number
(Mvl) of examples that do not include missing values by
using calcMissingV l(). Mvl is used to calculate possible
supports. The method discovers first characteristic patterns
whose numbers of items are 1. After that, it discovers higher
characteristic patterns whose numbers of items are larger than
1.
In the discovery of the first characteristic patterns, the
function createStruct() creates the structure that stores the
information related to each attribute Ai and the structure is
referred to as St. The method stores the attribute Ai as an
attribute pattern to St.PtAry and stores each attribute value
aij to St.VlPtAry. Here, PtAry and VlPtAry are members of
St. Next, the function calcFq() calculates frequencies of both
aij and the missing value of Ai by referring to the row of Ai
in DB. This function stores the frequency of aij to FqAry
and stores the frequency of the missing value to mvl. The
method calculates the characteristic support of aij by using
the formula: FqAry[j]N?mvl . If the characteristic support is larger
than or equal to MinChSp, the function output() outputs
the pattern (Ai : aij) corresponding to aij as a characteristic
pattern. Otherwise, the method calculates the possible support
of aij by using the formula: FqAry[j]N?Mvl . Then, if the possible
support is larger than or equal to MinChSp, the pattern is
kept to generate candidate patterns whose numbers of items
are larger. Otherwise, the pattern is deleted from St.V lP tAry.
The calculation of supports and the judgment are repeated for
each attribute value pattern. Lastly, the function judgeP t()
checks whether the number of remaining attribute patterns
is larger than or equal to 1. If at least one attribute pattern
remains, the function addQueue() stores the structure to
Queue1. Otherwise, the structure is deleted. Owing to these
processes, the method can output all first characteristic patterns
and can keep all possible patterns in order to generate higher
candidate patterns.
In the discovery of the higher characteristic patterns, the
function pickQueue() picks up one of the structures from
Queuei. Here, the structures include the information related
to attribute patterns composed of i attributes. The picked-up
structure is referred to as tSt1. The method picks up other
structures by referring to Queuei. The picked-up structure
is referred to as tSt2. The function createSt() creates the
structure that stores the information related to attribute patterns
composed of (i + 1) attributes and the structure is referred
to as St. The function genAtPt() generates an attribute
pattern composed of (i + 1) attributes by combining the
attribute pattern of tSt1 with the attribute pattern of tSt2. This
function stores the generated attribute pattern to St. We note
that Queuei stores the structures related to attribute patterns
such that each attribute pattern includes the (i ? 1) common
attributes from the top of the pattern. Therefore, this function
generates the pattern by adding the last attribute in the pattern
of tSt2 to the attribute pattern of tSt1. Next, the function
genV lP t() generates attribute value patterns corresponding
to the generated attribute pattern by combining the attribute
value patterns of tSt1 with the attribute value patterns of
tSt2. The generated attribute value patterns are composed
of (i + 1) attribute values. They are stored to St. Here, we
note that this function picks up one of the attribute value
patterns of tSt1 and picks up all attribute value patterns of
tSt2 in order. After that, this function picks up the next
attribute value pattern of tSt1. It repeats the pick-up until
all attribute value patterns of tSt1 are picked up. Owing to
these processes, this function can generate all attribute value
patterns. Next, the function calcFq() calculates frequencies
of both the generated attribute value patterns and the missing
value of the generated attribute pattern by referring to rows of
the generated attribute patterns in DB. This function stores the
frequencies of the generated attribute patterns to FqAry and
stores the frequency of the missing value to mvl. In a process
similar to the discovery of first characteristic patterns, the
method calculates characteristic supports and possible supports
of the generated attribute patterns. The method also outputs
characteristic patterns and keeps possible patterns in St. In
addition, if at least one attribute patterns remains, St is stored
to Queuei+1. Otherwise, St is deleted.
The method starts the growth corresponding to the top
structure of Queuei+1 when the growth corresponding to the
top structure of Queuei terminates. Therefore, the method can
investigate higher characteristic patterns in the order of A1,
A2, · · ·, and Am?1. We note the attribute Am is not processed.
This is because Am is the last attribute and higher charac-
teristic patterns based on Am are still being investigated for
other attributes. Also, we note the algorithm does not include
the deletion process of discovered patterns. If it is necessary
to delete them, the method requires administrating the link
relations among higher attribute patterns in the attribute Ai.
The method can delete the discovered patterns in Ai when the
method starts the discovery of higher characteristic patterns in
the attribute Ai+1.
III. NUMERICAL EXPERIMENTS
A. Data
This paper performs numerical experiments by using syn-
thetic data. The data is generated based on the algorithm as
shown in Figure 4. That is, the data is composed of examples
whose number (trnum) is defined by the user. Each example
is composed of attributes whose number (atnum) is user-
defined. Also, each attribute is composed of attribute values
whose number (atvnum) is user-defined. In addition, missing
values are inserted in the data based on the user-defined
missing rate (mrate). Here, the missing rate is defined by
the formula: mvnumtrnum×atvnum , where mvnum is the number
of missing values. trnum, atnum, atvnum, and mrate are
input to the algorithm, and the algorithm outputs the tabular
structured data as shown in Figure 5.
In these experiments, trnum is 1,000, atnum is 2, 3, 5,
and 10, atvnum is 2, 3, 5, and 10, and mrate is 0.1, 0.2,
0.3, 0.4, and 0.5. For each combination of trnum, atnum,
atvnum, and mrate, 10 example sets are generated based on
the random numbers using different initial value.
71
//Initialization
A = ?m1 Ai;
calcMissingV l(A,&Mvl);
//Discovery of first characteristic patterns
For each attribute Ai ? A
createStruct(&St);
St.AtP tAry = Ai;
St.V lP tAry = ?;
For each attribute value aij ? Ai
add aij to St.V lP tAry;
calcFq(St,DB,&FqAry,&mvl);
j = 0;
For each attribute value pattern V lP tj ? St.V lP tAry
csup = FqAry[j]
N?mvl ;
If csup ? MinChSp;
Then output(St, j);
Else psup = FqAry[j]
N?Mvl ;
If psup < MinChSp;
Then delete V lP tj from St.V lP tAry;
j = j + 1;
If judgeP t(St) == true;
Then addQueue(St,&Queue1);
Else delStruct(St);
//Discovery of higher characteristic patterns
i = 1;
while(true){
while((tSt1 = pickQueue(Queuei))! = NULL){
For each attribute pattern tSt2 ? Queuei
createStruct(&St);
genAtPt(tSt1.AtP tAry, tSt2.AtP tAry,&St.AtP tAry);
genV lP t(tSt1.V lP tAry, tSt2.V lP tAry,&St.V lP tAry);
calcFq(St,DB,&FqAry,&mvl);
j = 0;
For each attribute value pattern V lP tj ? St.V lP tAry
csup = FqAry[j]
N?mvl ;
If csup ? MinChSp;
Then output(St, j);
Else psup = FqAry[j]
N?Mvl ;
If psup < MinChSp;
Then delete V lP tj from St.V lP tAry;
j = j + 1;
If judgeP t(St) == true;
Then addQueue(St,&Queuei+1);
Else delStruct(St);
i = i+ 1;
}
i = i? 1;
If i == 0;
Then break;
}
end;
Fig. 3. A discovery method of characteristic patterns
B. Method
This paper discovers characteristic patterns from generated
example sets based on the proposed method. Also, it discovers
frequent patterns based on a previous method. That is, the
patterns are discovered from generated example sets where
examples including missing values are excluded. They corre-
spond to characteristic patterns. This paper uses 0.01, 0.05,
1. Read trnum, atnum, atvnum, and mrate.
2. Set trcnt to 0 and set atcnt to 0.
3. If trcnt is larger than or equal to trnum, terminate this
algorithm.
4. Add trcnt to 1.
5. If atcnt is larger than or equal to atnum, go to step 10.
6. Add atcnt to 1.
7. Generate a random number in the range [0, 1).
8. If the random number is smaller than or equal to mrate,
set a missing value to the attribute value corresponding
to trcnt and atcnt. Otherwise, generate another random
number, select a value from {1, 2, · · · , atvnum} based
on the random number with equal probability, and set the
value to the attribute value.
9. Return to step 5.
10. Set atcnt to 0 and return to step 3.
Fig. 4. A generation method of synthetic data
Fig. 5. An example of the synthetic data
0.1, and 0.2 as the minimum characteristic support and the
minimum support (MinSp). Also, it verifies the effectiveness
of the proposed method by evaluating the method from three
viewpoints. First, the number of examples used in order
to discover patterns is evaluated. Second, the difference of
discovered patterns between the proposed method and the pre-
vious method is evaluated. Third, the number of redundantly
stored possible patterns is evaluated.
C. Results
Figure 6 shows parts of the numerical experiments. In each
figure, the horizontal axis shows the number of items. In Fig-
ure 6(a) and Figure 6(b), the vertical axis shows the number of
examples. In Figure 6(c), Figure 6(d), Figure 6(e), and Figure
6(f), the vertical axis shows the number of patterns. Figure
6(a), Figure 6(c), and Figure 6(e) show the result in the case
that trnum=1,000, atnum=10, atvnum=2, mrate=0.1, and
MinChSp/MinSp=0.01. Figure 6(b), Figure 6(d), and Figure
6(f) show the result in the case that trnum=1,000, atnum=10,
atvnum=10, mrate=0.4, and MinChSp/MinSp=0.01. In
addition, Figure 6(a) and Figure 6(b) show the relationship
between the number of examples that do not include missing
values in the attribute patterns of discovered characteristic
patterns and the number of items in the discovered patterns.
In these figures, “Proposed” shows the results of the proposed
method and “Previous” shows the results of the previous
method. Figure 6(c) and Figure 6(d) show the difference of
both the discovered characteristic patterns and the discovered
frequent patterns. In these figures, “Common” shows the
72
(a) The number of evaluated examples (the case:
t1000a10v2m0.1sup0.01)
(b) The number of evaluated examples (the case:
t1000a10v10m0.4sup0.01)
(c) The difference of discovered patterns (the case:
t1000a10v2m0.1sup0.01)
(d) The difference of discovered patterns (the case:
t1000a10v10m0.4sup0.01)
(e) The number of characteristic patterns and possible
patterns (the case: t1000a10v2m0.1sup0.01)
(f) The number of characteristic patterns and possible
patterns (the case: t1000a10v10m0.4sup0.01)
Fig. 6. Experimental results
results in the case that both the proposed method and the
previous method discover the patterns. “Proposed only” shows
the results in the case that the proposed method discovers them
and the previous method does not discover them. “Previous
only” shows the results in the case that the proposed method
does not discover them and the previous method discovers
them. Figure 6(e) and Figure 6(f) show the relationships of
both the number of characteristic patterns and the number of
possible patterns that do not include characteristic patterns.
In these figure, “Char” shows the results in the case of the
characteristic patterns and “Pos” shows the results in the case
of the possible patterns.
D. Discussions
1) Number of evaluated examples: We note the results of
Figure 6(a) and Figure 6(b). The results show that the previous
method uses much smaller examples than the proposed one in
order to discover characteristic patterns. In particular, in the
case that the number of items is small and the rate of missing
values is high, this trend tends to be apparent. The proposed
method can discover the characteristic patterns based on many
examples. We can anticipate that the proposed method more
validly discovers the patterns. Therefore, the proposed method
is more efficient than the previous ones.
2) The difference of discovered patterns: We note Figure
6(c) and Figure 6(d). In the case that the number of attributes
is small, the difference of the discovered patterns is slight.
This is because the variation of characteristic patterns is
small. On the other hand, in the case that the number of
attributes is large, we can confirm that the difference become
huge. The results show that the previous method may fail
73
to discover important characteristic patterns. Therefore, the
proposed method is particularly important in the case that the
number of attributes is larger.
3) Characteristic patterns and possible patterns: We note
the results of Figure 6(e) and Figure 6(f). The results show
many possible patterns are discovered in the case that the
number of attributes is large and the rate of missing values
is high. The proposed method uses the number of examples
that do not include missing values in all attributes in order
to calculate possible supports. In this case, many examples
tend to include missing values in some attributes. The number
tends to be huge. Therefore, the possible supports tend to be
evaluated as larger than the characteristic supports. We think
this is the reason many possible patterns are discovered.
If it is necessary to discover all characteristic patterns, the
proposed method needs to keep all possible supports. However,
it is important to decrease the number of possible patterns
because many possible patterns require much memory space.
The space may give large impacts to the calculation speed.
On the other hand, as the number of attributes is large, the
possibility that characteristic patterns including all attributes
are discovered tends to be small and the characteristic patterns
tend to be composed of only parts of attributes. So, if we aim
at the discovery of characteristic patterns whose numbers are
smaller than or equal to the user-defined number of attributes,
we may be able to estimate smaller possible supports. For
example, we arrange attributes in descending order of the fre-
quencies of examples including missing values and accumulate
the frequencies in the order until the number of attributes
arrives at the user-defined maximum number of attributes. In
the case that the number of examples that include multiple
missing values is small, this method can appropriately estimate
the number (n(P?)) of examples that do not include missing
values in attribute patterns (P?). Here, P? is composed of
attributes whose number is user-defined maximum number. On
the other hand, in the case that the number of the examples is
large, this method may not be able to estimate it appropriately.
This is because many examples may be doubly counted. The
number of examples that does not include the missing values
in P? may be smaller than n(Pall). Therefore, it is necessary
to apply other methods to evaluate the number in the latter
case. We may be able to use sampling methods of attribute
patterns. In future work, we will try to consider estimation
methods for a smaller number.
4) Generality of the proposed method: Real data sets
include many missing values. For example, in the case of
a questionnaire, respondents do not always answer all the
questions. The unanswered questions become missing values.
Also, in the case of medical field, doctors do not always per-
form all tests on all patients and ask them the same questions
owing to the restrictions of time and cost. The unperformed
tests and the unanswered questions become missing values. In
addition, parts of the collected data may be destroyed owing to
human errors, breakdown of machines, and poor transmission
environments. The destroyed data becomes missing values.
On the other hand, the proposed method deals with missing
values without depending on the application tasks. Therefore,
the method can be applied to many application tasks.
In this paper, we showed only the experimental results based
on the synthetic data. However, numerical experiments based
on real medical data are performed and the results are similar
to the results shown in this section. In future work, we are
planning to consider whether the characteristic patterns have
larger impact. We believe the proposed method efficiently
acquires characteristic patterns composed of items from the
tabular structured data.
IV. SUMMARY AND FUTURE WORK
In this paper, we proposed a method that discovers charac-
teristic patterns from tabular structured data including missing
values. The method efficiently uses the information included
in the data without deleting examples including missing values
or completing missing values. Also, the method generates both
attribute patterns and attribute value patterns step-wisely. This
paper applies the proposed method to synthetic numerical data
and verifies the effectiveness of the proposed method.
In future work, we will try to apply the proposed method to
many application fields and to verify the effectiveness of the
method. For example, we are planning to apply the method
to medical data, financial data, and so on. Also, we will try
to consider a method that estimates possible supports more
appropriately in order to decrease the number of generated
possible patterns. Lastly, we will try to expand the method to
cover the sequential data including missing values.
REFERENCES
[1] R. Agrawal and R. Srikant, Fast Algorithms for Mining Association Rules
in Large Databases, Proc. 20th Intl. Conf. on Very Large Data Bases,
487-499, 1994.
[2] J. Han, J. Pei, and Y. Yin, Mining Frequent Patterns without Candidate
Generation, ACM SIGMOD Intl. Conf. on Management of Data, 1-12,
2000.
[3] T. Morzy and M. Zakrzewicz, Group Bitmap Index: A Structure for As-
sociation Rules Retrieval, Proc. 4th Intl. Conf. on Knowledge Discovery
and Data Mining, 284-288, 1998.
[4] L. B. Othman and S. B. Yahia, Yet Another Approach for Completing
Missing Values, Proc. 4th Intl. Conf. on Concept Lattices and Their
Applications, 155-169, 2006.
[5] J. R. Quinlan, Induction of Decision Trees, Machine learning, 1, 1, 81-
166, 1986.
[6] A. Ragel, Preprocessing of Missing Values Using Robust Association
Rules, Proc. 2nd European Sympo. on Principles of Data Mining and
Knowledge Discovery, 414-422, 1998.
[7] A. Ragel, Treatment of Missing Values for Association Rules, Proc. 2nd
Pacific-Asia Conference on Research and Development in Knowledge
Discovery and Data Mining, 258-270, 1998.
[8] S. Sakurai, Y. Kitahara, and R. Orihara, Discovery of Sequential Patterns
Coinciding with Analysts’ Interests, J. of Computers, 3, 7, 1-8, 2008.
[9] J. -J. Shen and M. -T. Chen, A Recycle Technique of Association Rule
for Missing Value Completion, Proc. 17th Intl. Conf. on Advanced
Information Networking and Applications, 526, 2003.
[10] T. Shintani, Mining Association Rules from Data with Missing Values
by Database Partitioning and Merging, Proc. 5th IEEE/ACIS Intl. Conf.
on Computer and Information Science and 1st IEEE/ACIS International
Workshop on Component-Based Software Engineering, Software Archi-
tecture and Reuse, 193-200, 2006.
[11] M. J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li, New Algorithms for
Fast Discovery of Association Rules, Proc. 3rd Intl. Conf. on Knowledge
Discovery and Data Mining, 283-286, 1997.
74
