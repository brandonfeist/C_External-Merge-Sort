2010 International Conference on Computer and Communication Technologies in Agriculture Engineering 
Research on Mining Positive and Negative Association 
Rules 
LUO lunwei 
College of Computer Science and Technology 
Henan Polytechnic University 
JiaoZuo, Henan Province China 
ljwonly@yahoo.com.cn 
Abstract - Positive and negative association rules are 
important to find useful information hided in massive data sets, 
especially negative association rules can reflect mutually 
exclusive correlaiton among items. Despite a great deal of 
research, a number of challenges still exist in mining positive and 
negative association rules. In order to solve the problem of 
"difficult to determine frequent item sets" and "how to delete 
contradictive positive and negative association rules", the paper 
presents a new algorithm for mining positive and negative 
association rules. The algorithm applies a new measurement 
framework of support and confidence to solve the problems 
existing. The performance study shows that the method is highly 
efficient and accurate in comparison with other reported mining 
methods. 
Index Terms - Association Rules; Data Mining; Negative 
Association Rules; Positive Association Rules; 
I. INTRODUCTION 
Association rules provide a convenient and effective way 
to identifY and represent certain dependencies between 
attributes in a database. Association rules mining includes 
positive and negative association rules mining. In the 
traditional approach to find association rules, one merely 
thinks in terms of positive association rules: especially when 
determining the degree of support and confidence. Previous 
studies propose that negative association rules have high 
accuracy at handling massive unstructured data [1]. 
However, these approaches may also suffer some 
weakness. 
On one hand, it is not easy to find interesting or useful 
frequent itemsets. Traditional techniques always select some 
simple criterions based on only positive frequent itemsets such 
as confidence, distance functions, rough set and so on, while 
ignoring the value of negative frequent itemsets. The simple 
pick may affect the mining accuracy. 
On the other hand, a training dataset often generates a 
huge set of positive and negative association rules. It is 
challenging to delete contradictive positive and negative 
association rules efficiently [2]. 
To solve these problems, in this paper, we develop a new 
mining algorithm for positive and negative association rules. 
Because the traditional approaches often are based on simple 
criterions. 
Therefore, in this paper we will find the positive and 
negative association rules from training dataset and prune 
contradictory positive and negative association rules. 
978-1-4244-6947-5/10/$26.00 ©2010 IEEE 
302 
ZhangBo 
School of Mathematics and Information Science 
Henan Polytechnic University 
JiaoZuo, Henan Province China 
zhangbocnu@sina.com 
According to the new measurement framework of support and 
confidence, this paper presents a new algorithm for data 
mining. The final comparative experiment shows that the 
algorithm has a higher recall rate and precision rate, is feasible 
and effective [3]. 
This work makes the following contributions: 
(1) It proposes a new measurement framework instead of 
relying on positive frequent itemsets for finding accurate 
frequent itemsets. This measurement framework uses a small 
set of high confidence positive and negative rules to determine 
the frequent itemsets, and experimental results show that this 
way is, in general, more accurate than other techniques; 
(2) It proposes a new mining algorithm for collecting 
useful and non-contradictory positive and negative associative 
rules based on an improved correlation function. 
The remaining of the paper is arranged as follows. Section 
IT revisits the general idea of association rules. Section m 
presents the algorithm for generating frequent itemsets and 
determining positive and negative association rules . The 
experimental results on accuracy and the performance study 
on efficiency and scalability are reported in Section N .The 
paper is concluded in Section V. 
II. ASSOCIA nON RULES 
This paper assumes that the training dataset is a normal 
relational table, which consists of N data objects described by 
L distinct attributes. Attributes can be categorical or 
continuous. For a categorical attribute, we assume that all the 
possible values are mapped to a set of consecutive positive 
integers. For a continuous attribute, we assume that its value 
range is discredited into intervals, and the intervals are also 
mapped to consecutive positive integers. So, we treat all the 
attributes uniformly in this study. 
Let D be the training dataset. In the training dataset, Let I 
be the set of all items in D , that means every data object has 
some attributes following the form I={h i2, • • •  , in} . We say 
that a data object d E D contains X ? I , a subset of 
items(called itemsets). A association rule is an implication of 
the form X ? Y where X ? I , and Y ? I . The number of 
data objects in D matching X and Y is called the support of the 
rule X ? y, denoted as sup(X ?J?The ratio of the number of 
objects matching X and Y versus the total number of objects 
matching X is called the confidence of the rule X ? Y, denoted 
as conf(X ?J? In general, given a training dataset, the task of 
CCTAE2010 
mining association rules is to find all association rules with 
high accuracy [4]. 
For example, if 80% of customers who have bought apples 
also buy oranges, i.e., the confidence of rule: apple-+oranges 
is 80%. To avoid noise, a rule is effective only if it has enough 
support. Given a support threshold and a confidence 
threshold, the method finds the complete set of association 
rules passing the thresholds [5]. 
In the training dataset, there also exists other association 
rules: X -+- Y, -X -+Y and -X -+- Y. The rule X -+- Y 
means the data objects which have itemset X do not have the 
itemsets Y. The rule -X -+ Y means the data objects which do 
not have itemsets X have the itemsets Y. The rule -X -+ - Y 
means the data objects which do not have itemsets X do not 
have the itemsets Y. These rules can be called negative 
association rules. The rule X -+ Y can be called positive 
association rule. The support and confidence of negative 
association rules can be defined as the support and confidence 
of positive association rule [6]. 
III. ALGORITHM FOR MINING POSITIVE AND NEGATIVE 
ASSOCIATION RULES 
The algorithm in this paper consists of two phases: 
frequent itemsets generating and mining all association rules. 
In the first phase: the algorithm computes the complete set 
of positive and negative frequent itemsets that pass the given 
support and confidence thresholds, respectively. Furthermore, 
it will prunes some contradictory itemsets between positive 
and negative frequent itemsets and only selects a subset of 
high quality frequent itemsets. 
In the second phase: Furthermore, the algorithm will find 
all positive and negative association rules according to 
frequent itemsets computed in the first phase, also it will 
prunes some contradictory rules based on an improving 
correlation function. 
A. Generating Frequent Itemsets 
So, the first step is to generate all the frequent itemsets by 
making multiple passes over the data. In the first pass, it 
separately counts the support of individual positive and 
negative itemsets and determines whether it is frequent. So, 
we can get positive and negative frequent itemsets(P _FI and 
N _Fl). If there is an itemsets X not only existing in P _FI and 
also in N _FI, we should compare sup(X) with sup( -X) and 
delete the itemsets which one is small. In each subsequent 
pass, it starts with the seed set of itemsets found to be frequent 
in the previous pass. It uses this seed set to generate new 
possibly frequent itemsets, called candidate itemsets. The 
actual supports for these candidate itemsets are calculated 
during the pass over the data. At the end of the pass, it 
determines which of the candidate itemsets are actually 
frequent. So, we will compute frequent itemsets(Fl). 
The algorithm of generating frequent itemsets is shown as 
follow: 
Algorithm 1 
Input: N _FI and P _FI 
Output: F 
(J)PJ=InitPass(P _FI), NJ=InitPass{N_FI); 
(2)P _Fh={flfEPJ), N_Fh={flfENJ }, 
(3)for(k=2; hJ!=NULL; k++) 
( 
Sk=CandidateGen(P _Fh N_Fh Fk-J); 
for (each t ET) 
{ 
for (each candidate s ES,J 
303 
{ 
if (s is contained in t) 
(the number of s)++; 
) 
Fk={S ESk I sup(s»=minsup }; 
} 
(4)Return FI= Uh· 
In this algorithm, there is an important function 
CandidateGenO which generates k-itemsets based on Fk.J. The 
code of it is shown as follow: 
Algorithm 2 
Input: P _FI, N _FI, hJ 
Output: Sk 
(J)Sk=NULL; 
(2)for(allfi EFk_f, ,12 EN_FIJ or P _FIJ andh ! EhJ) 
{ 
s= {if, i2, ••• ik-2, ik-f, h }; 
Sk=SkU{S }; 
For(each (k-1)-subset s of s) 
( 
if (s! ESk_J) delete s from Sk; 
} 
} 
B. Generating Positive and Negative Association Rules 
Then, the next step is to generate positive and negative 
association rules. It firstly finds the frequent itemsets 
contained in FI which satisfy min_sup and min_corif 
threshold. Then, it will determined the rules whether belong to 
the set of positve correlation rules P _ AR or the set of negative 
correlation rules N AR. 
The algorithm uses the correlation between itemsets to 
find positive and negative association rules. The correlation 
between itemsets can be defined as: 
(X Y) sup(X u Y) corr , = --'--''-- -'-
sup (X)sup(Y) (1) 
X and Y are itemsets. 
When corr()(, Y» 1, X and Y have positive correlation. 
When corr()(, Y)= 1, X and Y are independent. 
When corr(}{, Y)<1, X and Y have negative correlation. 
Also when corr(}{, Y» 1, we can deduce that corr(}{, 
Y)<1 and corr( -)(, Y)<1. 
The algorithm of generating positive and negative 
association rules is shown as follow: 
Algorithm 3 
Input: training dataset T, min_sup, min_corif 
Output: P_AR, N_AR 
(l)P _AR=NULL, N_AR=NULL; 
(2)for (any frequent itemsets X and Yin FI) 
{ 
if (sup(X -+ }j> min _sup and corif(X -+}j> min _conj) 
if( corr()(, }j> I) 
{ 
P _AR= P _AR U{X-+Y}; 
} 
} 
(3)for (any frequent itemsets X and -Y in Fl) 
( 
if (sup(X-+ -}j>min_sup and corif(X-+ -}j> min_conj) 
if( corr()(, -}j<l) 
{ 
N_AR= N_AR U{X-+ -Y}; 
} 
} 
(4)for (any frequent itemsets -x and -Y in FI) 
( 
if (sup( -X-+ -}j>min_sup and conf( -X-+ -}j> min_conf) 
if( corr( -)(, -}j> I) 
{ 
N_AR= N_AR U{ -X-+ -Y}; 
} 
} 
(5) return P_AR and N_AR; 
In this algorithm, we use a new method generates the set 
of frequent itemsets FI, In FI, there are some itemsets passing 
certain support and confidence thresholds. And the correlation 
between itemsets is used as an important criterion to judge 
whether or not the correlation rule is positve. Lastly, P _AR 
and N AR are returned. 
IV. EXPERIMENTAL RESULTS 
To evaluate the accuracy and efficiency of the algorithm, 
in this section, we report our experimental results on 
comparing the algorithm against the popular method: Apriori. 
All the experiments are performed on a 2.20Hz Core PC with 
10 main memory, running Microsoft Windows Server 2003. 
Apriori was implemented by its authors, respectively. We 
choose a training dataset including 1000 objects which have 
12 attributes. 
In our experiments, min_corifis set to 50%. For min_sup, 
it is more complex, because min_sup has a strong effect on the 
quality. If min_sup is set too high, those possible rules that 
cannot satisfy min_sup but with high confidences will not be 
included, and also the rules may fail to cover all the training 
cases. In the experiments reported before, we set min_sup to 
6%. The results are shown in Table I. 
304 
TABLE I 
EXPERIMENT RESULTS 
? 
Apriori 
Rules 
Positve Association Rules 199 
Negative Association Rules 0 
Total 199 
Recall Ratio 91.2% 
Precision Ratio 89.2% 
Algorithm in this 
paper 
142 
67 
209 
93.4% 
90.5% 
As can be seen from the table, the algorithm outperforms 
Apriori on recall ratio and precision ratio. Our Recall Ratio 
and Precision Ratio is higher than Apriori. So, it shows that 
the algorithm outperforms Apriori in terms of average 
accuracy and efficiency. 
V. CONCLUSIONS 
In this paper, we examined a number of problems that 
exist in current techniques of positive and negative association 
rules. A new algorithm is presented to generate all positive 
and negative association rules. The method has several 
distinguished features: (1) its frequent itemsets is performed 
based on positive and negative frequent itemsets, which leads 
to better overall accuracy; (2) it prunes contradictory positive 
and negative association rules effectively based on correlation 
between itemsets. Our experiment shows that the algorithm is 
highly effective at classification and has better average 
classification accuracy and efficiency in comparison with 
Apriori. In our future work, we will focus on building more 
accurate algorithm by using more sophisticated techniques and 
mining more useful positive and negative association rules. 
REFERENCES 
[1] Agrawal R, Imielinski T and Swami A, "Mining association rules between 
sets of items in large database," Proceeding of the 1993 ACM SIGMOD 
InternationalConference on Management of Data, ACM Press, Dec. 
1993, pp. 207-216. 
[2] Brin S, Motwani R, Silverstein C, "Beyond market Generalizing 
association rules to correlations," Processing of the ACM SIGMOD 
Conference 1997, ACM Press, 1997, pp. 265-276. 
[3] Dong Xiangjun, Song Haitao, Jiang He, Lu Yuchang, "Minnmum 
Interestingness Based Method For Discovering Positive and Negative 
Association Rules," Computer Engineering and Application, vol. 24, no. 
27, pp. 24-28, August 2004. 
[4] X Wu, C Zhang, S Zhang, "Both Positive and Negative Association 
Rules," Proceeding of the 19'h International Conference on Machine 
Learning, pp. 658-665, August 2002. 
[5] Han J, Pei J, Yin YW, "Mining frequent patterns without candidate 
generation," DataMining and Knowledge Discovery, vol. 8, pp. 53-87, 
September 2004. 
[6] Savasere. A , Omiecinski. E, Navathe.S, "Mining for Strong Negative 
Associations in a Large Database of Customer Transactions," Proceedings 
of IEEE 14th Internal Conference on Data Engineering, December 1998, 
pp. 369-376. 
