AVI:Based on the vertical and intersection operation of  
the improved Apriori algorithm 
Yan  Zhang  
School of Computer Science and Technology 
China University of Mining and Technology 
Xuzhou, China 
 e-mail: zyan@cumt.edu.cn 
Jing  Chen  
School of Computer Science and Technology 
China University of Mining and Technology 
Xuzhou, China 
e-mail: 506187212@qq.com 
Abstract—Association Rules is the extraction of data mining in 
the important research, analysis and research in the Apriori 
algorithm for association rule, the algorithm for computing 
efficiency is not high, proposing an improved algorithm AVI 
(Apriori with vertical and intersection operation) that use 
itemset union and identification intersection, just scanning the 
transaction database one time that can get the identity of a set 
of first-order and large set of vertical. K-order designate the 
options set for this operation as long as the first order of the 
large itemsets, scanning the database without having to repeat, 
thereby improving the efficiency of the mining algorithms.
Keywords- data mining; association rules; Apriori algorithm; 
AVI algorithm 
I. INTRODUCTION
Date Mining often referred to as knowledge discovery in 
databases (KDD), precisely, and KDD of knowledge in the 
learning stage, known as data mining. Data mining is a very 
important processing step of KDD, and it is a crossover field 
of study, Integration of the database, AI, Machine Learning, 
Statistics, Knowledge Engineering, Object-Oriented Method, 
High Performance Computing and Data visualization. The 
latest technology of research results is shown in [1]. 
II. ASSOCIATION RULES ANALYSIS
An association rule is an important element of Data 
Mining research. Association rules showing attribute - value 
is frequently given along with data set conditions for the 
emergence , can be described as Supposing I is a k-sets 
composed of a collection of different projects. 
A Transaction database D, in which each transaction T is 
a set consisting of a group items of I, namely T ? I, T has the 
only identification TID. If itemsets X ? I and X ? T, then 
transaction set T contains the itemsets X. An association rule 
is an implication, such as the form X => Y, ? ?
and ? ? . Association rule X => Y condition is 
established: It has degree of support for Supp, namely, there 
is at least Supp% of the transaction in database D that 
contains X ? Y. It has degree of confidence for Conf, 
namely, the transaction database D contains X, there are at 
least Conf% transaction also contains Y[5]. 
Association rule mining problem is found to have user-
specified minimal support(min_sup) and minimal 
confidence(min_conf) of association rules. This problem is 
usually decomposed into two subproblems. One is to find 
those itemsets whose occurrences exceed a predefined 
threshold in the database; those itemsets are called frequent 
or large itemsets. The second problem is to generate 
association rules from those large itemsets with the 
constraints of minimal confidence. 
III. APRIORI ALGORITHM
The greatest impact of mining association rules algorithm 
is the Apriori algorithm presented by RakeshAgrawal and 
others[2]. It is a Boolean association rule mining frequent 
itemsets algorithm, That is, first generate frequent itemsets 
(completed by the two-step connection and pruning), and 
then generate association rules. The basic idea of the 
algorithm is that scanning the transaction database many 
times, by using the principle that in a given transaction 
database D, any frequent itemset is a subset of frequent 
itemsets; superset of any weaknesses is the weaknesses set. 
This principle is scanned many times on the transaction 
database. The Apriori algorithm uses breadth-first search and 
a tree structure to count candidate item sets efficiently. It 
generates candidate itemsets of length i from itemsets of 
length i-1. 
First of all, to identify a group of frequent sets, the 
collection is recorded as L1, L1 is used to find the set of 
frequent 2-itemsets L2; L2 is used for L3, If it goes on, until 
you can not find frequent k-itemsets Lk, Li needs to find a 
database for each scanning, Li-1 to find out how the Li is 
divided into two processes composed of connections and 
pruning. According to the core idea of the Apriori algorithm, 
the functional block diagram of the Apriori algorithm is 
shown in Fig.1[3].  
By the Apriori algorithm can be seen, Each Ci have to 
scan the database once, but this time some business have no 
effect on the generation of frequent itemsets. Thus reducing 
the number of scanning the database D, For improving the 
efficiency of the algorithm it is necessary [4].based on the 
Apriori algorithm is low operating efficiency of existing 
inadequate, This paper presents an improved association rule 
mining algorithm - AVI algorithm.  
V2-718978-1-4244-5824-0/$26.00 c©2010 IEEE
Figure 1. the function block diagram of the Apriori algorithm
IV. AVI ALGORITHM DESIGN AND IMPLEMENTATION
A. A  Algorithm Optimization 
Definition 1: For itemset X, t (X) = {tid | tid is the 
identity of t, t  D and t supports X}; For the identification 
set Y, i(Y) = y Y itemset(y), itemset (y) that y 
corresponding to the transaction itemset[4]. For example:  
t({ A ,B ,D}) = { 2 , 6} 
i({2,6}) ={A,B,D} {A,B,C,D,E}={A,B,D} 
(See Table ) 
TABLE I. TRANSACTION DATA SHEET
TID R-listitem TID
1 A,B,E 
2 B,D 
3 B,C 
From Table , we can get: 
Itemset X = {A}: the vertical itemsets of corresponding 
identification sets is {1}; 
Itemset X = {B}: the vertical itemsets of corresponding 
identification sets is {1, 2, 3};  
Itemset X = {C}: the vertical itemsets of corresponding 
identification sets is {3};  
Itemset X = {D}: the vertical itemsets of corresponding 
identification sets is {2};  
Itemset X = {E}: the vertical itemsets of corresponding 
identification sets is {1}; 
AVI algorithm uses the vertical itemsets, by a second-
order frequent itemsets generated set of candidate processes, 
Through the key set and operations, identification sets of 
intersection operations are completed, for example:  
A B={1}{1,2,3}={1} 
B. The counting of support and the description of the AVI 
algorithm 
Let the database of transactions D = (T1, T2, ..., Tn) ,it 
consist of n transactions. Symbol X.sup means the degree of 
support, itemset X in D, Symbol X.count means the degree 
of support counts, itemset X in D. 
For each candidate k-itemsets, connected with it as long 
as from the frequent (k-1)-key focus, Optional 2 sets of 
vertical identity representations are bitwise intersection 
operations, statistical results and then identify the number of
concentrated elements, as a support for counting [6].  
Input : ( k-1) - frequent itemsets candidate k-itemsets to 
connect nodes in R, two-node P and Q identification sets 
vertical are Pb and Qb; 
Output: the counts of support of the candidate itemsets k 
nodes R.count 
Algorithm: 
C. The illustration of the AVI Algorithm 
Supposing min_sup=2/9=22%, minimum support count 
is 2, sample data are shown in Table . 
TABLE II. TRANSACTION  DATABASE  D 
TID Itemset TID Itemset 
1 ABE 6 BC 
2 BD 7 ACD 
3 BC 8 ABCE 
4 AB 9 ABC 
5 AC   
The AVI Algorithm steps are shown as follows: 
Step 1: Scanning transaction database D, the entry in D 
will be converted to table , and to count the number of 
elements identified focus. 
Step 2: Because of min_sup = 2, can get the candidate 
1-itemsets. Remove items that are not frequent 1-itemsets, 
Generate the vertical set of frequent 1-itemsets.(See Table 
)
TABLE III. CANDIDATE  1-ITEMSETS VERTICAL
Itemset Identification vertical Sup.count 
{A} {145789} 6 
{B} {1234689} 7 
{C} {356789} 6 
{D} {27} 2 
{E} {18} 2 
 Step 3: According to candidate set of trees and are 
known to frequent 1-itemsets to find frequent 2-itemsets. 
Process is as follows: according to the first layer of the tree 
nodes is frequent 1-itemsets node, that the second layer of 
each node in each subset is frequent, so generated by the 
Table  that for any two vertical lines marking sets 
intersection operations. (See Table
Step 4: Then counting the number of elements and the 
count as its degree of support, If support min_sup, the 
[Volume 2] 2010 2nd International Conference on Future Computer and Communication V2-719
1) Rb=Pb Q? b
2) For (i=0;i<n and Rb!=0;i++)    do begin 
3) If ((Rb  Qb)! =0) then   //Indicated that Rb and Qb 
are included the first same element. 
4) R..count++ 
5) Shr(Rb,1)    //To shift to the right one 
6) End
corresponding combination is a frequent itemset 2-itemsets 
[8], By frequent 2-itemsets, get a new identification of 
vertical. (See Table )
TABLE IV. CANDIDATE  2- ITEMSETS VERTICAL
Itemset Identification vertical Sup.count 
{A,B} {1489} 4 
{A,C} {5789} 4 
{A,D} {7} 1 
{A,E} {18} 2 
{B,C} {3689} 4 
{B,D} {2} 1 
{B,E} {18} 2 
{C,D} {7} 1 
{C,E} {8} 1 
{D,E} {} 0 
TABLE V. FREQUENT 2- ITEMSETS VERTICAL
Itemset Identification vertical Sup.count 
{A,B} {1489} 4 
{A,C} {5789} 4 
{A,E} {18} 2 
{B,C} {3689} 4 
{B,E} {18} 2 
Step 5: Found the frequent 3-itemsets. The first step 
generate a candidate 3-itemsets, according to candidate set of 
trees, for the third layer node, if it is the second layer of non-
frequent 2-itemsets linked nodes, pruning a link with the 
second-tier node, the remaining node with the second level 
linking to third-layer nodes is a candidate 3- itemsets, can get 
ABE nodes, choose any two of frequent 2-itemsets linked to 
the identity of the second-tier of nodes in the vertical set of 
operations that a transaction, count the number of elements, 
if support  min_sup then it is a frequent 3-itemsets, or 
not , by processing we can get  L3={(A B C) (A B
E)}.According to the calculations we get a frequent 3-
itemsets. (See Table   
REQUENT  3-VERTICAL 
Itemset Identification vertical Sup.count 
{A,B,C} {89} 2 
{A,B,E} {18} 2 
Step 6: Find frequent 4-itemsets. According to the above 
steps, there is no candidate 4-itemsets, therefore, there is no 
frequent 4-itemsets. 
Step 7: The process of frequent itemsets found in the 
end, get all the frequent itemsets: 
L1= {A, B, C, D, E} 
L2={{A,B},{A,C},{A,E},{B,C},{B,E}} 
L3= {{A, B, C}, {A, B, E}} 
V. EXPERIMENT AND RESULT ANALYSIS
After the text edit has been completed, the paper is ready 
for the template. Duplicate the template file by using the 
Save As command, and use the naming convention 
prescribed by your conference for the name of your paper. In 
this newly created file, highlight all of the contents and 
import your prepared text file. You are now ready to style 
your paper. 
In order to facilitate research and experience the AVI 
algorithm, this article uses Visual FoxPro to achieve the AVI 
algorithm. 
A.  design ideas 
In this method, for ease of operation, the introduction of 
the following data table, And made the following 
assumptions: 
• Define a transaction data table TRAN.DB, it consists 
of two fields, field names respectively T, ITEMS 
which are character [7],In turn behalf of the Panel 
ID, things included in the project record. 
• Create a project data table ITEM.DBF, it contains a 
field, called 11, Data type is character, used to store 
all the items, each record in this table represents a 
project. The number of records in the table is the 
number of projects. 
• Create a number of empty tables 
ITEM1.DBF,ITEM2.DBF,ITEM3.DBF,ITEM4.DB
F,……, these were used to store an item frequent 
sets,2 project frequent itemsets,3 project frequent 
itemsets,4 project frequent itemsets. Before you run 
the program, these tables, only the structure, there is 
no data. 
• Generate a set of frequent. By 1 project frequent, we 
can construct a candidate itemset tree. 
• Generate two frequent sets. The use of ITEM1.DBF 
And the candidate item set the length of the tree 
produces two candidate itemsets and the 
corresponding identification set into the 
ITEM2.DBF.count the number of identification, 
namely, the degree of support the candidate itemsets, 
remove support for less than a given minimum 
support of all records, Find all the length of two of 
the frequent itemsets into ITEM2.DBF, The rest of 
analogy, until you find all the frequent itemsets. If it 
is found the number of candidate itemsets is zero, 
then stop operation.  
• Finally, the output of all frequent itemsets.The 
programs need only scan the transaction database 
one time. 
B.  Experimental results 
In this paper, AVI algorithms experiments, The execution 
time of the algorithm were compared, use a real 
experimental data(March 2008 non-computer professional 
computer grade test scores library in China University of 
Mining and Technology ),A total of 1200 experimental 
database records, in order to facilitate excavation, the 
number of attributes and types of property data into Boolean 
types, Transformed total of 20 properties, The length of each 
transaction 4, taking into account the actual situation, 
respectively, a minimum degree of support for the 
1%,2%,3%,4%,5%and 6%, Tested this algorithm and the 
Apriori algorithm to improve execution time, shown in Fig.2. 
V2-720 2010 2nd International Conference on Future Computer and Communication [Volume 2]
TABLE VI. F
VI
Figure 2.  Different supports for  the algorithm execution time 
In order to improve the performance of mining frequent 
closed itemsets, AVI can be used in the algorithm to sort the 
child nodes, as well as dynamic optimization strategy. The 
newly generated child nodes according to their support for 
the size of the small to large re-sequencing, Thereby 
reducing the number of branches, speed up the frequent 
closed itemsets mining process. 
This paper uses a comparison with the Apriori algorithm, 
the experimental data are from http://www.Cs.Rpi.Edu/za-
ki/software/. Experiments show that degree of support 
between the two larger only slight changes, reduce the 
degree of support when the change will be sharply increased. 
In particularly, for the minimum degree of support of 45%, 
AVI faster than the Apriori is about 8 times, shown in Fig.3. 
Figure 3.  AVI performance analysis 
VI. CONCLUSION
The AVI algorithm presented in this paper, it is an
identity-based set of vertical, through the intersection 
operations, and generate candidate itemsets tree and the 
corresponding degree of support. Comparing with classic 
Apriori algorithm, AVI algorithm only needs to conduct a 
scan of the transaction database, to significantly reduce I / O 
frequency, so the AVI algorithm is simple and moderate 
memory overhead. Through relevant experiments, for 
different minimum support, to compare the execution time of 
these two algorithms, the superiority of the AVI algorithm in 
the larger minimum degree of support is not very clearly, 
however, with the minimum degree of support reduction, its 
superiority gradually increases. 
REFERENCES
[1] Zeng Xiao-wen , the study of the association rules and data mining 
methods[J]. Computer and Modernization,2006,9:90~93
[2] Jia wen Han,Jian pei, Yi wen Yin. Mining frequent patterns without 
candidate generation [J]. SIGMOD,2001.1~12 
[3] Chen Yu-sheng,Deng Xiao-guang,Jiang Xiao-yao.Apriori Algorithm 
for Frequent Itemsets Mining in the Realization.[J]. Computer 
Technology and Development,2006,16(3):58~60 
[4] Zaki M J, Hsiao C-J. CHARM: An Efficient Algorithm for Closed 
Association Rules Mining. Technical Report 99-10. Computer 
Science Department of Rensselaer Polytechnic Institute , 1999 
[5] ChenAn,ChenNing.DataMiningTechnologyandApplication[M].Beijin
g: Science Press,2006 
[6] Zaki M J. Generating Non-redundant Association Rules Proc. of 
the 6th ACM sIGKDD Int’l Conf. on Knowledge Discovery and Data 
Mining. 2000 :34-43 
[7] Zhou Cui-hong,HeJian-jun.An improvement of the Apriori algorithm 
in Mining Association Rules[J].Hunan City 
College,2006,15(4):68~69 
[8] Banks J, Bennamoun M. A constraint to improve t he reliability of 
stereo matching using t he rank transform IEEE International 
Conference. Vol. 06, 1999Huang Hai-yan, Gu Xing-sheng, Liu 
Mandan. Research on Cultural Algorithm for Solving Nonlinear 
Constrained Optimization [J].ACTA AUTOMATICA SINICA, 2007, 
33 (10):1115~1120 
[Volume 2] 2010 2nd International Conference on Future Computer and Communication V2-721
