May 12, 2010 15:23 RPS : Trim Size: 8.50in x 11.00in (IEEE) icfcc2010-lineup?vol-1: F340
An Incremental Associative Classification algorithm used for Malware Detection 
 
FENG Shaorong 
School of Information Science and Technology, Xiamen 
University 
Xiamen, Fujian, 361005 China 
shaorong@xmu.edu.cn 
HAN Zhixue 
School of Information Science and Technology, Xiamen 
University  
Xiamen, Fujian, 361005 China 
Jessehan@yahoo.cn
 
 
Abstract—Associative classification(AC) is a promising 
approach used for auto malware detection. However, when 
data operation occurs (training data added over time), 
traditional AC algorithms have to re-learn repetitive which is 
expensive or even become invalidly because of massive data 
and limited computing resource. To resolve the challenges 
above, an efficient incremental associative classification 
algorithm (EIAC) is proposed which can keep the last mining 
results and learn from the new data set. First, EIAC learns 
new potential ruleitems from the new data set; and then 
updates the frequent count of original and potential ruleitems 
by constructing and searching two trees based on FP-Tree 
respectively; at last, updates the classification association rules 
with the frequent information of updated ruleitems. The 
promising studies on real daily data collection and prediction 
illustrate that: compared with the traditional AC and other 
classification methods, EIAC can maintain the classification 
association rules effectively and ensure a higher predictability 
of the classification model. So it can be well used for malware 
detection. 
Keywords-Association; Classification;  Incremental Learning;  
Malware Detection 
I.  INTRODUCTION 
Malware is a program designed to infiltrate or damage a 
computer system without the owner's consent. Nowadays, 
massive malware has presented a major security threat to 
computer users. Auto Malware Detection becomes a key 
technique for Network Security [1]. Associative 
classification(AC)[2-5] can be successfully used for malware 
detection. In malware detection, the data set for training is 
always very large and frequently updated. However, when 
the training set changeed, the existing AC algorithms always 
scan the changed data set in order to reflect the changes 
done[3, 6]. We propose an efficient incremental associative 
classification algorithm(EIAC) based on FP-Tree. It 
maintains the classification association rules (CARs) 
incrementally by keeping the last mining results and learning 
the new data. A comprehensive experimental study on a 
large collection of PE files obtained from the anti-virus 
laboratory of KingSoft Corporation is performed to compare 
the incremental algorithm with non-incremental algorithm. 
Promising experimental results demonstrate that EIAC 
outperforms non-incremental algorithms and other 
classification methods in efficiency and accuracy both. 
II. RELATED WORK 
Utilizing association rule discovery method to construct 
classification systems is a promising approach[2]. Associative 
classification method has been used in malware detection 
system IMDS[4], which adopts the improved OOA_Fast_FP-
growth algorithm to generate CARs for building the 
classifier. But without considering the efficient incremental 
maintain problem, whitch restricts the practical application 
of this method seriously. In malware detection, the data set 
for training is always very large and frequently updated, as a 
result the traditional method has to relearn the complete 
updated database every time to reflect the changes of the 
training data. Furthermore, as the training data set grows 
rapidly, it wastes the learned information seriously, and will 
aggravate the exhaustion of computational resource for one 
time mining. 
So far, there are rarely studies on the incremental 
learning of associative classification[3, 6]. But there had been 
some studies on incremental association rule discovery 
algorithms, and we can use their ideas for reference to solve 
the incremental AC mining problem. FUP[7] and FUP2[8] are 
proposed to handle all update cases when data is added to 
and deleted from a database. So far, researchers had 
proposed some incremental association rule algorithms based 
on FP-Tree[9]. Such as methods in [10, 11], dealing with the 
association rule updating problem with minimum support 
changed and data operations occurred respectively. In [11], 
the authors focus on how to generate new frequent patterns 
after new transaction data added to training database. 
Considering the challenges above, we propose an 
efficient incremental associative classification algorithm 
(EIAC) based on FP-Tree. It maintains the classification 
association rules (CARs) incrementally by keeping the last 
mining results and learning the new data. As avoiding re-
learning the history data and using the compact FP-Tree 
structure, EIAC outperforms non-incremental algorithm and 
other classification methods in efficiency and accuracy both. 
III. THE EFFICIENT INCREMENTAL ASSOCIATIVE 
CLASSIFICATION ALGORITHM 
      In this paper, resting on the analysis of Windows APIs 
called by PE files, we apply associative classification 
method in malware detection. That is to find out how a set 
of API calls support the specific objectives: Class= Malware 
and Class= Benign. 
This research is supported by the National Natural Science 
Foundation of China under Grant No. 50604012 
V1-757978-1-4244-5824-0/$26.00 c©2010 IEEE
May 12, 2010 15:23 RPS : Trim Size: 8.50in x 11.00in (IEEE) icfcc2010-lineup?vol-1: F340
A. Problem description 
In this section, we will introduce the definitions used in 
EIAC algorithm. 
Definition 1 Support and Confidence[2, 4]  Let 
I={I1,I2,…,Im} be an itemset. The support and confidence of 
the corresponding class association rule are defined as: 
( { }, )% *100%num I Class DBos
DB
?  (1) 
( { }, )% *100%
( , )
num I Class DBoc
num I DB
?  (2) 
Where the function num(I Class, DB)?  returns the 
number of data in DB where I?Class hold and num(I, DB) 
means the number of data which contains I. 
Definition 2 Frequent Ruleitem[2, 4]  Given mos% as the 
user-specified minimum support. I is called frequent ruleitem 
in DB if os%?mos%.
Definition 3 Class Association Rules[2, 4] Given moc% as 
the user-specified minimum confidence. The rule is a class 
association rule if oc%?moc%. And I is called an accurate 
ruleitem. 
B. EIAC algorithm 
Let L be the set of frequent ruleitems in original database 
DB; MC be the set of frequent ruleitems relating to Malware; 
BC be the set of accurate ruleitems relating to Benign; MRS 
be the set of CARs related objective is Malware; BRS be the 
set of CARs related objective is Benign. After an incremental 
db (db contains only new malware data) is added to DB.NC 
is the set of frequent ruleitems in db which are relating to 
Malware. There are two steps in EIAC algorithm: Updating 
the frequent ruleitems and updating the CARs. 
1) Update of frequent ruleitems based on FP-Tree 
The following properties are useful: 
Lemma 1: For each frequent ruleitem which is not 
accurate in MC. It may generate a new malware rule: 
I?Malware(os,oc). We can compute the rule’s os'?oc' in 
updated database according to the following formulates: 
' ( { }, ) ( { }, )
| | | |
num I Malware DB num I Malware dbos
DB db
?  ? 

 
' ( { }, ) ( { }, )
( { }, ) ( { }, )
num I Malware DB num I Malware dboc
num I Malware DB db num I Benign DB
?  ? 
? ?  ?
(4) 
Lemma 2: A frequent ruleitem I in db can become a 
winner in the updated database DB?db if and only if 
( { }, ) * |num I Class db mos db? ! |
)
 (5) 
( { }, ) (| | | |num I Class DB db mos DB db? ? !    (6) 
Lemma 3: If an original frequent ruleitem I becomes a 
loser, then every longer ruleitem containing it cannot be a 
winner. 
Based on the above analysis and properties, the update 
of frequent ruleitems in DB?db is outlined as follows: 
a) Filtering out the ruleitems which appeared in MC 
from NC: NC=NC-NC?MC. Then for every ruleitem I?NC, 
updating its corresponding support count in DB db?  and 
choose the new frequent ruleitems: First, scan DB to delete 
items which are not 1-ruleitem in NC and order remainder 
items descending by their frequent count in NC; Second, 
construct a FP-Tree for pruned DB; Finally, update the 
frequent count of each potential ruleitem in DB?db by 
searching the FP-Tree; 
b) For every ruleitem I?MC, updating its 
corresponding support count in DB?db: First, scan db to 
delete items which are not 1-ruleitem in MC  and order 
remainder items descending by their frequent count in MC; 
Second, construct a FP-Tree for pruned db; Finally, update 
the frequent count of each original ruleitem by searching the 
FP-Tree; 
c) For every ruleitem , ? %& , calculate its 
corresponding support count in db by constructing and 
searching it like above. 
For every ruleitem above, if num(I ? Class, 
DB?db)<mos*(|DB|+|db|), means that it is not a frequent 
ruleitems in DB db? , we call it as a loser. Then we can 
delete it. At last, we get the frequent ruleitem set NC?MC 
relating to Malware and the frequent ruleitem set BC relating 
to Benign in updated database. 
2) Update of CARs 
There are only new malware data in db, so only new 
malware rules may be generated. The process of CARs 
update consists of three major parts: updating original 
malware rules; updating benign rules; generating new 
malware rules. Based on the enough information of frequent 
ruleitmes, the updating of CARs is very easy as follows: 
a) Update original malware rules: For every CAR in 
MRS: I?Malware(os,oc). Check I in updated MC. If I 
disappeared, which means the rule is invalid in the updated 
database. We can remove this rule from MRS straightly. Else 
we need to get the corresponding support count of I from MC. 
And compute the rule’s os' and oc', in updated database 
according to formulates (3) and(4). If oc'?moc , then the 
rule is still invalid in the updated database. So we remove 
this rule from MRS straightly. Else, update the rule’s value 
of support and confidence in updated database DB?db; 
b) Updating benign rules: For every CAR in BRS:  
I?Benign(os,oc) . Read its support count in db. Then 
compute the rule’s os' and oc' according to the following 
formulates: 
| |'
| | |
os DBos
|DB db
 

 (7) 
| |'
| | ( { },
os DBoc
os DB num I Malware db
 
  ? )
 (8) 
If os'<mos or oc'<moc, then the rule invalidates in the 
updated database. So we remove this rule from BRS 
straightly. Else, update the rule’s value of support and 
confidence in updated database DB?db; 
c) Generating new malware rules: New malware rules 
come from two parts: Original frequent ruleitems which 
were not accurate in MC but become accurate in updated 
database generating new malware rules; New frequent 
ruleitems may generate new malware rules. 
V1-758 2010 2nd International Conference on Future Computer and Communication [Volume 1]
May 12, 2010 15:23 RPS : Trim Size: 8.50in x 11.00in (IEEE) icfcc2010-lineup?vol-1: F340
For every frequent ruleitem which was not accurate in 
MC. It may generate a new malware rule:
I?Malware(os,oc). We can compute the rule’s os' and oc' in 
updated database according to formulates (3) and(4).
For each potential frequent ruleitem in NC, it may 
generate a new malware rule: I?Malware(os,oc). We 
compute its os' and oc' in updated database according to 
formulates (3) and(4) too. 
For every ruleitem I above, if its os'> mos and oc'?moc,
the ruleitem do generate a new malware rule in updated 
database. Then we add the new rules into MRS straight. Else, 
this ruleitems is still just frequent but not accurate to 
compose a rule. 
3) The EIAC algorithm outline 
Input:  
(1) DB: the original database; 
(2) db: an incremental malware data set;  
(3) MC: the set of frequent ruleitems relating to Malware in 
DB;
(4) BC: the set of frequent and accurate ruleitems relating to 
Benign in DB;  
(5) MRS: the set of CARs related objective is Malware; 
(6) BRS: the set of CARs related objective is Benign; 
(7) NC: the set of frequent ruleitems in db, and relating to 
Malware;
(8) mos: user-specified minimum support; 
(9) moc: user-specified minimum confidence. 
Output: 
(1) MC': the set of frequent ruleitems relating to Malware in 
DB?db;  
(2) BC': the set of frequent ruleitems relating to Benign in 
DB?db; 
(3) MRS': the set of CARs related objective is Malware in 
DB?db; 
(4)BRS': the set of CARs objective is Benign in DB?db; 
Algorithm:  
/* update the frequent ruleitems set*/ 
(1) NC'=NC-MC?NC;
(2) MC'=Update_FrequentItemsets(MC, db); 
(3) NC'=Update_FrequentRuleitems(NC, DB); 
(4) BC'=Update_FrequentRuleitems(BC, db); 
/* update original rules and generate new rules*/ 
(5) MRS'=Update_Rules(MRS, MC'); 
(6)BRS'=Update_Rules(BRS, BC'); 
// Generate new rules from the frequent itemsets which 
aren’t accurate ruleitems in MC'// 
(7) NewMRS=Generate_NewRules(MC',MRS'); 
// Generate new rules from potential frequent itemsets in 
NC'// 
(8) NewMRS=NewMRS?Generate_NewRules(NC', MRS'); 
//Union all rules// 
(9) MRS'=MRS'?NewMRS. 
IV. EXPERIMENT AND ANALYSIS
We conduct two sets of experiments on the large 
collection of PE files obtained from the anti-virus laboratory 
of KingSoft Corporation. First, we evaluate the performance 
of EIAC and traditional OOA_Fast_Fp-growth on a large 
data set. The second, we verify the precision of our classifier 
and other classification methods. Both the experiments are 
conducted under the environment of Windows XP OS plus 
Intel Core Duo 1.66GHz CPU and 1GB of RAM. 
A. Experimental data set 
In this study, we use 10,000 malicious executables and 
10,000 benign executables as training data set, other 5,000 
malicious and 5,000 benign executables as test data set. 
Useful information include: FileSort, as the PE type; 
FileName, as the name of the executables; APISeq, as 
Windows APIs called by PE files. 
B. Results and analysis 
1) Evaluation between EIAC and OOA_Fast_Fp-growth 
Because of the higher efficiency of OOA_Fast_Fp-
growth than Apriori and Fp-growth methods[4], we can 
directly use OOA_Fast_Fp-growth as a non-incremental 
comparison algorithm. 
The experimental data is arranged as follow: split the 
20,000 training data set DB as 5 parts: DB={DB0, DB1, 
DB2, DB3, DB4}; Where DB0 includes 6,000 malicious and 
benign executables respectively; DB1 and DB2 are both 
2000 malicious executables;DB3 and DB4 are both 2000 
Benign executables. 
The experiments were done as follow: Set mos=0.05?
moc=0.7 (recommend by malware analysis  and mining 
experts); EIAC incremental learning: mining DB0 as the 
initial learning database using OOA_Fast_Fp-growth in 
order to get the initial frequent patterns and CARs. Then 
mining the other 4 parts of data using the incremental 
methods EIAC; as the comparison experiments, re-learn the 
updated data set as a whole repetitive using OOA_Fast_Fp-
growth, when the other 4 data sets were added one by one. 
The results are shown in table , whe? re FAIL means the 
algorithm is invalid to get rules within limited computational 
recourse. 
TABLE I. COMPARISON OF TWO ALGORITHMS
Data
set
Re-learning with 
OOA_Fast_Fp-growth 
Numbers of CARs/CPU 
Time(hour)
Incremental learning 
with EIAC 
Numbers of CARs/CPU 
Time(hour) 
DB0 40035/1 40035/1 
DB1 40329/2 40329/0.5 
DB2 40147/5 40147/0.45 
DB3 FAIL 41786/0.6 
DB4 FAIL 43467/0.65 
We can find that, re-learning the whole updated 
database waste the learned information, and also exhaust the 
computational resource as the database becoming too large. 
However, using our incremental learning algorithm EIAC 
avoid re-learning. So with the same values of parameters 
[Volume 1] 2010 2nd International Conference on Future Computer and Communication V1-759
May 12, 2010 15:23 RPS : Trim Size: 8.50in x 11.00in (IEEE) icfcc2010-lineup?vol-1: F340
(mos, moc) EIAC can maintain the CAR set efficiently, 
solved the incremental learning problems of frequent data 
insertion. 
2) Comparison of different classification algorithms 
In this set of experiments, we compare EIAC with 
Support Vector Machine and Decision Tree methods. That 
is using the J4.8 version of Decision Tree implemented in 
WEKA[12] and the SVM implemented in LIBSVM 
package[13], setting mos to 0.294 and moc to 0.98 
respectively for EIAC. Then mine the 20,000 executables as 
training data, and validate the accuracy of each classifier on 
other 10,000 executables. Results shown in Table? indicate 
our method achieve most accurate malware detection. 
Where TP, TN, FP, FN, DR, and ACY refer to True Positive, 
True Negative, False Positive, False Negative, Detection 
Rate, and Accuracy respectively. 
TABLE II. COMPARISON OF DIFFERENT CLASSIFIERS
Classifiers TP TN FP FN DR ACY 
Decision 4078 4885 115 922 81.56% 89.63%
Lib SVM 4401 4857 143 599 88.02% 92.58%
EIAC 4453 4897 103 547 89.06% 93.50%
From the comparison, we observe that EIAC outperforms 
other classification methods in both detection rate and 
accuracy. Decision Tree method exhaust too much RAM. In 
order to deal with large data set, researchers improved it with 
many techniques (sampling, scattering), which descend the 
accuracy especially with noise. For SVM, the overlapping 
between Malware and Benign data cause the trouble to 
separate them exactly, especially the unbalance of data in the 
overlapping field. All of these reasons declined the accuracy. 
And some works [5] proved that a classifier with frequent 
pattern analysis is generally effective to solve the scalability 
and over fitting issues. 
V. CONCLUSIONS AND FUTURE WORK
In this paper, we introduce an efficient incremental 
updating associative classification algorithm (EIAC) based 
on Fp-Tree, the main contributions are: (1) Considering the 
frequent adding operation on training database, EIAC can 
maintain the set of CARs incrementally; (2) use Fp-tree for 
reference, it can maintain the classifier efficiently; (3) 
Considering the relationship between data attributions with 
class entirely, it achieve higher detection accuracy than other 
classifiers. Experimental results improved that EIAC 
algorithm can quickly and effectively maintain the 
classification rules, and ensure the predictability of the 
classification model used for malware detection. 
Furthermore, as a universal classification algorithm, EIAC 
can be directly used for many other categorization tasks, 
such as text mining and News classification etc. 
ACKNOWLEDGMENT
This research is supported by the National Natural 
Science Foundation of China under Grant No. 50604012, 
which title is “Research on Key Techniques in mining 
industry Data Warehouse”. Financial supports from this 
Program(NNSF) are highly appreciated. The helpful 
comments from two anonymous reviewers are also gratefully 
acknowledged. 
REFERENCES
[1] N. Idika, A. P. Mathur. A Survey of Malware Detection 
Techniques[R]. Department of Computer Science, Purdue University, 
2007. 
[2] B. Liu, W. Hsu, and Y. Ma. Integrating classification and association 
rule mining [C]. Proceedings of the Fourth International Conference 
on Knowledge Discovery and Data Mining. New York City, 1998: 
80-86. 
[3] Fadi Thabtah. A review of associative classification mining [J]. 
Proceedings of The Knowledge Engineering Review, 2007, 22(1): 37-
65 
[4] Yanfang Ye, Dingding Wang, Tao Li, and Dongyi Ye. IMDS: 
Intelligent malware detection system [C]. Proceedings of ACM 
International Conference on Knowledge Discovery and Data Mining 
(SIGKDD), San Jose, California, 2007: 1043-1047. 
[5] H. Cheng, X. Yan, J. Han, and C. Hsu. Discriminative frequent 
pattern analysis for effective classification [C]. Proceeding of the 
2007 international conference on data engineering ICDE-07, Istanbul, 
Trukey, 2007. 
[6] Fadi Thabtah. Challenges and Interesting Research Directions in 
Associative Classification [C]. Proceedings of the sixth IEEE 
International Conference on Data Mining-Workshops (ICDMW’06), 
Hong Kong: IEEE, 2006:785-792. 
[7] Cheung D W et al. Maintenance of discovered association rules in 
large databases: an incremental updating technique [C]. Proceedings 
of the 12th International Conference on Data Engineering. New 
Orleans, Louisana, 1996: 106-114 
[8] Cheung D W et al. A General incremental technique for maintaining 
discovered association rules [C]. Proceedings of the 5th International 
Conference on Database Systems for Advanced Applications 
(DASFAA’97). Melbourne, Australia, 1997. 
[9] Han J, Jian P, Yiwen Y. Mining frequent patterns without candidate 
generation [C]. Proceedings of the 2000 ACM SIGMOD International 
Conference Management of Data. Dallas, 2000: 1-12. 
[10] Zhu Yu-Quan, Sun Zhi-Hui, Ji Xiao-Jun. Incremental updating 
algorithm based on frequent pattern tree for mining association rules 
[J]. Journal of Computers , 2003 , 26 (1) : 91-96 
[11] Yang Ming et al. Fast incremental updating of frequent itemsets [J]. 
Journal of Applied Sciences, 2003 , 21 (4) : 367-372. 
[12] H. Witten and E. Frank. Data mining: Practical machine learning 
tools with Java implementations [M]. Newark City: Morgan 
Kaufmann, 2005. 
[13] C. Hsu and C. Lin. A comparison of methods for multiclass support 
vector machines [J]. IEEE Transactions on Neural Networks, 2002, 
13(2):415-425. 
V1-760 2010 2nd International Conference on Future Computer and Communication [Volume 1]
