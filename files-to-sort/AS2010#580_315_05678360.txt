Associative classification over Data Streams 
 
 
Zhen-Hui Song 
Department of Computer 
ShiJiaZhuang Vocational Technology Institute  
ShiJiaZhuang?China  
E-mail: songzhenhui_sjz@163.com  
Yi Li 
Department of electrical and electronic engineering 
ShiJiaZhuang Vocational Technology Institute  
ShiJiaZhuang?China  
E-mail: liyi_sjz@126.com
 
 
Abstract—Based on association rules, Associative classification 
(AC) has shown great promise over many other classification 
techniques on static dataset. However, the increasing prominence 
of data streams arising in a wide range of advanced application 
has posed a new challenge for it. This paper describes and 
evaluates AC-DS, a new associative classification algorithm for 
data streams which is based on the estimation mechanism of the 
Lossy Counting (LC) and landmark window model. We apply 
AC-DS to mining several datasets obtained from the UCI 
Machine Learning Repository and the result show that the 
algorithm is effective and efficient. 
Keywords--data streams; associative classification; frequent 
itemsets 
I.  INTRODUCTION  
Data-steam mining is a technique which can find valuable 
information or knowledge from a great deal of primitive data. 
Unlike mining static databases, mining data streams poses 
many new challenges [1]. 
First, each data element should be examined at most once. 
It is unrealistic to keep the entire stream in the main memory. 
Second, each data element in data streams should be processed 
as fast as possible. Third, the memory usage for mining data 
streams should be bounded even though new data elements are 
continuously generated. Finally, the results generated by the 
online algorithms should be instantly available when user 
requested. 
A. Data Stream 
Let I={i1,i2,…,im} be a set of literals, called items. An 
itemset is a subset of I. An itemset consisting of m items is 
called a m-itemset. Let us assume that the items in an itemset 
are in lexicographical order.  A transaction is a tuple, (tid, 
itemset), where tid is the ID of the transaction. 
A transaction data stream DS={B1, B2, … , BN , …} be an 
infinite sequence of blocks, where each block is associated 
with a block identifier n, and N is the identifier of the latest 
block BN.  Each block Bi consists of a set of transactions, that is, 
Bi=[T1, T2, …, Tk], where k>0. Hence, the current length of the 
data stream is defined as CL=|B1|+|B2|+…|BN|. 
The frequency of an itemset, X, denoted as freq(X), is the 
number of transactions in B that support X. The support of X is 
defined as fre(X)/N, where N is the total number of transactions 
received. X is a Frequent Itemset (FI) in B, if 
sup(X)>=minSupport, where minSupport (0<=minSupport<=1) 
is a user defined minimum support threshold. 
Many algorithms have been proposed for mining data 
stream. According to the processing model on the data stream, 
we can classify the research work into three fields: landmark 
windows, sliding windows and damped windows. Manku and 
Motwani[2] proposed a single-pass algorithm, Lossy Counting, 
to mining frequent itemsets, and the algorithm is based on a 
well known Apriori-property. Yu et al. [3] proposed a 
algorithm, FDPM, which is derived from the Chernoff bound, 
to approximate a set of FIs over a landmark window. Li et al.[4] 
propose a single-pass algorithm, DSM-FI, to mine all frequent 
itemsets over the entire history of data streams. Pedro 
Domingos and Geoff Hulten [5] described and evaluated an 
algorithm, VFDT, to predict the labels of the records it 
received. 
B. Associative Classification 
Let D be the dataset. Let I be the set of all items in D and C 
be the set of class labels. We say that a data case di?D 
contains X?  I, a subset of items, if X? di. A class association 
rule (CAR) is an implication of the form X?c, where X?  I, 
and c?C. 
Bing Liu et al. [6] first proposed the AC approach, named 
classification based on association algorithm (CBA), for 
building a classifier based on the set of discovered class 
association rules. The difference between rule discovery in AC 
and conventional frequent itemset mining is that the former 
task may carry out multiple frequent itemset mining processes 
for mining rules of different classes simultaneously. Data 
mining in associative classification (AC) framework usually 
consists of two steps: 
(1) Generating all the class association rules (CARs) 
which has the form of iset??c, where iset is an 
itemset and c is a class. 
(2) Building a classifier based on the generated CARs. 
Generally, a subset of the association rules was 
selected to form a classifier and AC approaches are 
based on the confidence measure to select rules [7]. 
978-1-4244-7941-2/10/$26.00 ©2010 IEEE
Input: DS---a data stream in which each record has N items. 
          Swindow---the window size, Swindow=| Bi |. 
minSupport--- Support threshold.  
Output: M---a classifier with a lot of association rules whose confidence are 
great than 50% and support value great than minSupport. 
Method:  
Initial the rule memory  M=? 
Do 
Read in a data block Bi={T1, T2, …, Tk} 
m=0; Am=?         //Clear the set of candidate itemsets A 
        Am+1=Gen(Bi, Am)        // Generate n candidate frequent itemsets, 
// Itemset1, Itemset2,…, Itemsetn,  
//each Itemseti in A has 1 items. 
While A??  
       For  i=1 to n 
                   S=Supp(Itemseti)           //Calculate the support of Itemseti  
                   If  S>= minSupport  then 
                        M?M+ Itemseti           //Put the itemset into memroy 
                           Endif 
               Endfor 
               m=m+1 
               Am+1=Gen(Bi, Am)       //Generate the (m+1) generation A 
        Endwhile 
        M=Rank(M)                     //Rank rules by their confidence values 
        M=Decay(M)                   //Decay the rules in memory 
While 
Most of the algorithms shown above were used for finding 
frequent itemsets. Some of them were used to classify the data 
steams with a decision tree. In this paper, we present an 
approach to mine class association rules and then to make a 
classifier. Classifying a data stream with an association 
classifier is a newly explored area of research, which may be 
viewed as a further extension to the earlier work on mining 
frequent itemsets over data stream. 
II. ASSOCIATIVE CLASSIFICATION ON DATA STREAMS 
A. Problem Definitions 
A data stream is a massive unbounded sequence of data 
elements continuously generated at a rapid rate. Due to the 
unique characteristics of streaming data, most one-pass 
algorithms have to sacrifice the correctness of their analysis 
results by allowing some errors. Hence, the True support of an 
intemset X, denoted by Tsup(X), is the number of transactions 
seen so far in which that itemsets occurs as a subset. The 
estimated support of the itemset X, denoted as Esup(X), is the 
estimated support of X stored in the summary data structure 
constructed by the one-scan approaches, where 
Esup(X)<=Tsup(X). An itemset X is called a frequent itemset if 
Tsup(X)>=Minsupport*CL. 
Hence, given a user-defined minimum support threshold 
minSupport (0<= minSupport <=1) and a data stream DS, our 
goal is to develop a single-pass algorithm to classify the 
streaming data in the landmark windows model using as little 
main memory as possible. 
Figure 1.  The proposed associative classification algorithm. 
B. Associative Classification Algorithm for Data Stream 
We can see the completed algorithm from Fig.1. Our 
algorithm accepts two user-specified parameters: a support 
threshold minSupport and a window size Swindow=| Bi |. Let 
N denote the current length of the stream, i.e., the number of 
records seen so far. Every time received a record, our algorithm 
can forecast its class label based on the association rules 
extracted from the records before. Each of the rules has a 
estimated support Esup(X), whose value is great than 
minSupport. 
For a given data block Bi, the first pass of the algorithm 
counts item occurrences to determine the frequent 1-itemsets. 
As long as the set of 1-itemsets was not empty, the algorithm 
subsequently carry out the next pass to find frequent (m+1)-
itemset. When the algorithm obtains all of the frequent itemsets, 
it will calculate the confidence of the rules and sort them in the 
memory. Then, if there is a request of classification, the 
classifier will predict the class label of a record. At the bound 
of block Bi, the memory rules will be pruned and the rules with 
low support value will be deleted. 
C. Funtions and Data Structure 
1) Data struture M 
The data structure M is a set of entries of the form (itemset, 
fclass(1), fclass(2),…, fclass(i), t), where itemset is a subset of 
conditional attributes, fclass(i) is an interger representing the 
approximate frequency of class attributes i, and t is the number 
of the data blocks in which the itemset appeared firstly. 
Initially, M is empty. Whenever a new association rule (itemset, 
class(i)) arrives, we examine M to see whether an entry mj 
already exists or not. If exists, we update the entry by 
incrementing its corresponding frequency fclass(i) by one. 
Otherwise, we create a new entry of the form (set, fclass(1), 
fclass(2),…, fclass(i), t).  The parameter t is the number of the 
data block and fclass(x) is the frequency of class x. 
fclass(x) =0 (class(x) ?class(i)) 
fclass(x) =1 (class(x) =  class(i))                                    (1)  
2) Function Gen(Bx, Am) 
The Gen(Bx, Am) function takes as argument Am, the set 
of all frequent m-itemsets. It returns a superset of the set of all 
(m+1)-itemsets. The function works as follows [8]:  
a) Step 1, we join Am with Am: 
                Insert into Am+1: 
                Select p.item1, p.item2, …, p.itemm, q.itemm 
                From Am  p,  Am  q 
                         Where p.item1=q.item1,…, p.itemm-1=q.itemm-1, 
                             p.itemm<q.itemm;          
b) Step 2, we delete all itemsets a?Am+1 such that some 
m-subset of a is not in Am: 
              Forall  itemsets  a?Am+1 do 
                    Forall   m-subsets s of a do 
                          If   NOT (s?Am)  then 
                                  Delete  a  from  Am+1; 
3) Function Supp(Itemsetx) 
The function Supp(Itemsetx) was used to calculate the 
support value of Itemsetx. The value of the function was 
calculated like this: 
Supp(Itemsetx)= MAX(fclass(1), fclass(2),…, fclass(i))/N          (2) 
Where N is the number of current data block. 
4) Function Rank(M) 
The function Rank(M) was used to sort the rules in M with 
their confidence values. The confidence value of a entry in M 
was calculated like this: 
Confidence(mi)=MAX(fclass(1), fclass(2),…, fclass(i))/ 
 (fclass(1), +fclass(2), +…+, fclass(i) )            (3) 
5) Function Decay(M) 
The function Decay (M) was used to delete some entries at 
the boundary of a data block. If the expression below is true, 
the entry will be deleted from the memory M. N is the number 
of current data block. 
 MAX(fclass(1), fclass(2),…, fclass(i))+minSupport*(t-1) *CL 
< MINSUPPORT*N*CL           (4) 
III. EXPERIMENTS 
In this section, we provided a formal model of associative 
classification and further examine how such algorithm can be 
applied in data stream mining. We have conducted an 
experiment on a 3.0 GHz Pentium PC with 512MB of memory 
running with Microsoft Windows XP to measure the 
performance of the proposed approach. The datasets used in the 
experiment are obtained from the UCI Maching Learing 
Repository[9].  
In order to compare our algorithm with other classification 
algorithms and make the evaluation more credible and reliable, 
we choose some large datasets from UCI. The Entropy method 
was used in the progress of discretization of continuous 
attributes[10]. 
A. Effects of Parameters 
We investigated the effects of the different block sizes on 
the effectiveness and efficiency of the algorithm and found that 
the accuracy of the algorithm was not affected obviously by it. 
But the number of the rules found and the time cost were 
affected by it largely.  
 
Figure 2.  The accuracy of the classification algorithm and the number of the 
rules found. 
As can be seen in Fig.2, when the initial data block size 
was set to 1000, 2000, 3000, 4000 and 6000, and minimum 
support threshold was set to 10%, we got the similar forecast 
results.  However, the numbers of the found rules are very 
different. As the block size getting larger, the number of the 
found rules became smaller. 
This is because that, although we delete some entries at the 
boundary of a data block, there still are some un-frequent rules 
in the memory and they will be deleted in the later decay step. 
The large the data block size is, the less the number of un-
frequent rules exists. 
 
 
Figure 3.  The run time of the classification algorithm. 
We further investigated the effects of different block sizes 
on the time cost by the algorithm and found that with the same 
dataset, as the increasing of the block size, the total run time 
becomes more and more long. With the different dataset, the 
run time was decided mainly by the number of attributes and 
the number of the items of the dataset.  We can see this from 
Fig.3, the numbers of the attributes of the four datasets, 
Nursery, Krkopt, Poke and Adult, are 8, 6, 10 and 14. And the 
numbers of the items of them are 27, 40, 85 and 127. Although, 
the support thresholds and the block sizes of them are the same, 
the run time of them are so different and it increased obviously 
with the increasing of the item and attribute. 
  
Figure 4.  The effects of different support thresholds. 
When the block size was set to 2000, we tested the 
performance of the algorithm with the different support 
thresholds. As it can be seen from Fig.4, with the decreasing of 
the support threshold value, the more accurate classification 
result we can get. But, at the same time, more classification 
rules will appear in our classifier, it means more run time. 
B. Comparision with different algorithms 
We implemented the CBA algorithm [8] and AC-DS 
algorithm for comparison. In the implementation of the CBA 
algorithm, the minimum confidence was set to 50%. In the 
implementation of the AC-DS algorithm, the confidence 
threshold was set to 50% too, and the data block size was set to 
1000. The support thresholds of the two algorithms were 
shown in the Table.1. 
Column 1: It lists the names of the 6 datasets. 
Column 2: It lists the number of attributes in the dataset. 
Column 3: It lists the number of classes in the dataset 
Column 4: It shows the support threshold used in the 
algorithm. 
Column 5: It shows the classification accuracy of the 
algorithm CBA. 
Column 6: It shows the number of the records which had to 
been read in the memory by CBA. 
Column 7: It shows the classification accuracy of the 
algorithm AC-DS. 
Column 8: It shows the number of records in a data block 
which was read in the memory by AC-DS. 
 We test 6 datasets come from UCI Machine Learning 
Repository. As can be seen from table.1, the mean accuracy of 
the two algorithms is similar, but the memory used by CBA is 
obviously greater than AC-DS. Since CBA is a classification 
algorithm for static dataset and AC-DS is a classification 
algorithm for mining data streams, the more total time cost by 
AC-DS than CBA should be think acceptable as long as the 
two accuracy rates of them are similar and the memory cost by 
AC-DS is in a special limit. 
TABLE I.   COMPARISION WITH DIFFERENT ALGORITHM 
Dataset #attr#class#supp CBA  AC-DS 
#accu#mem#accu#mem
Adult 14 2 10% 83.7% 48842 82.1% 1000 
Adult 14 2 5% 83.9% 48842 81.9% 1000 
Adult 14 2 1% 85.3% 48842 83.7% 1000 
Digit 16 10 10% 78.7% 10992 75.6% 1000 
Digit 16 10 5% 86.0% 10992 83.4% 1000 
Digit 16 10 1% 87.2% 10992 86.0% 1000 
Letter 16 26 10% 59.2% 20000 62.8% 1000 
Letter 16 26 5% 69.0% 20000 67.4% 1000 
Letter 16 26 1% 70.1% 20000 68.6% 1000 
Mushroom 22 2 20% 94.0% 8124 91.4% 1000 
Mushroom 22 2 10% 96.4% 8124 93.2% 1000 
Mushroom 22 2 2% 96.6% 8124 94.6% 1000 
Nursery 8 5 10% 89.9% 12960 90.4% 1000 
Nursery 8 5 5% 91.5% 12960 89.5% 1000 
Nursery 8 5 1% 96.1% 12960 93.8% 1000 
Krkopt,  6 18 10% 89.2% 28056 87.4% 1000 
Krkopt, 6 18 5% 94.2% 28056 91.8% 1000 
Krkopt, 6 18 1% 97.9% 28056 97.1% 1000 
 
IV.  DISCUSSIONS AND CONCLUSION 
 Since the transactions were not processed one by one, we 
always try to read in the available main memory as many 
transactions as possible. So, we always select a large data block 
size to process. When the support threshold is fixed, the more 
the data block size is, the less combinatorial explosion of 
itemsets takes place. 
One important fact we should notice is that our algorithm 
was designed to deal with dataset in which the all data was 
generated by a single concept. If the concept function is not a 
stationary one, in other words, a concept drift takes place in it, 
our algorithm will not output an accurate result. 
This paper described an associative classification approach 
based on association rules for mining data streams. Empirical 
studies show its effectiveness in taking advantage of massive 
numbers of examples. AC-DS’s application to a high-speed 
stream is under way. 
REFERENCES 
 
[1] R. Agrawal, T. Imielinski and A. Swami. “Mining association rules 
between sets of items in large databases”. In Proc. of the ACM 
SIGMOD Conference on Management of Data, Washington, D.C, May 
1993. 
[2] Gurmeet Singh Manku and Rajeev Movtwani, “Approximate Frequency 
Counts over Data Streams”. Proceedings of the 28th VLDB conference, 
Hong Kong, China, 2002. 
[3] Yu J, Chong Z, Lu H et al. “False positive or false negative: ming 
frequent itemsets from high speed transactional data streams. In: 
Nascimento et al.(eds) Proceedings of the thirtieth international 
conference on very large data bases”, Toronto, Canada, September 3-
August 31, 2004, pp 204-215. 
[4] Li H, Lee S, Shan M. “An efficient algorithm for mining frequent 
itemsets over the entire history of data streams”. Proceedings of the first 
international workshop on konwledge discovery in data streams, Pisa, 
Italy, 2004. 
[5] Pedro Domingos and Geoff Hulten. “Mining high-speed data 
streams”,In Proceedings of the sixth ACM SIGKDD international 
conference on knowledg discovery and data ming, pape 71-80, Boston, 
MA, 2000. ACM Press. 
[6] B. Liu, W. Hsu, and Y. Ma. “Integrating classification and association 
rule mining”. In KDD 98, New York, NY, Aug.1998. 
[7] B. Liu, Y. Ma, and C.-K. Wong, “Improving an association rule based 
classifier,” in Proc.4th Eur. Conf. Principles Practice Knowledge 
Discovery Databases(PKDD-2000),2000. 
[8] R. Agrawal and R.Srikant, “Fast algorithms for mining association 
rules”. In Proc. 20th Int. Conf. Very Large Data Bases(VLDB), 1994, 
pp.1-12 
[9] D. J. Newman, S. Hettich, C. Blake, and C. Merz, “UCI Repository of 
Machine Learning Databases”. Berleley, CA: Dept. Information 
Comput. Sci., University of California,1998. 
[10] R. Kohavi, D. Sommerfield, and J. Dougherty, “MLC++: A machine 
learning library in C++,” in Proc.6th Int. Conf. Tools Artificial 
Intelligence, New Orleans, LA, 1994, pp.740–743. 
 
