Data Quality Improvement using Fuzzy Association Rules 
 
Fatemeh Ghorbanpour Alizamini 
Computer Engineering Department, 
Islamic Azad University, Science and Research Branch, 
Tehran, Iran 
f.ghorbanpour@srbiau.ac.ir 
Mir Mohsen Pedram 
Computer Engineering Department, 
Faculty of Engineering, Tarbiat Moallem University, 
Karaj/Tehran, Iran 
pedram@tmu.ac.ir
Mohammad Alishahi 
Industrial Engineering Department, 
Sharif University of Technology,  
Tehran, Iran 
m_alishahi@ie.sharif.edu 
Kambiz Badie 
Information Technology Research Center, 
Telecom Research Center, 
Tehran, Iran 
k_badie@itrc.ac.ir 
 
Abstract— The activities and decisions of organizations and 
companies are based on data and the information obtained 
from data analysis. Data quality plays a crucial role in data 
analysis, because the incorrect data leads to wrong decisions. 
Nowadays, improving the data quality manually is very 
difficult and in many cases is impossible as data quality is one 
of the complicated and non-structured concepts and data 
refinement process can not be done without the help of 
professional domain experts, and detection and correction of 
errors require a thorough knowledge in the related domain of 
the data. Thus, the necessity of using (semi-)automatic 
methods is discussed to find data defects and errors and solve 
them. Because data mining methods are designed to discover 
interesting patterns in datasets, we can use them efficiently to 
improve different dimensions of data quality. In this paper, a 
new method is presented to measure the accuracy dimension 
of data quality using fuzzy association rules. Finally, 
Experimental results of the proposed method show the 
effectiveness of the proposed method to find incorrect values 
in datasets. 
 Keywords-data quality; data mining; fuzzy association rules 
I. INTRODUCTION 
Widespread use of data and decisions based on data 
analysis focuses on data quality in today’s business success. 
Nevertheless, a study by the Meta group reveals that 41% of 
projects based on data analysis will be failed. As one of the 
main reasons, they identified insufficient data quality 
leading to wrong decisions [1]. 
Therefore, traditional methods of cleaning data can be 
used rarely. It is normally infeasible to guarantee data 
quality by manual inspection, especially when data are 
collected over long periods of time and through multiple 
generations of databases. Therefore, (semi-)automatic data 
cleaning methods have to be used [1].  
 
 
 
 
 
 
Since the early 90s, knowledge discovery in databases 
(KDD) has been introduced as a well established field of 
research, and over the years new methods together with 
scalable algorithms have been developed to analyze 
efficiently very large datasets. Unfortunately most of the 
orientation is towards the particular and theoretical problems. 
The application of data mining methods to improve data 
quality is a relatively new and promising approach from 
research and usage viewpoint and can present new domains 
of their application outside the domain of pure data analysis 
[2]. 
Data quality studies have been accomplished using data 
mining methods. For example, Hipp and et al proposed 
developed an algorithm to extract association rules for data 
cleaning in which they ranked deviation from these rules 
based on confidence of violated rules [3]. 
In addition, Brodley and Friedl used data mining in 
preprocessing data in the process of knowledge discovery in 
database. By filtering the probably incorrect training data, 
they can significantly reduce the misclassifications, while 
their goal is to improve final classification accuracies, not 
detection of errors in training data [4]. Marcus and Maletic 
used different methods of data mining which include 
statistical methods, clustering, pattern-based methods and 
ordinal association rules for automatic error detection in real 
dataset. Their experiments showed that ordinal association 
rule is more efficient than the other methods, but it is 
appropriate only for the datasets whose lots of their attribute 
type is decimal or date [5].  
Grüning showed that classifiers can be used in detection 
of conflicts in datasets and gave practical recommendations 
for data correction.  This approach uses support vector 
machines as a classification algorithm [6].  
In this paper fuzzy association rules are used to enhance 
the data quality. The structure of this paper is as follows: 
Section 2 presents an introduction of data quality and 
different dimension of data quality. In Section 3, the data 
mining and its application for improving data quality is 
stated, and in Section 4, the proposed method is explained. 
The experimental results of proposed method have been 
given and analyzed.  
V1-468 Volume 1
2010 International Conference on Electronics and Information Engineering (ICEIE 2010)
C978-1-4244-7681-7/$26.00      2010 IEEE
II. DATA QUALITY 
Definition of data quality depends on the considered 
purpose, so there is no unique definition which can be stated 
formally.  In literature, "appropriate for use" or "to meet end 
user needs" are used.  According to the general definition of 
quality management, we define quality of data to meet 
customer needs. Contrary to popular belief, the quality is not 
necessarily error at the level of zero [7]. Data will have high 
quality if they are appropriate for applications, decision 
making and planning.  
Researches have defined different dimensions for data 
quality. Each dimension shows a particular view of quality. 
The more interesting dimensions are as follow: 
 Accuracy: the distance between value of v  and 
value of 'v  that represents the entity which v  tries 
to portray it. 
 Completeness: the database includes all of the 
related entities. 
 Update: the rate of fast updating of data. 
 Consistency: the rate of violation from the defined 
significant rules on the dataset. 
 Availability: the rate of data availability, being easy 
and the speed of data retrieval. 
 Timeless: over the time some of information looses 
its credit. For example, information about the 
address of person at the time of entrance is 
completely correct and has enough quality but just 
this information would loose its credit over the time. 
 Value added: the rate of being useful. 
Different dimensions of data quality are not independent 
of each other, and there may be a direct or indirect relation 
between them [6]. 
In this paper, the accuracy dimension has been 
considered. 
III. DATA MINING AND ITS APPLICATION IN DATA 
QUALITY IMPROVEMENT 
Data mining is an automatic process to extract the 
patterns as an implicit knowledge in the database. Data 
mining utilizes multiple scientific fields simultaneously, 
such as the technology of data base, artificial intelligence, 
machine learning, neural networks, statistics, pattern 
recognizer, knowledge based systems and information 
retrieval. 
In 2001, Hipp introduced data quality mining as a new 
approach from research and application viewpoint. The goal 
of data quality mining is to employ data mining methods in 
order to detect, quantify, explain and correct data quality 
deficiencies in huge databases.  
Generally, four important aspects are considered in data 
quality mining [3]:  
a) Employment of data mining methods to measure 
and explain the data quality deficiencies 
b) Employment of data mining methods to correct 
deficient  data 
c) Extension of knowledge discovery process models  
to reflect potential of data quality mining 
d) Development of specialized  process models for 
pure data quality mining 
IV. THE PROPOSED METHOD 
Among data mining methods, association rules is a very 
active research topic that has been used widely in lots of 
cases such as basket analysis, decision maker support 
systems, theft detection, etc. Data quality is also one of the 
topics where using association rules can be useful, because: 
 In comparison with other methods of data mining, 
association rules are understandable and can clearly 
discover and describe the dependency between data. 
 Association rules are independent of each other. 
Therefore, recision of simpler rules has no effect on 
the other rules. 
 The applications of association rules have proved 
its usability in data quality. The evaluations done in 
[3, 5] show that association rules can improve data 
quality. 
In [3], a method has been presented which uses 
association rules to discover error of transactions. In this 
method, rules and patterns available in data are initially 
discovered via Apriori algorithm. Then, each of the 
transactions is compared with discovered rules and any 
deviation from rules will be marked as error. In this method 
discovered association rules have a very important role in 
error detection.  
One of the defects of presented method in [3] is that 
association rules can not show properly the associations 
between quantitative values. As an example in Table I, 
occupation and education are categorical attributes and 
income is numerical attribute. If we want to obtain 
association rules in these data with support >1, via Apriori 
algorithm, only we can find some BS degrees, whereas there 
is an association between income and occupation or 
education and income. For example, some one who have a 
high educational level have more income than the people 
who have a mean educational level. Apriori algorithm is not 
capable of discovering such associations between 
quantitative values. As much of data in real world contain 
quantitative values and there may be errors in this type of 
values, the proposed method in [3] cannot be an applicable 
to detect error. In this paper, the presented method in [3] will 
be extended and association rules between quantitative 
values will be discovered with the help of fuzzy sets concept. 
A. Fuzzy Association Rule  
Fuzzy association rules are the rules which are extracted 
from a fuzzy dataset. A fuzzy rule in the form of BA   
where A, B are fuzzy sets and 0 BA , is called fuzzy 
association rule.  
Using of fuzzy concepts has some advantages. First, the 
discovered association rules are more understandable. These 
concepts make a transition between numerical values of data 
and categorical concepts. Second, these concepts help 
discover the rules between quantitative values. For example, 
V1-469 Volume 1
2010 International Conference on Electronics and Information Engineering (ICEIE 2010)
the following fuzzy sets are defined for the data shown in 
Table I: 
High occupational level = {manager, deputy} 
Mean occupational level = {designer} 
High educational grade = {PHD, MS} 
Mean educational grade = {BS, 12th} 
High income = {500-700} 
Mean income = {200-300}   
 
TABLE I.  INSTANCE DATABASE 
OccupationIncome Education Transaction_id 
designer 200 BS 1 
designer 250 BS 2 
designer 300 12th 3 
manager 500 PHD 4 
manager 550 MS 5 
deputy 600 PHD 6 
 
The following fuzzy association rules may be stated by 
the mentioned fuzzy sets: 
High occupational level High educational grade 
Mean occupational level Mean educational grade 
High occupational level High income  
Mean occupational level Mean income  
 
Now assume the following transaction: 
Transaction_id 7: occupation= deputy, education= PHD, 
income =200 
Considering the dataset shown by Table I, an error can 
be detected in the above transaction, as the person who is 
deputy cannot have income of 200, while his income is 
expected to be much higher. This transaction violates "High 
occupation level High income" fuzzy rule. It should be 
noted that the error would not be detected by the rules 
extracted by Apriori algorithm. 
B. proposed framework 
The proposed framework is consisted of steps shown in 
figure 1. 
      
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1.  Proposed framework  
The description of each step is as follows: 
a) Data preprocessing: in this step, data is converted to 
standard format and lost values are managed. For example, 
the values of date attribute in some transactions may be in 
the form of YYYY/MM/DD and in some other in the form 
of YYYY-MM-DD. To obtain a correct model of the 
associations, implementation of this step is necessary. 
 
b) Mapping data to fuzzy values: fuzzy sets and fuzzy 
membership functions for quantitative data are defined in 
accordance with the knowledge of experienced experts. 
Then, data values are mapped to fuzzy values by fuzzy 
membership functions. Table II shows fuzzy mapping for 
income and occupation attributes of Table I.  
 
TABLE II.  MAPPING TABLE I TO FUZZY VALUES 
Mean 
income
High 
income 
Mean 
occupational 
level 
High 
occupational 
level 
Fuzzytrans
_id 
1 0 1 0 1 
0.98 0.01 1 0 2 
0.90 0.02 1 0 3 
0.08 0.85 0.03 0.95 4 
0.04 0.90 0.01 0.95 5 
0.03 0.93 0 1 6 
 
 
c) Fuzzy association rules extraction: Fuzzy 
association rules are extracted, via the presented algorithm 
in [9], in which all attributes are weighted equally. In this 
step, the rules with high confidence level are considered. 
Thus, rule set R is limited to 
 

	
 )(| rconfidenceRrR  where   is the 
minimum confidence. 
Table III shows some extracted rules from Table II. 
 
TABLE III.  FUZZY ASSOCIATION RULES EXTRACTED FROM TABLE II 
ConfidenceFuzzy association rule 
0.52 High occupational level High income  
0.41  Mean occupational level Mean income 
 
 
d) Consistency check of transactions with discovered 
rules: the consistency of transactions with discovered rules 
is checked in this step. Each of transactions may violate 
some rules and may be consistent with other rules. Also, it 
may not fire some of the rules. Let R be the set of fuzzy 
association rules and Let D be the database of fuzzy 
transactions. Let YXr   be a Fuzzy association rule 
and X be membership function of X.  the mapping that 
determines whether a fuzzy transaction DT 
 violates a 
rule Rr 
 , is defined as:  
 



	

else
satisfyruleTsatisfyruleTifT
RDViolate
YXY
0
_)(_)()(1
]1,0[:

 
 
 
rule_satisfy parameter is the threshold for fuzzy rule 
satisfaction.  
Database
Data preprocessing 
Mapping data to 
fuzzy values
Association 
rules extraction 
Consistency check of 
transactions with 
discovered rules 
Scoring and ranking 
the transactions
V1-470 Volume 1
2010 International Conference on Electronics and Information Engineering (ICEIE 2010)
In (1), )(1 TY  shows the degree that fuzzy transaction T 
violates r, the more )(1 TY , the more inconsistency of 
transaction T with rule r. 
As an example, the consistency check for transaction 6 is 
shown in Table IV. 
 
TABLE IV.   CONSISTENCY CHECK FOR TRANSACTIONS 6 WITH RULES 
IN TABLE III     
Inconsistent 
rule  
Consistent 
rule  
Non-correlated 
rule 
Fuzzytrans_id
_ 
High 
occupational 
level High 
income  
Mean 
occupational 
level Mean 
income 
6 
 
 
e) Scoring and ranking the transactions: In order to 
assign scores to the transactions, the highest confidence of 
violated rules will be assigned to the corresponding 
transaction. Then the transactions are sorted in descending 
order. It is well-worth mentioning that the highest 
confidence of violated rules is not only simple and 
acceptable measure, but also eliminates extra calculations. 
Because, if a rule is violated by a transaction, there will be 
no need to check rules having confidence less than 
violated rule. 
V. EVALUATION RESULTS 
 In this section, proposed method is evaluated and the 
results are compared with presented method in [3]. All the 
evaluations have been implemented on a system equipped 
with Core 2 Duo 2.26 GHz CPU, 3GB RAM and Win 7. 
Usually for evaluating the performance of information 
retrieval and classification algorithms, precision and recall 
measures are used. These measures are defined by (2) and (3) 
based on the confusion matrix entries, shown in table V. In 
this paper, in addition to these two measures, the runtime of 
the two methods has been also compared. 
 
 
TABLE V.  CONFUSION  MATRIX 
Transactions 
classification by proposed 
method 
 
Incorrect 
transactions 
Correct 
transactions 
FN  TP Correct transactions  Real 
Classification TN  FP  Incorrect transactions  
 
    Precision =
FPTP
TP

                                         (2) 
    Recall = 
FNTP
TP

                                            (3) 
 
In order to evaluate the proposed method, Adult dataset 
from UCI has been used. This dataset includes two 
numerical attributes called income and hpw and two 
categorical attributes called occupation and education. 1000 
instances of the transactions of this dataset have been 
selected and manipulated to make imitation errors. Then, 
proposed method and the presented one in [3] are applied to 
the manipulated dataset. In the tests related to precision and 
recall, min_support, min_confidence and min_rule_satisfy 
are considered to be 0.2, 0.7 and 0.4, respectively.  
 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 20 40 60 80 100
Number of Error IN Dataset
Pr
ec
is
io
n
Fuzzy Association Rule Apriori
 
Figure 2.  The precision measure for the proposed method and the method 
described in [3] 
 
Figure 2 compares the precision of the proposed method 
with the method proposed in [3], in terms of number of 
created errors in dataset. The figure shows that the proposed 
method outperforms the method proposed in [3], as lots of 
the association between quantitative data in dataset are not 
discovered by Apriori algorithm and play no role in 
transactions’ ranking. Some attributes’ values are in a range 
which causes rules’ support would not be satisfied (above 
example). Precision curves have wavy shapes, because any 
change in data causes a change in the support of rules which 
can affect their detections. Figure 5 also compares the recall 
measure for the two methods. It is clear that the proposed 
method delivers better results. 
In the evaluation related to runtime of the two methods, 
min_support parameter has been changed to 0.05 in order to 
discover more rules by both algorithms. According to figure 
4, runtime for the proposed method is better. The main 
reason is due to the difference in runtime of association 
rules discovery algorithm and ranking the transactions. 
Fuzzy association rules algorithm is much faster than 
Apriori algorithm, because Apriori algorithm must check all 
of the numerical and categorical data in the dataset to find 
frequent itemsets. Also, it should find the support of each of 
itemsets which is time consuming due to variety of numeric 
values. While fuzzy association rules algorithm only works 
with the fuzzy sets whose numbers are more limited.  The 
ranking of transactions, based on highest confidence of 
violated rules, needs fewer computations, because when a 
V1-471 Volume 1
2010 International Conference on Electronics and Information Engineering (ICEIE 2010)
transaction violates a rule, there would be no need to check 
rules having confidence less than that of violated rule. 
 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 20 40 60 80 100
Number of Error In Dataset
R
ec
al
l
Fuzzy Association Rule Apriori
 
Figure 3.  The recall measure for the proposed method and the method 
described in [3] 
 
0
40
80
120
160
200
240
0 100 200 300 400 500 600 700 800 900 1000
Number of Transactions
R
un
Ti
m
e(
se
co
nd
)
Fuzzy Association Rule Apriori 
Figure 4.  The runtime for the proposed method and the mothod presented 
in [3] 
 
VI. CONCLUSION 
Automated data quality improvement is a necessity for 
finding incorrect data in large databases. In this paper, a 
method based on data mining approaches is presented to 
improve data quality. This paper’s proposed approach for 
data quality improvement uses fuzzy association rules, by 
which hidden rules in datasets will be discovered. Then, the 
consistency of transactions with these rules is checked and a 
score is assigned to each transaction. This score generally 
shows inconsistency of a transaction with the rules set. In 
the proposed method, high scores are assigned to the 
transactions which are suspicious to have defects. User’s 
knowledge on suspicious transactions and background 
knowledge about data will be used in the process of 
determining the accuracy of transactions and ultimately total 
quality of dataset. Evaluation results show the effectiveness 
of the proposed method.  
 
REFERENCES 
[1] D. Luebbers, U. Grimmer, M. Jarke, “Systematic Development of 
Data Mining-Based Data Quality Tools”, Proc. of the 29-th 
International Conference on Very Large Data Bases, Berlin, 
Germany, pp. 548-559, 2003. 
[2] J. Hipp, M. Müller, J. Hohendorff, F. Naumann, “Rule-Based 
Measurement of Data Quality in Nominal Data”, Proc. of the 12th 
International Conference on Information Quality (ICIQ), Cambridge, 
USA, 2007. 
[3] J. Hipp, U. Güntzer, U. Grimmer, “Data Quality Mining – Making a 
Virtue of Necessity”, Proc. of the 6-th ACM SIGMOD Workshop 
on Research Issues in Data Mining and Knowledge Discovery, 
Santa Barbara, California, pp. 52-57, 2001. 
[4] C. Brodley, M. Friedl, “Identifying Mislabeled Training Data”, 
Journal of Artificial Intelligence Research. Vol. 11, pp. 113-167, 
1999. 
[5] J. Maletic, A. Marcus, “Data cleansing: Beyond integrity analysis”, 
Proc. of the Conference on Information Quality, MIT, Boston, pp. 
200-209, 2000. 
[6] F. Grüning, “Data Quality Mining: Employing Classifiers for 
Assuring consistent Datasets”, Proc. of the 21-th Information 
Technologies in Environmental Engineering, Springer-Verlag 
Berlin Heidelberg, pp. 58-94, 2007. 
[7] J. Geiger, “Data Quality Management: The Most Critical Initiative 
You Can Implement”, Intelligent Solutions, Inc., Boulder, Paper 
098-29, 2004. 
[8] J. Maletic, A. Marcus, K. Lin, “Ordinal Association Rules for Error 
Identification in DataSets”, Proc. of 10-th Intl. conf. Information 
and Knowledge Management(CIKM), Atlanta, GA, pp. 589-591, 
2001 
[9] D. Olson, Y. Li, “Mining Fuzzy Wighted Association Rules”, Proc. 
of the 40th IEEE Intl. Conference on System Sciences, Hawaii, 
2007. 
V1-472 Volume 1
2010 International Conference on Electronics and Information Engineering (ICEIE 2010)
