A New Association Rules Mining Algorithm Based on Vector 
 
Xin Zhang, Pin Liao 
College of Science and Technology 
Nanchang University 
Nanchang, Jiangxi, China 
zhangxin77@gmail.com 
Huiyong Wang 
School of Mathematics and Computation Science 
Guilin University of Electronic Technology 
Guilin, Guangxi, China 
why608@guet.edu.cn
 
 
Abstract—As a classical algorithm of association rules mining, 
Apriori algorithm has two bottlenecks: the large number of 
candidate itemsets and the poor efficiency of counting support. 
A new association rules mining algorithm based on vector is 
proposed, which can reduce the number of candidate frequent 
itemsets, improve efficiency of pruning operation and count 
support quickly using vector inner product operation and 
vector addition operation between transaction vector and 
itemset vector. According to the results of the experiments, the 
proposed algorithm can quickly discover frequent itemsets and 
is more efficient than Apriori algorithm. 
Keywords-data mining; association rules; vector 
I.  INTRODUCTION 
Association rules mining is an active research topic in the 
data mining field, which is the key step in the knowledge 
discovery process [1]. Association rules mining may 
discover interesting associations or dependency relations 
between item sets among mass data, helping people make 
better decisions. 
Mining association rules have two problems: frequent 
itemsets discovery and association rules generation. Since 
the overall performance of mining is greatly determined by 
the first problem, this paper focuses on the process of 
discovering frequent itemsets. 
Apriori algorithm [1], as a classical algorithm of 
association rules mining, adopts an iterative method to 
discover frequent itemsets. It consists of two steps: the join 
step and the prune step. In the join step, a candidate k-
itemsets is generated by joining two frequent (k-1)-itemsets. 
Then in the prune step, all itemsets whose (k-1)-subset is not 
a frequent k-itemsets are removed from the candidate k-
itemsets. Then, the database is scanned to compute the 
support of the candidate k-itemsets. This process is repeated 
until no new candidate k-itemsets is generated. A huge 
calculation and a complicated transaction process are 
required during the algorithm. Therefore, the mining 
efficiency of Apriori algorithm will be poor if the transaction 
database is large. 
Recently, many methods have been proposed to improve 
Apriori algorithm’s efficiency. AprioriTid [2] reduces the 
overhead of I/O by scanning the database only once in the 
first iteration. DHP [3], a hash-based algorithm, is especially 
effective for the generation of frequent 2-itemsets. Some 
algorithms avoid generating candidate itemsets, such as DLG 
[4] and EDM [5]. Max-Miner algorithm [10] efficiently 
identifies long frequent itemsets, which, in turn, can be used 
to generate other frequent itemsets. 
In order to avoid the deficiency of Apriori algorithm, a 
new algorithm is proposed in this paper. The transaction 
database is scanned only once and every transaction is 
transformed into vector at the same time. Each itemset is 
expressed as a vector. Lastly, with vector inner product 
operation and vector addition operation between the 
transaction vectors and the itemset vectors, the new 
algorithm reduces the number of candidate frequent itemsets, 
improves the efficiency of counting support and generates 
frequent itemsets quickly. 
II. BASIC CONCEPTION AND PROPERTIES 
Let I = {i1,i2,…,im} be a universal set of items. Let 
D={T1,T2,…,Tn} be a set of transactions. Each transaction T 
is a subset of I, viz. T ? I. All items in transaction and 
itemset are arranged in dictionary order. 
Definition 1: Transaction Vector (TV). 
A transaction is expressed as a m-dimensional vector. 
Transaction vector of transaction T is expressed as TV= 
(x1,x2,…,xm), xk?[0,1], (k=1,2,…,m). If ik?T, and then 
xk=1, otherwise xk=0, and the dimension of TV is equal to the 
number of items in I.  
Example 1: Let I = {A,B,C,D,E} be a universal set. If a 
transaction is expressed as Ti={A,B,E}, then TVi=(1,1,0,0,1). 
Definition 2: k-Itemset Vector (IVk). 
A k-itemset is expressed as a n-dimensional vector, k-
itemset vector of k-itemset c is expressed as IVk = 
(y1,y2,…,ym), yk?[0,1], (k=1,2,…,m). If ik?c, and then yk=1, 
otherwise yk=0. The dimension of IVk is equal to the number 
of items in I. The value of k is equal to the number of digital 
“1” in IVk. 
Example 2: Let I={A,B,C,D,E} be a universal set.  If a 2-
itemset is expressed as c={A,B}, then IV2=(1,1,0,0,0). 
Definition 3: Cumulative k-Itemset Vector ( kCIV ). 
C is a set of k-itemsets. In C, the total number of all the k-
itemsets having a equal last item as ij is rj. The k-itemset 
vectors of these k-itemsets are expressed as s
k
ii jxIV )( ,... , 
1 ? s ? rj. And the cumulative k-itemset vector of all k-
itemsets ended with item ij in C is expressed as: 
2009 Third International Conference on Genetic and Evolutionary Computing
978-0-7695-3899-0/09 $29.00 © 2009 IEEE
DOI 10.1109/WGEC.2009.64
429
?
=
=
j
jxj
r
s
s
k
ii
k
i IVCIV
1
,... )( .                          (1) 
Example 3: Let I={A,B,C,D,E} be a set of items. If a 
frequent 3-itemsets is expressed as L3 = { {A,B,C}, {A,B,E}, 
{B,C,E} }, then 3CCIV  = (1,1,1,0,0),  
3
ECIV  = (1,2,1,0,2). 
Property 1: All nonempty subsets of a frequent itemset 
must also be frequent [1]. 
Property 2: Let X be a k-itemset. If X has a (k-1)-subset 
which is not a frequent (k-1)-itemset, then X is not a frequent 
k-itemset [1]. 
Inference 1: Let Lk-1 be a frequent k-itemsets, let l = 
{ip1,ip2,…,ip(k-1)} be a frequent (k-1)-itemset, and l?Lk-1, let iq 
be a item contained in Lk-1, and q > p(k-1). A k-itemset X, 
which is generated by joining l with iq, is expressed as 
X={ip1,ip2,…,ip(k-1),iq}. A vector S is expressed as: 
11 ?? += ki
k
l qCIVIVS .                             (2) 
Suppose there exists a number r such that for each 
r?(p1,p2,…,p(k-1),q) we have S[r]<k-1, then X is not a 
frequent k-itemset. S[r] presents the value of the rth element 
in S. 
Proof: Suppose X is a frequent k-itemset, then all (k-1)-
subsets of X are contained in Lk-1. According to definition 2, 
each value of the p1th, p2th, … , p(k-1)th element in 1?klIV  is 
equal to 1.  
There may be some itemsets ended with item iq in Lk-1, 
which is not (k-1)-subset of X. Then according to definition 
3, the value of the p1th, p2th, … , p(k-1)th element in 1?kiqCIV  
is at least equal to k-2, and the value of the qth element is at 
least equal to k-1. Therefore for each number r ?
(p1,p2,…,p(k-1),q), there is  S[r] ? k-1, which contradicts with 
the conditions. So X is not a frequent k-itemset. 
III. A NEW ALGORITHM BASED ON VECTOR 
The proposed algorithm is similar to Apriori algorithm, 
which can be divided into two steps. First, the algorithm 
finds out all frequent itemsets. Then it generates all 
association rules from frequent itemsets. Obviously, the 
second step is easier than first step, so we only describe the 
algorithm of discovering frequent itemsets. 
At first, we generate all transaction vector which only 
entries either 0 or 1 by scanning the database once. Every 
transaction T is expressed as transaction vector TVr, 1 ? r ? n. 
At the same time, we count the support number of every item 
ij in T, which is expressed as sup({ij}). If the support number 
of {ij} is beyond the user specified threshold 
MinSupNumber, viz. sup({ij}) ? MinSupNumber, then {ij}?
L1. 
Secondly, the set of candidate 2-itemsets C2 is generated 
by joining L1 with itself. Each 2-itemset in C2 has the form 
{ip,iq}, p<q. Then each {ip,iq} in C2 is expressed as a 2-
itemset vector 2, qp iiIV , and the support number of the set 
{ip,iq} is 
)](
2
1[int}),sup({ 2,
1
qp iir
n
r
qp IVTVii •=?
=
,            (3) 
where “• ” is the inner product operator, int[ ] is the round 
function that transforms a real number to an integer by 
truncating the fractional part. For example, int[1.2] = 1, and 
int[0.7] = 0. If sup({ip,iq}) ?  MinSupNumber,  then {ip,iq}?
L2. 
After the frequent 2-itemsets L2 is obtained, the frequent 
k-itemsets Lk-1 can be iteratively used to generate candidate 
k-itemests Ck. According to the sort rule, a k-itemset 
{ip,…,iq,ij} can be generated by joining a arbitrary (k-1)-
itemset {ip,…,iq} in Lk-1 with a item ij in Lk-1 which is bigger 
than iq. 
Then, we generate cumulative (k-1)-itemset vectors of (k-
1)-itemests in Lk-1, which is 1?ki jCIV , k ? j ? m. Then with 
plusing every 1?ki jCIV  (j=q+1,q+2,…,m) with the 
1
,,
?k
ii qpIV "  of 
every frequent (k-1)-intemset {ip,…,iq} in Lk-1, we can get 
S= 11,,
?? + ki
k
ii jqp SVIV " . Based on vector S and Inference 1, it 
can be determined {ip,…,iq,ij}?Ck or {ip,…,iq,ij}?Ck. 
At last, we compute the support number of each 
candidate k-itemset {ip1,ip2,…,ipk} in Ck according to the 
following formula: 
?
=
•=
n
r
k
iiirpkpp pkppIVTVk
iii
1
,,,21 )](
1int[})...,sup({
21 " .    (4) 
If the support number is beyond the user specified threshold 
MinSupNumber, then {ip1,ip2,…,ipk}?Lk. 
Repeat the above process with successively increasing 
number k until either Ck or Lk is empty. At the end of 
procedure, we can get the all frequent itemsets. 
A detailed description of the proposed algorithm is given 
in Figure 1. 
IV. EXPERIMENTAL RESULTS 
To assess the performance of the new algorithm, we 
performed several simulation experiments. We compare the 
performance of the new algorithm with the Apriori 
algorithm. The new algorithm and the Apriori algorithm is 
programmed in Matlab, all experiments are performed on a 
PC with 2.5GHz Intel Dual-Core E5200, 2GB memory. We 
use synthetic data to evaluate two algorithms, which is 
generated by the QUEST data generator of IBM Almaden 
Lab. The way of generating synthetic data is similar to that 
of [2]. The parameters used in our experiments are defined 
as: N is the number of items; |D| is the number of 
transactions; |T| is the average size of transactions; |I| is the 
average size of the maximal frequent itemsets. We generated 
430
four datasets as: T10I2, T10I4, T20I2, T20I4, by setting 
N=500 and |D|=1000. 
Input: a transaction database D, the minimum threshold 
of support minSupNum 
Output: the set of frequent itemsets L 
1)  for all transaction t?D do 
2)      generate TV;  
3)  L1 = (frequent 1-itemsets); 
4)  C2 = L1 ? L1; 
5)  L2 = {c?C2 | sup(c) ? MinSupNum};  // sup(c) is the 
 result of the formula (4) 
6)  for ( k=3; Lk-1? ø; k++ ) do begin 
7)      for( j=k; j ? m; j++ ) do  
8)          generate 1?ki jCIV ;  
             Ck = candidate_gen(Lk-1);  
9)          Lk = {c?Ck | sup(c) ? MinSupNum}; 
10) end 
11) Return L = ?Lk; 
candidate_gen( frequent itemsets Lk-1 ) 
1)  for all (k-1)-itemset l?Lk-1 do 
2)      for all ij?Lk-1 do 
3)          // S is the result of the formula (2) 
         if for every r(1 ? r ? k) such that S[r] ? k-1 then 
4)              add l?{ij} to Ck; 
Figure 1. The proposed algorithm. 
The following four figures show the comparison of the 
execution time of the Apriori algorithm and the proposed 
algorithm, along with the minimum support increasing.  
The results illustrate that the execution time of two 
algorithms decrease along with the minimum support 
increases, but the performance of the new algorithm is much 
better than that of the Apriori. Moreover, the smaller the 
minimum support is, the more efficient the performance of 
the new algorithm is. This is because that as minimum 
support reduces, Apriori produces more candidate itemsets 
and spends more time making pruning operation and 
counting support operation. However, by inner product 
operation and addition operation, the new algorithm can 
reduce candidate itemsets and simplify pruning operation 
and counting support operation. 
 
Figure 2. The execution time in T10I2D1K. 
 
Figure 3. The execution time in T10I4D1K. 
 
Figure 4. The execution time in T20I2D1K. 
 
Figure 5. The execution time in T20I4D1K. 
V. CONCLUSION 
In this paper, a new association rules mining algorithm 
based on vector has been presented in order to easily 
generate candidate frequent itemsets and quickly compute 
support of itemsets. The new algorithm only scans the 
database once, stores all transaction data into transaction 
vectors. And, for a more convenient calculation, express 
431
itemsets as the form of vector. In the joint step, candidate 
frequent k-itemsets can be generated by enlarging the last 
item of frequent (k-1)-itemstes only. This step can 
significantly reduce the times of comparison among items. In 
the prune step, the k times comparison of numerical value is 
much more efficient than the circular comparison in the 
Apriori algorithm. Then, inner product operations between 
transaction vector and itemset vector are performed to count 
the support of items. Therefore, frequent itemsets is 
generated quickly. The experiment results indicate that the 
proposed algorithm is much more efficient than Apriori 
algorithm for mining association rules. 
 
REFERENCES 
 
[1] R. Agrawal, T. Imielinski, and A. Swami, “Mining association rules 
between sets of items in large databases,” Proceedings of the ACM 
SIGMOD International Conference on Management of Data, 
Washington DC, pp. 207-216, May 1993. 
[2] R. Agrawal and R. Srikant, “Fast algorithms for mining association 
rules in large databases,” Proceedings of the 20th International 
Conference on Very Large Data Bases, September 1994. 
[3] J. S. park, M.-S. Chen, and P. S. Yu, “An effective hash based 
algorithm for mining association rules,” ACM SIGMOD International 
Conference on Management of Data, pp. 175-186, May 1995. 
[4] S.-J. Yen and A. Chen, “An efficient approach to discovering 
knowledge from large databases,” Proceedings of the International 
Conference on Parallel and Distributed Information Systems, pp. 8-
18, 1996. 
[5] S.-J. Yen and A. Chen, “An efficient data mining technique for 
discovering interesting association rules,” Proceedings of the 
International Conference and Workshop on Database and Expert 
System Applications, pp. 664-669, 1997. 
[6] S.M. Pauray and Chien-Ming Chen, “Mining interesting association 
rules from customer databases and transaction databases,” 
Information Systems, vol. 29(3), pp. 685-696, 2004. 
[7] F. Berzal, J.C. Cubero, N. Marrin and J.M. Serrano, “TBAR: An 
efficient method for association rules mining in relational databases,” 
Data and Knowledge Engineering, vol. 37, pp. 47-64, 2001. 
[8] D. Holt John and Soon M. Chung, “Mining association rules using 
inverted hashing and pruning,” Information Processing Letters, vol. 
83, pp. 211-220, 2002. 
[9] D. K. Sotiris Kotsiantis, “Association Rules Mining: A Recent 
Overview,” GESTS International Transactions on Computer Science 
and Engineering, vol. 32(1), pp. 71-82, 2006. 
[10] B. J. Roberto, “Efficiently mining long patterns from databases,” 
ACM SIGMOD International Conference on Management of Data, 
pp. 85-93, 1998. 
 
 
 
432
