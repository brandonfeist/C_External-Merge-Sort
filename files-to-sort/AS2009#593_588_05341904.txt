2009 2nd Conference on Data Mining and Optimization  
27-28 October 2009, Selangor, Malaysia 
 
 
978-1-4244-4944-6/09/$25.00 ©2009 IEEE   84 
 
 
Capturing Uncertainty in 
Associative Classification Model 
 
Yun-Huoy Choo1, Azuraliza Abu Bakar2, Azah Kamilah Muda1 
1Faculty of Information and Communication Technology, 
Universiti Teknikal Malaysia Melaka (UTeM). 
2Faculty of Information Science and Technology,  Universiti Kebangsaan Malaysia, 
43600 Bangi, Selangor, Malaysia 
 
 
Abstract—This paper aims to propose a weighted linguistic 
associative classification model for uncertainty data analysis 
using rough membership function. Transformation of 
quantitative association rules into linguistic representation can 
be achieved in discretizing the numerical interval into rough 
interval described with respective rough membership values. 
Transformation of linguistic information system is suggested 
prior to the frequent pattern discovery. Neither pruning of 
association rules nor classifier modelling is needed. The rough 
membership values of the each linguistic frequent item are 
composited to form the weighted associative classification rule. 
Simulated results on Iris Plant dataset were shown in the paper. 
The future work of the research will focus on implementing the 
model with more experimental dataset. 
 
Keywords— associative classification, rough set theory, rough 
membership function, uncertainty. 
 
I. INTRODUCTION 
Human tend to employ natural language when describing a 
certain situation when communicating with others, while the 
same situation is not easily explainable when using 
numerical values. Quantitative rules that convey in numbers 
are sometimes less practical in conversation. Thus, 
describing the interval data in natural language has lead to 
the emergence of the usage of linguistic terms in association 
rule which are later known as linguistic association rule. 
Linguistic association rules mining is derived from the 
original concept of association rules mining [1]. The term  
“Linguistic association rule” was coined in order to replace 
the numerical values with linguistic terms in quantitative 
association rules mining. With the introduction of 
associative classification task in [2], linguistic association 
rule mining can be further explored and transformed into 
associative classification rule mining. Besides fuzzy set 
theory, rough sets suggested by Zdzislaw Pawlak is another 
technique in soft computing that is able to deal with 
ambiguous boundary data sets [3]. Rough sets theory uses 
the upper and lower approximation as well as the rough 
membership function to determine the items in a set which 
can be obtained through self discovery in learning data. They 
are similar but different from the uncertainty handling 
proposed by fuzzy theory.  Hence, it has gained the interest 
of researchers in the development of rough sets techniques as 
a complement to fuzzy techniques.  
Currently, most researchers focus only on the concept of 
approximation. Hence, this paper attempts to investigate the 
opportunity and strength of rough sets theory in linguistic 
association rules mining from the perspective of rough 
membership function and embed the proposed model into 
associative classification task. The expected output of the 
model is the weighted associative classification model for 
uncertainty data analysis. The organization of the paper 
starts with the brief explanation on the theoretical concepts 
of associative classification in section II-A and section II-B, 
capturing uncertainty in linguistic association rule in section 
II-C, the related rough set concepts in section III, the 
proposed associative classification model and the detailed 
discussion on every phases in section IV before discussion 
on the simulation results in section V and lastly the 
conclusion and future work of this research in section VI. 
II. ASSOCIATIVE CLASSIFICATION 
The task of associative classification (AC) was born from 
combination of two frequently used data mining tasks, i.e. 
the association rules discovery and the classification task. Its 
main purpose is to model the classifier for prediction akin to 
the goal of classification [4]. AC has been initiated to 
complement whatever inadequacies in both the association 
rules mining and classification tasks [5-8]. The task of AC 
starts from the frequent itemsets discovery, association rules 
generation, pruning and rules selection to the regular 
classification with AC rules.  
A. Association Rules Mining 
Association rule mining was first proposed by Agrawal, 
Imielinski, and Swami [9]. It is a technique to extract 
patterns from huge data sets to obtain useful information 
generally from the transaction data. Earliest example of 
association rule mining is presented by the market basket 
analysis, which forms the foundation of today’s association 
rule mining. Later, the emergence of the categorical and 
quantitative association rules mining has widened the 
 978-1-4244-4944-6/09/$25.00 ©2009 IEEE   85 
 
discussion of association rules mining. The conventional 
quantitative association rules mining with A priori algorithm  
proposed by Agrawal and Srikant [10]. It has become the 
foundation of many new techniques in the later work of 
association rules mining. The A priori algorithm consists of 
two major tasks, i.e. the frequent pattern discovery to scan 
for frequent occurring itemsets that satisfied the predefined 
support and confidence thresholds and the association rules 
generation.  
B. Classification 
Classification is one of the most important tasks in data 
mining. The classification task models a classifier from the 
data records, with the intention of categorizing objects into 
different decision classes as precisely as possible according 
to the values of each attributes in the dataset. Classification 
is commonly carried out in two phases, i.e. the training and 
the testing. The training phase aims to generate classification 
patterns where each of the rules links a decision class while 
the testing phase is to verify the generated classification 
rules on the unseen data record. The higher the accuracy of 
generated classification rules set on the test data, the better 
the classifier is.  
C. Capturing Uncertainty with Linguistic Association 
Mining 
Linguistic association mining describes data attributes in 
a human understandable language. It conveys the message in 
linguistic terms instead of numerical values in quantitative 
attributes. Linguistic association mining task is generally 
being divided into two phases. The first phase is to discretize 
the continuous numeric data and represent those intervals 
with a suitable linguistic term [11] before proceeding to the 
second phase of frequent pattern mining. Partitioning the 
numeric values into interval sets will cause crisp boundary 
problems. This may create some contradiction and 
overlapping when these intervals are being presented in 
linguistics term. Hence, there is a need for a soft boundary 
instead of crisp boundary. Since the partitions are divided by 
linguistic terms, it is not easy to differentiate the boundary of 
the set [12]. There are several techniques in mining 
quantitative and categorical attributes in association rules. 
Some of the attributes may not be suitable to be described by 
intervals. To make them more recognizable to humans, it is 
advisable to use the natural language terms. However, this 
introduces some other problems. The linguistic terms used 
may not be sufficient to embody the actual distribution of 
data [12]. Problems happen due to the difficulty of 
distinguishing the exact boundary of a particular interval set. 
Thus, concept of uncertainty is used to soften the boundary 
of interval sets in data analysis [13]. 
Data discretization has arisen since the earlier work on 
quantitative association rule mining, while the linguistic 
terms representation is always being related to Fuzzy sets 
theory. Soft partitioning technique and some soft computing 
related techniques were introduced to create vague interval 
sets to handle this problem. Although typical analysis such 
as binning [14, 15], statistical [16] and evolutionary 
techniques [17] have contributed to a certain extent in 
dealing with quantitative attribute partitioning, it seems that 
soft computing methods especially involving fuzzy theory or 
its hybrids techniques with A priori algorithm and genetic 
algorithm are more promising techniques in managing soft 
partition problems. Fuzzy algorithm has been improved from 
time to time to more efficiently and accurately mine the 
linguistic terms used in association rules mining. Since then, 
fuzzy sets theory has become the more prominent technique 
in dealing with linguistic representation on soft boundary 
interval. 
On the other hand, rough sets theory introduced by Pawlak 
also deals with vagueness and uncertainty. With embedded 
foundation of classical theory, rough sets theory has its own 
perspective towards vagueness which is similar to fuzzy set 
theory [18]. According to Pawlak, it is a unique 
implementation of Frege’s idea of vagueness. The rough 
membership function is a special case of fuzzy membership 
function [19]. Hence, it is expected that rough sets theory 
can be used to describe linguistic knowledge as an 
alternative to fuzzy set theory in a different view of 
vagueness and uncertainty. 
III. ROUGH SET THEORY 
Rough sets theory was introduced by Pawlak [5] to deal 
with vagueness and uncertainty. According to [20] an 
information system can be presented as a pair >=< AUS , , 
or a function VAUf ?×: , where U is the nonempty 
universe object set, A an nonempty attribute set, and V a  
value set of attribute a such that V aUa ?:  for 
every Aa ? . A decision system is any information system of 
the form >?=< }{, dAUS , where Ad ?  is the decision 
attribute. The elements of A are called conditional attributes. 
The universe, U can be partitioned into disjoint subsets 
known as equivalence classes which are discernible among 
one another. All the elements in one particular subset are 
considered equal or indiscernible based on the selected 
attribute subset. This gives the definition of indiscernibility. 
Let A=(U, A) be an information system and each subset of 
attributes AB ?  defines an equivalence relation, called an 
indiscernibility relation as in (1): 
 
))}()((,|2),{()( jxaixaBaUjxixBIND =???=?     
(1) 
 
The core of rough sets approach is the idea of 
indiscernibility between objects. Thus, the equality operator 
is needed to define every attribute domain. This makes the 
method suitable to apply on any problem with discrete or 
categorical values.  Therefore, continuous or real values have 
to be combined or discretised in the pre-processing step in 
order to take on a suitable level of granularity. Discretisation 
 978-1-4244-4944-6/09/$25.00 ©2009 IEEE   86 
 
on quantitative values will reduce and optimize the 
determining input before the process of rules generation. 
Rough membership function is another uncertainty 
representation in rough sets theory. Let AB ?  a set of 
attributes, the rough membership function ]1,0[: ?UBXµ  is 
defined as in (2): 
        
|][|
|][|
)(
x B
Xx B
x
B
X
?
=µ            
(2) 
Equation (2) is an unbiased estimation of conditional 
probability ),|Pr( BxXx ? , the probability of object x 
belongs to set X in terms of B. Similar to fuzzy membership 
value, the operation of union and intersection of two RMVs 
[19] is defined in (3) and (4) as follows: 
                            (3) 
                            (4) 
Reference [21] shows the further details in the notation of 
rough membership function. Rough membership derived 
from accuracy of the rules will be used as the rough accuracy 
value to generate membership reference in transforming 
numerical values into linguistic variable. Instead of getting 
the membership value from the expert, our method is able to 
auto generate membership references through learning from 
data. Similar to fuzzy membership values, the generated 
rough membership reference will be applied in transforming 
the instances value into rough approximation region in 
linguistic rules mining. 
 According to [20], default rules generation framework 
proposed by Mollestad to discover more general rules to 
capture useful knowledge in incomplete dataset. The rules 
generated have two advantages compared to the 
deterministic rules. First, they are usually simpler in 
structure because they cover larger classes defined over a 
few condition attributes. Secondly, they are in many cases 
proven to be better when handling unseen cases. The default 
rules are generated through creating the indeterminacy in a 
decision system. The generation of indeterminacy is done 
through selection of projections over the condition attributes 
allowing certain attributes to be excluded from 
consideration. The removal of the information in terms of 
condition attributes from a given decision system allows the 
generation of rules from the resulting decision tables. The 
underlying idea is to search for default rules by removing 
reducts from the original system. The set of full reducts 
computed from the decision system determines the set of 
attributes to be retained in the projection. Refer to [20] for 
further information on default generation framework. 
IV. THE PROPOSED MODEL 
The proposed associative classification model employs the 
similar architecture to common AC technique. A new phase 
of generating rough membership value is embedded prior to 
the two major steps of original associative classification 
model to capture and define uncertainty in dicretized 
attribute. Some modifications have been made to the AC 
phases to facilitate rough associative classification rules 
generation. Fig. 1 shows the proposed associative 
classification model. The proposed rough associative 
classifier model comprises of three major phases serving 
different functionalities as below: 
A. Generating Rough Membership Value (RMV) 
This phase serves the purpose of inducing rough 
intervals with the corresponding rough membership values 
for linguistic representation and transforms the linguistic 
decision system. It is an automated membership generation 
process similar to the automated fuzzy membership function 
in fuzzy system. Once the rough intervals had been 
determined from the training data, they can be used for 
association rules mining process in any relevant testing data. 
Basically, there were two major steps involved in the rough 
variables generation, i.e. the discretisation and the rough 
intervals induction.  
The Boolean reasoning suggested as the discretisation 
method in rough classification is implemented as a necessary 
step in rough interval generation [22]. Rough intervals 
induction was the second step in generating rough variables. 
The purpose of rough interval generation is to obtain the 
rough membership values (RMV) corresponding to each 
discretised interval [23]. RMV can be derived from two 
sources, either the right hand side accuracy of the rough 
classification rules [23] or the probability of occurrence of 
the respective intervals in the data [21]. In the proposed 
model, default rules generation framework is used to obtain 
the rough classification rules and the accuracy values. RMV 
indicates the probability of an object being classified in the 
set of a decision class [19, 21, 24, 25]. It is used as the 
numerical weightage in AC. RMV for all output class of 
every rough interval is determined according to the formula 
defined in (5) as follows: 
 
;    repeat for every 0 < i ? x and 0 < j ? y  
(5) 
 
Let x denotes the number of rough interval and i is the i-th 
rough interval. Card (i) is the total instance in the i-th rough 
interval. Card (Oj) is the total instance from i-th rough 
interval that concludes to decision class Oj while j is the j-th 
decision class and y is the total decision class.  
 
 978-1-4244-4944-6/09/$25.00 ©2009 IEEE   87 
 
  
Fig. 1: The proposed Rough AC model 
 
B. Association Mining 
The association mining is the second phase in the 
proposed model. Modification on the conventional 
association rule mining has been made in the proposed 
model. Instead of generating association rules from the 
frequent itemset, the proposed model starts with 
transforming the decision system into linguistic information 
system (IS). The next step is to scan for frequent occurring 
linguistic itemsets that satisfy the user-defined support 
thresholds.  
The linguistic IS transformation serves a two-fold purpose: 
i.e. replaces the decision classes in the original decision 
system with linguistic terms and transform the decision table 
into a linguistic IS table. This is an important step before the 
frequent pattern mining in order to generate linguistic 
frequent pattern instead of quantitative frequent pattern. The 
linguistic terms carry different RMV weightage towards the 
discretised RMR intervals. The decision classes will be 
replaced by user-defined linguistic terms. Replacing the 
decision classes with predefined linguistic terms will  change 
the decision classes into different rough linguistic sets [19]. 
User-defined linguistic terms were used in the experiments. 
Therefore this process requires the intervention of expert 
knowledge. Rough membership function helps in capturing 
the uncertainty boundary of the rough linguistic set, but user 
understanding of the dataset used is still necessary in order to 
determine suitable linguistic terms to be replaced. Imprecise 
terms chosen may result in deviation of the meaning of the 
rules. However, this would not affect the process of rules 
generation and the performance of the association rules. 
Choosing suitable linguistic terms with expert knowledge is 
not the focus of this paper. The second part of the linguistic 
decision system transformation is to convert the original 
decision system into a linguistic decision system. Similar to 
fuzzy decision system proposed in [26], the attributes in the 
linguistic decision system are the linguistic terms chosen 
previously while the attributes values are the RMV attained 
from respective rough intervals. All the instances that fall in 
the same rough interval will be assigned the same RMV.  
Iteration of the algorithm finds different large itemsets 
(frequent combination of items that appears together). The 
frequent pattern discovery algorithm [9] is referred in this 
phase. It starts with first scan pass the dataset to determine 
the occurrences to produce the large 1-itemsets. This is 
denoted as L1. L1 is then used to find L2, which is the large 2-
itemsets in the 2-pass scanning. The process is carried on 
until no more frequent k-itemsets can be found. Since the 
associative mining process stops at frequent pattern 
discovery, only the measurement of minimum support is 
needed in this phase. The higher the minimum support 
thresholds the lesser the frequent pattern  discovered in the 
experiments but it can increase the accuracy since only 
strong frequent itemsets passed the threshold constraints [27, 
28].  
C. Weighted Linguistic Associative Classification Rules 
Generation 
This is the last major phase in the proposed AC model. 
Some modification has been made in this phase compared to 
the conventional concept of AC model. This phase aims to 
generate weighted associative classification rules from the 
linguistic frequent pattern in the previous phase. Neither 
pruning of association rules nor classifier modelling is 
needed. The RMV values of the each linguistic frequent item 
are composited to form the weighted associative 
classification rule. Refer to section II. 
V. SIMULATION RESULTS 
The proposed model was simulated and discussed here. 
The simulation results shown in this section is based on the 
Iris Plant (IP) dataset from UCI repositories [29]. The rough 
interval of IP dataset and their respective RMV is shown in 
Fig. 2. Three decision classes were replaced with linguistic 
terms describing the size of the Iris plant, i.e. short (Iris-
setosa), medium (Iris-versicolor) and long (Iris-virginica). 
 
 
Fig. 2: Example of rough intervals with respective RMV 
 
The original decision system was transformed into 
linguistic information system. Fig. 3 is an example of 
linguistic data table for IP dataset. The RMV was used as the 
linguistic attributes value in the new IS. No decision class is 
Generating RMV: 
Association Mining: 
Discretisation 
Induce Rough Intervals 
Linguistic IS Transformation 
 
Frequent Pattern Discovery 
 
Weighted Linguistic Associative Classification 
Rules Generation 
 
 978-1-4244-4944-6/09/$25.00 ©2009 IEEE   88 
 
needed as each linguistic attribute itself embodies a rough 
decision class set. The associative relation between these 
rough decision class set need to be identified. 
 
 
Fig. 3:  Example of linguistic data table  
The frequent pattern mining was done on the new 
linguistic information system to get the large itemset. An 
example of the associative frequent pattern generated is 
shown in table I. Each of the generated items contains 
multiple relations between the rough decision classes. All 
the relations were composited into one AC rule. 
 
TABLE I.  AN EXAMPLE OF LINGUISTIC FREQUENT PATTERN FOR IP 
DATASET  
No. Linguistic Frequent Pattern 
1 PL.Medium^PW.Medium^PW.Short 
 
For example, the linguistic frequent L3 itemset consists of 
three rough decision classes. The rough intervals and their 
respective RMVs generated in the first phase of the proposed 
model were referred. The composition of RMVs is shown as 
follows: 
 
 
 
 
 
 
 
 
 
 
 
 
 
                     
Thus, weighted linguistic AC rules generated from 
linguistic frequent L3 itemset is as shown in table II. 
TABLE II.  THE WEIGHTED LINGUISTIC ASSOCITIVE CLASSIFICATION 
RULES 
No. Linguistic AC Rules Rule Weightage 
1 IF PL.Medium^PW. Medium^PW.Short   => Iris-setosa 0.51 
2 IF PL.Medium^PW. Medium^PW.Short  => Iris-
versicolor 1 
3 IF PL.Medium^PW. Medium^PW.Short => Iris-virginica 0 
 
VI. CONCLUSION AND FUTURE WORK 
This paper has proposed an associative classification 
model for uncertainty data analysis using rough membership 
function. Rough membership value is suggested as the 
weightage for capturing uncertainty in linguistic 
representation of discretized numerical intervals. With the 
new phase of generating rough membership values 
embedded prior to the two major steps of original associative 
classification model, the proposed model is able to handle 
continuous real-value attributes to generate weighted 
linguistic AC rules instead of crisp quantitative AC rules. 
The linguistic terms carry different RMV weightage. The 
weighted attribute sets will then go through the proposed 
task of associative classification including linguistic IS 
transformation, frequent pattern discovery, weighted 
associative classification rule generation. The generated 
weighted linguistic associative classification rules set can be 
used to perform classification operation on respective unseen 
data records. The future work of this research is to verify the 
proposed model with real dataset. The results will be 
reported in near future.  
 
 
ACKNOWLEDGMENT 
The work described in this paper was partially supported 
by the Faculty of Information and Communication 
Technology, Universiti Teknikal Malaysia Melaka (UTeM), 
grant no. PJP/2008/FTMK(15)-S494. We also thank the 
anonymous reviewers for their helpful comments. 
REFERENCES 
[1] R. Srikant and R. Agrawal, "Mining Quantitative Association Rules 
in Large Relational Tables," in Proc. 1996  ACM SIGMOD 
International Conference on Management of Data, pp. 1-12. 
[2] B. Liu, W. Hsu, and Y. Ma, "Integrating classification and 
association rule mining," in Proc. 1998 KDD-98, pp.  
[3] Z. Pawlak, "Rough Sets and Data Analysis," in Proc. 1996 Asian 
Fuzzy Systems Symposium, 'Soft Computing in Intelligent Systems 
and Information Processing', pp. 1-6. 
[4] F. Thabtah, "A Review of Associative Classification Mining," The 
Knowledge Engineering Review, vol. 22:1, pp. 37-65, 2007. 
[5] A. Veloso and W. Meira Jr, "Rule generation and rule selection 
techniques for cost-sensitive associative classification," in Proc. 
2005, pp. 295–309. 
Instances 
Linguistic terms 
RMV 
 978-1-4244-4944-6/09/$25.00 ©2009 IEEE   89 
 
[6] W. Li, J. Han, and J. Pei, "CMAR: Accurate and Efficient 
Classification Based on Multiple Class-Association Rules," 2001. 
[7] D. Mallett, "A Paper Review on Integrating Classification and 
Association Rule Mining," 2003. 
[8] D. Janssens, G. Wets, T. Brijs, and K. Vanhoof, "Integrating 
Classification and Association Rules by proposing adaptations to the 
CBA Algorithm," in Proc. 2003 Proceedings of the 10th 
International Conference on Recent Advances in Retailing and 
Services Science, pp.  
[9] R. Agrawal, T. Imielinski, and A. Swami, "Mining Association Rules 
Between Sets of Items in Large Databases," in Proc. 1993 ACM 
SIGMOD Conference, pp. 1-10. 
[10] Q. A. Al-Radaideh, M. N. Sulaiman, M. H. Selamat, and H. Ibrahim, 
"Approximate reduct computation by rough sets based attribute 
weighting," in Proc. 2005 IEEE International Conference on 
Granular Computing, pp. 383 - 386. 
[11] H. Ishibuchi, T. Nakashima, and T. Yamamoto, "Fuzzy Association 
Rules for Handling Continuous Attributes," in Proc. ISIE 2001 
International Symposium on Industrial Electronics, pp. 118-121. 
[12] J. Lu, B. Xu, L. Xu, D. Kang, H. Chen, and H. Yang, "Mining 
Association Rules with Linguistic Terms," in Proc. 2003 The 15th 
IEEE International Conference on Tools with Artificial Intelligence 
(ICTAI’03), pp. 129-133. 
[13] H. Ishibuchi, T. Yamamoto, and T. Nakashima, "Fuzzy Data Mining: 
Effect of Fuzzy Discretization," in Proc. 2001 IEEE 2001 
International Conference on Data Mining, pp. 241-248. 
[14] R. J. Miller and Y. Yang, "Association Rules Over Interval Data," in 
Proc. 1997 The 1997 ACM SIGMOD international conference on 
Management of data, pp. 452-461. 
[15] B. Pôssas, W. M. Jr., M. Carvalho, and R. Resende, "Using 
Quantitative Information for Efficient Association Rules Generation," 
in ACM SIGMOD Record, vol. 29, 2000, pp. 19-25. 
[16] Y. Aumann and Y. Lindell, "A Statistical Theory for Quantitative 
Association Rules," Journal of Intelligent Information Systems, vol. 
20, pp. 255-283, 2003. 
[17] J. Matra, J. L. Alvarez, and J. C. Riquelme, "An Evolutionary 
Algorithm to Discover Numeric Association Rules," in Proc. 2002 
The 2002 ACM symposium on Applied computing, pp. 590-594. 
[18] Z. Pawlak, "Some Issues on Rough Sets," in Transactions on Rough 
Sets vol. LNCS 3100, Lecture Notes in Computer Sciences, J. F. P. e. 
al., Ed.: Springer-Verlag Berlin Heidelberg, 2004, pp. 1-58. 
[19] Y. Yao, "A Comparative Study of Fuzzy Sets and Rough Sets," 
Journal of Information Sciences, vol. 109, pp. 227-242, 1998. 
[20] A. Abu Bakar, "Propositional Satisfiability Method In Rough 
Classification Modeling For Data Mining," vol. PhD: Universiti Putra 
Malaysia, 2002. 
[21] L. Polkowski, "Rough Set Theory: An Introduction," in Rough Sets 
Mathematical Foundations, Advances in Soft Computing: Physica-
Verlag, Heidelberg, 2002, pp. 5-6. 
[22] Z. Pawlak and A. Skowron, "Rough sets and Boolean reasoning," 
International Journal of Information Sciences, vol. 177, pp. 41-73, 
2007. 
[23] A. Ohrn, "Technical reference manual," vol. Ph.D. Trondheim, 
Norway: Norwegian University of Science and Technology, 2001, 
pp. 16-18. 
[24] Z. Pawlak and A. Skowron, "Rudiments of Rough Sets," Journal of 
Information Sciences, vol. 177, pp. 3-27, 2007. 
[25] Y. Yao, "Two Views of the Theory of Rough Sets in Finite 
Universes," International Journal of Approximate Reasoning, vol. 15, 
pp. 291-317, 1996. 
[26] T.-P. Hong, K.-Y. Lin, and S.-L. Wang, "Fuzzy Data Mining for 
Interesting Generalized Association Rules," Fuzzy Sets and Systems, 
vol. 138, pp. 255-269, 2003. 
[27] T.-P. Hong, C.-S. Kuo, and S.-L. Wang, "A fuzzy AprioriTid mining 
algorithm with reduced computational time," Applied Soft 
Computing, vol. 5, pp. 1-10, 2004. 
[28] R. S. Gy?rödi, "A Comparative Study of Iterative Algorithms in 
Association Rules Mining," Studies in Informatics and Control, vol. 
12, pp. 205-213, 2003. 
[29] A. Asuncion and D. J. Newman, "UCI Machine Learning 
Repository," University of California, School of Information and 
Computer Science, 2007. 
 
 
 
