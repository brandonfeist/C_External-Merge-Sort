An Efficient Close Frequent Pattern Mining Algorithm 
 
Jun TAN 
College of Mechanical and Electrical Engineering 
 Central South University 
Changsha,   Hunan Province, China 
e-mail: tanjun007@vip.sina.com 
 Yingyong BU and Bo YANG 
College of Mechanical and Electrical Engineering 
 Central South University 
Changsha,   Hunan Province, China 
e-mail: buyingyong43@126.com
 
 
Abstract—Efficient algorithms for mining frequent itemsets are 
crucial for mining association rules and for other data mining 
tasks.  FP-growth algorithm has been implemented using a 
prefix-tree structure,   known as a FP-tree,   for storing 
compressed frequency information.  Numerous experimental 
results have demonstrated that the algorithm perform 
extremely well.  But In FP-growth algorithm,   two traversals 
of FP-tree are needed for constructing the new conditional FP-
tree.   In this paper we present a novel FP-array technique that 
greatly reduces the need to traverse FP-trees,   thus obtaining 
significantly improved performance for FP-tree based 
algorithms.  We then present a very effective closed frequent   
pattern   algorithm which uses a variation of the FP-tree data 
structure in combination with the FP-array technique 
efficiently.  In the algorithm,   an efficient closeness-testing 
approach is also given for mining closed frequent itemsets.  
Experimental results show that the new algorithm outperform 
other algorithm in not only the speed of algorithms,   but also 
their scalability. 
Keywords- Closed FP-growth algorithm;  Closed FP-tree;     
sparse datasets;  FP-array 
I. PREFACE 
The core of mining association rules is mining frequent 
itemsets efficiently.  An important property of frequent 
itemset is “anti-monotonicity”,   which means any subset of a 
frequent itemset is frequent.  When a transaction database is 
very dense,   i.e. when the database contains large number of 
long frequent itemsets,   mining all frequent itemsets might 
not be a good idea.  For example,   if there is a frequent 
itemset with size L, then all 2L nonempty subsets of the 
itemset have to be generated.  For overcome the difficult,   
scholars put forward the concept of closed frequent itemsets.  
The closed frequent itemsets not only is less several orders of 
magnitude   than   frequent   itemsets,   but also contains 
complete information.  So it is widely used. Some 
representative research results are made.  for example &+$50
>@&/26(7>@&/26(7>@HWF
This paper proposes an effective closed frequent pattern 
mining algorithm at the basis of frequent itemsets mining 
algorithm FP-growth >@ >@ >@.  A novel FP-array 
technology is proposed in the algorithm,   a variation of the 
FP-tree data structure is used which is in combination with 
the FP-array.  Experimental results show that the new 
algorithm has very good performance. 
II. CLOSED FREQUENT ITEMSETS CONCEPT  
Mining association rules is divided into two steps.  Fist 
step:  find all frequent itemsets;   Second step:  generate 
strong association rules.  The second step costs much lower 
than first step,   so the performance of mining association 
rules is decided by the first step.  For reduce the number of 
frequent itemsets,   we give the concept of maximal frequent 
itemset. 
Definition 1:  A frequent itemset X is called maximal if 
there does not   exist a   frequent itemest Y such that X?Y. 
Sometimes we only want to know whether an itemset is 
frequent or not.  Then it is sufficient to discover only all the 
maximal frequent itemsets (MFI).  Many existing algorithms 
only mine maximal frequent itemsets (MFI). However,   
mining only MFI’s has the following deficiency.  From a 
MFI and its support,   we know that all its subsets are 
frequent and the support of any of its subset is not less than 
MFI’,   but we do not know the exact value of the support.  
For generating association rules,   we do need support of all 
frequent itemsets.  To solve this problem,   another type of a 
frequent itemset,   the Closed Frequent Itemset (CFI) was 
proposed. 
Definition 2:  A frequent itemset X is called closed if the 
support of all its supersets is less than the support of X. 
So,   the first step of mining association rules is changed 
to find all closed frequent itemsets.  Mining closed frequent 
itemsets is divided two steps. (1) find frequent itemsets;   (2) 
verify that the new frequent itemset is closed.   
III. CLOSED FREQUENT PATTERN MINING ALGORITHM 
A. Closed  FP-tree construction 
We proposed an effective closed frequent pattern mining 
algorithm based on FP-growth,   which is named,   closed   
FP-growth.  In the algorithm,   a variation of the FP-tree is 
used,   named Closed FP-tree.  In a Closed FP-tree,   each 
node in the subtree has four fields: item-name, count,   node-
link and level.  Level is used for closed frequent itemset 
testing.  
An example is used to illustrate the construction of the 
Closed FP-tree.  a transaction dataset is shown in table 1.  the 
minimum support is supposed to be 20%.  figure 1 shows 
FP-tree which contains all frequent itemset;   table 2 shows 
that each item of FP-tree’s conditional pattern base and 
conditional FP-tree.  the constructed Closed   FP-tree is 
2009 Second International Conference on Intelligent Computation Technology and Automation
978-0-7695-3804-4/09 $26.00 © 2009 IEEE
DOI 10.1109/ICICTA.2009.134
528
shown in figure 2 which contains all closed frequent itemsets. 
A node x: l: c means that the node is for item x,   its level is l 
and its count is c.  The header table in the CFP-tree is the 
same as that of the FP-tree. 
 Start from the first FP-tree T in figure 1.  Since T 
contains more than one path,   a bottom-up search has to be 
done.  First,   because item g’conditional FP-tree contains 
only one path,   we get the first frequent itemset {a, g}. 
Obviously it is closed,   so it is inserted into the Closed FP-
tree directly.  Similarly,   for item b and e,   we can get 
closed frequent itemsets {d, b} and {a, c, e}.  For item f,   
construct f’conditional FP-tree (Tf),   because Tf contains 
more than one path, Closed FP-growth is recursively called,   
we get closed frequent itemsets {a, d, f},   {a, f}.  Similarly,   
we deal with item d until item a.  Finally,   we get Closed 
FP-tree as shown   in   figure2. 
TABLE I.    A TRANSACTION SET 
 
 
 
 
 
 
 
 
 
TABLE II.  CONDITONAL FP-TREE 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1.  FP-tree 
 
 
 
 
 
Figure 2.  Closed  FP-tree 
 
B. FP-array technique 
The core of Closed FP-growth algorithm is traversing 
Closed   FP-trees and constructing new conditional Closed 
FP-tree after the first Closed   FP-tree is constructed   from 
the original database.  From a great deal of experiments we 
found out that about 80% of the CPU time was used for 
traversing Closed   FP-trees.  Therefore, the question is,   can 
we reduce the traversal time so that the method can be sped 
up? 
The traversal time can be reduced by using a simple 
additional data structure.  For each item i in the header of a 
conditional FP-tree TX,   two traversals of TX are needed for 
constructing the new conditional FP-tree TX? {i}.  The first 
traversal finds all frequent items in the conditional pattern 
base of X? {i},  and initializes the FP-tree T X? {i} by 
constructing its header table,  The second traversal constructs 
the new tree T X?{i}.  We can omit the first scan of TX by 
constructing a frequent pair array AX while building TX.  TX 
is initialized with an attribute AX. 
Definition: Suppose the T is a conditional FP-tree, I= {i1, 
i2…im} is the set of items in the header table of T.  A 
frequent pair array of T is a (m-1)*(m-1) matrix,   where 
each element of the matrix corresponds the counter of an 
ordered pair of items in I.  Obviously, there is no need to set 
a counter for both item pair (ij,   ik) and item pair (ik, ij).  
Therefore we only set the counters for all pairs (ij, ik) (j<k). 
We explain the construction of the FP-array based on 
datasets shown in table 1.  After the first scan of the original 
database,   we get frequent items as a:5,  c:5,  d:5,  f:4,  e:2, 
b:2, g:2.  During the second scan of the database we will 
construct FP-tree T and a FP-array A.  The FP-array will 
save the counts of all pairs of items,   each cell is a counter of 
a pair of items,   and   All cells in the FP-array are initialized 
as 0. For example,   A [a, b] is the counter for itemset {a, b}. 
During the second scan,   for each transaction,   first all 
frequent items in the transaction are extracted.  Suppose 
these items form itemset I.  To insert I into T,   the items in I 
are sorted according to the order in header table of T.  When 
we insert I into T,   at the same time A [i, j] is incremented 
by 1.  if {i, j} is contained in I.  For example,   for the fourth 
transaction, {a, c, e, f} is extracted (item h is infrequent  )  
Tid Transactions 
1 cdb 
2 acde 
3 acg 
4 acefh 
5 cdfk 
6 adfgi 
7 adfbj 
item Conditional pattern base Conditional FP-tree 
g {(fda:1),  (ca:1)} {(a:2)} 
b {(fda:1),  (dc:1)} {(d:2)} 
e {(dca:1),  (fca:1)} ((c:2, a:2)) 
f ((da:2),  (ca:1),  (dc:1)) {(a:3, (d:2, c:2)), (d:1, 
c:1)) 
d {(a:2),  (ca:1),  (c:2)} (((c:3, a:1), a:2)) 
c {(a:3)} {(a:3)} 
a Null set Null set 
529
This itemset is inserted into T as usual,  and at the same time, 
A[a, c], A[a, e], A[a, f], A[c, e], A[c, f], A[e, f] are all 
incremented by 1.  After the second scan, FP-array A 
contains the counts of all pairs of frequent items as shown in 
table 3. 
Next,   the FP-growth method is recursively called to 
mine frequent itemsets for each item in header table of T. 
However,  now for each item i,  instead of traversing T along 
the linked list starting at i to get all frequent items in i’s 
conditional pattern base,  A gives all frequent items for i. For 
example, by checking the third line in the table for A, 
frequent items a; c; d for the conditional pattern base of f can 
be obtained.  Therefore,   for each item i in T the FP-array A 
makes the first traversal of T  unnecessary,   and T {i} can be 
initialized directly from A. 
For the same reason,  from a conditional FP-tree TX, 
when we construct a new conditional FP-tree for X?{i},  for 
an item i,  a new FP-array AX?{i} is calculated.  During the 
construction of the new FP-tree T X?{i},  the FP-array A X?{i} 
is filled. For example,  if the conditional FP-tree T{f} is 
constructed, the cells of its FP-array A{f} will be the table 4. 
This FP-array is constructed as follows.  From the FP-array 
A,  we know that the frequent items in the conditional pattern 
base of {f} are,  in order, a, d, c.  By following the linked list 
of f,  from the first node we get {a, d }:1,  so it is inserted 
into the new FP-tree T{f}. At the same time, A{f}[a, d] is 
incremented by 1.  From the second node in the linked list, 
{c,  d}:1 is extracted,  and it is inserted  into T{f}.  At the 
same time,  A{f}[c, d] is incremented by 1.  From the third 
node in the linked list,  {a, b}:2 is extracted,  and it is 
inserted into T{f}. At the same time, A{f}[a, b] is incremented 
by 2. Since there are no other nodes in the linked list,  the 
construction of T{f} is finished, and FP-array A{f} is ready to 
be used for construction of FP-tree in next level of recursion. 
The construction of FP-arrays and FP-trees continues until 
the FP-growth method terminates. 
 
TABLE III.  A 
 
 
 
 
 
 
 
 
TABLE IV.  A{F} 
 
 
 
 
 
IV. ALGORITHM  DISCRIPSION 
According to the FP-array technique, we present an 
improved FP-growth algorithm (FP-growth# ).  T means an 
FP-tree, its attribute contains base, header and FP-array.  
T.base contains the itemsets X,  while T is X’s a conditional 
FP-tree,  T.FP-array contains FP-array AX.  The algorithm 
detail is described as follows: 
Procedure Closed  FP-growth (T,C)  
Input: A conditional FP-tree: T 
          The Closed FP-tree for T.base: C 
Output: updated C 
Method: 
1. If  T is single path P 
2.        generate all Closed FP-tree’s from P 
3.    for each Closed FP X   
4.           if   not closed_checking (X,C)  
5.                insert X into C 
6.          else for each I in T.header { 
7.                       set Y=t.base?{i}; 
8.                       if  not closed_checking (Y,C) 
9.                           if  T.FP-array is not NULL 
10.                           construct the header of Y’ conditional 
FP-tree from  T.FP-array 
11.                         else 
12.                               scan T to construct the header  
13.            construct the FP-tree TY and its FP-array AY; 
14.            initialize Y’s conditional Closed FP-tree CY; 
15.            call Closed  FP-growth(TY,CY); 
16.            merge CY with C 
17.  end 
In the algorithm,   FP-array technology is used from line 
9 to line 12.  Function closed_checking (Y, C) is called to 
test if Y is closed frequent itemset in line 4 and line 8.  if it is, 
function returns true.  Y’conditional frequent pattern tree and 
closed frequent pattern tree are constructed in line 14 and 
line 15. Closed  FP-growth is called recursively. 
V. EXPERIMENT ANALYSIS 
We implemented Closed FP-growth and CLOSET+ by 
running them on a virtual dataset.  The virtual dataset is 
downloaded from the website of research center of IBM [7], 
which contains 100, 000 transactions and 1000 items. The 
average transaction length is 40, and the average frequent 
pattern length is 10. it is a sparse dataset. The experiments 
were performed on a Lenevo computer with Intel 2.8 GHz 
Processor, and 1 GB of memory. The  operating system was  
Windows 2003 Server.  The algorithm is developed by 
VC++.  Source codes of CLOSET+ algorithm is download 
from [8]. 
Figure 3 shows that respective runtime of two algorithms. 
Because of the usage of FP-array technique,  For a sparse 
dataset, Closed FP-growth turns out to be faster than  
CLOSET+.  But when the minimum support is very low,   
the FP-tree contains good compressibility,   thus the FP-array 
has no too great advantage,  Consequently, as showed in 
Figure 3,  for very low minimum support,  Closed FP-growth 
and CLOSET+ have almost the same runtime. 
 
c 3 
d 3 3 
f 3 2 3 
e 2 2 1 1 
b 1 1 2 1 0 
g 2 1 1 1 0 0 
  a c d f e b 
d 2 
c 1 1 
 c a 
530
 




    
?????
?
?
?
?
V
HF

&)3JURZWK
&/26(7
 
Figure 3.  Compare on sparse dataset 
VI. CONCLUSIONS 
In this paper, we combine FP-tree with FP-array 
efficiently and present an  improved closed frequent pattern 
mining algorithm. Experiment result indicates,   for the 
sparse database,  that the algorithm has more excellent 
performance than the FP-growth algorithm. How to apply 
FP-array technique to mine the data streams based on 
frequent itemsets,   it need further research. 
ACKNOWLEDGMENT 
Financial supports from National Natural Science 
Foundation of China (Grant NO.50474052) are highly 
appreciated. 
REFERENCES 
[1] M.J..Zaki, C.Hsiao, “CHARM:An efficient algorithm for closed 
itemset mining”. .In Proceeding of The 2nd SIAM International 
Conference on Data Mining, Arlington, April  2002 
[2] J.Pei, J.Han, R.Mao, “CLOSET:An efficient algorithm for mining 
frequent closed itemsets”. In ACM SIGMOD Workshop on Research 
Issues in Data Mining and Knowledge Discovery, 2000,   pp.21-30 
[3] J.Wang, J.Han, and J.Pei, “CLOSET+: Searching for the best 
strategies for mining frequent closed itemsets”. In Proc.2003 ACM 
SIGKDD Int. Conf on Knowkedge Discovery and Data Mining, 
Washington, D.C.,Aug.2003 
[4] J.Han, J.Pei, and Y.Yin,  “Mining frequent patterns without candidate 
generation”.  In Proc. 2000 ACM- SIGMOD Int. Conf.  Management 
of  Data (SIGMOD’00),   pp.  1–12,  May 2000 
[5] Jiawei Han, Micheline Kamber, Data Mining Concepts and 
Techniques,  mechanism industry publishing, Bei Jing, 2002, pp.156-
160 
[6] K.Wang, L.Tang, J.Han. ”Top down FP-growth for association rule 
mining”  In proc.of the 6th Pacific Area Conference on Knowleage 
Discovery and Data Mining (PAKDD), 2002, pp.108-112 
[7] http://www.almaden.ibm.com/software/quest/resources/index.shtml 
[8] http://www-sal.cs.uiuc.edu/hanj/pubs/software.htm 
531
