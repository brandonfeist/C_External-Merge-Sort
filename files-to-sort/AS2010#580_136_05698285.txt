Nonfuzzy Classification using Rules annotated with Weight of Evidence from 
Statistical Data
Raja K
Department of Information Technology
MIT, Anna University
Chennai, India
raja_koth@yahoo.co.in
Raghavendran N, Vasudha V
India Development Center
Oracle Corporation,
Bangalore, India
raghavan291@gmail.com, vasu18@gmail.com
Rashmi Magdalene J
Software Private Limited
D.E.Shaw & Co,
Hyderabad, India
rashmi.magdalene@gmail.com
Abstract—To design and develop a nonfuzzy classification 
paradigm from a statistical data set. The event association 
patterns of different orders are detected which provides a 
probabilistic inference mechanism to achieve flexible 
classification and prediction. To detect significant event 
associations, residual analysis in statistics is used. Patterns are 
detected and rules are generated based on the deviations of the 
observed patterns from a default model. The discriminative 
power of each rule generated is described using Weight of 
Evidence (WOE) statistic. Classification decisions are made 
using WOE based estimation of the relative likelihoods of each 
possible labeling. Estimates are calculated by using the set of 
rules triggered by matching input values. Experimental results 
are discussed towards the end of the paper.
Keywords-Maximum marginal entropy; Event generation; 
Residual analysis; Pattern discovery; Nonfuzzy
I. INTRODUCTION
One of the basic tasks of data analysis is to automatically 
uncover the qualitative and quantitative patterns in a data set. 
When a large database is given, discovering the inherent 
patterns and regularities becomes a challenging task, 
especially when no domain knowledge is available or when 
the domain knowledge is too weak [1]. Because of the size 
of the dataset, it is almost impossible for a decision maker to 
manually abstract from it the useful patterns. Hence, it is 
desirable for automatic pattern discovery tools to support 
various types of decision-making tasks.
Consider a set D of M observations or samples obtained 
from a database. Each observation is described by N 
attributes. Thus, for any distinct attribute, there is a domain 
of possible values which can be numeric, boolean, and/or 
symbolic. Here, all the possible values are assumed to be 
discrete. The purpose of pattern discovery is to find the 
relations among the attributes and/or among their values. 
Those events whose occurrences are significantly different 
from those based on a “random” model are extracted. Thus, 
the co-occurrence of events detected may reflect the nature 
of the data set and provide useful information for future 
inferences and reasoning.
To design a pattern discovery system, some specific 
requirements must be addressed [2].
x The discovery process and the representation of the 
patterns should be transparent and easily 
interpretable.
x To account for probabilistic uncertainty in the data, 
the description of patterns should have a 
probabilistic indication of statistical relevance. 
x As real data is often noisy, the discovery system 
must be able to filter random noise from useful 
information. 
x To alleviate an often-vexatious data-preprocessing 
step, the discovery process should not be sensitive to 
monotonic, linear scaling of the data.
In order to meet these objectives, past methods have 
often sacrificed interpretability. A pattern discovery method 
is presented that aims at meeting the above challenges under 
a single framework, while remaining fairly transparent. The 
discovery method is based on the statistically guided 
construction of events in the sample space.
A. Theoretical background
The fundamental concepts and definitions are presented 
follows.
Consider a data set D containing M samples. Every 
sample is described in terms of N attributes, each of which 
can assume values in a corresponding discrete finite 
alphabet.
1) Feature: A feature, X, is a random variable which, in 
general, may be a nominal, ordinal, or numerical attribute or 
characteristic of an object or process.
2) Event: An event is the elemental construct in the 
present formulation of pattern discovery. An event is just a 
subspace of the continuous sample space.
3) Primary Event: A primary event of a random variable 
Xi is a realization of Xi which takes on a value ‘a’ from Di.
&i  a
4) Compound Event: A compound event associated with 
the variable set Xs is a set of primary events. A ‘k’ 
compound event is made up of k primary events of k 
distinctive variables. Every sample in the data set is an N 
compound event.
Third International Conference on Emerging Trends in Engineering and Technology
978-0-7695-4246-1/10 $26.00 © 2010 IEEE
DOI 10.1109/ICETET.2010.114
27
5) Observed Occurrence: The observed occurrence is 
the actual occurrence of the compound event Xs in data set 
D.
6) Expected Occurrence: The expected occurrence of a 
compound event Xs in data set D is its expected total under 
the assumption that the variables in Xs are mutually 
independent. The expected occurrence of Xs is denoted as 
Ex.
7) Pattern: If a compound event Xsj passes the statistical 
significance test T then Xsj is a significant pattern, or, 
simply, a pattern, of order |s|.
II. DISCRETIZATION BY PARTITIONING
In order to function in a continuous domain the pattern
discovery algorithm has been adapted to use discretized data. 
The approach for event construction is to partition the 
continuous sample space into nonoverlapping sections [3].
Figure 1. Observed data events and MME partitioning
The following are the reasons for partitioning
x The continuous sample must be discretized because 
the pattern discovery cannot work on continuous 
data values.
x Each event can be described probabilistically i.e. the 
probability of each event can be estimated.
A. Maximum marginal entropy partitioning
The marginal maximum entropy criterion is derived from 
Information Theory and prescribes a method of partitioning 
the sample space [4]. The subspace of interest is segmented 
in a way, which approximately maximizes the overall 
entropy H, of the partition. If the subspace is partitioned into 
J events, then the entropy of the partition is expressed as
                                       J
                              H = -? Pj log Pj                                                       (2)
                                      j=1
The approximate nature of the method is due to the fact 
that partitioning is not performed in the full dimensionality, 
but rather marginally along each dimension.
1)  Marginal Maximum Entropy (MME) Criterion:  To 
approximately maximize the entropy of a d-dimensional 
partition maximize the entropy of each one-dimensional 
(marginal) partition [17]. Thus discretization is performed 
via independent analysis of the density of feature values 
along each dimension using marginal maximum entropy 
partitioning in which the values observed in each input 
feature is divided among Q quantization bins, such that each 
bin contains the same number of values as shown in Fig. 1. 
B. Choice of Q value
   The choice of partition size Q is governed by the trade-
off between maximizing statistical significance of the 
resulting events while minimizing computational complexity 
[5].
x Too low value of Q will cause coarse partition and 
hence exact specific and true patterns cannot be 
identified.
x Too high value of Q will cause sparse number of 
samples in each partition and hence the confidence 
of the rules identified cannot be asserted.
Q is chosen such that each d-dimensional event has at 
least the minimum requisite number of data points for 
statistical testing.
C. Algorithm
For a subspace ?, with ? as the threshold sample size 
and vj as the volume of the event in a d-dimensional 
partition, the simple recursive partitioning with MME 
criterion is:
Choose a partition size Qo where Qo >= 2
Call Partition(?,Qo)
Procedure Partition(S,Q)
Partition S into Qd events according to the MME 
criterion
For each event Ej, j=1….Qd
If nj  ?,
            Estimate probability density as pj = nj / Nvj
Else
            Choose a partition size Qj
            Call Partition(Ej,Qj)
End if
End For
End Partition
III. PATTERN DISCOVERY
An event in a dataset is said to be a statistically 
significant pattern if there is significant deviation difference 
between its expected and observed occurrence [6][13].
x A positive pattern is one where the observed 
occurrence is much more than the expected 
occurrence.
x A negative pattern is one where the observed 
occurrence is much less than the expected 
occurrence.
28
This deviation is measured in terms of residuals
x Simple residual
x Standard residual
x Adjusted residual
For an event Xi
x ex indicates the number of occurrences of xlm
expected from the observation of a training set of 
known size under an assumed model of uniform 
random chance. 
x ox is the observed number of occurrences of xlm  in a 
training data set.
A. Simple Residual 
Simple Residual is just the absolute difference between 
the expected and observed occurrence
               sx = ox - ex                                                                              (3)
But it does not show the actual deviation, as it is not 
scalable. It depends on the size of the input dataset.
B. Standard residual
Standard residual is defined as the ratio of the simple 
residual and the square root of the expected occurrence. The 
standardized residual provides a normalization of the simple 
residual
zx = ox – ex / ¥ex                                                                    (4)
But it does not have a unit standard deviation.
C. Adjusted residual
Adjusted residual is defined is the ratio of the standard 
deviation and its variance
               dx =zx  / ¥vx                                                                                 (5)
The benefit of the adjusted residual is that it scales the 
space of the standardized residual into that of a normal 
deviate with unit standard deviation. Using the resulting N(0, 
1) space, we can easily calculate observed deviation from 
expectation[7].
It is important to note that the expectation value expected 
value cannot fall to zero under the assumed model used here. 
This is because it is equivalent to a linear scale of the number 
of available training examples, as it is produced by dividing 
the available training examples among the number of quanta. 
The only way a zero value could therefore be produced is by 
having no observed values for some feature. If this were to 
occur (i.e. if all the values for a given column in the training 
data were missing) the adjusted residual would be undefined, 
however in this pathological case there is no discovery 
system that would be able to proceed. It suffices to proceed 
under the assumption that there will be a non-zero number of 
examples available for every input feature.
D. Pattern discovery by residual analysis
   To select significant events, the adjusted residual is 
used as it provides an N (0, 1) distributed z statistic (i.e., a
statistic drawn from a normal distribution with zero mean 
and unit standard deviation). The value of the adjusted 
residual defines the relative significance of the associated 
event. The pattern discovery algorithm determines an event 
to be significant if the adjusted residual is greater than a 
threshold value [14].
E. Logic
   A compound event is said to be a pattern if the absolute 
value of the adjusted residual is greater than the threshold 
value. If the value is positive it is a positive pattern else it is a 
negative pattern. The testing can be formalized as follows
H0 : x is random
  H1 : x is significant
The decision rule has the following form:
( |dx|  >  threshold ) => H1
F. Selecting the threshold value
The threshold value determines the statistical 
significance the compound event has to pass to be considered 
as a pattern. So the selection of the threshold H is very 
crucial. It depends on the confidence support [8].
For a confidence level of 95 percent, H is chosen as 1.96
Figure 2.  Region of significance and threshold determination
G. Elimination and validation of test cases
As a precaution against recording spurious patterns when 
is low, the pattern discovery classifier implemented will not 
consider as patterns any events for which the expected 
number of occurrences is less than a cut off [9]. This is 
desired in order to prevent the discovery of high-order 
patterns caused by the occurrence of only one or two 
instances of a high-order event (instances that were possibly 
generated by chance). The existence of such patterns may 
diminish classifier performance, especially when the total 
number of features, M, is quite large. The use of a spurious 
pattern would then corrupt the evaluation of any remaining 
features in the induction of the correct class label value.
So a cut off ‘?’ is decided. Only when expected 
occurrence of an event is greater than or equal to the cut off, 
29
the compound event is eligible for residual analysis. Only 
when
ex !  ?
The test is valid.
This has two purposes
x Elimination of spurious candidate patterns early in 
the pattern discovery phase.
x Avoiding exhaustive search of all combinations of 
primary events.
Usually the value for is taken as any integer greater than 
3. For extremely stringent cases a large cut of value can be 
assumed.
H. Algorithm for pattern discovery by residual analysis
Procedure: PatternDiscovery()
Input: Data Set
Output: {Pattern, Label}
Begin Procedure
Let Order=2
While Order<=N, Do
For all compound events xi of order, Do
Calculate R, adjusted residual
If |R|>Threshold
Output xi as a rule  
End if
End for 
Let Order = Order + 1
End while
End Procedure
I. Output of pattern discovery
The algorithm takes as input the given dataset and 
outputs a set of patterns by calculating the residual analysis 
and comparing with the threshold.
x It outputs a set of patterns with statistical 
significance.
x If the pattern contains a label it becomes a rule
IV. WEIGHT OF EVIDENCE 
In order to measure the discriminative power of a pattern, 
“weight of evidence” statistic or WOE is used [11].
Definition: Let (Y=yk) represent the label portion of a given 
pattern xlm, the remaining portion (consisting of the input 
feature values) is referred to as x*l. The mutual information 
between these two components can be calculated using:
                           (6)
A WOE in favor of or against a particular labeling yk ?Y 
can be calculated as
        (7)
or
                 (8)
WOE thereby provides a measure of how discriminative 
a pattern x*l is in relation to a label yk and gives us a measure 
of the relative probability of the co-occurrence of x*l and yk
(i.e., the “odds” of labeling correctly).
The domain of WOE values is [í1Â Â Â1], where í1 
indicates those patterns (x*l) that never occur in the training 
data with the specific class label yk; 1 indicates patterns, 
which only occur with the specific class label yk. These ±1 
valued WOE patterns are the most descriptive relationships 
found in the training data set as any non-infinite WOE 
indicates a pattern for which conflicting labels have been 
observed.
V. NONFUZZY CLASSIFICATION
   WOE-based support for each yk (possible class label) is 
evaluated in turn by considering the highest order pattern 
with the greatest adjusted residual from the set of all patterns 
occurring in an input data vector to be classified, and 
accumulating the WOE of this pattern in support of the
associated label. All features of the input data vector 
matching this pattern are then excluded from further 
consideration for this yk, and the next-highest order 
occurring pattern is considered [12]. This continues until no 
patterns match the remaining input data vector, or all the 
features in the input data vector are excluded. This 
“independent” method of selecting patterns attempts to 
accumulate their WOE in way, which estimates the 
accumulation of the probabilities of independent random 
variables. Once this is repeated to consider each yk, the label 
with the highest accrued WOE is assumed as the highest 
likelihood match and this class label is assigned to the input 
feature vector. The classification is done using independent 
rule firing.
A. Independent Rule Firing 
The algorithm proceeds through the following steps:
1) Produce a list L of all input variables 
2) Repeat steps 3-9 for each classification label.
3) Set search order o to be N, the number of inputs to 
the system
4) Place all rules of order o whose precedents exist in 
the list L into a list of matches, M. If this search 
fails, repeat after setting o = o í 1
5) If o = 1 has been reached, and no matches have 
been found, then stop
6) Find the rule Rmax in M which has the highest 
adjusted residual
7) Add the WOE of Rmax to the WOE support of the 
label
8) Remove from L all the variables matching the 
precedents of rule Rmax
9) If L is empty, then stop
10) The class label with the highest accrued WOE is 
assigned to the input feature vector
30
The occurrence of a pattern is assumed to consume the 
information associated with the primary events describing 
the pattern. For this reason, each input feature can support at 
most one rule firing, in order to maintain the assumption of 
statistical independence of the input features [15] [16].
VI. EXPERIMENTAL RESULTS
   The pattern discovery and classification modules were 
successfully implemented. A large sample dataset was used 
as input and the rules were got as output. The modules were 
successfully tested on various datasets and results were 
obtained. 
TABLE I. INPUT DATA SET
A training dataset D is used as input to the algorithm 
[10].
x D consists of six input features with five attributes 
“Red”, “Blue”, “Green”, “Hue”, “Saturation” and a 
label “Texture”.
x 1500 record sets were considered
A. Parameters
The three important parameters, which had to be decided 
on, are
x Q – number of quantization bins
x H – threshold level for statistical significance
x ? – valid significance test cut-off 
1) Q value: The value of Q was a tradeoff between the 
classification precision and the computational complexity. 
Taking all the parameters into consideration, the value of Q 
was fixed as 5.
2) H value: The threshold level was chosen to be a 
standard value of 1.96 which corresponds to 95 percent 
confidence level which ensures a reasonable assertion to 
make a strong suggestion in decision support system.
3) ? value: The valid significance test cut off was 
decided based on the size of the database. To eliminate the 
spurious patterns early in pattern discovery phase, and to 
avoid exhaustive search of the data set, an optimum value of 
was chosen as 10.
B. Quantization
   The numeric data of all the 5 input features were 
discretized using maximum marginal entropy partitioning. 
x Each sample space was divided into quantization 
bins along each margin.
x The value of Q was chosen as 5.
x The input data was distributed over the 5 quantized 
bins in such a way that the entropy was maximized.
x The number of sample data in each bin was equal.
1) Quantized bins
TABLE II. QUANTIZED BINS
C. Events generated
After quantization the input sample space is partitioned 
into events each of which can be determined 
probabilistically.
Primary events are the basic building blocks
A :  [a1] [a2] [a3] [a4] [a5]
B :  [b1] [b2] [b3] [b4] [b5]
C :  [c1] [c2] [c3] [c4] [c5]
D :  [d1] [d2] [d3] [d4] [d5]
E :  [e1] [e2] [e3] [e4] [e5]
The primary events are connected to form higher order 
events.
Second order [a1, b1] [a2, b2]…….
       Third order [a1, a2, a3]   [b1, b2, b3] …
And so on…
D. Rules generated
303 rules were generated. Each rule mapped an event to a 
label value and the following are some of the rules generated
IF (Green 0.0...3.55556) and (Hue 0.0...8.22222) THEN 
Window
IF (Blue 0.0...7.33333) and (Green 3.55556...16.8889) 
THEN Cement
IF (Blue 0.0...7.33333) and (Hue 8.33333...23.6667) 
THEN Cement
x
x
x
IF (Red 0.0...5.44444) and (Blue 7.33333...20.0) THEN 
Foliage
31
IF (Red 0.0...5.44444) and (Green 3.55556...16.8889) 
THEN Foliage
No of rules generated.. 303
E. Weight of Evidence
Weight of Evidence for each rule is calculated
IF (Green 0.0...3.55556) and (Hue 0.0...8.22222) THEN 
Window
WOE : 1.5279335293281375
IF (Blue 0.0...7.33333) and (Green 3.55556...16.8889) 
THEN Cement
WOE : 7.067363981745421
IF (Blue 0.0...7.33333) and (Hue 8.33333...23.6667) 
THEN Cement
WOE : Infinity
x
x
x
IF (Red 0.0...5.44444) and (Blue 7.33333...20.0) THEN 
Foliage
WOE : 5.495014176104029
IF (Red 0.0...5.44444) and (Green 3.55556...16.8889) 
THEN Foliage
WOE : 4.35485123066857
F. Classification
Same training data was used as input to the classification 
algorithm. Of the 1500 input sets, 949 were correctly labeled.
26.3333,35.2222,25.6667,35.2222,-2.04915,Brickface,
Miss : LABEL  :  Window
13.1111,17.8889,21.5556,21.5556,2.69011,Cement,
Hit : LABEL  :  Cement
0,1.33333,0,1.33333,-2.0944,Foliage,
Hit : LABEL  :  Foliage
6.55556,6.44444,11.5556,11.5556,2.09315,Cement,
Hit : LABEL  :  Cement
0.777778,3,0,3,-1.82221,Grass,
Miss : LABEL  :  Foliage
0.444444,6.44444,1.44444,6.44444,-2.26954,Foliage,
Miss : LABEL  :  Path
x
x
x
115.111,142.222,121.333,142.222,-2.33375,Path,
Hit : LABEL  :  Path
51.8889,72.4444,49.6667,72.4444,-1.99159,Brickface,
Hit : LABEL  :  Brickface
7.44444,7.11111,14.6667,14.6667,2.06945,Cement,
Hit : LABEL  :  Cement
41,57.2222,39.5556,57.2222,-2.01072,Brickface,
Hit : LABEL  :  Brickface
No of hits : 949
CONCLUSION
Hence a framework for nonfuzzy classification is 
proposed and proved experimentally. This model identifies 
statistically significant event associations inherent in a given 
data set, using residual analysis to test whether or not a 
pattern candidate is significantly deviated from a default log-
linear model. The generated rules annotated with weight of 
evidence are fired independently to classify the input feature 
vector. 
In scenarios where labeling requires further statistical 
augmentation, a deterministic means of confidence could be 
derived. The confidence measure can be further combined 
with the label to supplement the decision analogous to 
multiple votes. The proposed non-fuzzy model leveraging 
such weighted classifiers increases the accuracy of decision-
making.
REFERENCES
[1] M. Holsheimer and A. Siebes, “Data Mining: The Search for 
Knowledge in Databases,” Technical Report CS-R9406, CWI, 
Amsterdam,1994.
[2] R. Agrawal, S. Gliosli. T. Imielinski. B. Iyer. and A. Swami, An 
interval classifier for database mining applications. In VLDB-92, pp. 
560-573: 2002.
[3] D.K.Y. Chiu, A.K.C. Wong, and B. Cheung, “Information Discovery 
through Hierarchical Maximum Entropy Discretization and 
Synthesis,” Knowledge Discovery in Databases, G. Piatetsky-
Shapiroand W.J. Frawley, eds. AAAI Press/The MIT Press, 1991.
[4] D.V. Gokhale, “On Joint and Conditional Entropies,” Entropy, vol. 1, 
no. 2, pp. 21-24, 1999.
[5] J. Han, Y . Cai, and N. Cercone, Data-driven discovery of 
quantitative rules in relational databases, IEEE Trans. on Knowledge 
and Data Engineering, 5(1):29-40. 1993.
[6] A.K.C. Wong and Y. Wang, “Pattern Discovery: A Data Driven 
Approach to Decision Support,” IEEE Trans. Systems, Man, and 
Cybernetics C, vol. 33, no. 1, pp. 114-124, Feb. 2003.
[7] A. Hamilton-Wright and D.W. Stashuk, “Comparing Pattern 
Discovery and Back-Propagation Classifiers,” Proc. Int’l Joint Conf 
Neural Networks (IJCNN), July 2005.
[8] [S.J. Haberman, “The Analysis of Residuals in Cross-Classified 
Tables,” Biometrics, vol. 29, no. 1, pp. 205-220, Mar. 1973.
[9] Estimation of Normal parameters -
http://www.weibull.com/LifeDataWeb/estimation_of_the_parameters
.htm#probability_plotting
[10] Datasets - http://www.cs.waikato.ac.nz/ml/weka/
[11] Wang, Y. and A. K. C. Wong. From association to classification: 
Inference using weight of evidence. IEEE Transactions on 
Knowledge & Data Engineering, 15(3):764–767, May-June 2003
[12] Duda, R. O., P. E. Hart and D. G. Stork. Pattern Classification. John 
Wiley & Sons, 2nd edition, 2001.ISBN 0-471-05669-3.
[13] Becker, P. W. Recognition of Patterns: Using the Frequencies of 
Occurrence of Binary Words. Springer- Verlag, 2nd edition, 1968.
[14] Chan, K. C. C. and D. K. Y. Wong, Andrew K. C. Chiu. Learning 
sequential patterns from probabilistic inductive prediction. IEEE 
Transactions Systems, Man, Cybernetics, 24(10):1532–1547, October 
1994.
[15] Levitin, G. Evaluating correct classification probability for weighted 
voting classifiers with plurality voting. European Journal of 
Operations Research, 141:596–607, 2002.
[16] Gurov, S. I. Reliability estimation of classification algorithms 
Introduction to the problem – point frequency estimates. Computation 
Mathematics and Modeling, 15(4):365–376, 2004.
[17] Tom Chau, “ Marginal Maximum Entropy Partitioning Yields 
Asymptotically Consistent Probability Density Functions,” IEEE 
transactions On Pattern Analysis And Machine Intelligence, Vol.23, 
No.4, April 2001.
32
