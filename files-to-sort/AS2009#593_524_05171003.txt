Confidence-based Concept Discovery in Relational Databases
Yusuf Kavurucu?
yusuf.kavurucu@ceng.metu.edu.tr
Pinar Senkul†
senkul@ceng.metu.edu.tr
Ismail Hakki Toroslu‡
toroslu@ceng.metu.edu.tr
Abstract
Multi-relational data mining has become popular due to
the limitations of propositional problem definition in struc-
tured domains and the tendency of storing data in relational
databases. Several relational knowledge discovery sys-
tems have been developed employing various search strate-
gies, heuristics, language pattern limitations and hypoth-
esis evaluation criteria, in order to cope with intractably
large search space and to be able to generate high-quality
patterns. In this work, we improve an ILP-based concept
discovery method, namely Confidence-based Concept Dis-
covery (C2D) by removing the dependence on order of tar-
get instances in the relational database. In this method, the
generalization step of the basic algorithm of C2D is modi-
fied so that all possible frequent rules in Apriori lattice can
be searched in an efficient manner. Moreover, this improved
version directly finds transitive rules in the search space. A
set of experiments is conducted to compare the performance
of proposed method with the basic version in terms of sup-
port and confidence.
1 Introduction
Real life data intensive applications require the use of
databases in order to store complex data in relational form.
The need for applying data mining and learning tasks on
such data has led to the development of multi-relational
learning systems that can be directly applied to relational
data [3]. Relational upgrades of data mining and con-
cept learning systems generally employ first-order predicate
logic as representation language for background knowl-
edge and data structures. The learning systems, which in-
duce logical patterns valid for given background knowl-
edge, have been investigated under a research area, called
Inductive Logic Programming (ILP), a subfield of Machine
Learning and Logic Programming [8].
?Middle East Technical University Computer Eng. Dept.
†Contact author. Middle East Technical University Computer Engi-
neering Department 06531 Ankara, Turkey
‡Middle East Technical University Computer Eng. Dept.
Confidence-based Concept Discovery (C2D) [5, 4] is a
predictive concept learning ILP system that employs rela-
tional association rule mining concepts and techniques to
find frequent and strong concept definitions according to
given target relation and background knowledge. C2D uti-
lizes absorption operator of inverse resolution for general-
ization of concept instances in the presence of background
knowledge and refines these general patterns into frequent
and strong concept definitions with an Apriori-based spe-
cialization operator based on confidence.
In this work, C2D is improved by modifying the gener-
alization step of the main algorithm to overcome the draw-
backs of dependence on target instance order and limita-
tions on constructing certain types of transitive rules.
This paper is organized as follows: Section 2 presents
the related work. Section 3 gives an overview of C2D al-
gorithm. Section 4 describes the modification in the algo-
rithm. Section 5 presents the experiments to discuss the per-
formance of improved version according to the basic C2D
algorithm. Finally, Section 6 includes concluding remarks.
2 Related Work
FOIL, PROGOL and WARMR are some of the well-
known ILP-based systems in the literature. FOIL [9] is one
of the earliest concept discovery systems. It is a top-down
relational ILP system, which uses refinement graph in the
search process. In FOIL, negative examples are not explic-
itly given; they are generated on the basis of CWA.
PROGOL [7] is a top-down relational ILP system, which
is based on inverse entailment. A bottom clause is a max-
imally specific clause, which covers a positive example
and is derived using inverse entailment. PROGOL extends
clauses by traversing the refinement lattice.
Design of algorithms for frequent pattern discovery has
become a popular topic in data mining. Almost all algo-
rithms have the same level-wise search technique known
as APRIORI algorithm. WARMR [1] is a descriptive ILP
system that employs Apriori rule to find frequent queries
having the target relation by using support criteria.
C2D is similar to PROGOL as both systems produce con-
cept definition from given target. WARMR is another simi-
2009 World Congress on Computer Science and Information Engineering
978-0-7695-3507-4/08 $25.00 © 2008 IEEE
DOI 10.1109/CSIE.2009.267
2872
lar work in a sense that, both systems employ Apriori-based
searching methods. Unlike PROGOL and WARMR, C2D
does not need input/output mode declarations. It only re-
quires type specifications of the arguments, which already
exist together with relational tables corresponding to pred-
icates. Most of the ILP-based systems require negative in-
formation, whereas C2D directly works on databases which
have only positive data. Similar to FOIL, negative infor-
mation is implicitly described according to CWA. Finally, it
uses a novel confidence-based hypothesis evaluation crite-
rion and search space pruning method.
PROGOL and WARMR can generate transitive rules
only by using strict mode declarations. In this improved
version of C2D, transitive rules are generated without the
guidance of mode declarations.
3 C2D: Confidence-based Concept Discovery
Method
The proposed technique is an improved version of the
concept discovery system named C2D [5, 4] and is coupled
with the rule generation heuristics of the system. Therefore,
in this section, we describe the outline and the important
features of C2D.
C2D is a concept discovery system that uses first-order
logic as the concept definition language and generates a set
of definite clauses having the target concept in the head.
When pure first-order logic is used to generate patterns in
learning systems, it may also generate unreasonable clauses
such as the ones in which predicates having arguments with
different types are related. C2D prevents this by disallowing
predicate arguments with different types to be unified.
In C2D, three mechanisms are effective for pruning the
search space.
1. The first one is a generality ordering on the con-
cept clauses based on ?-subsumption and is defined as fol-
lows: A definite clause C ?-subsumes a definite clause C?,
i.e. at least as general as C?, if and only if ?? such that:
head(C) = head(C ?) and body(C ?) ? body(C)?.
2. The second one, which is novel in C2D, is the use of
confidence as follows: If the confidence value of a clause
is not higher than the confidence values of the two parent
clauses in the Apriori search lattice, then it is pruned. A
similar approach is used in the Dense-Miner system [10]
for traditional association rule mining.
3. The third pruning strategy utilizes primary-foreign
key relationship between the head and body relations. If
such a relationship exists between the head and the body
predicates, the foreign key argument of the body relation
can only have the same variable as the primary key argu-
ment of the head predicate in the generalization step.
Another new feature is the parametric structure for sup-
port, confidence, recursion and f-metric definitions. The
user can set support and confidence thresholds in order to
find rules having acceptable support and confidence values.
Similarly, it is possible to allow/disallow recursion in dis-
covered concepts. F-metric is the rule evaluation function
whose definition is given in [5]
The database given in Figure 1, is used as a running ex-
ample in this section. In this example, daughter (d) is the
concept to be learned, and two concept instances are given.
Background facts of two relations, namely parent (p) and
female (f) are provided. Finally, types of the attributes of
relations are listed.
Concept Instances Background Facts Type Declarations
d(mary, ann). p(ann, mary). d(person, person).
d(eve, tom). p(ann, tom). p(person, person).
p(tom, eve). f(person).
f(ann).
f(mary).
f(eve).
Figure 1. The daughter database
The algorithm of C2D, given in Figure 2, starts with
selecting a positive concept instance. The most general
clauses with two literals, one in the head and one in the
body, that entail the positive example are generated and then
the concept rule space is searched with an Apriori-based
specialization operator. In the refinement graph, if support
parameter is on and the frequency of a clause is below the
support threshold, it is pruned as an infrequent clause. Ad-
ditionally, clauses whose confidence values are not higher
than their parents’ confidence values are also eliminated.
When the maximum depth reached or no more candidate
clause can be found, if confidence parameter is on, then the
clauses that have less confidence value than the confidence
threshold are eliminated for the solution set. Among the
produced strong and frequent rules, the best clause (accord-
ing to f-metric value) is selected and the rule search is re-
peated for the remaining concept instances that are not in
the coverage of the selected hypothesis clauses.
Generalization: The generalization step is similar to the
approach in [5]. After picking the first uncovered positive
example, C2D searches facts related to selected concept in-
stance in the database, including the related facts that be-
long to the target concept in order for the system to induce
recursive rules. Two facts are related if they share the same
constant in the predicate argument positions of the same
type. Then, the system generalizes the concept instance
with all related facts.
In the daughter example, C2D selects the first concept
instance, which is daughter(mary, ann), and then finds the
related fact set as {parent(ann, mary), parent(ann, tom), fe-
male(ann), female(mary)}. The system generalizes daugh-
ter(mary, ann) with each related fact. For instance, by ap-
plying absorption operator to the daughter(mary, ann) and
2883
- Input: I: Concept Instance set, BF: Background Facts (from DB)
- Output: H: Hypothesis set (initially H=?)
- Parameters: min-sup, min-conf, B (used in f-metric)
md: maximum depth (body literal count)
- Local Variables: Ci: Candidate clauses set at level i, d: level
G: Generalization clauses, FSC: Frequent and Strong clauses set
- Repeat until I is covered by H (until I = ?) or
No more possible G can be found according to given parameters:
1. Select p (first uncovered instance) from I.
2. GENERALIZATION: Generate G of p by using BF
3. Initialize C1:=G, FSC:=?, d:=1
4. REFINEMENT OF GENERALIZATION:
While Cd = ? and d?md
a. FSC = FSC ? FREQ STR CLS(Cd, min-sup)
b. Cd+1= CAND GEN(FSC, min-sup)
i.UNION: For each clause pair in level d
compute each possible union clause.
ii.For each union clause satisfying min-sup
SPECIALIZATION: Unify existential variables
FILTERING: Discard infrequent and less-confident clauses
c. d:=d+1.
5. EVALUATION: Eliminate clauses according to min-conf
Select cbest from FSC
6. COVERING: Compute Ic ? I covered by cbest
7. H := H ? cbest, I := I - Ic
-Return H.
Figure 2. C2D Algorithm
parent(ann,mary), the concept descriptions of the form
{daughter(mary, ann) : ? parent(ann,mary)}??12
are derived.
Refinement of Generalization: C2D refines the two lit-
eral concept descriptions with an Apriori-based specializa-
tion operator that searches the definite clause space in a top-
down manner, from general to specific. As in Apriori, the
search proceeds level-wise in the hypothesis space and it is
mainly composed of two steps: frequent clause set selection
from candidate clauses and candidate clause set generation
as refinements of the frequent clauses in the previous level.
The standard Apriori search lattice is extended in order to
capture first-order logical clauses and the candidate genera-
tion and frequent pattern selection tasks are customized for
first-order logical clauses.
In the daughter example, for the concept instance daugh-
ter(mary, ann), two literal generalizations are generated in
the presence of related facts and the first level of the search
lattice is populated with these generalizations. With the
support threshold value 0.8, the system eliminates the in-
frequent clauses. Notice that among 18 clauses generated
for daughter(mary, ann) and parent(ann, mary), only 6 of
them satisfy the threshold (bold ones below). Other clauses
below are generated from other generalizations.
d(A, B) :- p(C, mary). d(A, B) :- p(B, A).
d(A, B) :- p(C, A). d(A, B) :- p(ann, C).
d(A, B) :- p(B, C). d(A, B) :- p(C, D).
d(A, B) :- p(C, tom). d(A, B) :- f(A).
d(A, B) :- f(C).
The candidate clauses for the next level of the search
space are generated in three important steps:
1. Frequent clauses of the previous level are joined to
generate the candidate clauses via union operator. In order
to apply the union operator to two frequent definite clauses,
these clauses must have the same head literal, and bodies
must have all but one literal in common.
2. For each frequent union clause, a further specializa-
tion step is employed that unifies the existential variables of
relations having the same type in the body of the clause. By
this way, clauses with relations indirectly bound to the head
predicate can be captured.
3. Except for the first level, the candidate clauses that
have confidence value not higher than parent’s confidence
values are eliminated.
Evaluation: For the first instance of the target concept,
which has not been covered by the hypothesis yet, the sys-
tem constructs the search tree consisting of the frequent and
confident candidate clauses that induce the current concept
instance. Then it eliminates the clauses having less confi-
dence value than the confidence threshold. Finally, the sys-
tem decides on which clause in the search tree represents a
better concept description than other candidates according
to f-metric definition.
Covering: After the best clause is selected, concept in-
stances covered by this clause are removed from the concept
instances set. The main iteration continues until all concept
instances are covered or no more possible candidate clause
can be found for the uncovered concept instances.
In the daughter example, the search tree constructed for
the instance daughter(mary, ann) is traversed for the best
clause. The clause d(A, B) :- p(B, A), f(A) with support value
of 1.0 and the confidence value of 1.0 (f-metric=1.0) is se-
lected and added to the hypothesis. Since all the concept
instances are covered by this rule, the algorithm terminates
and outputs the following hypothesis:
daughter(A,B) :- parent(B,A), female(A).
4 Improved C2D: C2D v2
In the basic algorithm of C2D, the experiments show that
the selection order of the target instance (the order in the tar-
get relation) may change the result hypothesis set. In each
coverage set, the induced rules depend on the selected tar-
get instance and the covered target instances in each step
do not have any effect on the induced rules in the following
coverage steps.
As a remedy to this problem, a new mechanism is de-
veloped and used in the improved version of C2D, namely
C2D v2. For the target relation t(a, b), the induced rules
has a head including either a constant or a variable for each
argument of t. Each argument can be handled indepen-
dently to find the possible head relations for the hypoth-
2894
esis set. As an example, for the first argument a, a con-
stant must be repeated at least min sup multiplied by num-
ber of uncovered instances in the target relation so that it
can exist as a constant in a possible induced rule. To find
the possible constants for a, the following SQL statement
must be executed in the database:
SELECT a FROM t
GROUP BY a
HAVING COUNT(*) ? (min sup * num of uncov inst)
For example, in the PTE-1 database [11] the target rela-
tion pte active has only one argument (arg1). At the begin-
ning, there are 298 uncovered instances in pte active. The
min sup parameter is set as 0.05. However, the following
SQL statement returns an empty set which means there can-
not be a constant for the argument arg1 of pte active:
SELECT arg1 FROM pte active
GROUP BY arg1
HAVING COUNT(*) ? 298 * 0.05
The result of the SQL statement (empty set) means that
the argument arg1 of pte active can only be a variable for
the head of the solution hypothesis clauses.
In the same manner, for a background relation r(a, b, c),
if a constant exists many times for the same argument in r,
then it is a frequent value for that argument of r and may
exist in a solution clause for the hypothesis set. As an ex-
ample, in the PTE-1 database, atom(drug, atom, element,
integer, charge) is a background relation and possible con-
stants which can exist in the hypothesis set can be found for
each argument of atom by executing SQL statements. The
possible constants for each argument of atom are as follows:
drug: empty set (only variable)
atom: empty set (only variable)
element: c, h, o (also variable)
integer: 3, 10, 22 (also variable)
charge: empty set (only variable)
As a result, the following clauses (totally (1*1*4*4*1)*2
= 32 different clause) are created as candidate in the gener-
alization step of the improved version.
pte active(A) :- atom(A, B, c, 3, C)
pte active(A) :- atom(A, B, c, 10, C)
pte active(A) :- atom(A, B, c, 22, C)
pte active(A) :- atom(A, B, c, C, D)
pte active(A) :- atom(A, B, h, 3, D)
...
Support and confidence values for each clause are calcu-
lated and the infrequent clauses are eliminated. The special-
ization and coverage steps are same as in the basic version.
Another drawback of the generalization step of the basic
C2D algorithm is about generating transitive rules. In the
basic version, only the related facts of the selected target in-
stance are used in the algorithm. If a background relation
does not have any argument with same type of any argument
of the target relation, then that background relation is called
an unrelated relation and has no effect in the result hypoth-
esis set. However, as the modified version of C2D handles
all the background relations independently, it can find the
transitive rules in the search space.
In the daughter example, the target and background re-
lations can only have variables for the arguments in the hy-
pothesis set. So, the following clauses are produced in the
generalization step:
d(A,B):-p(A,B). d(A,B):-p(A,C). d(A,B):-p(C,B).
d(A,B):-p(C,D). d(A,B):-p(B,C). d(A,B):-p(C, D).
d(A,B):-f(A). d(A,B):-f(B). d(A,B):-f(C).
At the end of the first coverage step, the following clause
which covers all the target instances is induced:
daughter(A,B) :- parent(B,A), female(A).
5 Experimental Results
5.1 Constructing Transitive Rules
The generation phase of basic C2D algorithm considers
only related facts to construct the 2-predicate (one head and
one body predicate) generalized rules. However, this ap-
proach falls short for the cases where the domain includes
many unrelated facts. Michalski’s trains problem [6] is a
typical case for this situation. In this data set, the target re-
lation eastbound(train) is only related with has car(train,
car) relation. The other background relations have an argu-
ment of type car and are only related with has car relation.
The basic version of C2D finds the following rules:
eastbound(A :-has car(B,car 11). eastbound(A):-has car(east1,B).
eastbound(A):-has car(B,car 12). eastbound(A :-has car(A,B).
eastbound(A):-has car(B,car 13). eastbound(A):-has car(B,C).
eastbound(A):-has car(B,car 14).
As seen above, the generated rules are very general and
can not include any information about the properties of the
cars of the train. However, C2D v2 takes all of the back-
ground relations into consideration and can find the follow-
ing transitive rule in the search space:
eastbound(A):-has car(A,B),closed(B). (s=5/5,c=5/7).
5.2 Finite Element Mesh Design
In Mesh Design, the task is to learn the rules to determine
the number of elements for a given edge in the presence of
the background knowledge. Four different structures called
(b-e) in [2] are used for learning in this experiment. Then,
the structure a (having 55 examples) is used for testing the
accuracy and coverage of the induced clauses. For this ex-
periment, recursion is disallowed, support threshold is set
as 0.1, confidence threshold is set as 0.1, B is set as 1 and
maximum depth is set as 3.
In this experiment, C2D finds rules that cover 31 of the
55 records in the test data set [5]. The improved version,
C2D v2, finds rules that cover 29 of the records in the test
29085
Method Type Pred. Acc.
C2D v2 (improved ver.) ILP + DM 0.80
Ashby Chemist 0.77
PROGOL ILP 0.72
RASH Biological Potency An. 0.72
C2D (basic ver.) ILP + DM 0.69
TIPT Propositional ML 0.67
Bakale Chemical Reactivity An. 0.63
DEREK Expert System 0.57
Figure 3. Predictive accuracies for PTE-1
data set. However, the accuracy of the rules found by C2D
is 0.29, whereas the accuracy of the rules by C2D v2 is 0.49.
In other words, the improved version finds rules that have
nearly same coverage but higher accuracy according to the
basic version of C2D.
5.3 Predictive Toxicology Evaluation
The carcinogenecity tests of compounds are vital to pre-
vent cancers; however, the standard bioassays of chemicals
on rodents are really time-consuming and expensive. In the
National Toxicity Program, the tests conducted on the ro-
dents results in a database of more than 300 compounds
classified as carcinogenic or non-carcinogenic. Among
these compounds, 298 of them are separated as training set,
39 of them formed the test set of PTE-1 and the other 30
chemicals constitute the test set of PTE-2 challenge [11].
In this experiment, PTE-1 data set is used, recursion
is disallowed, support threshold is set as 0.05, confidence
threshold as 0.7, B is set as 1 and max depth is set as 3.
The predictive accuracies of the state-of-art methods and
C2D with its improved version for PTE-1 data set are listed
in Figure 3. As seen from the figure, C2D v2 has a better
predictive accuracy than the basic C2D algorithm. In addi-
tion, it finds the best results (having highest accuracy) with
respect to other systems.
6 Conclusion
This work presents an improvement for an ILP-based
concept discovery system, named C2D, which combines
rule extraction methods in ILP and Apriori-based special-
ization operator. By this way, strong declarative biases are
relaxed; instead, support and confidence values are used for
pruning the search space. In addition, C2D does not re-
quire user specification of input/output modes of arguments
of predicates and negative concept instances. Thus, it pro-
vides a suitable data mining framework for non-expert users
who are not expected to know much about the semantic de-
tails of large relations they would like to mine, which are
stored in classical database management systems.
In C2D v2, generalization step of C2D is modified in
such a way that most general rules are constructed by con-
sidering the number of occurrences of constant arguments
in the rules. By this mechanism, the effect of target instance
order on rule generation is eliminated and rule quality is im-
proved. Another improvement is on constructing transitive
rules. Since this new method does not differentiate related
and non-related facts, transitive rule generation is not lim-
ited to related facts.
The proposed system is tested on several benchmark
problems including the Michaslki’s train problem, mesh de-
sign and predictive toxicology evaluation test. The experi-
ments reveal promising test results that are comparable with
the performance of basic C2D method and current state-of-
the-art knowledge discovery systems.
References
[1] L. Dehaspe and L. D. Raedt. Mining association rules in
multiple relations. In ILP’97: Proceedings of the 7th Inter-
national Workshop on Inductive Logic Programming, pages
125–132, London, UK, 1997. Springer-Verlag.
[2] B. Dolsak and S. Muggleton. The application of Induc-
tive Logic Programming to finite element mesh design. In
S. Muggleton, editor, Inductive Logic Programming. Aca-
demic Press, London, 1992.
[3] S. Dz?eroski. Multi-relational data mining: an introduction.
SIGKDD Explorations, 5(1):1–16, 2003.
[4] Y. Kavurucu, P. Senkul, and I. H. Toroslu. Aggregation in
confidence-based concept discovery for multi-relational data
mining. In IADIS European Conference on Data Mining
(ECDM), Amsterdam, Netherland, July 2008.
[5] Y. Kavurucu, P. Senkul, and I. H. Toroslu. Confidence-based
concept discovery in multi-relational data mining. In IAENG
International Conference on Data Mining and Applications
(ICDMA), pages 446–451, Hong Kong, March 2008.
[6] R. Michalski and J. Larson. Inductive inference of vl de-
cision rules. In Workshop on Pattern-Directed Inference
Systems, volume 63, pages 33–44, Hawaii, 1997. SIGART
Newsletter, ACM.
[7] S. Muggleton. Inverse entailment and Progol. New Genera-
tion Computing, Special issue on Inductive Logic Program-
ming, 13(3-4):245–286, 1995.
[8] S. Muggleton. Inductive Logic Programming. In The MIT
Encyclopedia of the Cognitive Sciences (MITECS). MIT
Press, 1999.
[9] J. R. Quinlan. Learning logical definitions from relations.
Mach. Learn., 5(3):239–266, 1990.
[10] J. Roberto J. Bayardo, R. Agrawal, and D. Gunopulos.
Constraint-based rule mining in large, dense databases. Data
Mining and Knowledge Discovery, 4(2-3):217–240, 2000.
[11] A. Srinivasan, R. D. King, S. H. Muggleton, and M. Stern-
berg. The predictive toxicology evaluation challenge. In
Proceedings of the Fifteenth International Joint Conference
on Artificial Intelligence (IJCAI-97), pages 1–6. Morgan-
Kaufmann, 1997.
29186
