An Improved Apriori Algorithm
 Weixiao LIU*, Junli CHEN, Shifu QU, Wanggen WAN 
* School of Communication and Information Engineering, Shanghai University, Shanghai 200072, China
(Email:lweixiao2008@yahoo.cn)
Keywords: Data Mining; Association Rule; Interest Items; 
Interest Measure 
Abstract
Apriori algorithm has some abuses, such as too many scans of 
the database, large load of system’s I/O and vast unrelated 
middle itemsets. This paper proposes an improved Apriori 
algorithm to overcome these abuses. The improved algorithm 
reduces the set of candidates and accelerated the speed of the 
algorithm by adding the interest items. Breaking the 
traditional steps of the algorithm to reduce the database scans 
and bring down the load of system’s I/O. The algorithm 
improves the readability of the strong association by 
constructing the model of the interest measure. Experimental 
results show that the algorithm can improve the speed and 
efficiency of operation effectively. 
1 Introduction 
Apriori algorithm is the most classic algorithm of Association 
Rule Mining[1]. Apriori algorithm needs to scan the database 
many times to find candidates of frequent itemsets and 
generates great amount of unrelated middle itemsets during 
the iterative process. With the evaluation criteria of support 
and confidence the algorithm may generate many 
uninteresting strong association rules. The drawbacks that 
proposed above increase the load of system I/O and greatly 
affect the efficiency of the algorithm. 
Against the drawbacks of the Apriori algorithm, this paper 
presents a fast and efficient Apriori algorithm. The algorithm 
introduces the concept of user interest items and with the user 
interest items to reduce the emergence of the candidate 
itemsets[2]. It changes the realization steps to reduce the 
number of database scans and the load of system I/O. In the 
stage of strong association rules evaluation, more useful 
strong association rules are mined with the establishment of 
the user interest model. 
2  Apriori algorithm 
2.1 The steps of the Apriori algorithm 
Apriori algorithm uses iterative method layer by layer to find 
candidate itemsets. The main steps of the Apriori algorithm 
are showed as follows[3]: 
Step 1: Producing one frequent itemset 
Scaning the database D to find the frequency itemsets L1.
Step 2: Connection 
Using the connection method to have Lk (K>1). Generating 
candidate itemsets k with itemsets Lk-1 and itself. 
Step 3: Pruning 
Supposed: ck?Ck, that ck is a itemsets of candidate k, ck +1 is 
a (k-1)subset of ck , if 11  ? kk Lc , that the itemsets of 
candidate ck should be deleted from the itemsets of candidate 
Ck.
Step 4: Generating strong association rules  
Strong association rules are mined according to the minimum 
confidence level and then end of the algorithm. 
2.2 Apriori algorithm analysis 
The Apriori algorithm has certain limitation on the speed and 
efficiency as follows: 
(1) Apriori algorithm will produce a large number of middle-
itemsets. Candidate itemsets Ck are generated by Apriori-Gen 
function with itemsets Lk-1.The number of itemsets Ck is 
k
LkC 1 . Obviously that the number of candidate itemsets Ck 
will be in large quantity if the k is big enough. 
(2) Scaning the database too many times. Apriori algorithm 
needs to scan the database every time when it generates 
candidate itemsets. It will increase the load of system I/O and 
impact the speed of the algorithm when it scans massive 
database. 
(3) Misleading strong association rules are mined. Apriori 
algorithm uses evaluation criteria of support and confidence. 
Not all of the strong association rules are useful to the users. 
For example, transaction database is showed as follows. We 
use TID to mark the transaction, Items to stand for itemsets in 
the transaction, Num to stand for the times of the itemsets and 
L stand for the length of the itemsets. 
Tab.1 Example of transaction database 
TID Items Num L 
1 I1,I2 20 2 
2 I1,I3,I4 5 3 
3 I2,I5,I6 70 3 
4 I6,I7 5 3 
For itemsets (I1,I2), the support of (I1,I2) is 
Sup(I1 ? I2)=20/100=0.2 and the confidence of (I1,I2) is 
conf(I1 ? I2)=20/25=0.8. When the min_sup is not bigger 
than 0.2 and the min_con is not bigger than 0.8, Itemsets (I1,I2)
is choosed as strong association rules. However, when I1 does 
not appear, the confidence of I2 is conf = 70/75 = 0.93. In 
other words, if we suppose I1 and I2 as products the effect of 
putting itemsets(I1,I2) together is not good to put them 
separately. So we need to find another evaluation criteria:
interest measure. 
221
3 Improved Apriori algorithm 
The improved Apriori algorithm made the following 
adjustments according to the three drawbacks. 
3.1 Constraints of the user interest itemsets 
Interest items are selected by users. We use Itsn to stand for 
them. Interest itemsets are collection of interest Itsn and 
Its={its1,its2,…itsn}.
The improved algorithm uses interest itemsets to exclude non-
relevant items in the transaction database D and the length of 
the itemsets will be changes too. For example: the number of 
uninterested items in itemsets whose length is n, then the 
length of the excluded itemsets is Ln =L-n. Shown in Table 1 
as an example of transaction database, the itemsets of 
transaction database is I={I1,I2,I3,I4,I5,I6,I7}. We want to know 
the strong association rules of I1,I2,I3,I4,I5, so the interest 
itemsets is Its={I1,I2,I3,I4,I5}. The excluded transaction 
database is shown in table 2. 
Tab.2 New example of transaction database 
TID Items Num L 
1 I1,I2 20 2 
2 I1,I3,I4 5 3 
3 I2,I5 70 2 
Compared the new transaction database with the original we 
know: 
(1) The number of database scans that the new transaction 
database compared to the original is seven to five. The 
efficiency improves about 30%. 
(2) We assume that I1,I2,I3,I4,I5,I6,I7 are frequent items. The 
number of candidate three sets is about 1330 which generate 
by the original transaction database however the number of 
candidate three sets is about 120 which generate by the new. 
The efficiency improves about 91%. 
3.2 The steps of the improved algorithm 
Against too many times of database scans, the improved 
Apriori algorithm changes the steps of the traditional 
algorithm. We use arrays to store the datas and reduce the 
number of the database scans. We introduce the steps of the 
improved algorithm with table 2. 
Step 1: Traverse the new database  
The existence form of transaction itemsets in table 2 is shown 
as follows: 
Tab.3 The existence form of transaction itemsets in table 2 
TID Items Num 
1 I1 20 
1 I2 20 
2 I1 5 
2 I3 5 
2 I4 5 
3 I2 70 
3 I5 70 
The unique identifier of itemsets in transaction database is 
TID. We put the itemsets into array A according to the TID. 
We use “,”to separate the items under the same TID with 
different items and “;” to separate itemsets with different TID. 
The organizational form is A[]={I1,I2;I1,I3,I4;I2,I5,I6}.
Step 2:Traverse the array A and generate frequent itemsets 
The improved algorithm generates frequent itemsets 
according to the min_sup by traversing the array A. We put 
the frequent itemsets and the corresponding support into 
{A1,A2,…An}, n stands for the length of the itemsets. We use 
“,” to separate the itemsets and the corresponding support and 
“;” to separate the different itemsets. If we set the min_sup is 
0.2 then we have frequent itemsets as follows: 
Tab.4 Frequent itemsets 
Items I1 I2 I5 I1,I2 I2,I5
Number 25 90 70 20 70 
Sup 0.25 0.9 0.7 0.2 0.7 
The organizational form of frequent itemsets in array is 
A1[]={I1,0.25; I2,0.9; I5,0.7};A2[]={I1,I2,0.2?I2,I5,0.7}. 
Step 3: Candidate frequent itemsets 
The improved algorithm generates frequent itemsets k by 
using Ak-1 and itself to do a connection. Compared the 
generated frequent itemsets with the itemsets in array Ak then 
add the non-existed itemsets in array orderly. 
Compared the implementation steps of the improved Apriori 
algorithm with the traditional algrorithm we know: 
(1) The improved algorithm greatly reduces the number of 
database traversal and the system I / O load because it scans 
the database only once.  
(2) The improved algorithm puts the frequent itemsets and 
support into one array. With this array we can generate strong 
association rules rapidly. 
(3) We can use the sequence of the frequent itemsets in 
corresponding array to know which frequent itemsets are 
generated by itemsets in database and which are generated by 
candidate itemsets. This improvement will help users to 
achieve practical applications. 
3.3 Model of interest  
The evaluation criteria which based on the measure of support 
and confidence will not reflect the real strong association 
rules in some cases. So we need to build a new model of 
interest. 
The interest measure mainly includes the subjective interest 
measure and the objective interest measure. Now studies 
mainly focused on the objective interest measure. Some 
models of the objective interest measure are shown as follows: 
(1) The interest model of Gray and Orlowska[4] 
The interest model of Gray and Orlowska is used to assess the 
associated degree between the itemsets. The definition is 
shown as follows: 
mk ypxp
ypxp
xypI ))()(()1)
)()(
)((( u 
                
(1) 
)()(
)(
ypxp
xyp
is the deviation. k and m stand for factor 
parameters.  
(2) The rule template of Klemettinen[5] 
The rule template is an extension of syntax. It is used to 
bound which properties can appear on the left side and which 
properties can appear on the right side. The definition of the 
222
rule template is A1,A2 …Ak =>A m. A j may be a name of 
property or class. The model is proposed by users and if a rule 
matchs this model then the rule is interest. 
(3)The function of J-measure[6] 
The function of J-measure is proposed by Gray and Orlowska. 
It is the average information of probability of classification 
rules and it is the best rule to find the properties of the 
discrete rules. The definition of function is : 
)
)(1
)(1log())(1()
)(
)(log())()((
XP
YXPYXp
XP
YXpYXPYP


         
(2) 
Close analysis of the interest model that proposed above we 
know: 
(1) The parameters k and m in the Gray and Orlowska's 
interest mode does not have determined computation rule. It 
is easy to add subjective factors and affect the accuracy of the 
evaluation model. 
(2) Klemettinen’s rule template is a rule-based method. 
Although the rule template can select the valid association 
rules accordance with the rule template, it does not give the 
effective template to suit every rule. 
(3) The model of function J-measure considers the coupling 
degree of the probability distribution between X and Y. But 
the function does not consider the impact of P(Y). 
So the interest model that we build must meet the following 
requirements: 
(1) The interest model of YX ? must meet that if the 
probability of X is big then the same to the model. 
(2) The new model must consider the coupling degree 
between X and Y. The value of the model is proportional to 
the coupling degree. 
(3) From the analysis of examples in chapter 3 we know if we 
do not consider the impact of post item I2 we will elicit 
misleading strong association rules. So we must pay attention 
to the post item Y. 
(4) The interest model of YX ?  should be proportional to 
the support of X and Y. 
Consider the analysis above we set the interest model as 
follows: 
)(
))(1))((1(
)(1)( YXSup
YXconfXSup
YSupYXInte ?u
?

 ?
         
(3) 
)(
))(1))((1(
)(1)( XYP
XYPxP
YPYXInte u

 ?             
(4) 
The interest model that we proposed above meets the premise 
that X is a big probability event and consider the coupling 
degree between X and Y. The model also adds the factor of 
probability Y. The practical application value of the strong 
association rules is proportional to the interest measure. 
We use the interest model to verify the example of chapter 2.2. 
)(
))(1))((1(
)(1
)( 21
211
2
21 IISupIIconfISup
ISup
IIInte ?u
?

 ? (5)
13.02.0
)8.01)(25.01(
9.01)( 21  u

 ? IIInte
                   
(6) 
From the result we know that the interest measure of I1? I2 is
0.13. It is too low to let us trust this association rule. 
According to the new interest model and combining the 
traditional evaluation criteria of support and confident we 
make the new definition of strong association rules. We 
assume that the minimum support threshold is min_sup, the 
minimum confidence threshold is min_con and the minimum 
interest measure is min_int. We call it strong association rules 
if the association rules meet that supmin_)( t? YXSup ,
conYXCon min_)( t?  and intmin_)( t? YXInt  . 
4 Experiment results 
The algorithm uses C # as a code development platform and 
Microsoft SQL Server2005 as a database development 
platform. The number of test record is 52761 which exist in 
the vAssocSeqLineItems of AdventureWorksDW sample 
database. OrderNumber and Model will be used as identifiers 
and items of the transaction. The operation platform of the 
algorithm is shown in table 5. 
Tab.5 Experiment platform 
Experiment platform
CPU Pentium(R) Dual E2200  2.2GHz 
Memory DDR2, 2048MB 
Hard Disk ST3160815AS,160G 
The runtime that the improved Apriori algorithm compares 
with the traditional is shown in figure 1. The abscissa of the 
figure stands for the threshold of support (100 /%) and the 
ordinate stands for the runtime. 
Fig.1 The time ratio of two ways to find frequent itemsets 
We assume the min_sup is 0.01, the min_con is 0.2 and the 
min_int is 1.45. The result that the improved algorithm 
compares with the traditional is shown in table 6. 
Tab.6 Compared the improved algorithm with the traditional 
 Apriori Improved 
Apriori 
Efficiency
improvement
Frequent 
itemsets 114 44 61.4% 
Strong 
association
rules
51 20 62% 
Time 79 24 69.62% 
The experiment results show that the improved Apriori 
algorithm can quickly and efficiently get the useful strong 
association rules. Figure 1 shows that the improved algorithm 
finds frequent itemsets faster than the traditional under 
different levels of support. It reflects that the rapid 
characteristics of the improved algorithms. As can be seen 
223
from Table 6, the number ratio of frequent itemsets that the 
improved algorithm compared with the traditional is 114:44. 
This stands for that the number of superfluous itemsets is 70. 
And the number ration of strong association rules is 51:20. It 
shows that the improved algorithm can get scientific strong 
association rules accurately. 
5 Conclusions 
This paper proposes an improved algorithm of Apriori. User 
interest items and a new model of interest measure are added 
into the algorithm and the steps of the algorithm are 
restructured. Compared with the traditional algorithm the 
improved algorithm has a great advantage in both speed and 
efficiency. But there is also room for improvement, such as 
improving the integrity of the frequent itemsets and doing 
more experiments  to verify the scientificalness of the interest 
model. 
Association Rules algorithm has broad application prospects 
of the market. How to optimize the efficiency of database 
operations, how to select the more scientific min_sup, 
min_con and min_int, how to further explore the practical 
application of the algorithm will be the future research work. 
Acknowledgements 
This research is supported by National Natural Science Fund 
for Nature Program (60872115), Shanghai’s Key Discipline 
Development Program (J50104). 
References 
[1] R. AGRAWAL, R.SRIKAN. “Fast algorithms for 
mining association rules in lager databases”, Santiago: 
Proceedings of the Twentieth International Conference 
on Very Large Databases , pp. 487-499,(1994)  
[2] CHAOHUI LIU, JIANCHENG AN. “Fast Mining and 
Updating Frequent Itemsets”,Volume 1, 3-4 ,pp365 – 
368, (2008) 
[3] S.MITRA, S. K. RAL, and P.MITRA. “Data mining in 
soft computing framework: a survey”. In IEEE 
Transactions on Neural Networks, Vo1.13, No. 1, (2002) 
[4] GRAY,ORLOWSKA.”Clustering Categorical Attributes 
into Interesting Association Rules”, Proceedings of the 
2nd Pacific-Asia Conference on Knowledge Discovery 
and Data Mining, Berlin: Springer,1998?
[5] M Klemettinen, H Mannila, R Ronkainen. “Finding 
intrersting rules from large sets of discovered association 
rules” New York, USA:ACM,1994. 
[6] SYMTH P,GOODMAN R M. “An information theoretic 
approach to rule induction from databases”,IEEE Trans 
on Knowledge and Data Engineering, 4(4),pp:301-316. 
(1992) 
224
