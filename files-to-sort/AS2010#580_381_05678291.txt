An Algorithm of Fast Mining Long Frequent 
Neighboring Class Set 
 
Cheng-sheng TU, Gang FANG 
College of Math and Computer Science 
Chongqing Three Gorges University 
Chongqing 404000, P.R. China 
cqwztcs@163.com, cqwzjsjfg@163.com 
 
 
Abstract—As present frequent neighboring class set mining 
algorithms inefficiently extract long frequent neighboring class 
set, and so this paper introduces an algorithm of fast mining long 
frequent neighboring class set. To fast search long frequent 
neighboring class set in large spatial data, this algorithm uses 
down search strategy to generate candidate frequent neighboring 
class set. But the course of down search strategy used by the 
algorithm isn’t different from present down search strategy, 
which need set position of k-subset when (k+1)-non frequent 
neighboring class set generates its all k-subset. By the method, 
the algorithm may delete repetitive candidate item sets and 
redundant computing. Because the algorithm creates digital 
database of neighboring class set via neighboring class weight, 
and so it computes support of candidate frequent neighboring 
class set by digit logical operation. The algorithm improves 
mining efficiency through these methods. The result of 
experiment indicates that the algorithm is faster and more 
efficient than present algorithms when mining long frequent 
neighboring class set in large spatial data.  
Keywords- Long frequent neighboring class set; down search; 
setting position; neighboring class weight; spatial data mining 
I.  INTRODUCTION 
Geographic Information Databases is an important spatial 
database in spatial data mining, mining spatial association rules 
from Geographic Information Databases is one important part 
of spatial data mining and knowledge discovery (SDMKD), 
which is also known as spatial co-location pattern as in [1]. 
Spatial co-location pattern are some implicit rules expressing 
construct and association of spatial objects in Geographic 
Information Databases, and also expressing hierarchy and 
correlation of different subsets of spatial association or spatial 
data in Geographic Information Databases as in [2]. At present, 
in spatial data mining, three kinds approaches are mainly used 
to mining spatial association rules as in [3], such as, layer 
covered based on clustering as in [3], mining approach based 
on spatial transaction as in [2, 4, 5, and 6] and mining approach 
based on non-spatial transaction as in [3]. The first two 
methods may be also used to mining frequent neighboring class 
set, The spatial association as in [4, 5, and 6] is very single, 
because they only express spatial association among these 
objects which are all close to objective. However, neighboring 
class set expresses another spatial association among these 
objects which are close to each other. MFNCS as in [2] uses 
the similar method of Apriori to search frequent neighboring 
class set, which gains some right instance of (k+1)-neighboring 
class set only through connecting right instance of k-
neighboring class set according to down-top strategy, and so 
this algorithm is only suitable for mining short frequent 
neighboring class set according to character of Apriori. Hence, 
this paper introduces an algorithm of fast mining long frequent 
neighboring class set, denoted by MLFNCS, which is faster 
and more efficient than present algorithms when mining long 
frequent neighboring class set in large spatial data. 
II. PRELIMINARY KNOWLEDGE 
According to these basic contents as in [2], a spatial data set 
is made up of all these objects in spatial domain. These objects 
are expressed as <Object Identify, Class Identify and Spatial 
Location>. Here, identify of different class in spatial data set is 
denoted by Class Identify, identify of different object instance 
in the same class is denoted by Object Identify, location 
coordinate of object is denoted by Spatial Location. We regard 
an object as an instance of corresponding class and so these 
instances of spatial Class Identify constitute spatial data set. 
Sets of Class Identify are thought as a class set, denoted by C = 
{C1, C2…Cm}, means there are m different classes. 
A. Definition and Property 
Definition 1 Neighboring Class Set, it is a subset of spatial 
class set in spatial data set, which is denoted by {Ct1, Ct2…Ctk} 
(tk ? m), denoted by NCS. 
Let I = {it1, it2…itk} be an instance of neighboring class set 
expressed as NCS = {Ct1, Ct2…Ctk}, here, itj is an instance of 
Ctj (j?1, 2…k). 
Example, let {V, W, X, Z} be a NCS, and I = {V2, W3, X4, 
Z1} is an instance of NCS. 
Definition 2 Neighboring Class Set Length, its value is 
equal to the sum of class contained in neighboring class set. If 
the length of NCS is equal to k, it is expressed as k-NCS. 
Example, let {W, X, Y, Z} be a NCS, and its length is 4. 
Definition 3 Right Instance of Neighboring Class Set, here, 
suppose I = {it1, it2…itk} be an instance of NCS, if ?  ip and iq 
(ip, iq ? I), and distance (ip, iq) ? d, and then we think I be an 
right instance of NCS. Here, d is the minimal distance used by 
This work was fully supported by science and technology research 
projects of Chongqing Education Commission (Project No. KJ061101), and it 
was also fully supported by science and technology research projects of 
Chongqing Three Gorges University (Project No. 10QN-22). 
978-1-4244-7941-2/10/$26.00 ©2010 IEEE
judging two spatial objects are close to each other, Euclidean 
distance is denoted by distance (ip?iq). 
Definition 4 Neighboring Class Set Support, it is equal to 
the sum of right instance of neighboring class set, which is 
denoted by support (NCS). 
Definition 5 Frequent Neighboring Class Set, its support is 
not less than the minimal support given by user. 
Property If (k+1)-NCS is frequent neighboring class set, 
and then k-NCS will be also frequent neighboring class set. 
Here, k-NCS ? (k+1)-NCS. 
B. Problem Description 
According to above these definitions, property and these 
contents as in [2], we describe frequent neighboring class set 
mining as follows: 
Input: 
(1) Class set is expressed as C = {C1, C2…Cm}, instance set 
is expressed as I = {i1, i2…in}, each ik (ik? I) is expressed as 
above defined data structure. 
(2) Minimal distance is expressed as d. 
(3) Minimal support is expressed as s. 
Output: 
Frequent neighboring class set. 
III. THE ALGORITHM  OF MINING LONG FREQUENT 
NEIGHBORING CLASS SET 
A. Creating Mining Database 
If we create digital database of neighboring class set, we 
will turn every corresponding NCS of every right instance into 
a digit. The course of turning used by the algorithm is 
expressed as follows: 
Step1, firstly, we define the order of class as C = {C1, C2… 
Cj…Cm}, Here j is defined as order of class, and so each class 
inside right instance has an order expressed as Noj. 
Step2, and then, we use weight to denote every class, if 
order of this class is expressed as Noj, and this neighboring 
class weight is expressed as 12 ?jNo . 
Step3, we will gain an integer by computing this value, 
expressed as?
=
?
L
j
Noj
1
12 , here L is Neighboring Class Set Length, 
and namely, it is the sum of class inside this neighboring class 
set. 
By this method, the algorithm will gain an integer through 
scanning every right instance and this integer also express this 
corresponding NCS of right instance. 
Now we save these integers by this data structure expressed 
as follows: 
Structure neighboring class set { 
Int value; // saving integer expressed as NCS of right 
instance 
Int count; // saving the sum of same integer expressed as 
NCS} NCS 
Example, here class set is denoted by C = {V, W, X, Y, Z}, 
the first three right instances are denoted by I1 = {V5, W4, X3, 
Y2}, I2 = {X2, Y3, Z4}, I3 = {V2, W3, X4, Y5}. 
Now we use the previous introductive method to create 
digital NCS database, this course is expressed as follows: 
NCS of I1 is denoted by {V, W, X, Y}, and these orders of 
classes are denoted by {1, 2, 3, 4}, and then integer = 2(1-1) + 
2(2-1) + 2(3-1) + 2(4-1) =15, NCS [0].value =15, NCS [0].count=1. 
NCS of I2 is denoted by {X, Y, Z}, and these orders of 
classes are denoted by {3, 4, 5}, and then integer = 2(3-1) + 2(4-1) 
+ 2(5-1) =28, NCS [1].value =28, NCS [1].count=1. 
NCS of I3 is denoted by {V, W, X, Y}, and these orders of 
classes are denoted by {1, 2, 3, 4}, and then integer = 2(1-1) + 
2(2-1) + 2(3-1) + 2(4-1) =15, because NCS [0] has already saved 
this information, and only NCS [0].count=NCS [0].count+1=2. 
....... 
B. The course of generating candidate frequent neighboring 
class set 
The algorithm generates candidate frequent neighboring 
class set via down strategy. Namely, it uses computing k-subset 
of (k+1)-non frequent neighboring class set to generate k-
candidate frequent neighboring class set. The strategy is similar 
to B_UDMA as in [7] and ITDASN as in [8], but the course of 
generating subset is not different from the two kinds of method. 
Now we introduce the difference of generating course. 
We save candidate frequent item by the structure as follows: 
Structure Candidate Neighboring Class Set { 
Int value; // saving integer expressed as NCS 
Int length; //saving length of NCS 
Int No; // saving setting position of generating subset 
Int [m] location; //saving the order of class in NCS 
} CNCSet, NFSet 
As this knowledge in [9], the principle of setting position 
about k-subset of (k+1)-non frequent neighboring class set is 
expressed as follows: 
Principle Each k-subset of (k+1)-non frequent neighboring 
class set is set position only once when it is firstly generated 
from (k+1)-non frequent neighboring class set, i.e. setting 
position of subset is only one when it is saved in CNCSet.No. 
Example, here are two 4-NCS denoted by  {30, 23}, 3-NCS 
denoted by 22 is not only 3-subset of 30, but also 3-subset of 
23, obviously, it is the third time that 30 generates 22 when 30 
generates its all 3-subsets, it is the first time that 23 generates 
22 when 23 generates its all 3-subsets, and so, we know that 
setting position of 22 is not 1 but 3 because 23 will need not 
generate 22 according to the method as follows, this is the 
difference of generating k-subset. 
Let C = {C1, C2… Cj…Cm} be NCS, and let CSet save 
neighboring class weight, namely CSet[t] = 2t (t = 0…m-1). 
Suppose there are N (k+1)-non frequent neighboring class sets, 
let NFSet save them, and let CNCSet save k-subset. Aiming to 
each NFSet[i] which saves No.i k-non frequent neighboring 
class sets, these rules of generating subset are expressed as 
follows: 
Rule 1 aiming to each (k+1)-non frequent neighboring class 
sets, namely NFSet[i], the sum of k-subset generated by 
NFSet[i] is denoted by Sum = NFSet [i].length- NFSet [i].No. 
Rule 2 when NFSet[i] generates No.j k-subset, which is 
saved by CNCSet[j], and then value of its domain are 
expressed as follows: 
CNCSet[j].value=NFSet[i].value&(~CSet[NFSet [i].location [ID]]); 
CNCSet[j].length=NFSet[i].length-1; 
CNCSet[j].No=NFSet[i].No + (j-1), namely ID; 
CNCSet[j].location[0...(CNCSet[j].length-1)]=NFSet[i].location[0…ID-1, ID+1 
… (NFSet[i].length-1)]. 
If a NCS with all spatial classes, its information of data 
structure is expressed as follows: 
NCS.value=2m-1, NCS.length=m, NCS.No=0; 
NCS.location [0] = 0…NCS.location [m-1] = m-1. 
C. The method of computing support 
Accordingly to chapter A and digit logical operation, we 
may gain this property as follows: 
Property Let p and q express two right instances, let Cp be 
a NCS denoted by p, let Cq be a NCS denoted by q, then 
Cp ? Cq ? p & q=p. 
This algorithm uses logical operation to compute support 
according to this property. The process is expressed as follows: 
Suppose class set is expressed as C = {V, W, X, Y, Z}, 
there are five neighboring class sets as follows: 
NCS1 = {V, Y, Z}; 
NCS2 = {W, X, Y, Z}; 
NCS3= {V, W, X, Y, Z}; 
NCS4 = {W, X, Z}; 
NCS5 = {X, Y, Z}; 
These integers denoting neighboring class set of right 
instance are expressed as {19, 15, 31, 13 and 7}. Suppose a 
candidate is 3 which is denoted by C-NCS = {Y, Z}, and then 
19&3=3, 15&3=3, 31&3=3, 13&3=1, 7&3=3. Because of this, 
we write 4 to support (C-NCS). 
D. The process of mining frequent neighboring class set 
Input: 
(1) Spatial class set is denoted by C = {C1, C2…Cm}. 
(2) Instance set is denoted by I = {i1, i2…in}. 
(3) The minimal distance is denoted by d.   
(4) The minimal support is denoted by s. 
Output: Frequent neighboring class set. 
Step1: firstly, we computes all right instances denoted by I’ 
from instance set in spatial database as I by the minimal 
distance as d. 
Step2: and then, we create neighboring class set database as 
NCS after scanning once right instance set as I’ via the 
presented method in chapter A of III. 
Step3: we use down strategy to generate candidate frequent 
NCS according to this chapter B of III. Namely, if a (k+1)-
candidate is frequent neighboring class set, and it is written to 
FNCS which saves frequent neighboring class set, otherwise, 
the algorithm will compute k-subset of (k+1)-candidate, and 
then continue to search frequent neighboring class set. 
Step4: finally, the algorithm output FNCS according to 
chapter A of III. 
IV. THE ANALYSIS AND COMPARING OF CAPABILITY 
At present, there are little documents of research frequent 
neighboring class set. Let ATDSS express an algorithm of 
traditional down search strategy. Here we compare MFNCS as 
[2] and ATDSS with MLFNCS as follows: 
A. The Analysis of Capability 
MFNCS: this algorithm uses idea of Apriori to find 
frequent neighboring class set, which is made of three stages, 
firstly, computing all the frequent 1-NCS, secondly, generating 
all the 2-NCS by range query, and generating all the k-NCS 
(k>2) by iteration. The algorithm has some repeated computing 
and superfluous candidate frequent neighboring class set, 
which gains some right instance of (k+1)-neighboring class set 
only through connecting right instance of k-neighboring class 
set. As this algorithm adopts down-top strategy to generate 
candidate frequent neighboring class set, it is only suitable for 
mining short frequent neighboring class set. 
ATDSS: this algorithm adopts down search strategy to 
generate candidate frequent neighboring class set, which is 
made of three stages, firstly, computing the 1st (k+1)-candidate 
frequent neighboring class set which contains all classes, and 
then generating k-candidate frequent neighboring class set, and 
generating all k-frequent neighboring class set (k>3) by 
iteration. But the algorithm has some repetitive candidate and 
redundancy computing, it is inefficient when mining long 
frequent neighboring class set. 
MLFNCS: this algorithm also adopts down search strategy 
to generate candidate frequent neighboring class set, but it is 
different from ATDSS, the difference is that it can delete 
repetitive k-candidate frequent neighboring class set of (k+1)-
non frequent neighboring class set to reduce redundancy 
computing via setting position of subset in B of III. And so this 
algorithm is more efficient than ATDSS when mining long 
frequent neighboring class set. 
B. The comparing of experimental result 
Now we use experiment to testify above analyses and 
comparison. Three mining algorithms are used to generate 
frequent neighboring class set from 12267 right instances, 
whose class sets are expressed as integer from 3 to 8191, 
neighboring class set does not include any simple class, namely, 
it has two classes at least, the number of spatial class is denoted 
by m=13, the number of right instance included by these 
neighboring class set observe the discipline which is expressed 
as follows: 
NCS of integer denoted by 8191 has one right instance. 
NCS of integer denoted by 8190 has two right instances. 
NCS of integer denoted by 8189 has one right instance. 
NCS of integer denoted by 8188 has two right instances. 
… 
Our experimental circumstances are expressed as follow: 
Intel(R) Celeron(R) M CPU 420 ?  1.60 GHz, 512MB, 
language of the procedure is Visual C# 2005.NET, OS is 
Windows XP Professional. 
The experimental result of three algorithms is expressed as 
Fig.1, here support is absolute. The runtime of algorithms 
MFNCS and MLFNCS is expressed as Fig.2 as support and 
length of neighboring class set change. The runtime of 
algorithms ATDSS and MLFNCS is expressed as Fig.3 as 
support and length of neighboring class set change. 
 
Figure 1.  The experimental result 
0
10000
20000
30000
40000
50000
1000(3) 500(4) 200(5) 100(6) 50(7) 20(8) 10(9) 5(10)
Absolute Support(Length)
Ru
nt
im
e(
Mi
ll
is
ec
on
d)
MFNCS MLFNCS
 
Figure 2.  The runtime of algorithms MFNCS and MLFNCS 
0
200
400
600
800
1000
1200
1400
1600
1000(3) 500(4) 200(5) 100(6) 50(7) 20(8) 10(9) 5(10)
Absolute Support(Length)
Ru
nt
im
e(
Mi
ll
is
ec
on
d)
ATDSS MLFNCS
 
Figure 3.  The runtime of algorithms ATDSS and MLFNCS 
V. CONCLUSION 
This paper introduces an algorithm of fast mining long 
frequent neighboring class set. The result of experiment 
indicates that the algorithm is faster and more efficient than 
present algorithms when mining long frequent neighboring 
class set in large spatial data. 
ACKNOWLEDGMENT 
This work was fully supported by science and technology 
research projects of Chongqing Education Commission 
(Project No. KJ061101), and it was also fully supported by 
science and technology research projects of Chongqing Three 
Gorges University (Project No. 10QN-22). 
REFERENCES 
[1] MA, R.H., PU, Y.X., Ma, X.D.: GIS Spatial Association Pattern Ming. 
Science Press, 2007 Beijing.(in Chinese) 
[2] MA, R.H., HE, Z.Y.: Mining Complete and Correct Frequent 
Neighboring Class Sets from Spatial Databases. Journal of Geomatics 
and Information Science of Wuhan University, vol.32(2), pp.112-114, 
2007. (in Chinese) 
[3] ZHANG X.W., SU, F.Z., SHI, Y.S., ZHANG, D.D.: Research on 
Progress of Spatial Association Rule Mining. Journal of Progress in 
Geography, vol.26(6), pp.119-128, 2007. (in Chinese) 
[4] LIU, Y.L., FANG, G.: The research and application of a transaction 
complement mining algorithm. Journal of Computer Engineering and 
Applications, vol. 44(35), pp.168-170, 2008. (in Chinese) 
[5] FANG, G., WEI, Z.K., YIN, Q.: Extraction of Spatial Association Rules 
Based on Binary Mining Algorithm in Mobile Computing. In: IEEE 
Information Conference on Information and Automation, pp. 1571-1575. 
IEEE press, 2008. 
[6] FANG, G., LIU, Y.L.: Application of Binary System Based Spatial 
Mining Algorithm in Mobile Intelligent Systems. Journal of Southwest 
University (Natural Science Edition), vol.31(1), pp. 95 - 99, 2009. (in 
Chinese) 
[7] FANG, G., WEI, Z.K., YIN, Q.: The Research of Association Rules 
Mining Algorithm Based on Binary. In Proc. of International Conference 
on Cybernetics and Intelligent Systems and International Conference on 
Robotics, Automation and Mechatronics. IEEE press, vol 1. pp. 406-410, 
2008. 
[8] FANG, G., LIU, Y.L. XIONG, J., YING, H.: An Improved Top-Down 
Data Mining Algorithm for Long Frequents. Lisa O’Conner ed. 
International Conference on Artificial Intelligence and Computational 
Intelligence, IEEE press, vol. 4, pp.312-316, 2009. 
[9] Gang F., Hong Y., Jiang X., Yong-jian Z.. An Algorithm of Locating 
order Mining Based on Sequence Number, In Proc. of International 
Conference on Machine Learning and Cybernetics, IEEE press, vol 1. pp. 
403-407, 2010. 
 
