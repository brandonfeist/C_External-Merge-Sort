Weaknesses Of DAGs For Imprecise General 
Causal Representations 
Lawrence J. Mazlack 
Applied Artificial Intelligence Laboratory 
University of Cincinnati 
Cincinnati, Ohio 45221-0030 
Mazlack@uc.edu 
 
 
Abstract— Causal reasoning occupies a central position in hu-
man reasoning. In order to algorithmically consider causal 
relations, the relations must be placed into a representation 
that supports manipulation. The most widespread causal rep-
resentation is directed acyclic graphs (DAGs). However, DAGs 
are severely limited in what portion of the every day world 
they can represent. Both possible causal relationships and 
shifts in grain size are overly limited. Commonsense reasoning 
recognizes causal granularization. Sometimes, the details un-
derlying an event can be known to a fine level of detail, some-
times not; causal representations must accommodate shifts in 
grain size. Every day reasoning approaches are used that do 
not require complete knowledge. An algorithmic way of han-
dling and representing causal imprecision is needed. 
Keywords-causality; representation; common sense reason-
ing; DAGs; complexes; imprecision 
I. INTRODUCTION 
Causal reasoning occupies a central position in human 
reasoning. It has a core position in human decision-making. 
For thousands of years, philosophers, mathematicians, 
computer scientists, cognitive scientists, psychologists, 
economists, and others have formally explored causation. 
Whether causality can be precisely defined or can be 
recognized at all has long been the subject of speculation. At 
the same time, people operate on the commonsense belief 
that causality exists. 
Of particular interest to this paper are areas where the 
analysis is observational (non-experimental). The analytic 
world is not subject to experimentation. Data mining is of 
interest; one product of data mining is association rules.  
 
Customers who buy strawberries 
also tend to buy whipped cream  
with {confidence = 0.8} 
in {support = 0.15} 
Customers who 
buy beer and sausage 
also tend to buy hamburger 
with {confidence = 0.7} 
in {support = 0.2} 
Figure 1. Association rules. 
At first glance, association rules seem to imply a causal 
or cause-effect relationship. That is:  
A customer’s purchase of both sausage and beer 
causes the customer to also buy hamburger.  
But, all that is discovered is a degree of joint occurrence. 
The nature of the relationship is not understood. It is not 
known whether an item or set of items causes another item 
or set of items; or the converse, or some other phenomenon 
causes them to occur together. 
Purely accidental relationships do not have the same 
decision value, as do causal relationships. For example,  
IF it is true that buying strawberries somehow causes 
someone to buy whipped cream,  
• THEN: A merchant might profitably put strawberries 
on sale 
• AND at the same time: Increase the price of whipped 
cream to compensate for strawberries sale price 
On the other hand, knowing that  
Bread and milk are often purchased together.  
may not be useful information as the purchase of one does 
not normally influence the purchase of the other; both may 
often be independently purchased on the same store visit.  
Consequently, when typically developed, association 
rules do not necessarily describe causality. The confidence 
measure is simply an estimate of conditional probability. 
Support indicates how often the joint occurrence happens 
(the joint probability over the entire data set). The joint 
occurrence count is symmetric; that is, it does not matter 
what we count first.  The strength of any causal dependency 
may be very different from that of a paired association 
value.  In all cases 
confidence ? causal dependence 
All that can be said is that associations describe the strength 
of joint co-occurrences.  
Association rules can be used as an aid in retail mar-
keting. However, naïve use of association rules may lead to 
errors. Errors might occur; either if causality is recognized 
where there is no causality; or if the direction of the causal 
relationship is wrong [1] [2]. For example, if 
A study of our customers shows that 94% are sick. 
• Is it the following rule?  
Our customers are sick, so they buy from us. 
• Or, is it the following complementary rule? 
If people use our products, they are likely to become 
sick. 
978-1-4244-7858-3/10/$26.00 ©2010 IEEE
From a decision making view, it is not enough to know that  
People both buy our products and are sick. 
what is needed is knowledge of what causes what; if at all.  
People do things in the world by exploiting 
commonsense perceptions of cause and effect. Manipulating 
perceptions has been explored [3]. The interest here is how 
perceptions affect commonsense causal reasoning, granu-
larity, and precision. 
To precisely reason about causality, complete 
knowledge of all of the relevant events and circumstances 
would be needed. In commonsense, every day reasoning, 
approaches are used that do not require complete 
knowledge. Often, approaches follow a satisficing [4] 
paradigm.  
Commonsense understanding of the world tells us that 
we have to deal with imprecision, uncertainty and imperfect 
knowledge. A way of handling imprecision is needed to 
computationally handle causality. A difficulty is striking a 
good balance between precise formalism and commonsense 
imprecise reality. 
II. COMPLEXES 
In many ways, causality is granular. This is true for 
commonsense reasoning as well as for more formal 
approaches. Commonsense perception of causality is often 
large grained while the underlying causal structures may be 
more precisely described in a more fine-grained manner. A 
comprehensive causal representation structure should be 
able to accommodate changes in grain size. 
In applying commonsense causal reasoning, it may be 
recognized that a complex collection of causal elements can 
result in a particular effect, even if the precise elements of 
the complex are unknown. It may not be known what events 
are in the complex; or, what constraints and laws the com-
plex is subject to. Sometimes, the details underlying an 
event are known to a fine level of detail, sometimes not. 
Events are rarely known to the finest possible grain size.  
When events happen, there are usually other related 
events. The entire collection of events can be called a com-
plex. A “mechanism” [5] or a “causal complex” [6] [7] is a 
collection of events whose occurrence or non-occurrence 
results in a consequent event happening.  
Each complex, taken as a whole, can be considered to be 
a granule if the grain size is increased. Larger complexes 
can be decomposed into smaller complexes; going from 
large grained to small grained. For example, when describ-
ing starting an automobile, A large-grained to small-grained, 
nested causal view could start with  
When an automobile’s ignition switch is turned on, this 
causes the engine to start.  
But, it would not happen if a large system of other nested 
conditions were not in place.  
There has to be available fuel. The battery has to be 
good. The switch has to be connected to the battery so 
electricity can flow through it. The wiring has to connect 
the switch to the starter and ignition system (spark 
plugs, etc.). The engine has to be in good working or-
der; and so forth.  
Turning the ignition switch on is one action in a complex of 
conditions required to start the engine. The largest grained 
view is: turning on the switch is the sole causal element; the 
complex of other elements represents the finer grains. These 
elements in turn could be broken down into still finer grains; 
for example, “available fuel” could be broken down into:  
fuel in tank, working fuel pump, intact fuel lines, etc. 
Sometimes, it is enough to know what happens at a large 
grained level; at other times it is necessary to know the fined 
grained result. For example, if  
Bill believes that turning the ignition key of his automo-
bile causes the automobile to start. 
It is enough if  
Bill engages an automobile mechanic when his automo-
bile does not start when he turns the key on.  
As the automobile mechanic knows a finer grained view 
of an automobile’s causal complex than does Robin. 
Nested granularity may be applied to causal complexes. 
A complex may be several larger grained elements. In turn, 
each of the larger grained elements may be a complex of 
more fine-grained elements. In turn, these elements may be 
made up still finer grained elements. In general, people are 
more successful in applying commonsense reasoning to a 
few large grain sized events than the many fine-grained 
elements that might make up a complex.  
III. PRECISELY DEFINING CAUSAL RELATIONSHIPS 
Coming to a precise description of what is meant by cau-
sality is difficult. There are multiple and sometimes con-
flicting definitions.  Zadeh [8] suggested that a precise, for-
mal definition may not be possible. To this, Pearl [9] sug-
gested: “For me, the adequacy of a definition lies not in 
abstract argumentation but in whether the definition leads to 
useful ways of solving concrete problems.”  Regardless, we 
do have a commonsense belief that there are causal relation-
ships. Satisfactorily and explicitly specifying them may 
sometimes be difficult. This paper is not dependent on 
precise notation; it approaches the issues from a common-
sense view and occasionally comments on more formal 
notations. 
A common, simplistic conception [10] is that causality 
depends on one-way, time ordering. In contrast, Simon [11] 
[12] provides an analysis of causality that does not rely on 
time order. For a more extensive definition of various 
aspects of causality definition, see Mazlack [13]. 
Perhaps, complete knowledge of all possible situational 
factors might lead to a crisp description of whether an effect 
will occur. However, it is unlikely that the identity all possi-
ble elements can or may come to be known. It is also un-
likely that it may be possible to fully know the certainty all 
of the elements involved. Consequently, the extent or actu-
ality of missing elements may not be known. Additionally, 
some well described physics as well as neuro-biological 
events appear to be truly random [14]; and some mathemati-
cal descriptions randomly uncertain. Consequently, there is 
no way of avoiding causal imprecision. 
Friedman [15] argues that any cause that we isolate is 
never the whole cause and that every direct cause itself has 
its own direct causes, so that networks of causation spread 
synchronically across the economy and diachronically back 
into the mists of time. If this is true, granules must neces-
sarily be imprecise as separation through truncation from a 
network would be implicitly required by any representation. 
A. Positive Causation 
Commonsense understanding of causation focuses on 
positive causation. Causal relationships exist in the com-
monsense world; for example, 
When a glass is pushed off a table and breaks on the 
floor 
it might be said that  
Being pushed from the table caused the glass to break. 
Although,  
Being pushed from a table is not a certain cause of 
breakage; sometimes the glass bounces and no break-
age occurs; or, someone catches the glass before it hits 
the floor. 
Counterfactually and more weakly, usually 
Not falling to the floor prevents breakage.  
A counter factual statement is weaker as other factors can 
come into play; for example, sometimes  
A glass breaks when an errant object hits it, even 
though it does not fall from the table.  
Positive causal relationships can be described as: if ? 
then ? (or, ? ? ?). For example:  
When an automobile driver fails to stop at a red light 
and there is an accident it can be said that the failure to 
stop was the accident’s cause.  
However, negating the causal factor does not mean that the 
effect does not happen, sometimes effects can be overdeter-
mined. For example: 
An automobile that did not fail to stop at a red light can 
still be involved in an accident; another car can still hit it 
for a number of reasons, such as the other car’s brakes 
failing. 
B. Negation 
Simple negation does not work; both because an effect 
can be overdetermined and because negative statement is 
weaker than positive statements as the negative statements 
can become overextended. It cannot be said that ¬? ? ¬?, 
for example: 
Failing to stop at a red light is not a certain cause of no 
accident occurring; sometimes no accident at all occurs. 
Instead of being concerned with all of the fined grained 
detail, a better approach may be to use larger grains to sof-
ten the need for preciseness.  
Negation or counterfactuals in causal reasoning has a 
place; although it may result in reasoning errors. One reason 
is that effects can be overdetermined; that is: more than one 
item can cause an effect. If so, eliminating one cause does 
not necessarily eliminate the effect. For example, the rule:  
If a person drinks wine, they may become inebriated.  
cannot be simply negated to  
If a person does not drink wine, they will not become 
inebriated. 
In this case: 
A person may also drink beer or whiskey to excess and 
become inebriated. 
 
Figure 2. Overdetermined effect (inebriation) 
Events that do not happen can similarly be overdetermined. 
From a commonsense reasoning view, it is more likely that 
things do not happen than they do. For example, Ortiz [16] 
states that it is not true that 
Closing the barn door caused the horse not to escape. 
because the horse might not have attempted to escape even 
if the door was open. Therefore, a false counterfactual is: 
If he had not closed the barn door, the horse would 
have escaped. 
Similarly, for example, the rule  
If a person smokes, they will get cancer.  
cannot be simply negated to 
 If a person does not smoke, they will not get cancer.  
Again, effects can be overdetermined. In this case,  
People who do not smoke may still get cancer from 
other causes. 
Negative causal relationships are less sure; but often stated; 
for example, it is often said that: 
Not walking under a ladder prevents bad luck.  
Or, usually (but not always),  
Stopping for a red light avoids an accident.  
Other ideas that are sometimes involved in causal rea-
soning are causal uncorrelatedness [17] where if two vari-
ables have no common cause they are causally uncorrelated. 
Similarly, Dawid [18] speaks in terms of unresponsiveness 
and insensitivity. If ? is unresponsive to ? if whatever the 
value of ? might be set to, the value of ? will be unchanged. 
In parallel, if ? is insensitive to ? if whatever the value ? 
may be set, the uncertainty about ? will be unaffected.  
Some describe events in terms of enablement and use 
counterfactual implication whose negation is implicit; for 
example [16]: 
Not picking up the ticket enabled him to miss the train. 
Along the same vein, Shoham [19] [20] distinguishes be-
tween causing, enabling, and preventing. The enabling fac-
tor is often considered to be a causal factor. Shoham distin-
guished between background (enabling) conditions and 
foreground conditions. The background (enabling) condi-
tions are inferred by default. For example [20]:  
“If information is present that the key was turned and 
nothing is mentioned about the stated about the state of 
the battery, then it is inferred that the motor will start, 
because the battery is assumed, by default to be alive.” 
Given this distinction, causing is taken to refer to the fore-
ground conditions where enabling and preventing refer to 
the background conditions (in this example, turning the key 
causes the motor to start, the live battery enables it, the 
dead battery prevents it).” 
C. Inherently Uncertain Recognition 
Recognizing many things with absolute certainty is 
problematic. As this is the case, our causal understanding is 
based on a foundation of inherent uncertainty and incom-
pleteness. Consequently, causal reasoning models must 
accommodate inherent ambiguity. For a more detailed dis-
cussion of inherently uncertain recognition, see [21]. 
IV. CAUSALITY RECOGNITION & REPRESENTATION 
Various causality descriptions and discovery tools have 
been suggested. It may eventually turn out that different 
subject domains may need different methodologies. 
Hobbs [6] uses first order logic to describe causal com-
plexes. Pearl [22] develops probabilistic causal networks of 
directed graphs (DAGs). The causal complexes explicitly 
considered by Hobbs and Pearl have a required structure 
that may be overly restrictive for commonsense causal un-
derstanding, namely: 
• If all of the events in the causal complex appropriately 
happen, then the effect will occur 
• There is nothing in the causal complex that is irrelevant to 
the effect 
These requirements are probably too precise and extensive 
to be realized in a commonsense world. Sometimes, only 
some of the events need to happen. For example,  
Someone may be able to save more money: 
• If their taxes are lowered or  
• If they earn more money.  
Either even may lead to greater savings. However,  
Neither may result in increased savings if they also have 
to pay a large divorce settlement.  
So, if all of the events happen, the effect may happen. If 
some of the events happen, the effect may happen. In the 
commonsense world, we rarely know whether all of the 
events are in a complex are necessary. For example,  
A man may want to attract the attention of a woman. He 
may do a large number of things (e.g., hair, clothes, 
learn to dance, etc.). If he does attract the woman, he 
may never know which things were relevant and which 
were not 
V. DIRECTED GRAPHS AND CAUSALITY 
Different aspects of causality have been examined. The 
idea of “positive” causation (? ? ?) is at the core of com-
monsense causal reasoning. Often a positive causal relation-
ship is represented as a network of nodes and branches [21]. 
!"
 
Figure 3. Diagram indicating that a is causally dependent on b. 
Various graph based Bayesian based methods have been 
suggested to describe causality. Probably the best known is 
the class of methods based on Directed Acyclic Graphs 
(DAGs). The most fully developed approach is Pearl [22]. 
Silverstein [23] [24] followed a similar approach. 
Pearl [25] and Spirtes [26] claim that it is possible to in-
fer causal relationships between two variables from associa-
tions found in observational (nonexperimental) data without 
substantial domain knowledge. Spirtes claims that directed 
acyclic graphs could be used if (a) the sample size is large 
and (b) the distribution of random values is faithful to the 
causal graph. Robins [27] argues that their argument is 
incorrect. Lastly, Scheines [28] claims that only in some 
situations will it be possible to determine causality. Their 
discussion is tangential to the focus of this paper; going 
deeply into their discussion is outside this paper’s scope. 
This paper focuses on the inadequacies of DAGs. 
From the commonsense causal reasoning view, the vari-
ous directed graph methods have similar liabilities, specifi-
cally: 
A. Liability: Cyclic needs that cannot be represented in a 
DAG.  
Causal relationships are not cyclic in a DAG, either directly 
or indirectly (through another attribute).  
This is at variance with our commonsense understanding 
of the world. Within cyclic dependencies, there are variants. 
Liability: Representing cycles with time lag: feedback 
There are many commonsense examples where cycles are 
needed. 
 
Figure 4. Positive feedback cycle: Robin tells Kim that I love you. Then, 
Kim tells Robin I love you. Then, Robin tells Robin I love you 
more than before. Then, Kim … and so forth and so on. The cy-
clic reinforcement could be substantial. 
In some cases, some cycles can be reasonably collapsed, 
some cannot be. In the following example, perhaps the right 
hand cycle can be collapsed without a loss of significant 
meaning. The problem with collapsing a feedback cycle is 
the loss of knowledge that a process is going on; also, iden-
tifying stable values may be difficult. 
 
Figure 5. Cyclic relationship that can be collapsed if process knowledge 
loss is acceptable. 
 
Figure 6. Cyclic relationship that cannot be collapsed 
Liability: Representing: concurrent cycles. A form of a 
cycle is concurrent joint mutual dependency with no time 
lag. Mutual dependencies are possible; i.e., ? ? ? as well as 
? ? ?. It seems to be possible that they do so with different 
strengths with equal strengths being a special case. The 
dependencies can be described as shown in the following 
figure where Si,j represents the strength of the causal de-
pendency from i to j .  
!
!,"
S
S
",!
"
 
Figure 7. Cyclic relationship: Mutual dependency. 
There are two variations: differing causal strengths for 
the same activity; and, different causal strengths for 
symmetric activities occurring at different times. 
Different causal strengths for the same activity, simultane-
ously occurring 
Some argue that causality should be completely asym-
metric and if it appears that items have mutual influences it 
is because there is another cause that causes both. A prob-
lem with this is that it can lead to eventual regression to a 
first cause. Whether this is true or not, it is not useful for 
commonsense representation. In contrast, Simon [5] and 
Shoham [20] identify cases where causality is simultaneous.  
It is also our commonsense experience. For example, in 
the preceding figure, ? could be short men and ? could be 
tall women. If S?,? meant the strength of desire for a social 
meeting that was caused in short men by the sight of tall 
women, it might be that S?,? > S?,? . 
Different causal strengths for symmetric activities, occur-
ring at different sequential times 
It would seem that if there were causal relationships in 
market basket data, there would often be imbalanced de-
pendencies. For example, if  
A customer first buys strawberries, there may be a rea-
sonably good chance that they will then buy whipped 
cream.  
Conversely, if  
They first buys whipped cream, the subsequent pur-
chase of strawberries may be less likely.  
B. Liability: Markov Stationary Condition holds: Probabili-
ties are time independent 
This does not correspond to our commonsense world under-
standing. If one event is dependent on two other events, if 
one causing event happens much earlier (or later) than the 
other causing event, there may well be a different result. 
 
Figure 8. Case where differing times in causal events affects probability of 
causal result. 
C. Liability: The Markov Condition holds: Memoryless 
States 
The Markov Condition is defined as: Let A be a node in a 
causal Bayesian network, and let B be any node that is not a 
descendant of A in the network. Then the Markov (Markoff) 
condition holds if A and B are independent, conditioned on 
the parents of A. The intuition of this condition is: If A and 
B are dependent, then B must either be (a possibly indirect) 
cause of A or (possibly indirectly) caused by A. In the sec-
ond case, B is a descendant of A, while in the first B is an 
ancestor of A and has no effect on A once A’s immediate 
parents are fixed. This makes sense in the example in the 
following figure. 
 
Figure 9. “Memoryless” Markov condition holds 
 
However, not all of our commonsense perceptions of 
causality work this way. Often, we believe that history mat-
ters as in the example shown in the following figure. 
 
Figure 10. Causality where memory play a part. 
D. Liability: Discrete or continuous data must be reduced 
to Boolean values 
This is an early technique that was and is used in data 
mining when analyzing market basket data. However, it is 
essentially flawed. Quantities do matter; some data co-oc-
currences are conditioned on there being a sufficiency of a 
co-occurring attribute. Also, some relationships may be 
non-linear based on quantity [2]. 
E. Liability: There can be no missing data 
This is at variance with day-to-day experience. There is 
almost always missing data of some sort. Data collection is 
rarely fully representative and complete. Incremental data is 
often acquired that is at variance with previously acquired 
data. What is needed is a methodology that is not brittle in 
the face of incompleteness. 
VI. CONCLUSIONS 
Causal reasoning occupies a central position in human 
reasoning.  
In many ways, causality is granular. Commonsense rea-
soning recognizes granularization and that objects may be 
made up out of granules. Knowledge of at least some causal 
effects is imprecise. Perhaps, complete knowledge of all 
possible factors might lead to a crisp description of whether 
an effect will occur. However, it is unlikely that all possible 
factors can be known. Commonsense understanding of the 
world deals with imprecision, uncertainty and imperfect 
knowledge. Even if the precise elements of the complex are 
unknown, people recognize that a complex collection of ele-
ments can cause a particular effect. They may not know 
what events are in the complex; or, what constraints and 
laws the complex is subject to. Sometimes, the details un-
derlying an event can be known to a fine level of detail, 
sometimes not. A causal model must accommodate shifts in 
grain size as well as imprecision and incompleteness. 
There are several needs of a causal model. Some are: 
• Represent imprecision: DAGs OK 
• Accommodate changes in grain size: DAGs not OK as not 
all cycles can be collapsed 
• Describe complexes: DAGs OK 
• Avoid over determination and over extension: Graphs in 
general, including DAGs, not OK 
• Support cyclic models of all kinds: DAGs not OK as can-
not represent any kind of cycles 
• Be time varying: DAGs not OK 
• Not be restricted by Markov conditions: DAGs not OK, 
they must meet Markov conditions 
• Handle incompleteness: DAGs, not OK; other models 
such as fuzzy model OK 
In order to algorithmically consider causal relations, the 
elements must be placed into a representation that supports 
manipulation. The most widespread causal representation is 
directed acyclic graphs (DAGs). However, DAGs are se-
verely limited in what portion of the common sense world 
they can represent. 
REFERENCES 
[1] S. Brin, R. Motwani, and C. Silverstein, "Beyond Market Baskets: 
Generalizing Association Rules To Correlations," presented at ACM 
SIGMOD International Conference, SIGMOD'97, 265-276, SIGMOD 
Record, 1997. 
[2] L. J. Mazlack, "Inexact Multiple-Grained Causal Complexes," in 
Studies in Computational Intelligence (SCI), vol. 118. Berlin: 
Springer-Verlag, 2008, 231-249. 
[3] L. A. Zadeh, "From Computing With Numbers To Computing With 
Words - From Manipulation Of Measurements To Manipulation Of 
Perceptions," IEEE Transactions on Circuits and Systems, vol. 45, 
pp. 108-119, 1999. 
[4] H. A. Simon, "A Behavior Model Of Rational Choice," Quarterly 
Journal of Economics, vol. 59, pp. 99-118, 1955. 
[5] H. A. Simon, "Nonmonotonic Reasoning and Causation: Comment," 
Cognitive Science, vol. 15, pp. 293-300, 1991. 
[6] J. R. Hobbs, "Causality," presented at Common Sense 2001, Fifth 
Symposium on Logical Formalizations of Commonsense Reasoning, 
145-155, New York University, New York, 2001. 
[7] J. R. Hobbs, "Causality And Modality: The Case Of ‘Would’," 
Journal of Semantics, 2003. 
[8] L. A. Zadeh, "Causality Is Undefinable," presented at Abstract Of A 
Lecture Presented At The BISC Seminar, University of California, 
Berkeley, January 16, 2001, 
http://www.cs.berkeley.edu/~nikraves/zadeh/Zadeh2.doc, 2001. 
[9] J. Pearl, "From a web page supporting Pearl’s 2000 book, in reply to a 
question, Date: September 28, 2001; From: Sampsa Hautaniemi, 
NIH; Subject: Zadeh's 'Causality Is Undefinable', 
http://bayes.cs.ucla.edu/BOOK-2K/hautaniemi.html," 2001. 
[10] C. Granger, "Investigating Causal Relations By Econometric Models 
And Cross-Spectral Methods," Econometrica, vol. 37, pp. 424-438, 
1969. 
[11] H. A. Simon, "On The Definition Of The Causal Relation," Journal of 
Philosophy, vol. 49, pp. 517-528, 1952. 
[12] H. A. Simon, "Causal ordering And Identifiability," in Studies in 
Econometric Method, Cowles Commission for Research in 
Economics, vol. 14, W. Hood and T. Koopmans, Eds. New York, 
N.Y.: Wiley and Sons, 1953, 49–74. 
[13] L. J. Mazlack, "Discouvering Mined Granular Causal Complexes," 
presented at IEEE International Conference on Data Mining (ICDM), 
Brighton, United Kingdom, 2004. 
[14] W. Freeman, Societies Of Brains: Lawrence Erlbaum, 1995. 
[15] M. Friedman, "The Marshallian Demand Curve," Journal of Political 
Economy, vol. 57, pp. 463-495, 1949. 
[16] C. L. Ortiz, "A Commonsense Language For Reasoning About 
Causation And Rational Action," Artificial Intelligence, vol. 108, pp. 
125-178, 1999. 
[17] G. Shafer, "Causal Conjecture," in Causal Models and Intelligent 
Data Management, A. Gammerman, Ed. Berlin: Springer-Verlag, 
1999. 
[18] A. Dawid, "Who Needs Counterfactuals?" in Causal Models and 
Intelligent Data Management, A. Gammerman, Ed. Berlin: Springer-
Verlag, 1999. 
[19] Y. Shoham, "Nonmonotonic Reasoning And Causality," Cognitive 
Science, vol. 14, pp. 213-252, 1990. 
[20] Y. Shoham, "Remarks on Simon's Comments," Cognitive Science, 
vol. 15, pp. 301-303, 1991. 
[21] L. J. Mazlack, "Commonsense Causal Modeling In The Data Mining 
Context," presented at IEEE International Conference on Data Mining 
series (ICDM), Melbourne, Florida, 2003. 
[22] J. Pearl, Causality. New York, NY: Cambridge University Press, 
2000. 
[23] C. Silverstein, S. Brin, and S. Mani, "Beyond Market Baskets: 
Generalizing Association Rules To Dependence Rules," Data Mining 
And Knowledge Discovery, vol. 2, pp. 39-68, 1998. 
[24] C. Silverstein, S. Brin, R. Motwani, and J. Ullman, "Scalable 
Techniques For Mining Causal Structures," presented at International 
Conference on Very Large Databases, 594-605, New York, NY, 
1998. 
[25] J. Pearl and T. Verma, "A Theory Of Inferred Causation," presented 
at Principles Of Knowledge Representation And Reasoning: 
Proceedings Of The Second International Conference, 441-452, 1991. 
[26] P. Spirtes, C. Glymour, and R. Scheines, Causation, Prediction, and 
Search. New York: Springer-Verlag, 1993. 
[27] R. Robins and L. Wasserman, "On The Impossibility Of Inferring 
Causation From Association Without Background Knowledge," in 
Computation, Causation, and Discovery, C. Glymour and G. F. 
Cooper, Eds. Menlo Park: AAAI Press/MIT Press, 1999, 305-321. 
[28] R. Scheines, P. Spirtes, C. Glymour, and C. Meek, Tetrad II: Tools 
For Causal Modeling. Hillsdale, NJ: Lawrence Erlbaum, 1994.
 
 
 
