Based on rough set of associative rules improve algorithm of data mining
 
Zhangkun    
Business College 
Liaoning technical university 
Huludao, China 
Zhangkunkun1984@163.com 
 
shaoliangshan    
Business College 
Liaoning technical university 
Huludao, China 
lnsongxia@sina.cn 
 
Abstract—According to information system theory and from 
the equivalent and support the concept of view, it is easy to 
find the frequency of collection and confirm the relevant rules 
of coarse. 
Under in-depth and systematic research on RS theory and 
associate rules mining algorithms, This paper make some 
improvement based on original algorithms. The first and the 
foremost, this paper proposes an efficient algorithm for 
counting core and a reduction algorithm of attributes based on 
discernibility matrix which can handle the knowledge system 
and make the extraction of decision-rules convinent. Secondly, 
it put forward a mining model of association rules with 
decision attributes based on Apriori ,AprioriTid and 
AprioriHybrid algorithms,which also optimize them. 
Keywords-Data Mining?Rough Set ?Asscoiate Rule  
I. INTRODUCTION 
Mining (Data Mining) is extracted from the mass of data 
known in advance, understandable, and ultimately the 
information available for the user, and knowledge. And the 
associated rules of research is a very hot of data mining. 
Purpose of mining association rules from large transaction 
databases mining history is to found the association between 
the project, The association between the items, including two 
steps: (l) to identify all frequent itemsets. (2) frequent 
itemsets generated by the trust to meet the minimum 
threshold rule Currently, the vast majority of studies focus 
on the first step, in which the most classic Apriori algorithm, 
most descendants to improve on this basis  
Rough set theory is a new deal with fuzzy and uncertain 
knowledge of the mathematical tools, the idea is to maintain 
the premise of the same classification ability, through 
knowledge reduction, export decision-making or 
classification rules. 
II. CONCEPTS AND MODELS OF  
ASSOCIATION  RULES  
Given a set of projects I=(I1,I2…Im) And a transaction 
database D=(t1,t2…tn) Among 
ti={ Ii1,,Ii2,…,Iim} and Iij I? ?Association rule is of the 
form X=>Y?among 
X,Y? I Is a collection of two projects, known as the 
project set and XnY=?? 
Association rule X => Y's support?sup (X => Y) is a 
database that contains X Y in the?  transaction account 
database the percentage of all transactions  
that sup (X => Y) = p (X Y).?  
Association rule X => Y's confidence: conf (X => Y) is 
the transaction that contains X  Y contains x number and ?  
 
ratio of the number of services that conf (X => Y) = sup 
(X  Y) / sup (X).?  
 
Association rule X => Y's confidence: conf (X => Y) is 
the transaction that contains X  Y contains x number and ?
ratio of the number of services that conf (X => Y) = sup (X 
 Y) / sup (X).?  
Confidence of association rules is a measure of accuracy, 
support is a measure of the importance of association rules. 
Help support this rule in all matters the representation of the 
extent, obviously the greater the support, the more to 
association rules. Although some confidence association 
rules is high, but the support is very low, indicating that the 
association rules and practical opportunity to small, it is not 
important. the problem of mining association rules is found 
to have a user-specified minimum support and minimum 
confidence of all association rules, which found that the 
support of association rules and confidence are not less than 
the minimum support and minimum confidence. 
III. ROUGH SET THEORY IN ASSOCIATION RULES MINING 
In practical application, a lot of knowledge are based on 
information form, Information systems generally deal with 
the following main steps: First, data preparation, including 
data discretization, data cleaning, depending on the issue of 
the form given information table knowledge representation 
system, incompatible with the object and remove redundant 
objects, the decision to establish the compatibility table is 
prepare  for the data reduction. And then examine whether 
the conditional attribute can be omitted, get the simple 
attribute set, a multi-lateral compression of information table, 
if the information table reflects the control rules, then the 
equivalent of all the control rules to reduce the antecedent 
conditions. On this basis, on the basis of value reduction to 
reduce the number of properties and individuals, the final 
extraction rules is to access information systems inherent 
laws. Using rough set theory for data mining, extraction of 
knowledge rules, the most important thing is based on rough 
set attribute reduction and rule redundancy value reduction. 
2010 International Conference on Computer Application and System Modeling (ICCASM 2010)
V11-236C978-1-4244-7237-6/$26.00     2010 IEEE
Through some simple operations, reduce the dimension 
attribute, summed up the knowledge for decision support in 
the rules is rough set theory is one of the most important 
applications.  
 
 
IV. DECISION-MAKING SYSTEM AND DECISION RULES 
APPLIED TO THE ASSOCIATION RULE MINING 
Rough set theory in decision-making systems and 
decision rules applied to the concept of mining association 
rules, attribute rules can also limit; before proceeding to 
association rules mining, to improve the efficiency of mining 
association rules. Mining Decision table or something before 
the association rules in the database, should be handled in 
accordance with the following general steps to reduce the 
mining complex, reduce errors, false, redundant rule 
generation 
Processing steps are as follows: 
Step 1: data preprocessing, the continuous attributes in 
decision table will be converted to discrete values. 
Step 2: Using rough set reducts of condition attributes, delete 
redundant attribute, use the resolution matrix (difference 
matrix) to complete the reduction and the core. When there 
are several core values, core values with the smallest 
component to extract simple rules. 
Step 3: Mining association rules is based on Boolean, 
continuous attribute values to go through a cluster analysis to 
classify the property value, obtained after the reduction of 
decision table into a Boolean type. The decision table can be 
treated as transaction databases. 
V. WITH CONCLUSIONS BASED ON ROUGH SET OF 
ASSOCIATION RULES MINING. 
A. Algorithm Apriori-MARDA Introduction 
In this paper, research project constraints algorithm, 
Apriori algorithm is improved by, a band of Association 
Rules Mining Algorithm Apriori-MARDA (Mining 
Associate Ruleswith Decision Atrribute) and optimized 
variants.The example indicates that this method can reduce 
the association rule mining of time and space complexity, 
improve efficiency of data mining. 
How to improve the efficiency of the current rules 
mining algorithm of association rule mining is an important 
research topic, the relevance of the general conclusion is no 
domain, although it can try to find the relationship between 
the various properties and dependence, and to fully explore 
the database .But aimlessly analysis have to pay a high price, 
when there is a definite goal, which is concluded when the 
domain is known (from the constraints of the project point of 
view are bound, that is, only certain items can appear in the 
rules right, as a rule after the piece), can reduce the number 
of candidate sets, the search space, and count, so to some 
extent improved the efficiency of the algorithm 
Apriori-MARDA algorithm aims to use the reduction of 
rough directly from the decision table after the extract with 
the rules of Association. Some scholars have raised the 
rough set decision table reduction, and then extracted 
directly from decision rules (with the conclusion of 
Association Rules) This idea, and verified by this method for 
mining association rules are meaningful, without modeling, 
to achieve such further work 
B. Apriori-MARDA algorithm model 
 First, the decision table using the previously mentioned 
steps to deal with common, the property values of all 
properties unique number, and then, using Apriori-MARDA 
algorithm to generate all the frequent item sets frequent 
itemsets generated by the length of the property does not 
exceed the conditions number with the number of decision 
attributes. 
To facilitate the description below, are agreed as follows: 
Decision table established by the transaction processing 
database (Transactions Database) to T. 
Lk: with minimum support set of frequent k itemsets  
Ck: Candidate itemset of k-dimensional set of (potentially 
the largest itemset)  
c: 1 ~ k-dimensional itemsets  
c ': 1 ~ k frequent itemsets  
d ': 1 ~ k frequent itemsets  
Count (C): condition attribute dimension  
Count (D): decision attribute dimension  
This Apriori-MARDA algorithm is as follows:  
Input: a database T, condition attribute set C, decision 
attribute set D, minimum support threshold (minsup)  
Output: T of all frequent itemsets  
(1) L1 = find_frequent_Itemset (large 1-itemsets); / * 
generate L1 * /  
(2) L2 = GenerateL2 (c '? L1, d' ? L1); / * generate C2 
and L2, with the SM-MARDA to store and calculate all the 
possible frequent 2 item set support * /  
(3) for (k = 3; k <= Count (D) + Count (C); k ++)/* 
produce L3 ~ frequent Count (D) + Count (C) itemset * /  
(4) (Ck = apriori_genC (c '?  Lk-1, d' ?  Lk-1) ? 
apriori_genD (c '? Lk-1, d' ? Lk-1); / * generate new 
candidate itemsets * /  
(5) for each transactions t ? T  
(6) (Ct = subset (Ck, t);  
(7) for each New candidates c ? C t  
(8) (c.count + +;  
(9) if (c.count> = minsup) / * c is greater than minsup is 
added Lk * / 
(10) Lk = (c ? Ck | c.count> = minsup); 
) 
Data Preprocessing 
Decision Table Reduction 
Rules acquisition 
Original information sheet 
2010 International Conference on Computer Application and System Modeling (ICCASM 2010)
V11-237
   ) 
     ) 
(11) Answer = ? Lk; 
 Algorithm GenerateL2 function is used to generate L2, 
and the function is divided into two steps: Join (link) and 
Prune (pruning). In the connection step, the frequent 
one-dimensional set of decision attribute value of item 1-D 
conditions and frequent item sets from the connection 
attribute values, until all the candidates generated two sets, 
functions described as follows: 
insert into C2 
(Select p [1], q [1] / * p, q as frequent a set * / 
from L1 p, L1 q 
where p [1] <> q [1] 
) 
Pruning function described as follows: 
for each transactions t ? T 
(Ct = subset (C2, t); 
for each New candidates c ? Ct 
(C.count + +; 
if (c.count> = minsup) / * c is greater than minsup are 
joined L1 * / 
L2 = (c ? C2 | c.count> = minsup) 
) 
) 
)   
)  
Algorithm in the function apriori_genD and apriori_genC 
can produce all the candidate itemsets. Their input 
parameters for the Lk-1, Lk-1 that all frequent k-1 itemsets, 
eventually returned to Ck, that all candidate k itemsets. Two 
numbers the same letter has two steps: Join and Prune. 
First of all, apriori_genD in the Join step, through the Lk-1 
in pre-k-2 Same as items of the item sets from the 
connection operations to obtain a superset of Ck, the steps 
described as follows:  
insert into Ck  
(Select p [1], q [1], p [2], q [2], ..., p [k-1], q [k-1] / * p, q 
k-1 for the frequent item sets * /  
from Lk-1 p, Lk-1 q 
where p [1] = q [1], ..., p [k-2] = q [k-2], p [k-1] <q [k-1]  
) 
Then, in the Prune steps that have been obtained for each 
k Ck itemsets c, if c k-1 some of the item set  
Not in Lk-1 in, it will delete c from Ck. The steps 
described below:  
for each New candicate c ? Ck  
for each s ? subset (c, k-1) 
if (! (s ? Lk-1))  
(Delete c from Ck;) 
Secondly, apriori_genC in the Join step, through the Lk-1 
in Same as items after the k-2 item sets from the connection 
operations to obtain a superset of Ck, the steps described as 
follows:  
insert into Ck  
(Select p [1], q [1], p [2], q [2] ..., p [k-1], q [k-1] / * p, q 
k-1 for the frequent itemsets * /  
from Lk-1 p, Lk-1 q  
where p [1] <q [1], ..., p [k-2] = q [k-2], p [k-1] = q [k-1]  
)  
 apriori_genC function Prune apriori_genD function 
with the same step, not repeat them here.  
AprioriTid-MARDA algorithm to set up a calculator to 
store the candidate itemsets c ? Ck the number of 
transactions when dealing with a certain time T, the counter 
initial value set to 0, the time when the scan Hash tree, if the 
transaction T reaches Hash tree one leaf node, then the 
counter plus 1, when scanned Hash tree after the counter 
trading on T has been included in the candidate the number 
of frequent k itemsets.  
In the worst case (all items are set to meet the minimum 
support, which are frequent itemsets), up to (C1n + C2n +...+ 
Cnn) (C1m + C2m +...+ Cmm) = (2n-1) ( 2m-1) = 2m + n-2m 
(2n-m +1) +1 second operation, get all the frequent itemsets 
(where m is the number of decision attribute value, n the 
number of attribute values for the conditions). If conditions 
do not distinguish between attributes and decision attribute, 
the simple use Apriori algorithm to generate all frequent 
itemsets when up to (C1m + n + C2m + n +...+ Cm + nm + n) = 
2m + n-1 time operations, and improve algorithm to reduce the 
2n +2 m-2 times operator (the above items in any length to 
satisfy the minimum support set all the time). Shows that the 
modified algorithm reduces the computation time and space 
complexity, improve efficiency of data mining. 
 Based measure of confidence from the frequent itemsets 
generated in the form of  
Des (C1) ? Des (C2) ? ... ? Des (Cn) => Des (d1) 
? Des (d2) ? ... ? Des (dn) association rules, in which 
Des (Cn) is one such condition attributes Cn price category 
values, Des (dn) for the decision attribute of an equivalence 
class values.  
VI. APRIORI-MARDA ALGORITHM OPTIMIZATION 
Because of previous mining algorithm Apriori-MARDA 
is based on the Apriori algorithm to adjust the efficiency of 
the traditional Apriori algorithm in mining on the shortage is 
also inherited, so the original algorithm of optimization is 
also essential. 
Apriori algorithm is the major shortcomings of the 
following two points:  
(1) k-1 by the frequent itemsets were generated from the 
connection candidate frequent k itemsets large quantities.  
(2) verify the candidate frequent k itemsets when the need to 
scan the entire database is very time-consuming. Therefore, 
based on the Apriori another scholar suggested AprioriTid 
algorithm is only the first scan the transaction database D 
with the calculation of the candidate frequent itemsets 
support, and other first k scans generated with its last set of 
Ck-scan 1 'to calculate the candidate frequent itemsets 
support, support to reduce the computation time required to 
scan the total number of transactions, thereby reducing the 
computation time support. 
 Rough set is an effective tool for data mining, has a 
solid theoretical basis. Pawlak has been proposed since 1982, 
has been applied in many fields, but as a new, rough sets are 
also encountered in many practical difficulties. Currently 
2010 International Conference on Computer Application and System Modeling (ICCASM 2010)
V11-238
there are two effective ways: First, the expansion of rough 
set theory, such as Ziarko's variable precision rough set 
model. Followed by the rough set combined with other 
methods.  
VII. CONCLUSION 
This article summarizes the major work done by the 
following: 
Using rough set theory on the advantages of attribute 
reduction, decision table attribute reduction, remove the 
irrelevant attribute, as extracted directly from the appropriate 
decision table decision-making rules, the same time, based 
on rough set attribute reduction algorithm is improved 
Classical association rules mining algorithm of the 
system and to make the existing algorithms on the proposed 
improvement and optimization 
 
REFERENCES 
[1] R.Agrawal,T.Imielinski.A.Swami.Mining Association Rules between 
Sets of Items inLarge Databases[A].Proceedings of the ACM 
SIGMOD Conference on Managementof Data[C],1993,207-216 
[2] Fayyad U.M,Piatetsky-Shapiro G,Smyth P.Knowledge Discovery and 
Data Mining:Towards a Unifying Framework[A].Proc of KDD 
96[C].Menlo Park,CA:AAAI Press,1996:82-88 
[3] M-S.Chen,J.Han,P.S.Yu.Data mining:an overview from a database 
perspective[J].IEEE Transactions On Knowledge And Data 
Engineering,1996(8):66-883 
[4] Z.Pawlak.Theorize with Data using Rough Sets[A].Proc of the 26th 
AnnualInternational Computer Software and Applications 
Conference[C].IEEE,2002,137-156 
[5] Z.Pawlak.Roughclassification[M].Int.J.Human-Computer 
Studies,1999,51:369-383 
[6] Z.Pawlak,JerzyGrzymala-Busse,RomanSlawinski .RoughSets[M].Co
mmunication of  the ACM,1995,38(11):89-95 
[7] LingrasPDaviesC.Roughoenetiealgorithms[A].zhongN ?
skowronAeds.Proe7 Intl WksPonRSFD[C].SPringe 1999.38 ? 46. 
2010 International Conference on Computer Application and System Modeling (ICCASM 2010)
V11-239
