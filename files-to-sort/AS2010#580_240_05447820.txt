Efficient Identification of Coupled Entities in
Document Collections
Nikos Sarkas #, Albert Angel #, Nick Koudas #, Divesh Srivastava ?
#University of Toronto
?AT&T Labs-Research
{nsarkas,albert,koudas}@cs.toronto.edu
divesh@research.att.com
Abstract— The relentless pace at which textual data are gener-
ated on-line necessitates novel paradigms for their understanding
and exploration. To this end, we introduce a methodology for
discovering strong entity associations in all the slices (meta-
data value restrictions) of a document collection. Since related
documents mention approximately the same group of core entities
(people, locations, etc.), the groups of coupled entities discovered
can be used to expose themes in the document collection.
We devise and evaluate algorithms capable of addressing two
flavors of our core problem: algorithm THR-ENT for computing
all sufficiently strong entity associations and algorithm TOP-ENT
for computing the top-k strongest entity associations, for each
slice of the document collection.
I. INTRODUCTION
Millions of bloggers author in excess of 5 million blog posts
daily, where they comment upon a wide variety of private mat-
ters and public on-going events. This collective “discussion”
takes place in a semi-anonymous manner, as bloggers typically
reveal their demographic profile (age, gender, occupation, lo-
cation). Valuable knowledge can be extracted from such user-
generated data by identifying the stories and events currently
capturing the attention of different demographics.
Consider an emerging crisis situation like the fatal 2008
Listeriosis outbreak in Canada. It would be invaluable for both
the authorities and citizens to identify active discussion about
such an event, before it reaches its climax. As another exam-
ple, a new TV series targeted at all US teenagers is launched.
An advertiser could compare the stories discussed by different
groups of teenagers and identify surprising divergences, e.g.,
that the series is surging in popularity among female teenagers,
but failing to capture the attention of males.
In order to provide such intuitive knowledge discovery fa-
cilities, we leverage mentions of entities extracted [3] from the
documents’ textual component and document meta-data (e.g.,
blogger demographics). The key observation is that related
documents, capturing the same story, mention approximately
the same group of core entities. E.g., blog posts discussing
the Canadian “Listeriosis” outbreak mention the disease in
conjunction with “Public Health Agency of Canada” and lo-
cations like “Toronto”. By identifying such groups of strongly
associated entities, i.e., groups of entities that are recurrently
mentioned together, we implicitly detect the underlying event
of which they are the main actors. Strong entity associations
can be computed for each demographic segment and presented
on demand (along with additional information such as relevant
blog posts) as we “drill-down” to the demographic of interest,
or compared for different demographics [2].
To support such interactive browsing and exploration of a
document collection, we need to pre-compute and materialize
strong entity associations for all meta-data value combinations
that can be possibly requested. We refer to the fraction of
documents matching a certain meta-data restriction as a slice
of the collection (e.g., a demographic segment such as “US
teenagers” in the case of blogs). Two variations of our core
problem are of interest: computing all sufficiently strong entity
associations (Threshold Variation) and computing the top-
k strongest entity associations (Top-k Variation), for all the
different slices of the document collection.
While there exist a significant number of set-similarity
and statistical correlation measures for assessing association
strength between two entities, we focus on the Likelihood
Ratio statistical test whose unique properties and intuitive
behavior render it ideal for exposing interesting and mean-
ingful entity associations in user-generated data ([6] and
Section IV). Existing techniques in association rule mining
and set-similarity search focus on measures inadequate for
our application (e.g., support/confidence [1], X 2 test [5], set-
similarity measures [4]) and cannot support the complex, non-
linear Likelihood Ratio test.
The relentless pace at which user-generated content is
accumulated necessitates highly efficient solutions. In order to
address this challenge, we make the following contributions.
• We develop algorithm THR-ENT (Thr. Variation), which
eliminates from consideration provably weak associations.
• We develop algorithm TOP-ENT (Top-k Variation), which
terminates as soon as it can guarantee that the top-k associ-
ations computed so far constitute the final result.
• We demonstrate the efficiency and applicability of the
proposed techniques using both real and synthetic data.
II. FORMAL PROBLEM STATEMENT
Consider a collection C of n documents d1, . . . , dn, anno-
tated with l categorical meta-data attributes A1, . . . , Al. We
denote with di(A1, . . . , Al) the meta-data values annotating
di. Let A ? Powerset(A1, . . . , Al) be a subset of the l meta-
data attributes and A× be the cartesian product of their corre-
sponding domains. Each element a of set A× defines a slice
sC(a) of the collection: sC(a) = {d ? C|a ? d(A1, . . . , Al)}.
We have at our disposal an entity extraction algorithm
whose application on the document collection reveals the
978-1-4244-5446-4/10/$26.00 © 2010 IEEE ICDE Conference 2010769
mentions of m entities e1, . . . , em. We define the document-
list ei = {dj |dj mentions ei} of entity ei and the entity-list
dj = {ei|ei mentioned in dj} of document dj . We denote
with ci = |ei| the number of documents that mention entity
ei and with cij = |ei ? ej | the number of documents where
entities ei and ej are mentioned together. We will also refer
to cij as the co-occurrence of ei and ej .
The Likelihood Ratio measure quantifies the likelihood
that, for the pair of entities examined, the assumption of
independent occurrence in documents does not hold. Higher
values indicate a higher likelihood that the assumption is
violated and therefore signify stronger association. Consider
two entities ei and ej . We denote with N11 the number of
documents containing both. Similarly, let N10 (N01) be the
number of documents containing entity ei (ej) but not entity
ej (ei) and N00 the number of documents containing neither
entity. We also denote with E11, E10, E01, E00 the expected
values of these quantities under the independence assumption.
These quantities can be easily expressed as a function of n,
ci, cj and cij . Then:
• Likelihood Ratio: L(ei, ej) =
?
x?{0,1}
?
y?{0,1}
2Nxy ln
Nxy
Exy
Our goal is to compute associated entities, for all the
slices of the document collection. We identify two interesting
variations of this problem.
• Problem 1 (Threshold Variation): For each slice s, compute
pairs (ei, ej) such that L(ei, ej) ? T .
• Problem 2 (Top-k Variation): For each slice s, compute the
k pairs (ei, ej) with the highest L(ei, ej) values.
III. ALGORITHMS
A. ALL-PAIRS: Evaluating All Pairs
Let us initially focus on a single slice. We assume that we
have in main memory the document-lists ei associated with
each entity ei. In order to compute value L(ei, ej) for pair
(ei, ej), we need four pieces of information: n, ci, cj and cij .
The main challenge in evaluating L(ei, ej) for every entity
pair is computing co-occurrences cij . This information can be
represented as a symmetric co-occurrence matrix Cm×m with
cij as its elements. Due to its symmetry, we are only interested
in elements that lie below its diagonal (Figure 1(a)).
Matrix C can be efficiently computed one row-at-a-time.
The row-at-a-time computation of C begins from the row
corresponding to entity e1 (Figure 1(a)) and progresses to-
wards the row of entity em. Consider the row of entity ei.
Its elements correspond to the co-occurrences of entity ei
with all preceding entities e1, . . . , ei?1. The entire row can be
computed by merging the sorted by entity identifier entity-lists
d corresponding to the documents where entity ei appears.
Example 1: Suppose that the document-list for entity e9
is e9 = [1, 5, 8] (e9 appears in documents d1, d5 and d8).
Respectively, the entity-lists for documents d1,d5 and d8 are
d1 = [2, 3, 6,9], d5 = [3, 7,9] and d8 = [2, 3, 7,9] (e.g.,
document d1 contains entities e2, e3, e6, in addition to e9).
The element sequence resulting from the merge of the three
e1
e2
e3
e4
e5
e6
e7
e8
e1 e2 e3 e4 e5 e6 e7 e8
(a)
e1
e2
e3
e4
e5
e6
e7
e8
e1 e2 e3 e4 e5 e6 e7 e8
(b)
Forward Merge
Reverse Merge
Fig. 1. The co-occurrences matrix C.
lists is [2, 2, 3, 3, 3, 6, 7, 7,9,9,9]. This implies that entity e2
appears twice in the documents where entity e9 occurs and
therefore c92 = 2. Similarly, we compute that c93 = 3, c96 = 1
and c97 = 2. We are certain that all other elements of row e9
are equal to zero.
Intuitively, by computing C in a row-at-a-time fashion we
search the entity-lists of all documents in which ei appears
and focus only on the entities that actually co-occur with
ei. The expected low number of entities per document and
heavy-tailed distribution of entity frequencies (most entities
mentioned in a few documents) benefit this approach.
The entity-lists d essentially correspond to an inverted
index of the entity document-lists e. This inverted index
is incrementally populated as entities (rows of matrix C)
are processed. We denote with Ii such an inverted index,
populated with entities e1, . . . , ei. Inserting an entity ei entails
identifying the appropriate entity-lists where ei should be
placed, and inserting it at the end of those lists. This index
growing process, in conjunction with the fact that entities are
processed in ascending identifier order e1, . . . , em, implies that
the index’s entity-lists are also maintained sorted in ascending
entity identifier order.
This observation has an interesting consequence that we will
utilize. Consider inverted index Ii?1 used to compute C’s row
corresponding to entity ei. The merging process produces co-
occurrences in a well-defined order, i.e., ci1, ci2, . . . , ci(i?1)
(left to right). We refer to this operation as forward merge (Fig-
ure 1(b)). However, this is not the only possibility. Although
the index’s inverted lists contain entities in ascending identifier
order, we can process the lists by scanning them backwards.
This reverse merge operation (Figure 1(b)) produces co-
occurrences in the order ci(i?1), ci(i?2), . . . , ci1 (right to left).
We note that in either case only non-zero co-occurrences are
produced. These techniques constitute algorithm ALL-PAIRS
which can be used to address both Problems 1 and 2.
B. THR-ENT: Problem 1 (Threshold Variation)
Given our interest only in pairs such that L(ei, ej) ? T ,
computing the entire matrix C is wasteful. We need to focus
our attention on computing the part of C that corresponds to
entity pairs whose association can exceed T .
Lemma 1: L(ei, ej) ? P (ei, ej), where P (ei, ej) is the
Likelihood Ratio value obtained by setting cij = min(ci, cj).
We refer to P (ei, ej) as the potential of entity pair (ei, ej),
since the association strength of an entity pair is maximized
when the less frequent entity always co-occurs with the most
frequent one. The lemma instructs us to focus only on elements
cij such that P (ei, ej) ? T . Let us concentrate on a single row
of matrix C, corresponding to entity ei. One could compute the
770
potential of every pair and identify the ones with P (ei, ej) ?
T . However, the row elements that need to be evaluated reveal
a “fragmented” image.
Consider Figure 2(a). Elements corresponding to pairs that
have the potential to exceed the association threshold are
highlighted in grey. This immensely complicates matters,
since the efficient row-at-a-time approach of computing row
elements generates those elements in a well specified order
(either left to right or right to left, Figure 1(b)). Consequently,
we need to compute all row elements sequentially, until we
reach the last element that is needed. In the worst case, the
entire row will be processed (e.g., row e5 in Figure 2(a)). The
following lemma helps alleviate this limitation.
Lemma 2: Suppose that entities e1, . . . , em are sorted in
descending order of their frequencies, i.e., ci ? cj for i < j.
Then, for any entity ei and its preceding entities e1, . . . , ei?1
it holds that P (ei, e1) ? P (ei, e2) ? · · · ? P (ei, ei?1).
Lemma 2 implies that if entities e1, . . . , em are sorted in
descending order of their frequencies, the potential of pairs
in a row is monotonically non-decreasing from left to right.
Consequently, a threshold T divides a row into two sections:
pairs on the left that do not have the potential to exceed the
threshold and pairs on the right that have the potential to
exceed it (Figure 2(b)).
Now, pairs that need to be computed are concentrated on the
right side of the row and can be accessed using a reverse merge
(Figure 2(b)). It should be noted that even among the elements
whose corresponding pair potentials exceed threshold T , only
those with cij > 0 will actually be computed. The techniques
described above constitute algorithm THR-ENT.
C. TOP-ENT: Problem 2 (Top-k Variation)
The potential of entity pairs can be used to devise a
“Threshold algorithm” for Problem 2. We can incrementally
generate candidate entity pairs in descending order of their
potential and evaluate their association in that order. As soon
as the potential of the next candidate entity pair is less than
the top-k “threshold”, i.e., the minimum association among the
top-k entity pairs, we can be certain that the top-k pairs en-
countered so far are the overall top-k pairs. The challenge lies
in reconciling the incremental, descending-potential candidate
generation process and the efficient row-at-a-time approach to
computing entity co-occurrences.
e1
e2
e3
e4
e5
e6
e7
e8
e1 e2 e3 e4 e5 e6 e7 e8
(c)
e1
e2
e3
e4
e5
e6
e7
e8
e1 e2 e3 e4 e5 e6 e7 e8
(d)
Frontier 1
Frontier 2
Frontier 3
e1
e2
e3
e4
e5
e6
e7
e8
e1 e2 e3 e4 e5 e6 e7 e8
(a)
e1
e2
e3
e4
e5
e6
e7
e8
e1 e2 e3 e4 e5 e6 e7 e8
(b)
Fig. 2. Principles of Algorithms THR-ENT & TOP-ENT.
The reconciliation can be performed by pre-sorting, as be-
fore, entities in descending order of their frequencies. Lemma
2 guarantees that for each row of matrix C, the corresponding
pair potential is monotonically non-decreasing from left to
right. Figure 2(c) illustrates: darker shades of gray signify
higher pair potential. The direction of the arrow is the direction
of decreasing pair potential. Note that high-potential elements
are concentrated towards the matrix diagonal.
Lemma 2 provides a guaranteed potential ordering within
each matrix row, but not for the entire matrix. Given two
matrix elements (entity pairs) in different rows, we can draw
no conclusions about their relative potential. Still, a total pair
ordering can be derived by progressively merging the rows of
matrix C. This operation results in a stream of candidate pairs
ordered in descending potential order.
Let us discuss this process in more detail. Each of the
m rows of matrix C is associated with an “iterator”. The
iterator ITi associated with row i incrementally performs a
reverse-merge and provides row pairs and their co-occurrences
in descending potential order. At any time, an iterator is
associated with the potential of the pair that it currently
points. The iterators are inserted in a heap H which maintains
them in descending order of their corresponding potential.
Then, the top of heap H provides the iterator pointing to the
pair with the maximum overall potential: it is the pair with
the maximum potential among all m matrix rows. After the
maximum potential pair is retrieved and evaluated, the iterator
that provided the pair is advanced, and heap H is updated.
Intuitively, heap H dictates how to alternate between the m
on-going reverse merges after each element computation.
The proposed algorithm “sweeps” the elements of co-
occurrence matrix C using a frontier. This is depicted in
Figure 2(d). The frontier connects the elements from each row
currently in heap H. All the matrix elements on the left of
the frontier have lower potential than those computed so far,
i.e., elements on the right of the frontier. Figure 2(d) presents
this frontier at three phases during the algorithm’s execution.
The threshold process described above is implemented by
algorithm TOP-ENT. Note that the algorithm has a starting
phase where the entire inverted index is populated and iterators
ITi and heap H are initialized.
D. Data-assembly Infrastructure
Algorithms THR-ENT and TOP-ENT need to be applied on
every slice of the document collection. Assembling the relevant
document-lists ei required for their application on each of the
numerous slices is a formidable challenge.
Our solution exploits the considerable overlap among slices:
except for the most “fine-grained” slices, all remaining ones
can be expressed as the union of other disjoint slices. E.g.,
consider a slice s={blogs by US bloggers} that is the union of
two disjoint slices s1={blogs by US males} and s2={blogs by
US females}. Then, the document-list esi of entity ei for slice
s is simply the concatenation of the corresponding document-
lists es1i , e
s2
i for slices s1 and s2. We refer to the process of
concatenating the document-lists from disjoint slices in order
to generate the lists of a new slice as slice merging.
Our data-assembly infrastructure is based on the MEMORY-
CUBE algorithm [7]. It first generates the document-lists for
the most “fine-grained” slices and then progressively merges
already processed slices to generate new ones.
771
IV. EXPERIMENTAL EVALUATION
All the techniques were implemented using 64-bit Java 6.
Our test platform was a 2.4Ghz AMD Opteron 850 processor.
The Java VM was constrained to use up to 10GB of memory.
A. Benefits of Likelihood Ratio Test
Set-similarity based measures (such as Jaccard) and the
X 2 test disproportionately “reward” high relative overlap
between the document-lists of two entities, without accounting
for entity frequencies [6]. As a result, the most associated
pairs identified by such measures involve infrequent entities
which occur (almost) always together. Such associations are
unrepresentative of interesting and prevalent stories.
The Likelihood Ratio (LR) test “rewards” in an intuitive
manner both considerable overlap and high entity frequency.
This behavior cannot be replicated by simply enforcing a
threshold on the minimum frequency of entities considered
and using a different association measure. As an example, the
following table presents the top-5 entity pairs under the LR
measure and their (extremely low) Jaccard values, discovered
in 290k blog posts collected on Feb 12, 2009.
ei ej ci cj cij Jaccard
Barack Obama White House 2006 9352 1304 0.13
Charles Darwin The Origin of Species 2927 848 664 0.21
Israel Gaza 2161 947 586 0.23
Microsoft MS Windows 2415 2549 736 0.17
David Letterman Joaquin Phoenix 1280 686 442 0.29
B. Performance on Real Data
Our real-data collection is comprised of 1.4M blog posts.
Each post is associated with author information (Age, Gender,
Country, Profession, Influence). Influence is quantified by the
number of links to the author blog during the previous year.
The domain size of attribute Age is 7 (age ranges), of Gender
is 2, of Country is 224, of Profession is 42 and of Influence
is 5 (ranges of in-link values). The posts were pre-processed
by the entity extractor of [2]. The extraction process revealed
the mentions of 280K unique entities.
Figure 3 presents the total time required by the different
techniques to address Problems 1 (Figure 3(a)) and 2 (Figure
3(b)). The time figures include the overhead of slice merging,
which was only 33 seconds. THR-ENT offers a substantial
performance benefit that becomes more pronounced as the
threshold value increases. Higher threshold values allow al-
gorithm THR-ENT to process only a small number of entity
pairs per row of matrix C (Section III-B) and even ignore rows
corresponding to less frequent entities entirely, since the LR
measure assigns small association values to such entities. TOP-
ENT clearly outperforms ALL-PAIRS, although expectedly
the performance gap decreases as the number of requested
associations k increases.
C. Performance on Synthetic Data
Algorithms ALL-PAIRS and THR-ENT (Problem 1) were
also applied on a single, synthetically generated slice. The
comparison of algorithms ALL-PAIRS and TOP-ENT (Problem
2) yielded similar results.
0 200 400 600 800 1000
0
100
200
300
400
500
600
(a): Threshold
Ti
m
e
 
(s)
 
 
All-Pairs
Thr-Ent
0 200 400 600 800 1000
0
100
200
300
400
500
600
(b): Top-k
Ti
m
e
 
(s)
 
 
All-Pairs
Top-Ent
Problem 2Problem 1
Fig. 3. Real data, Problems 1 and 2.
The parameters influencing performance are (a) the number
of documents n, (b) the number of entities m (c) the number
of entities per document p and (d) the specified association
threshold T . We set the default values of these parameters
to n = 500k, m = 500k, p = 10 and T = 200. The entity
frequency distribution followed Zipf’s Law with exponent 0.8,
the value witnessed in our real data set.
For each document we independently selected p entities ac-
cording to their occurrence probability. Populating documents
with independent entities fails to introduce strong associations.
However, this does not affect our results: algorithm THR-ENT
performs pruning based on the assumption that all entity pairs
are as strongly associated as possible (Section III-B).
Figure 4 presents a sample of our experimental results. All
parameters (n, m, p, T ), other than the one varied in each plot,
are set to the default values stated before. The figures verify
the performance improvement offered by algorithm THR-ENT
under a wide range of experimental parameters.
10k 100k 1M 10M
102
103
104
105
Number of posts (n)
Ti
m
e
 
(m
s)
 
 
0 5 10 15 20 25
0
2
4
6
8
x 104
Entities per document (p)
Ti
m
e
 
(m
s)
 
 
All-Pairs
Thr-Ent
All-Pairs
Thr-Ent
Fig. 4. Synthetic data, Problem 1.
V. CONCLUSIONS
We developed techniques for the discovery of entity asso-
ciations in all the slices of a meta-data annotated document
collection. Such associations can be used to expose interesting
underlying themes in the collection. Efficient association dis-
covery is performed by algorithms THR-ENT and TOP-ENT.
REFERENCES
[1] R. Agrawal and R. Srikant. Fast algorithms for mining association rules
in large databases. In VLDB, 1994.
[2] A. Angel, N. Koudas, N. Sarkas, and D. Srivastava. What’s on the
grapevine? In SIGMOD Conference, pages 1047–1050, 2009.
[3] D. E. Appelt and D. Israel. Introduction to information extraction
technology. In IJCAI Tutorial, 1999.
[4] R. J. Bayardo, Y. Ma, and R. Srikant. Scaling up all pairs similarity
search. In WWW, 2007.
[5] S. Brin, R. Motwani, and C. Silverstein. Beyond market baskets:
Generalizing association rules to correlations. In SIGMOD, 1997.
[6] T. Dunning. Accurate methods for the statistics of surprise and coinci-
dence. Computational Linguistics, 19(1), 1993.
[7] K. A. Ross and D. Srivastava. Fast computation of sparse datacubes. In
VLDB, 1997.
772
