Mining Generalized Actionable Rules Using Concept Hierarchies 
Li-Shiang Tsay1, Seunghyun Im2
1North Carolina A&T State Univ., School of Technology, USA 
2Univ. of Pittsburgh at Johnstown, Dept. of Comp. Science, USA 
ltsay@ncat.edu, lishiangtsay@yahoo.com, sim@pitt.edu 
Abstract 
A series of mining actionable rule methods have 
been proposed from various aspects, but the existing 
models do not incorporate the concept of 
hierarchy/taxonomy into the mining process and 
restrict the terms used to build actionable rules to 
atomic concepts.  In order to resolve this problem, an 
integrated framework for extracting multiple-level 
actionable rules with ontology support is proposed so 
more generalized knowledge from data can be 
extracted.  This type of generalized rules will contain 
not only the attribute values contained in data, but also 
some concepts encoded in a given taxonomy.  
Obtaining generalized actionable rules are a necessity 
since they provide a more general view of the domain.  
The proposed framework is based on a breadth-first 
top-downward model to be developed by extending the 
existing single-level actionable rule discovery methods.  
This framework can improve the quality of the 
extracted actionable rules in terms of their 
interestingness and understandability.   
1. Introduction
Discovering actionable patterns in a data set has 
been a focused topic in recent data mining research [6-
10][13-23].  This type of research moves further than 
the classical data mining methods [1][3][14][26] by not 
only summarizing the data but also comparing the 
context of segmentations. Given a historical database, 
where each record is described by a set of values, an 
actionable rule is an expression (?X ? ?Y), ?X and 
?Y are a set of attributes with value variation. The 
intuitive meaning of such a rule (what changes within 
attribute X), is needed to re-classify some objects from 
a lower profitability class to a higher one (in attribute 
Y). An example of such a rule might be that 94% of 
special need students will learn better and improve 
his/her grade by one letter grade if their teacher’s office 
hours are increased to 6 hours. In this case, 94% is 
called the confidence of the rule.  The left support of 
the rule ?X ? ?Y is the percentage of the records that 
contain the initial values on both X and Y and its right 
support is the percentage of records in X and Y after 
changing the values of both.   The problem of mining 
actionable rules is to find all rules that satisfy user-
specified minimum left support, right support, and 
confidence thresholds.   
Over the years many techniques have been 
proposed for discovering actionable rules from large 
collections, such as a domain-dependent way [13], a 
post-process analyzing rules manner [6] [9-10] [16-19] 
[23-25], and a domain-independent way [20-21].  
Unfortunately, those approaches do not incorporate the 
concept of hierarchy/taxonomy into the mining process 
and restrict the terms used to build actionable patterns 
to a single concept level.  The first issue is that the 
rules discovered at the atomic concepts level may not 
provide informative knowledge of the dataset due to 
specificity.  The second issue is that the rules at the 
atomic concepts level may not have minimum supports, 
and thus some significant actionable rules may not be 
discovered.  To increase the chance of finding 
significant actionable rules at primitive concept level, 
the minimum left and right support thresholds must be 
reduced substantially. By doing this, it may generate 
too many rules.  To reduce the number of valid rules, 
one may place higher confidence in the pattern 
extraction procedure.  Pursuing high confidence usually 
results in a complex model and overly detailed rules.  
In practice, the end users often prefer a simpler and 
more understandable model with slightly less accuracy.   
Therefore, there is a need to extract actionable rules 
using the information of pre-defined taxonomy over the 
attribute values. 
The concept hierarchies as background knowledge 
have long been successfully used to help generate a 
simpler and more meaningful result in information 
retrieval, association rule mining, etc. [2][5][15]. In 
many applications, the taxonomy information is either 
stored implicitly in the database or provided by domain 
experts [4] and can be used to generalize primitive 
level concepts to higher level ones. More initiative and 
informative regulations can be found by mining data at 
higher abstract concepts. By exploring multiple-level 
concepts, rules can be generated across different levels 
2009 Fifth International Joint Conference on INC, IMS and IDC
978-0-7695-3769-6/09 $26.00 © 2009 IEEE
DOI 10.1109/NCM.2009.410
200416
of concept hierarchy to derive more interesting, 
concise, simple, and understandable rules.   
The most intuitive approach to mine generalized 
actionable rules is to apply the existing single-level 
actionable rule mining methods to exam attribute 
value-pairs at multiple levels of abstraction under the 
same minimum left support, right support and 
confidence thresholds.  This approach is simple but it 
may lead to some undesired results. Since the size of 
granules at each level of the hierarchy is different, the 
higher level concepts tend to have higher support and 
the lower level concepts tend to have lower support.  
One is likely to find many strong rules at higher level 
concepts and those rules have high potential to echo to 
prior knowledge and expectations. This side effect 
exists in most of the generalized association rule 
methods [2][15]. To avoid this drawback, we use the 
same approach as in [5] -- a different set of thresholds 
will be used for different levels of abstraction.  
In this study, we assume that domain experts will 
provide a concept hierarchy for each nominal attribute 
and the Rosetta-type algorithm will be utilized to 
automatically discretize for continuous attributes. An 
integrated framework for extracting generalized 
actionable rules with ontology support is proposed so 
more generalized knowledge from data can be 
extracted.  This type of generalized rules will contain 
not only the attribute values in data, but also some 
concepts encoded in a given taxonomy. Obtaining such 
rules is a necessity since they provide a more general 
view of the domain and the discovered rules are also 
simpler and more understandable than classical 
actionable rules. The proposed framework is based on 
a breadth-first top-downward model to be developed by 
extending the existing single-level actionable rule 
discovery method, StrategyGeneratorIII [22].  It is a 
two-part process: identifying a set of frequent-closed-
factor-sets within the different level concepts at 
different thresholds and generating reclassification 
rules from these factors. The anti-monotonic property 
is applied, in order to avoid construction of 
meaningless factor-sets, by considering if a factor is not 
frequent then its descendants will not be frequent as 
well.  
This paper is organized as follows. In section 2, the 
concept of hierarchy is introduced.  An actionable rule 
and a non-redundant actionable rule are reviewed in 
sections 3 and 4, respectively.  The generalized 
actionable rules and the new algorithm are proposed in 
section 5. This study is concluded in section 6 along 
with future work.  
2. Concept Hierarchy
Let V = {v1, v2, … , vm} be a set of possible values 
for attribute A.  Let H be a concept hierarchy for the 
attribute A, which organizes relationships of values in a 
tree structure, shown in Figure 1.  In such tree, each 
level represents a different degree of abstraction. In [2], 
authors define the concept hierarchy as a partial order 
set and represent the special-general relationship 
between concepts as a partial order relation. Given two 
concepts v and w in a partial order relation P, i.e., (v, 
w) ?P and v?w where v is a descendant of w and w is 
an ancestor of v.  For example, the relationship of 
concepts middleAge  and Adult can be described as
middleAge ? Adult, where the concept of Adult is a 
more general than the concept of middleAge, or that 
middleAge is a more specific concept than Adult. The 
value sits at root node representing the most general 
concept and the leaf node representing the primitive 
concept. Note that only the leaf values of concept 
hierarchy are represented in the original database and 
the attribute name is represented in the root node.  
Intuitively, an attribute can be extended to have 
taxonomy by moving up from bottom to top. By 
grouping the similar primitive values into a general 
concept, the new-formed concept will be represented as 
a parent node of those primitive values.  The domain of 
the new-formed concept will be identified as the union 
of their children’s domain.  For example, the domain of 
Adult can be formed by adding the record ID that 
contains its children’ values: Young, MiddleAge, and 
Senior. 
3. Actionable Rules 
Reclassification Rule Mining (RRM) [21-23] and 
Action Rule Mining (ARM) [7-8], [16-20] both aim to 
build an actionable model that allows users to perform 
actions directly.  The structure of reclassification rules 
is similar to action rules; however, it differs in the 
following ways.  For RRM, the target of discovery is 
not pre-determined, while for ARM there is one and 
only one predetermined target.  RRM aims to discover 
Figure 1. Concept hierarchy for age
…..
MiddleAge …..
age
Children
Young
18      19 …30 
Adult
Senior
200517
all the patterns Z from a given data set, while ARM 
discovers only one specific subset of Z.  In addition, 
attributes are not required to be classified as stable, 
flexible, or decision attributes in reclassification rule 
mining.  RRM is more general than ARM.  It is 
important that the space of all actionable patterns is 
known.  If practical implementation of discovered 
actionable rules is too expensive to achieve, an 
alternative way is needed for searching the space of all 
patterns to obtain the same reclassification effect.    In 
this paper, we will focus on reclassification rule type of 
actionable rules. 
3.1. Information System
An information system [12] is used for representing 
knowledge and is defined as a pair S = (U, A), where U 
is a nonempty, finite set of objects, and A is a 
nonempty, finite set of attributes, i.e. a : U?Va is a 
function for any a ? A, where Va is called the domain 
of a.   Elements of U are called objects. In this paper, 
for the purpose of clarity, objects are interpreted as 
customers.   
3.2. Factor-Set  
Any set of factors is called a factor-set F.  It is 
partitioned into constant factors FS and flexible factors 
FF.   Let us assume that (v1, v2 ? Va ). Then, the 
expressions (a, v1Æ v2) are elements of FS? FF.  If v1?
v2, then (a, v1 Æ v2) refers to a flexible factor and it
denotes the fact that the value of attribute a can be 
changed from v1 to v2 for a number of objects in S.   
Similarly, the term (a, v1 ? v2)(o) means that the 
attribute value a(o) = v1 can be changed to a(o) = v2 for 
the object o.  If v1= v2, (a, v1 Æ v2) refers to a constant 
factor and its value does not change.  A factor-set is 
defined as any conjunction of factors where each factor 
is associated with a different attribute, for example, 
F=(a1, v1 Æ v2), (a2, v1Æ v2), …, (an, v1Æ v2) (o1, o2, 
…, om), where (?i ? n)(?j ? n)[(i ? j) ?  (ai ? aj)] for 
m objects [20].   
3.3. Frequent Factor-Set  
A factor-set ? which supports the user specified 
minimum left support threshold s1 ? 0 and minimum 
right support threshold s2 ? 0 is said to be frequent. 
The collection of all frequent factor-sets supporting 
these two thresholds is denoted by )(s1, s2)=^?1, ?2, …, 
?n`.  supL(? ) denotes left support and supR(? )  
denotes right support of ?.  A frequent factor-set ? in S
is defined as a term  
?= [(a1, ?1? ?1) ? (a2, ?2 ? ?2) ?… ? (ap, 
?p? ?p)],  where (?i ? p)(?j ? p)[(i ?
j) ?  (ai ? aj)], supL(? ) ? s1, and
supR(? ) ? s2 .  
The Left Support defines the domain of a factor-
set which identifies objects in U on which the factor-
set can be applied.  The larger its value, the more 
interesting the factor-set will be for a user. The left 
hand side of the factor-set ?,  is defined as PL  = {?1, 
?2,…, ?p}.  The domain DomS(PL) is a set of objects in
S that exactly match PL.  The left support supL of the 
factor-set ? is defined as  
supL(?) = Card[DomS(PL)] /Card[U]. 
The Right Support shows how well the factor-set is 
supported by objects in S. The higher its value, the 
stronger its effect will be.  The right hand side of the 
factor-set ?  is defined as  PR = {?1, ?2,…, ?p}.   The 
domain DomS(PR) is a set of objects in S that exactly 
match PR. The right support supR of the factor-set ? is 
defined as  
supR(?) = Card[DomS(PR)]/Card[U]. 
3.4. Reclassification Rule
The set of all candidate rules is constructed from 
frequent factor-sets.  The number of factors in a factor-
set will be called the length of the factor-set.  A factor-
set of the length k is referred to as a k-factor-set.  Each 
rule is extracted through k-factor-set having at least 
two flexible factors fF, when k ? 2.  A candidate rule 
whose confidence is greater than the user specified 
minimum confidence c is called a strong rule and it 
belongs to 5(s1, s2, c).  
A reclassification rule U in S is defined as a 
statement 
U  = ; ? <where ;, < ?), ; ?<, and
; ?< =?. 
The factor-set ; is called the action premise 
(antecedent) and < is called the effect 
(consequent) of the reclassification rule.  Let us 
assume that ; = [[(a1, ?1? ?1) ? (a2, ?2 ? ?2) 
?… ? (ap, ?p? ?p)] and < = [(b1, ?1 ? ?1) ? (b2, 
200618
?2 ? ?2) ? … ?(bq, ?q ? ?q)].  If we say that 
objects o1, o2 ? U  support the reclassification rule 
U  in S, if and only if:  
• (?i ? p) [[ai (o1) = ?i]  ? [ai (o2) = ?i]], 
• (?j ? q) [[bj (o1) = ?i] ? [bj(o2) = ?i]]. 
Statistical significance of a rule U is expressed in 
terms of left support and right support, denoted as 
supL(U) and supR(U), respectively.  The left support
measure defines the range of the rule, i.e., the 
proportion of objects which are applicable. The right
support measure defines the feasibility of the rule, i.e., 
the proportion of objects which are role models for 
others.  Because supports indicate the frequency of the 
factor-set occurring in the rule, then supL(U) and 
supR(U) are basically the same as the supports for the 
factor-set ?, i.e., supL(U)=supL(?) and supR(U)= 
supR(?).   
The strength of a reclassification rule U is 
characterized by confidence, which is denoted by 
conf(U).  Statistical significance of the antecedent of a 
rule U is called antecedent-right-support and 
antecedent-left-support which are denoted as a-supL(U) 
and a-supR(U), respectively. a-supL(U)  is defined as the 
number of objects in S which have property ?i, for i = 
1, 2, …, p.  a-supR(U) is defined as the number of 
objects in S that have property ?j, for j = 1, 2, …, q.   
The confidence of a reclassification rule U in S is 
defined as conf(U) = [supL(U)×supR(U)]÷[a-supL(U)×a-
supR(U)].  
4. Non-redundant Reclassification rules  
A set of non-redundant reclassification rules is 
based on the concept of closed frequent factor-set, 
which are sufficient to drastically reduce the rule set 
without information loss [20]. 
By a closed factor-set )in S we mean a term   
) = [(a1, ?1? ?1) ? (a2, ?2 ? ?2) ?… ?
(ap, ?p? ?p)] iff none of its supersets 
)’ satisfies supL()) = supL()’),  
supR()) = supR()’) . 
It means that ) is not closed if at least one of its 
immediate supersets has both supports the same as ).   
By a frequent closed factor-set ?, we mean that 
  
? = [(a1, ?1? ?1) ? (a2, ?2 ? ?2) ?… ? (ap, 
?p? ?p)] is a frequent closed factor-set 
iff  ? ? ), supL(?) ? s1 and supR(?)? s2. 
A closed factor-set ? which supports the user 
specified minimum left support threshold s1 ? 0
and minimum right support threshold s2 ? 0 is said 
to be a frequent and closed factor-set. supL(? ) 
denotes left support and supR(? ) denotes right 
support of ?.  If ? = ?, supL(? )= supL(?) and 
supR(? )= supR(?).  The collection of all such 
factor-sets supporting these two thresholds is 
denoted by )F(s1, s2)=^ ?1, ?2, …, ?n`.  
Let us first formally define what we mean by a 
redundant reclassification rule.  Assume Ui denotes 
the rule [(X,???)i ? (Y,???)i].  U1 is more 
general than rule U2 if U2 is valid, U1 and U2 have the 
same left/right support and confidence, (X,???)1 
?(X,???)2, and (Y,???)1?(Y,???)2, and it is 
denoted as U1 ?ULet 5 = {U1,U 2, …U n} be a set 
of rules that have the same left and right support 
counts and confidence, i.e., supL(U1) = supL(U2)= 
… = supL(Un), supR(U1) = supR(U2)= … = supR(Un), 
and conf(U1) = conf(U2)= … = conf(Un).  Um is a 
redundant rule if Un is valid and 
? Un ?Um
? (Y,???)n =(Y,???)mand(X,???)n 
?(X,???)m, but not
(X,???)m?(X,???)n, or 
? (X,???)n =(X,???)m and (Y,???)n ?
(Y,???)m. 
Therefore, the non-redundant rules are those that have 
the shortest antecedent and the longest consequent. A 
non-redundant reclassification rule15 is defined as  
15: [(X,???) ? (Y,???)] is  not 
redundant iff none of the existed rules 
5’: [(X’,?’??’) ? (Y’,?’??’)] with 
supL(15) = supL(5’),  supR(15) = 
supR(5’), conf(15) = conf(5’), 
satisfies (X’,?’??’) ? (X, ???) and
(Y,???) ? (Y’,?’??’).  
As mentioned in the introduction, some 
shortcomings in the existing methods for actionable 
rule mining are due to not incorporating the concept of 
hierarchy/taxonomy into mining procedure.   
200719
  
GAR( H, S, sl1, sl2,  cl) 
Input: H –
  
a hierarchy concept for every attribute  
          S = (U, A, V) – an information system, where U are objects, A are 
attributes, and V are their values.  
           sl1 –  threshold for a minimum Right Support at level l
           sl2 –  threshold for a minimum Left Support at level l
           cl –  threshold for a minimum Confidence at level l
            
//Step 1 Find the frequent generalized closed factor-sets: the sets of 
factors that have minimum left and right supports at highest 
level 
//Step 1.1 Find the domain of each concept at level l and its support 
count above the predefined sl1 or sl2. 
AV:= Get all distinct attribute-concept-value pairs
 for all attribute-concept-value pair(a,v) do begin 
    HD(A, CV, L, D):=Get domain of each concept value  
    //CV – concept value, L – Level value, 1, 2, 3,…., D – domain 
     While L ?  -1 do 
        For each concept value 
            If Card(HD) >= min (sl1 , sl2)   
               //qualified attribute-value pairs (a, v)
              distinctValueQueue:= ? (a,v) 
          end if
       end for
   end while 
end for 
//Step 1.2 Find the frequent generalized closed factor-set  
//Step 1.2.1 1-factor-sets 
N:= Get all elements at level  1 from distinctValueQueue
M:= Get all elements at level  1 from distinctValueQueue 
while subLevelDistinctValueQueue changed do   
   for each attribute-value pair (an,vn) do   
 For each attribute-value pair (am,vm)  do  
    If (an = am) 
        AtomicFactor:= (al, vn?vm)    
        If ( supL(AtomicFactor)? sl1 & supR(AtomicFactor)?sl2 ) 
             atomicFactorQueue:= ?( AtomicFactor) 
                   subLevelDistinctValueQueue:= ?(subLevelDistinctValue)
         end if 
     end if 
  end for 
      N:= Get all elements from subLevelDistinctValueQueue
      M:= Get all elements from subLevelDistinctValueQueue  
   end for 
end while    
       
//Step 1.2.2 2-factor-sets 
F:= Get all atomic factors  from atomicFactorQueue       
 // every factor denotes as f=(a, v?v) 
n:= F.size         // the total number of primitive factors in F 
    for (i:=0; i++; i>n) do 
        for (j:=(i+1); j++; j>n) do 
            if (fi.a? fj.a )
                 newCandidate:= {fi ?fj} 
                 if ( supL(newCandidate) >= s11  
                               & supR(newCandidate) >=s12 ) 
                   //add this new factor-set into the frequent factor-set  
                   // queue and mark its length with 2 
                   frequentFactorQueue:= ?( newCandidate)        
               end if 
           end if
       end for 
end for 
//Step 1.2.3 Find the frequent k-factor-sets, where k >2.  This step is 
iteratively generating new k-factor-sets using the frequent (k-
1) factor-sets found in the previous iteration until there are no 
new frequent factor-sets generated. 
P:=Get all frequent (k-1)-factor-sets from frequentFactorQueue  
n:= the total number of (k-1)-factor-sets  in P 
For (i:=0; i++; i>n) do 
    If the first (k-2) factors are identical in fi and fj
       newCandidate:= {fi ?fj} 
         If ( supL(newCandidate) >= s1 & supR(newCandidate) >=s2 ) 
              If all subsets of newCandidate ? frequentFactorQueue 
                   //add this new factor-set into the frequent factor-set       
                   // queue and mark its length with k 
                   frequentFactorQueue:= ?( newCandidate)  
              end if 
          end if
     end if 
end for 
//Step 1.2.4 
Fmax:= get all frequent factor-sets f with max Length from  
           frequentFactorQueue 
maxLength:= the length of a factor-set in Fmax 
CompactFactorQueue := ? Fk
for (i := (maxLength-1), i- -, i:=1) do 
       Fi:= get all frequent factor-sets f’ with length i from 
              frequentFactorQueue 
       for each f’ in Fi do 
              if f’.supL > max(? f.supL in Fi+1 ) 
                        if f’.supR > max(? f.supR in Fi+1 ) 
                  CompactFactorQueue := ? f’ 
          end if 
              end if
       end for 
end for 
//Step 2. Generate strong Rules having the confidence above c.  
For each frequent factor-set f in frequentFactorQueue do 
     n := f.length //the length of the frequent factor-set 
     // EQ for elementQueque   
    EQ:= Parse the factor-set into factors and store  
              each factor with level of 999  
     for the length of  the premise i from 1 to n-1
             get next available premise 
             for all factors in EQ 
                   Consequence := { f- premise.factor}
             end for 
             ruleCandidate := [ premise ? Consequence]
             if (conf(ruleCandidate)? c) 
                  // add rule the output and place a positive mark
                   results:= ? ruleCandidate
                   mark EQ.level with level i //place a Positive mark
            end if 
        end for 
 end for 
print all Reclassification rules from result. 
Figure 2. Pseudo-code of algorithm GAR
200820
5. Generalized Reclassification Rules
Authors in [15][5] introduce hierarchy/taxonomy 
into association rule mining to identify more interesting 
rules and their results show promise. In this paper, we 
also studied generalization in mining, but used a 
different mining paradigm, actionable rule mining.  We 
developed a similar approach, called Generalized 
Actionable Rules (GAR), in order to find a concise set 
of reclassification rules.  The algorithm GAR is a 
breadth-first approach to mining a set of all frequent 
generalized closed factor-sets within concept 
hierarchies from top to lower level abstraction and then 
constructs reclassification rules straightforward by 
partitioning the valid factor-sets into a IF-THEN 
representation.  
A generalized closed factor-set )G in S is defined 
as: 
)G = [(a1, ?1? ?1) ? (a2, ?2 ? ?2) ?… ? (ap, 
?p? ?p)] iff )G ? ) and none of its factors is 
the ancestor factor of others
A frequent generalized closed factor-set ?G is 
defined as: 
?G = [(a1, ?1? ?1) ? (a2, ?2 ? ?2) ?… ?
(ap, ?p? ?p)] is a frequent generalized 
closed factor-set at level l iff  ?G ? )G, 
supL(?G) ? sl1 and supR(?G)? sl2, where 
sl1 and sl2 are the user specified 
minimum left support  and minimum 
right support thresholds at level l, 
respectively.  
If a factor occurs rarely, its descendants will be 
even less frequent. For finding generalized 
actionable rules, different minimum left support, 
right support, and confidence thresholds can be 
specified at different levels.   By doing this, if 
users want to find actionable rules at relatively 
lower levels of abstraction, the minimum left and 
right supports can be set relatively low without 
compromise, generating many valid uninteresting 
rules at higher or intermediate levels.  
A generalized reclassification rule UG in S is defined 
as a statement  
UG  = ; ? < where ;, <??G, ; ? <, 
; ? <=?, and no factor-set in < is 
an ancestor of any factor-set in ; in 
the concept hierarchy. 
Algorithm GAR consists of two main steps: (1) 
generate all frequent generalized closed factor-sets, (2) 
generate strong generalized reclassification rules. 
Below is the short description of the algorithm and the 
pseudo-code of GAR is presented in Figure 2.   
5.1. Generate all Frequent Generalized Closed 
Factor-Sets.   
This step computes the frequent factor-sets in the 
data set through several iterations and the breadth-first 
top-downward approach is utilized. In each iteration, 
we generate new candidates from frequent generalized 
closed factor-sets found in the previous iteration; the 
left support and right support for each candidate is then 
computed and tested against the user-specified 
thresholds for that level.  It examines the highest top 
level of abstraction to generate frequent closed factor-
sets and then progresses deeper into their frequent 
descendants at lower concept levels. The lower level 
concepts will be evaluated only when its ancestor has 
large left and right supports at the corresponding upper 
level.  End users have the power to gradually decrease 
the minimum left and right support thresholds at lower 
levels of abstraction in order to discover rules at 
different levels.  
5.2. Generate Generalized Rules.   
A reclassification rule is extracted by partitioning 
the factor-set W into two non-empty subsets X and Y
and represented as X ? Y, where (X, Y? W), (X, Y?
FS), and X=W-Y.  A level-wise approach is utilized for 
generating rules, where each level corresponds to the 
number of factors belonging to the rule premise.  The 
rule-premise is extended one factor at a time.  It starts 
with one factor on the rule-premise, such as X.  If the 
rule confidence is above the predetermined threshold, it 
is called a strong rule and marked with a positive mark.  
The principle of minimum description length is also 
adopted to identify the general rules.  Therefore, in the 
next iteration, it generates new candidates from only 
unmarked rules found in the previous iteration by 
moving one of the consequence-type factors to the rule 
premise, now X is a 2-factor-set.  Repeat the previous 
step until there is only one flexible factor on the rule 
consequent, such as Y.   By doing this, the resulting set 
of rules is comprised of those with the shortest length 
of premise, not all the lengths.   
200921
6.   Conclusion 
This paper addressed the problem of discovering 
generalized actionable rules in a dataset.   We 
investigated the properties of Reclassification Rules, 
presented the notions of generalized closed factor-sets, 
generalized reclassification rules, and introduced 
algorithm GAR.  This new algorithm is based on the 
concept of frequent generalized closed factor-sets to 
generate a very concise set of rules. The proposed 
framework provides more generalized knowledge from 
data than previous existing methods by incorporating 
concept hierarchy into mining procedure.  In addition, 
the quality of the extracted rules in terms of their 
interestingness and understandability is improved.  
Therefore, it would be easier for the user to 
comprehend the rule result in a timely manner.   In the 
future, we intend to apply this approach to a large 
variety of databases and application domains.   
7. References 
[1] Agrawal, R., Imielinski, T., and Swami, A.: Mining 
association rules between sets of items in large databases. In: 
Proceedings of the ACM SIGMOD International Conference 
on the Management of Data. P. Buneman and S. Jajodia, Eds. 
ACM Press, Washington DC, 1993, pp. 207–216. 
[2] Beneditto, M. E. M. D. and Barros, L. N. d.: Using 
Concept Hierarchies in Knowledge Discovery. A.L.C. 
Bazzan and S. Labidi (Eds.): SBIA 2004, LNAI 3171, pp. 
255–265, 2004. Springer-Verlag Berlin Heidelberg 2004. 
[3] Grzymala-Busse, J.: A new version of the rule induction 
system LERS. In: Fundamenta Informaticae, 31(1), 1997, pp. 
27-39 
[4] Han, J., Cai, Y., and Cercone, N.: Data-Driven Discovery 
of Quantitative Rules in Relational Databases. In: IEEE 
Trans. Knowledge and Data Eng., vol. 5, pp. 29-40, 1993. 
[5] Han, J., and Fu, Y.: Discovery of multiple-level 
association rules from large databases. In: Proc. of the 21st 
Int'l Conference on Very Large Databases, Zurich, 
Switzerland, September 1995. 
[6] He, Z.., Xu, X., and Deng, S.: Mining Cluster-Defining 
Actionable Rules. In: Proceedings of NDBC’04, 2004.
[7] He, Z., Xu, X., Deng, S., and Ma, R.: Mining Action 
Rules From Scratch. Expert Systems with Applications. Vol. 
29, No. 3, 691--699 (2005) 
[8] Im, S. and Ra?, Z.W.: Action Rule Extraction from a 
Decision Table: ARED.  In: 17th International Symposium 
on Methodologies for Intelligent Systems (ISMIS), pp. 
160—168. Springer,  Toronto, Canada (2008) 
[9] Ling, C. X., Chen, T., Yang, Q., and Chen, J.: Mining 
Optimal Actions for Intelligent CRM. In: 2002 IEEE 
International Conference on Data Mining, pp.767--770. 
IEEE Computer Society, Maebashi City, Japan (2002) 
[9] Liu, B., Hsu, W., and Ma, Y.: Identifying Non-actionable 
Association Rules. In: Proceedings of KDD 2001, pp. 329--
334. San Francisco, CA, USA (2001)  
[10] Pasquier, N., Bastide, Y., Taouil, R., and Lakhal, L.: 
Discovering Frequent Closed Itemsets for Association Rules. 
In: the 7th international conference on database theory 
(ICDT’99), pp 398--416, Jerusalem, Israel (1999) 
[11] Pawlak, Z.: Information Systems - Theoretical 
Foundations. Information Systems Journal. Vol. 6, pp. 205--
218 (1981) 
[12] Piatesky-Shapiro, G. and Matheus, C. J.:  The 
interestingness of deviations. In: Proceedings of AAA 
Workshop on Knowledge Discovery in Database, pp. 25--36. 
AAAI Press, Menlo Park, CA (1994)  
[13] Quinlan, J. R.: C4.5: program for machine learning. 
Morgan Kaufmann, 1992. 
[14] Ramakrishnsn, S. and Rakesh, A.: Mining Generalized 
Association Rules. In: Proceedings of ICDM’03, pp. 685-
688. IEEE Computer Society, Florida, USA (2003) 
[15] Ra?, Z.W. and Tsay, L.-S.: Discovering Extended 
Action-Rules (System DEAR). In: Proceedings of the 
IIS'2003 Symposium, Advances in Soft Computing, pp. 293-
300. Springer, Zakopane, Poland (2003)  
[16] Ra?, Z.W. and Wieczorkowska, A.: Action Rules: How 
to Increase Profit of a Company. In: Principles of Data 
Mining and Knowledge Discovery, Proceedings of 
PKDD'00, LNAI, pp. 587-592. Springer, Lyon, France 
(2000)  
[17] Tsay, L.-S. and Ra?, Z.W.: Action Rules Discovery: 
System DEAR2, Method and Experiments. Journal of 
Experimental and Theoretical Artificial Intelligence, Vol. 17, 
No. 1-2, pp. 119-128. Taylor and Francis (2005) 
[18] Tsay, L.-S. and Ra?, Z.W.: E-Action Rules. In: Lin, 
T.Y., Xie, Y., Wasilewska, A., and Liau, C.-J. (eds) 
Foundations of Data Mining, Studies in Computational 
Intelligence, pp. 261--272. Springer, Berlin / Heidelberg 
(2007)   
[19] Tsay, L.-S. and Ra?, Z.W.: Discovering the Concise Set 
of Actionable Patterns. In: 17th International Symposium on 
Methodologies for Intelligent Systems (ISMIS), LNAI, Vol. 
4994, pp. 169--178. Springer (2008) 
[20] Tsay, L.-S., Ras, Z.W., and Im, S.: Reclassification 
Rules. In: IEEE/ICDM Workshop on Foundations of Data 
201022
Mining (FDM 2008), pp. 619--627. IEEE Computer Society, 
Pisa, Italy (2008) 
[21] Tsay, L.-S., and Im, S.: Mining Non-Redundant 
Reclassification Rules.  In: Proceedings of the Twenty 
Second International Conference on Industrial, Engineering 
& Other Applications of Applied Intelligent Systems 
(IEA/AIE'09), Tainan, Taiwan, June 24-27, 2009, 806-815 
[22] Wong, R. C.-W. and Fu, A. W.-C.: ISM: Item Selection 
for Marketing with Cross-selling Considerations. In: the 
Eighth Pacific-Asia Conference on Knowledge Discovery 
and Data Mining (PAKDD), pp. 431--440. Sydney, Australia 
(2004) 
[23] Yang, Q., Yin, J., Lin, C. X., and Chen, T.:  
Postprocessing Decision Trees to Extract Actionable 
Knowledge. In: Proceedings of ICDM’03, pp. 685-688. 
IEEE Computer Society, Florida, USA (2003) 
[24] Zhang, H., Zhao, Y., Cao, L., and Zhang, C.:  Combined 
Association Rule Mining. In: Advances in Knowledge 
Discovery and Data Mining, Proceedings of the PAKDD 
Conference, Lecture Notes in Computer Science, 5012, pp. 
1069-1074. Springer, Antwerp, Belgium (2008) 
[25] Zhang, T., Ramakrishnan, R., and Livny, M.: BIRCH: 
An efficient data clustering method for very large databases. 
In: Proceedings of ACM SIGMOD Conference, Montreal, 
Canada, pp. 103–114. 
201123
