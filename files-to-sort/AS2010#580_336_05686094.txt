An innovative algorithm for clustering retail items 
Pradip Kumar Bala 
Dept. of Operations Management & Decision Science,  
Xavier Institute of Management, Bhubaneswar (XIMB) 
Bhubaneswar, India 
e-mail: p_k_bala@rediffmail.com 
Abstract— Dendrogram for clustering retail items is a tool used 
in customer relationship management (CRM) for 
implementing homogeneous schemes for all the items in one 
cluster. In agglomerative hierarchical clustering, dendrograms 
are developed based on the concept of ‘distance’ between the 
entities or, groups of entities. These entities may be the 
customers, retail items, business units etc. as per the business 
problem. The present paper focuses on agglomerative 
hierarchical clustering of retail items based on the concept of 
‘similarity’ of the items which is regarded as the inverse of 
‘distance’ (or ‘dissimilarity’). Similarity is measured based on 
the strength of association rule/s containing the candidate 
items to be merged. An algorithm has been proposed for 
developing dendrogram and it has been explained with an 
example in retail sale. The algorithm overcomes the negative 
impact of negative association between the items on the quality 
of clustering. 
Keywords- association rule; clustering; CRM; dendrogram; 
data mining 
I. INTRODUCTION
In the recent years, the primary task in business analytics 
of customer relationship management (CRM) has been to 
integrate ‘relationship technology (i.e. data consolidating and 
data mining)’ with ‘loyalty schemes’. CRM is expected to 
enhance value to customers through raising satisfaction 
levels on transactions. If customers appreciate the value 
provided by a scheme in CRM, they are expected to 
continuously enhance the relationship with the firm involved 
through loyalty to the products/brands, purchasing more, 
advocating the firm to others, etc. With the advent of data 
mining technology, cluster analysis of items is frequently 
done in supermarkets and in other large-scale retail sectors. 
Clustering of items has been a popular tool for identification 
of different groups of items where appropriate programs in 
CRM are designed for each group separately with maximum 
effectiveness and return. For example, items frequently 
purchased together are placed in one place in the shelf of a 
retail store. A discount-oriented promotional offer is 
designed for a group of items with incidence of high 
association in purchase. Such clustering of items is useful in 
operations other than CRM also. One such illustration is seen 
in clustering of retail items for the purpose of inventory 
control or, placing joint replenishment order. 
Cluster analysis or clustering assigns a set of entities or 
observations or objects into subsets so that observations in 
the same cluster are similar in some sense. These subsets are 
called clusters. Clustering is a method of unsupervised 
learning, and a common technique for statistical data 
analysis used in many fields, including machine learning, 
data mining, pattern recognition, image analysis and 
bioinformatics. There are various algorithms used for 
clustering. Hierarchical algorithms find successive clusters 
using previously established clusters. These algorithms 
usually are either agglomerative ("bottom-up") or divisive 
("top-down"). Agglomerative algorithms begin with each 
element as a separate cluster and merge them into 
successively larger clusters. Agglomerative hierarchical 
clustering creates a hierarchy of clusters which may be 
represented in a tree structure called a dendrogram. The root 
of the tree consists of a single cluster containing all 
observations, and the leaves correspond to individual 
observations. Any valid metric may be used as a measure of 
similarity between pairs of observations. The choice of 
which clusters to merge or split is determined by a linkage 
criterion, which is a function of the pair-wise distances 
between observations. Different clusters are obtained at 
different levels of the tree diagram of dendrogram. This 
gives an opportunity to compare the performance of various 
clustering in different levels with respect to a selected 
performance criterion. In CRM, the performance of 
clustering may be evaluated based on total profit, customer 
retention, total sale etc. Inventory managers may choose 
total relevant inventory cost as a parameter for analyzing the 
performance of clustering at various levels. 
   An important step in any clustering is to select a distance 
measure, which will determine how the similarity of two 
elements is calculated. This will influence the shape of the 
clusters, as some elements may be close to one another 
according to one distance metric and farther away according 
to another distance metric. For example, in a 2-dimensional 
space, the distance between the point (x = 1, y = 0) and the 
origin (x = 0, y = 0) is always 1 according to the usual 
norms, but the distance between the point (x = 1, y = 1) and 
the origin can be 2, ¥2 or 1 if you take respectively the 1-
norm (Manhattan distance), 2-norm (Euclidean distance) or 
infinity-norm (Maximum norm) distance. The Mahalanobis 
distance corrects data for different scales and correlations in 
the variables. Another approach is based on inner product or 
scalar product where the angle between two vectors can be 
used as a distance measure when clustering high 
208978-1-4244-7896-5/10/$26.00 c©2010 IEEE
dimensional data. The Hamming distance measures the 
minimum number of substitutions required to change one 
member into another. Another important distinction is 
whether the clustering uses symmetric or asymmetric 
distances. Many of the distance functions discussed here 
have the property that distances are symmetric (the distance 
from object A to B is the same as the distance from B to A). 
In other applications, e.g., sequence-alignment methods in 
[1], this is not the case. A matrix populated with both upper 
and lower triangles gives complete measures of distance. 
    The concept of ‘distance’ for the purpose of clustering 
has been extended to various other aspects for measuring 
similarity or dissimilarity between the items. The work in 
the current research paper gives a novel algorithm for 
agglomerative hierarchical clustering based on the strength 
of association rule within the items. 
II. AGGLOMERATIVE HIERARCHICAL CLUSTERING AND 
DEDNDROGRAM 
  Agglomerative hierarchical clustering method builds the 
hierarchy from the individual elements by progressively 
merging clusters that operates in a bottom-up fashion [2]. In 
the example given in fig. 1, we have six elements {a}, {b}, 
{c}, {d}, {e} and {f} in the Euclidean field. Clustering is to 
be done on the basis of Euclidean distance of similarity. The 
first step is to determine which elements to merge in a 
cluster. Usually, we want to take the two closest elements, 
according to the chosen distance metric. Based on Euclidean 
distance metric, the dendrogram is given in fig. 2 which has 
been developed in four steps. In step-1, b and c form one 
cluster and similarly, d and e form one cluster and hence, 
there are four clusters obtained, viz. {a} {b, c} {d, e} and 
{f}. Similarly in the subsequent steps, merger happens till 
all fall in one cluster. 
Fig.1: Objects for clustering in Euclidean Field 
Fig.2: Hierarchical clustering dendrogram 
III. STRENGTH OF ASSOCIATION RULE AS A BASIS OF 
CLUSTERING
   Association rule is a type of data mining rule or pattern 
that correlates one set of items or events with another set of 
items or events. An association rule must satisfy the 
minimum values of support, confidence and lift which are 
measures of strength of a rule. Use of data mining algorithm 
in [3] helps in finding frequent itemsets efficiently and 
hence in mining association rules efficiently from sale 
transaction data or, order list data. There are other data 
mining algorithms also to find frequent itemsets for mining 
association rules [4]. With given minimum threshold values 
of support, confidence and lift, all possible association rules 
can be mined. An association rule is represented in the form, 
X => Y (read as ‘X associates Y’) where X and Y are two 
mutually exclusive sets of items or events. X is known as 
antecedent and Y is known as consequent.  ‘Support’ of an 
itemset in a transaction database is defined as the 
percetntage of occurrence of the itemset, out of all the 
transactions.  X=>Y holds with support s%, if s% of the 
transactions in the database contain ‘X’ and ‘Y’ both. 
‘Confidence’ of an association rule X=>Y is defined as the 
percentage of transactions containing X and Y both, out of 
all the transactions containing X. X=>Y holds with 
confidence c%,   if c% of the transaction in the database that 
supports ‘X’ also supports ‘Y’. ‘Lift’ of an association rule 
X=>Y is computed as the confidence of the rule divided by 
the confidence assuming independence of consequent from 
antecedent, i.e., lift = (percentage confidence of the 
rule)/(percentage support of Y).  A lift ratio greater than 1.0 
2010 International Conference of Soft Computing and Pattern Recognition 209
suggests that the rule is positive and hence, X gives lift to Y. 
Lift value less than 1.0 implies a negative association rule 
and hence, implication is that  purchase of X reduces 
purchase of Y. For most of the uses, association rules can 
not be directly applied in business decision-making. 
   The work in [5] gives an association clustering method for 
grouping of inventory items for designing an optimum can-
order inventory policy. The method can be used to develop 
dendrogram for agglomerative hierarchical clustering with 
‘support’ of an itemset as the measure of ‘similarity (seen as 
inverse of ‘distance’)’ between or amongst the items. 
Support of an itemset, as defined earlier, is the percetntage 
of occurrence of the itemset, out of all the transactions of 
order list or in sale. The paper in [5] considers a list of items 
from order list for clustering and does not consider 
confidence and lift in the similarity measure. Lift gives a 
meaningful interpretation in terms of positive and negative 
association. If we do not address the issue of lift, there may 
come many items in one cluster with negative association 
effect which is not desirable for the clusters. Moreover, 
confidence is an important measure of strength of 
association between two itemsets. Hence, it is expected that 
confidence as a measure of strength of association must find 
place while clustering the items. 
IV. ASSOCIATION RULE-BASED CLUSTERING
As we understand, strength of association is described 
with three relevant measures, viz., rule support, confidence 
and lift, due regard must be given to all three while 
clustering of items based on their association. Moreover, 
negative association rule suggests that item associated are 
not purchased together. In view of this, items correlated by 
negative association rule are not to be clustered in one group 
if we are clustering items based on the coincidence of 
purchase in the same market basket. In the following sub-
section, implication of negative association rule has been 
explained. 
A. Implication of Negative Association Rule 
Positive association rules are generally mined with the 
threshold values of support and confidence . However, this 
may be misleading sometimes which is explained below with 
the example given in table 1. Table 1 shows the number of 
transactions with respect to presence and absence of two 
items, X and Y in a transaction.  For the association rule, ‘X 
=> Y’, where, X is called antecedent and Y is called 
consequent, support and confidence are computed as given 
below. 
Support of the rule = (8,000/20,000)*100 % = 40% 
Confidence of the rule = (8,000/12,000)*100% = 
66.67% 
   Support value of 40% and confidence value of 66.67% are 
reasonably good for the rule, ‘X = Y’, to qualify for being 
called a valid association rule. An association rule of the 
form, ‘X => Y’, implies that purchase of X causes purchase 
of Y in many transactions and it happens in certain 
percentage of transactions which is given by the confidence 
value of the rule. 
Table 1. Illustration of Negative Association Rule 
X is 
present 
X is 
absent 
?row
Y is 
present 
8,000 7,000 15,000 
Y is 
absent 
4,000 1,000 5,000 
?column 12,000 8,000 20,000 
   However, if we analyze further, it is observed that X in 
fact inhibits purchase of Y. The reality is that item, Y, is 
purchased on its own in most of the transactions with a very 
high support of 75% (in 15,000 transactions out of total of 
20,000 transactions). However, when X is purchased, Y is 
purchased only in 66.67% of those transactions. As 66.67% 
is less than 75%, it is interpreted that X inhibits the purchase 
of Y. This fact is captured by ‘lift’. Lift of the association 
rule, ‘X => Y’ is given by the expression, “(Confidence of 
the rule)/(Support of the consequent)”. For the example 
under discussion, lift is 0.89, i.e., (66.67%)/(75%). It is 
evident that if the confidence of the rule is less than the 
support of Y, the lift value is less than’1’ and it implies that 
X inhibits purchase of Y and it is a negative association 
rule.  
   Positive association rules are with lift value of more than 
‘1’ and a lift value of less than ‘1’ implies a negative 
association rule [4]. According to [5], when lift is less than 
1, negating the rule produces a better rule. If the rule, “IF B 
an C THEN A”, has a lift value less than ‘1’, then the rule, 
“IF B an C THEN not A” is a better rule.  
B. Details of the proposed Clustering Tehniques 
   The present work in this paper complements the existing 
literature on the technique of association clustering. The 
proposed technique in this work considers only the positive 
associations for clustering. There are two steps in this 
technique. First step is mining association rules with 
threshold value of ‘1.00’ for lift. Suitable threshold values 
for support and confidence are also chosen. In the second 
step, these association rules along with the support, 
confidence and lift are taken as input for clustering. Hence, 
this step gives an algorithm for developing the dendrogram. 
As the proposed algorithm for clustering makes use of ‘all 
strength measures of association rule/s’ in stead of only 
210 2010 International Conference of Soft Computing and Pattern Recognition
‘support of itemset’, the proposed algorithm has been named 
as ‘association rule based clustering’.
   As discussed, there are two phases involved in the 
proposed clustering technique as mentioned below. 
   Phase-I: Mine association rules from the transaction data 
with some threshold values of rule support and confidence 
with lift more than 1.00. The threshold values of rule 
support and confidence are chosen with low values so that 
all the items under consideration find place at least in one 
rule. Amongst these rules, we will select only the positive 
rules, i.e., the rules with lift value of greater than 1.00. 
The input and output in this step are as given below.  
Input:  
I = {I1, I2,……,Im}      //set o items 
D = {t1, t2,………tn}    
     //a transaction database with ti as one transaction 
Threshold rule support = s 
Threshold rule confidence = c 
Threshold lift = 1.00 
Output: 
A = { {r1, s1, c1, l1}, {r2, s2, c2, l2},……,{ rn, sn, cn, ln}}
         // set of positive association rules (ri)                                                                 
with rule support (si) , confidence (ci) and lift (li)
   Phase-II: Make use of the output in phase-1 for 
developing the dendrogram. The algorithm is given in fig.3. 
Key steps involved in this phase are described below. 
Step 1: Input to this phase is the output of phase-I, i.e., the 
set of association rules with the values of rule support, 
confidence and lift, given by set A. 
Step 2: Obtain the set of individual items present in at least 
one of the association rules in A by I = {I1, I2,……,Im}. 
Step 3: Start with tree level s initiated as 1, the itemset 
similarity is defined as very high value (tending to infinity), 
and number of clusters (itemsets) is m (the total number of 
itemsin I). Hence, the set of clusters at level 1, L
{1}
, contains 
all 1-item clusters in I. That is, L
{1}
 = {{I1},
{I2},……….,{Im}}. 
Step 4: To generate a set of candidate itemsets for next level 
(C
(s+1)
) each pair of itemsets in the previous level are joined. 
Step 5: To evaluate itemset similarity, i.e., similarity 
amongst the items in a cluster, each of the association rules 
is checked if all the items in the candidate set exist in the 
rule (either in the antecedent or in the consequent). If all the 
items exist in a rule and no other item is present in the rule, 
then sum up rule support, confidence and lift for the rule. 
Similarly, sums are obtained for all other rules where all the 
items are present in a rule. Sum of all such sums is taken as 
the measure of similarity. 
Step 6: To generate L
(s+1)
 (i.e.,  the set of itemsets in level 
(s+1)), the two itemsets are merged if their similarity is the 
highest value among all itemsets in C
(s+1)
.  Hence, L
{s+1}
 = 
{L
{s}
 - La
{s}
- Lb
{s}
}U{La
{s}
 U Lb
{s}
}.
Input: A = { {r1, s1, c1, l1}, {r2, s2, c2, l2},……,{ rn, sn,
cn, ln}}// set of  positive association rules (ri)
with rule support (si) , confidence (ci) and lift (li)
Output: DE  / Dendrogram: a set of ordered tuples 
Association rule based clustering algorithm: 
I = {I1, I2,……,Im}      //set of individual items present in 
at least one of the association rules in A 
s =1;              // the tree level 
sim = Infinity;            // the itemset similarity (inversely 
proportional to distance) is very high 
k = m;         // the total number of itemsets (clusters) at 
tree level L 
L{s} = {{I1}, {I2},……….,{Im}};    // the set of itemsets 
(clusters) at level s 
DE =  <s, sim, k, L{s}>;
for (s=1; k=1; s++)   //generate all candidate itemsets 
(join step) 
  for each itemset Li
{s} € L{s}  // € is for ‘belongs to’ 
  for each itemset Lj
{s} € {L{s} - Li
{s} }  
      for each association rule ri
          where all items in Li
{s} and Lj
{s} exist either in 
antecedent or in consequent of a rule   
and no other item is present in the rule 
{
 [sim]i = [rule support]i + [confidence]i + [lift]i // the 
itemset similarity equals   
the sum of rule support, confidence and lift 
  [sim]i++;  
  sim{Li
{s}, Lj
{s}} =  [sim]i++;  // the similarity for Li
{s}
and Lj
{s}
 { La
{s}, Lb
{s}} = arg max sim{Li
{s}, Lj
{s}};     
                  Li
{s}€ {L{s}, Lj
{s}€ L{s}      
// the two clusters with the highest similarity 
L{s+1} = {L{s} - La
{s}- Lb
{s}}U{La
{s} U Lb
{s}}; // the set of 
itemsets (clusters) in the next level 
sim = sim{La
{s}, Lb
{s}; k = k-1; L{s} = L{ s+1};  //update 
the parameters 
DE = DE U <s, sim, k, L{s}>;
}
return DE; 
Fig.3: The pseudo-code of the association rule based 
clustering algorithm
Step 7: The steps 4-6 are iterated with updating the 
dendrogram (DE) by adding the tuple <s, sim, k, L
{s}
> into 
DE where s =s+1, sim = sim{La
{s}
, Lb
{s}
,  k = k-1,  L
{s}
 = L
{
s+1}
.  Itereation stops when there is no association rule with 
all items of any pair of combined clusters/itemsets in a level 
and this level is the last level of clustering. Hence, all items 
may not be merged in one cluster as per the proposed 
algotithm in most of the cases. 
2010 International Conference of Soft Computing and Pattern Recognition 211
V. APPLYING THE PROPOSED CLUSTERING TECHNIQUE
The proposed technique of clustering was applied on the 
sale transaction data of nine grocery items with 10,000 sales 
transactions (at least one of the selected nine items is present 
in each transaction). Moreover, the customers have been 
chosen who are loyal to a store and hence, not moving from 
one store to another. This is guaranteed to a great extent by 
the location of the stores chosen where no other store is 
located nearby. Association rules were mined from the 
transaction data with different threshold values of rule 
support and confidence. It was found that for maximum 
threshold values of 0.20 for rule support and 0.65 for 
confidence, all nine items found place. With threshold values 
above this, it was found that at least one item was missing. 
For the chosen threshold values of rule support and 
confidence, only 15 association rules were found to qualify 
as given in table 2.  
Table 2. Association rules mined covering all the items 
Rule 
#
Antecedent  Consequent 
Rule 
Support 
Confidence Lift Sum 
1
Noodles – Maggi 
(200 g). and Tomato 
Sauce (Maggi) (200 
g). 
MDH Chicken 
Masala. 
0.23 0.93 2.76 3.92 
2
MDH Chicken 
Masala. 
Noodles – Maggi 
(200 g). and 
Tomato Sau 
0.23 0.68 2.76 3.67 
3
MDH Chicken 
Masala. and 
Noodles – Maggi 
(200 g). 
Tomato Sauce 
(Maggi) (200 g). 
0.23 0.87 2.57 3.67 
4
Tomato Sauce 
(Maggi) (200 g). 
MDH Chicken 
Masala. and 
Noodles – Maggi 
0.23 0.68 2.57 3.48 
5 Kismis (100 g). Basmati (1 kg). 0.21 0.71 1.25 2.17 
6
Tomato Sauce 
(Maggi) (200 g). 
MDH Chicken 
Masala. 
0.25 0.74 2.19 3.18 
7
MDH Chicken 
Masala. 
Tomato Sauce 
(Maggi) (200 g). 
0.25 0.75 2.19 3.19 
8
Coffee – Nescafe 
(50 g). 
Bournvita - 
Cadbury (500 g). 
0.23 0.67 1.32 2.22 
9
MDH Chicken 
Masala. and Tomato 
Sauce (Maggi) (200 
g). 
Noodles – Maggi 
(200 g). 
0.23 0.92 1.57 2.72 
10
Poha (Chura) (1 
kg). and Sooji (1 
kg). 
Noodles – Maggi 
(200 g). 
0.20 0.89 1.52 2.61 
11
Poha (Chura) (1 
kg). 
Noodles – Maggi 
(200 g). 
0.34 0.88 1.5 2.72 
12 Sooji (1 kg). 
Noodles – Maggi 
(200 g). 
0.35 0.85 1.46 2.66 
13
MDH Chicken 
Masala. 
Noodles – Maggi 
(200 g). 
0.26 0.78 1.35 2.40 
14
Tomato Sauce 
(Maggi) (200 g). 
Noodles – Maggi 
(200 g). 
0.25 0.73 1.25 2.23 
15 Sooji (1 kg). Basmati (1 kg). 0.37 0.72 0.85 1.94 
All nine items find place in one or more rules, either in 
the antecedent or in the consequent. The rules are given with 
the value of rule support, confidence and lift whereas the last 
column is the sum of these three measures of strength of 
association. Amongst these 15 rules, 14 rules were positive, 
i.e., with lift of more than 1.00 and only one rule was 
negative, i.e., with lift value less than 1.00.  
   For applying the proposed technique for clustering of the 
items, we consider only the positive association rules (i.e., 
with lift > 1.00). In 14 positive association rules, nine items 
(as selected for the purpose of clustering) are as given 
below. 
   Basmati (1 kg), Bournvita - Cadbury (500 g), Coffee – 
Nescafe (50 g),  Kismis (100 g),  MDH Chicken Masala, 
Noodles – Maggi (200 g), Poha (Chura) (1 kg),  Sooji (1 
kg), Tomato Sauce (Maggi) (200 g). 
   As per the proposed technique, in level 1 of the 
dendrogram, all individual items will be differen clusters as 
given below. 
   Level 1 (9 itemsets/clusters): {Basmati (1 kg)}, 
{Bournvita - Cadbury (500 g)}, {Coffee – Nescafe (50 g)}, 
{Kismis (100 g)}, {MDH Chicken Masala}, {Noodles – 
Maggi (200 g)}, {Poha (Chura) (1 kg)}, {Sooji (1 kg)}, 
{Tomato Sauce (Maggi) (200 g)} 
   {MDH Chicken Masala} and {Tomato Sauce (Maggi) 
(200 g)}are merged in level 2 as the similarity is highest for 
these two itemsets which is obtained from rules # 6 and #7. 
Each of these two rules contains only the two items, i.e., 
{MDH Chicken Masala} and {Tomato Sauce (Maggi) (200 
g)}. For rule#6, sum is 3.18 and for rule#7, the sum is 3.19 
and total is (3.18 +3.19), i.e., 6.37. This similarity of 6.37, 
as given by the sum of sums is higher than the similarity 
between any two itemsets. Hence, these two items are 
merged in level 2 as given below. 
  Level 2 (8 itemsets/clusters):  {Basmati (1 kg)}, 
{Bournvita - Cadbury (500 g)}, {Coffee – Nescafe (50 g)}, 
{Kismis (100 g)}, {MDH Chicken Masala, Tomato Sauce 
(Maggi) (200 g)}, {Noodles – Maggi (200 g)}, {Poha 
(Chura) (1 kg)}, {Sooji (1 kg)} 
   Adopting the similar logic, we obtain the clusters in 
different levels as given below. 
   Level 3 (7 itemsets/clusters):  {Basmati (1 kg)}, 
{Bournvita - Cadbury (500 g)}, {Coffee – Nescafe (50 g)}, 
{Kismis (100 g)}, {MDH Chicken Masala, Tomato Sauce 
(Maggi) (200 g), Noodles – Maggi (200 g)}, {Poha (Chura) 
(1 kg)}, {Sooji (1 kg)} 
   Level 4 (6 itemsets/clusters):  {Basmati (1 kg)}, 
{Bournvita - Cadbury (500 g), Coffee – Nescafe (50 g)},
{Kismis (100 g)}, {MDH Chicken Masala, Tomato Sauce 
(Maggi) (200 g), Noodles – Maggi (200 g)}, {Poha (Chura) 
(1 kg)}, {Sooji (1 kg)} 
   Level 5 (5 itemsets/clusters):  {Basmati (1 kg), Kismis 
(100 g)}, {Bournvita - Cadbury (500 g), Coffee – Nescafe 
(50 g)}, {MDH Chicken Masala, Tomato Sauce (Maggi) 
(200 g), Noodles – Maggi (200 g)}, {Poha (Chura) (1 kg)}, 
{Sooji (1 kg)} 
212 2010 International Conference of Soft Computing and Pattern Recognition
   Level 5 of the dendrogram with 5 clusters is the last level 
as there is not possibility of further clustering as no 
association rule exists in the list for further clustering. 
VI. CONCLUSION
   If we apply the algorithm of [5] for clustering which is 
based on only the strength of the rule support, in level 2, 
Basmati (1 kg) and Sooji (1 kg) are merged in one cluster as 
the distance (= 1- rule support) between them is minimum if 
we compare it amongst the distances between all the pairs of 
items. However, these two items are correlated by a 
negative association rule with lift value of 0.85 (which is 
less than 1.00). As per the interpretation of negative 
association rule, the antecedent item inhibits the purchase of 
the consequent item. With the objective of clustering items 
which are purchased together, this pair should not find place 
in any level of clustering in the dendrogram. Moreover, if 
Basmati (1 kg) and Sooji (1 kg) are put in one cluster, the 
former item (i.e., Basmati (1 kg)) no more gets an 
opportunity to be in one cluster with Kismis (100 g), 
whereas Kismis (100 g) and Basmati (1 kg) are positively 
associated as given in rule#5. The proposed algorithm in 
this paper is designed to avoid such undesirable clustering 
in the dendrogram. As CRM makes an attempt to design 
homogeneous strategy for all the items in a cluster, items 
associated by negative rule in the same cluster will have a 
negative effect on the objectives of a program. 
   Further work on devising more appropriate utility based 
clustering may be done as an extension of this work. The 
dendrogram can be used in designing promotional offer of 
itemsets/clusters obtained in various levels where based on a 
performance indicator, the best level of clustering can be 
identified for implementation. The same can be used for 
designing joint inventory replenishment policies with 
minimum total relevant cost. 
REFERENCES
[1] Prinzie, A., Poel, D. Van den, “Incorporating sequential information 
into traditional classification models by using an element/position-
sensitive SAM”,  Decision Support Systems 42 (2): 508-526, 2006. 
[2] Dunham, M. H..”DataMining:IntroductoryandAdvancedTopics.”, 
Pearson Education Inc.,NewJersey,USA, 2003. 
[3] Srikant, R. & Agrawal, R., “Mining Generalized Association Rules”, 
Proceedings of the 21st International Conference on Very Large 
Databases, Zurich, Switzerland, 407-419, 1995.
[4] Han, J., Kamber, M., “Data Mining: Concepts and Techniques”, 
Morgan Kaufmann Publishers, 2006.
[5] Tsai, Chieh-Yuan, Tsai, Chi-Yang, Huang, Po-Wen, “An association 
clustering algorithm for can-order policies in the joint replenishment 
problem”, International Journal of Production Economics, 117 (1), 
pp. 30–41, 2009. 
[6] Berry, M.J.A. and Linoff, G.S., “Data Mining Techniques”, pp. 311, 
Wiley India, 2008. 
2010 International Conference of Soft Computing and Pattern Recognition 213
