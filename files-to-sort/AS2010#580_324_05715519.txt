Text Categorization by MILO Tree Traversals 
Jau-Ji Shen 
Dept. of Management Information 
System  
National Chung Hsing University 
Taichung, Taiwan R.O.C 
e-mail: jjshen@dragon.nchu.edu.tw 
Wei-Cheng Huang 
Dept. of Management Information 
System  
National Chung Hsing University 
Taichung, Taiwan R.O.C 
e-mail: g9829004@mail.nchu.edu.tw 
Chia-Chuan Wu 
Dept. of Management Information 
System  
National Chung Hsing University 
Taichung, Taiwan R.O.C 
e-mail: g9729004@mail.nchu.edu.tw
  
Abstract—This paper presents a new method based on 
MILO for automatic text categorization. MILO classification 
technique is a new rule-based classification technique, which is 
different from traditional rule-based technique such as 
decision tree and association rule. MILO-based classification 
technique further analyzes the content structure of documents, 
and classifies them by finding underlying term that links 
across paragraphs. The previous research based on MILO 
extracted the classification rules from a single document at a 
time, and these extracted rules are locally associated with the 
document’s subject and independent with the rules that are 
extracted from other documents. Hence, this paper presents a 
tree structure which stores local rules from each document, 
and then through three tree: traversals, pre-order, post-order 
and breath-first-search order to transform local rules into 
global rules for text categorization. The experimental results 
have shown that our method has comparable accuracy against 
other techniques. 
Keywords-Rule-based; pattern matching; text categorizaton; 
term distribution 
I.  INTRODUCTION 
Text categorization is the task to assign unlabeled 
documents into predefined categories according to its 
content. In recent years, thanks to the fast development of 
Internet technology, there are enormous amounts of 
information available every day. Hence it is extremely 
important to efficiently organize this large amount of 
information. Now the text categorization is focusing on using 
a lot of automatic techniques. There are many well-known 
automatic techniques such as SVM [1], decision tree [2], and 
Bayes classifier [3]. These techniques can quickly and 
accurately classify documents. Among all kinds of 
classifiers, the rule-based classifiers do provide an interesting 
advantage. The classification rules are easy to understand 
and thus they can be added, deleted, and modified through 
human knowledge. There are many representative techniques 
such as decision tree [2], Olex [4], and association rule [5]. 
Generally speaking, the rule-based classifiers construct the 
classification rules by finding valuable patterns from training 
documents. Then, the document can be classified by 
performing pattern matching between rules and documents.  
For decision tree, a document is classified through 
checking the presence or absence of important terms, and the 
original content structures of the documents are ignored. To 
focus on this issue, previous MILO research [6] learned the 
rules by observing the distribution of terms in document 
paragraphs, and these classification rules are related with the 
topic of a certain document. But, usually the topic is not 
included in a single document. Therefore, these documents 
with related topic may conceal valuable classification rules. 
To establish these rules, this paper uses pre-order, post-order, 
and the breath-first-search order to convert local rules that 
are learning from single documents into global rules to 
classify documents. 
The structure of this paper is arranged as follows: Section 
2 describes the concept of our categorization method; 
Section 3 illustrates the experimental results, and Section 4 
provides the conclusions. 
II. BUILDING AN MILO CLASSIFIER 
A. Basic Concepts 
In general, a document should have the basic structure 
that can roughly be separated into three paragraphs, the 
beginning, the middle, and the end. We named these 3 
paragraph as P1, P2, and P3 respectively; P1 usually is the 
beginning part of a document; P2 is the main body in a 
document; and P3 draws the conclusion. Whereas between 
terms in different paragraphs should have the relation of 
special meaning, this reflected the semantics of language. 
We defined the relationship between terms as meaningful 
inner link objects, abbreviated as MILO. The MILO example 
diagram is shown as Figure1. 
 
 
Figure 1.  The concept of MILO. 
In Figure 1, the first paragraph illustrates origin of 
National Basketball Association (NBA) and its history of 
2010 Fourth International Conference on Genetic and Evolutionary Computing
978-0-7695-4281-2/10 $26.00 © 2010 IEEE
DOI 10.1109/ICGEC.2010.169
663
development in the past; the second paragraph describes the 
NBA’s 100th celebration of anniversary, including the 
names of retired basketball players at the ceremony; the third 
paragraph records the reflections of players on this 
celebration. In between the paragraphs, the relation of link 
constituted by terms is MILO, shown in Figure 1, and the 
dotted line represents different MILO patterns between the 
paragraphs. There are 3 kinds of MILOs with lengths of 1, 2, 
3 respectively. According to the example of MILO in Figure 
1, we can illustrate the composition of MILO as Table . 
TABLE I.  EXTRACTED MILOS FROM DOCUMENT 
MILO Content Direction pattern 
m0 [league?retire?player] [P1?P2?P3] 
m1 [league?celebrate?player] [P1?P2?P3] 
m2 [league?retire] [P1?P2] 
m3 [league?celebrate] [P1?P2] 
m4 [retire?player] [P2?P3] 
m5 [celebrate?player] [P2?P3] 
m6 [league?player] [P1?P3] 
m7 [league] [P1] 
m8 [retire] [P2] 
m9 [celebrate] [P2] 
m10 [player] [P3] 
By Table , we know that MILO has 7 kinds of direction 
patterns, so we will illustrate how to use MILO to build the 
classifier and to classify documents. Figure 2 contains the 
concept diagram of text categorization. 
 
Figure 2.  Construction for MILO classifier. 
First, according to pre-defined proportion, data set are 
divided into training sets and testing sets, and the training set 
is used to train classifier while the testing set is used for 
measuring classifier and producing accurate ratio. Each 
document in the training set initially went through paragraph 
normalization, and then the chi-square function was used to 
extract those terms considered important from training set. 
When all documents have been preprocessed, the documents 
are stored into a database accordingly with the category of 
the document. Then through the proposed categorization 
algorithm, the MILO patterns are extracted from database of 
each category, and stored into a tree structure, called MILO 
tree, which completed the construction of the classifier. 
When we want to classify the unlabeled document, we need 
to extract all MILOs in the unlabeled document, and then 
match them with the MILO classifier. Lastly, we assign one 
or more categories to the unlabeled document, and the 
process of text categorization is completed. 
B. Preliminaries and Notations 
To simply illustrate the detailed algorithm, we have 
defined some notations shown as follows: 
c:  a category, 
dc: a training document of c, 
Dc: a set of training documents in c, 
Pn: the n-th paragraph of dc, n = 1, 2, 3 in this paper, 
tni: an arbitrary term in Pn, 
DBPn: the database which stores all Pn of each dc in Dc, 
T: the number of selected distinct terms with highest chi-
squared value, 
TS(DBPn)T: a reduced term set from DBPn consisting with 
selected T terms, 
Treec: a tree structure which stores the MILOs of c. 
C. Data Set Preprocessing 
We must normalize each document and divide them into 
three paragraphs before finding MILO patterns. Initially the 
number of distinct terms in a document would be counted. 
After that, depending on the appearing sequence of these 
terms in the document, they would be separated into P1, P2, 
and P3, such that these three paragraphs have the same 
number of distinct terms. If the number of distinct terms 
cannot be divided by three, then P2 would have the greatest 
number of terms. After all documents have been 
preprocessed, P1, P2 and P3 in all documents are stored into 
DBP1, DBP2, and DBP3, respectively. Referred on Stopwords 
List [7], some commonly used words from three database 
DBP1, DBP2, and DBP3 are deleted, after that, let TS(DBPn) 
presents the reducing set of database DBPn. However, we 
need to consider the weightiness of term in each paragraph, 
thus finding out great weightiness of terms in each paragraph 
is required. Then we calculate weightiness of each term by 
using the chi-square function with TS(DBP1), TS(DBP2), 
TS(DBP3), for measuring the relationship between term ti and 
category c, the formula of chi-square function is developed 
as shown below. 
))()()((
)**(*),(
2
2
DCBADBCA
CBDANcti ++++
?
=?
             
(1) 
664
A: occurrences of term ti in category c, 
B: occurrences of other terms in category c, 
C: occurrences of term ti in other categories, 
D: occurrences of other terms in other categories, 
N: A+B+C+D, total occurrences of all terms in all 
categories. 
Through Formula (1), we could calculate the weightiness 
of terms with TS(DBP1), TS(DBP2), and TS(DBP3), 
respectively. The selected T terms, that have greatest chi-
square value, are used to compose the set of important terms 
from each paragraph. 
D. Construction of MILO Classifier 
Algorithm of extracted MILOs in a document dc is shown 
as follows: Mc is a set of MILOs in category c, OMc is a set 
of number of occurrences with respect to each MILO in M, 
DMc is direction of MILOs in category c. 
Algorithm Extract MILOs  
Input: P1, P2 and P3 of dc, TS(DBP1)T, TS(DBP2)T, 
TS(DBP3)T,  
Output: Mc, DMc, OMc 
Methods: 
(1) /* t1i, t2j and t3k represent the arbitrary terms in P1, P2,
and P3, respectively*/  
(2) /* o1i,o2j and o3k represent the occurrences of t1i, t2j and t3k
in P1, P2, and P3, respectively*/ 
(3)  For each distinct t1i in P1 and t1i ?  TS(DBP1)T
(4)    For each distinct t2j in P2 and t2j ?  TS(DBP2)T 
(5)      For each distinct t3k in P3 and t3k ?  TS(DBP3)T
(6)         If dc has length 3 MILO 
(7)           Mc?Mc ? {[ t1i?t2j?t3k]}  
(8)           DMc?DMc ? {[P1?P2?P3]}  
(9)           OMc?OMc ? {( o1i + o2j + o3k)/3}  
Steps(3)-(5) delete unimportant terms from each 
paragraph, for example, Step(3) would delete those terms 
that do not belong to TS(DBP1)T. Step(6) checks whether has 
length 3 MILO in dc, if so, then steps(7)-(9) would be 
executed; otherwise, do not extract MILO from dc. Step(7) 
stores the content of a MILO into Mc, and Step(8) stores the 
direction of a MILO into DMc. Step(9) stores the occurrences 
of a MILO in dc into OMc. 
MILOs would be stored into Treec while extracted 
MILOs by Algorithm Extract MILOs from documents in Dc, 
and the concept diagram is shown as Figure 3. For example, 
referred to Figure 3, if a length 3 MILO 
[league?retire?player] in dc, “league” would be stored into 
Level 2 that is the node of “root”; “retire” would be stored 
into Level 3; “player” would be stored into Level 4. In Level 
5, the square cell would be stored with the occurrence of 
MILO in category c, so if a MILO [against?star?retire] is 
extracted from a document, the occurrence is 8, and if the 
same MILO is extracted from another document, then the 
content of square cell in Level 5 would be 15. In this paper, 
only length 3 MILO was taken to construct Treec, because 
length 3 MILO is the most integrated and the most 
diversified across the whole content of document, and it also 
has the greatest relationship with the topic of a document, 
and the content includes the contents of length 2 and length 1 
MILO, but other length MILO are still used in next section. 
After all the MILOs are extracted from documents in Dc, the 
construction of Treec is then finished. Now, for controlling 
the quality of the MILO classifier, we must filter out some of 
the non-representative MILOs, so we let ? presents one of the 
MILOs in Treec. If support of ? is less than threshold ?, then 
we delete ? from Treec. The formula for calculating support 
is shown as follows: 
c in  MILOslla of occurences The
c in  of occurences The ?
                 (2) 
The above process is used to construct a MILO tree of 
category c. After all of the MILO trees have been dealt with 
by the above process, the MILO classifier is then finished. 
 
Figure 3.  MILO tree of Category c. 
E. Docunemt Categorization with MILO 
When an unlabeled document performs paragraph 
normalization and filters out common words depending on 
Stopwords List, and then we have to use all the terms to 
construct MILOs from unlabeled documents. Difference 
from the construction of classifier is that we only uses length 
3 MILO in this section, thus we will take all kinds of length 
to match the MILO patterns. The algorithm of categorization 
is shown as follows. 
Algorithm Categorization
Input: classifier, d: unlabeled document, ?: a categorization 
factor, C: a set of categories
Output: classification results
Method:
(1) Choose a type of traversal  
(2) For each MILO ??d
(3)     For each c?C 
(4)        Trace Treec by the selected type of traversal and 
store the result into Sc 
(5)             Matched_time[c] ? ?’s matched times in Sc 
(6)     For each c?C, Total?Total + Matched_time[c] 
(7)     /* Total is used to store the total matched times */
(8)     For each c?C, Decision array[c] ? Decision array[c] 
+ Matched_time[c] /Total 
(9)    Find the max total confidence value in Decision array
and store into MaxDecideConf 
(10) For each c?C, if(Decision array[c] ? MaxDecideConf 
* ?), then assign c to d
In Step(1), we first choose a type of traversal for trace 
Treec, all of three types traversal can be chosen and Step(4) 
665
stores the string of result into Sc by trace Treec. For example, 
referred to Figure 3, we traced the MILO tree, the result is as 
shown follows: [league, retire, player, star, retire, celebrate, 
season, against, effort, league, star, retire], and then, Step(5) 
would perform pattern matching between the MILOs of d 
and Sc. After that the times they matched will be produced 
and stored into the array Matchedtime[c], for example, if one 
of MILO ? in d is [star?retire], after performing pattern 
match with Sc as mentioned above, and the result is matched 
2 times in this case. Steps(6)-(8) would then calculate the 
confidence value in each category and store it into Decision 
array. Step(9) would pick the greatest confidence value from 
Decision array and present MaxDecideConf, and Step(10) 
determines whether the confidence value of category c in 
Decision array[c] is not less than that of MaxDecideConf * 
?; if so, then the category c would be assigned to the 
unlabeled document d. 
TABLE II.  RESULTS OF CATEGORIZATION TAKING ON THE WHOLE TRAINING SET (T=100, µ=0.000053, ? = 0.75)  
 Preorder  Postorder 
Breadth-First 
-Search Naive Bayes 
Bayes 
 Nets C4.5 
Rocchio’s  
method 
Rullo  
et al. 
Pietramala  
et al. HARMONY
acq 93.0 93.6 93.1 91.5 88.3 85.3 92.1 87.0 87.2 95.3 
corn 66.7 69.2 67.7 47.3 76.4 87.7 86.2 87.9 90.8 78.2 
crude 79.9 80.0 79.2 81.0 79.6 75.5 81.5 80.1 77.0 85.7 
earn 96.0 95.9 95.7 95.9 95.8 96.1 96.1 96.1 95.3 98.1 
grain 36.1 36.1 35.5 72.5 81.4 89.1 79.5 90.8 91.6 91.8 
interest 72.4 73.2 71.4 58.0 71.3 49.1 72.5 82.2 64.2 77.3 
money-fx 79.1 76.6 77.8 62.9 58.8 69.4 67.6 69.3 66.5 80.5 
ship 87.3 87.4 87.4 78.7 84.4 80.9 83.1 78.2 74.1 86.9 
trade 80.9 82.8 81.3 50.0 69.0 59.2 77.4 52.6 61.8 88.4 
wheat 71.0 67.1 68.8 60.6 82.7 85.5 79.4 91.4 87.5 62.8 
Micro F 87.5 87.6 87.1 84.2 85.0 85.2 87.2 86.5 86.4 92.0 
III. EXIPERMTNTAL RESULTS 
We have employed micro F1 in the survey of F. 
Sebastiani [8] for classifier evaluation. Table ? shows the 
experimental results. The results of naive Bayes, Bayes nets, 
C4.5, and Rocchio’s method are obtained from [9]; the 
results of Rullo et al. and Pietramala et al.’s method are 
obtained from [10] and [11], respectively. Table ? clearly 
reveals the comparison results, it shows that our 
classification method based on three kinds of traversal are 
better than the other methods except HARMONY [12].  
We have also conducted another interesting experiment: 
we used all the training documents to select the three terms 
set, but only take a few training documents to construct the 
classifier, the results of selecting top 5 training documents 
for each category (the documents are sorted according to 
their file ID) are shown in Table ? . It reveals that the 
accuracy of our method is better than HARMONY. 
TABLE III.  RESULTS OF SELECTING 5 TRAINING DOCUMENTS FOR 
EACH CATEGORY (T=100, µ=0, ? = 0.75) 
Preorder  Postorder Breadth-First-Search HARMONY
Micro F 83.2  83.3  83.0  53.1 
IV. CONCLUSION 
This paper presents a classification technique by 
transforming local patterns to global patterns. The 
experimental results shows that the three kinds of traversal 
have a comparable accuracy against other techniques, and 
the proposed method can reach a high accuracy by selecting 
five training document documents in each category. On the 
internet, there are many kinds of documents such as free talk 
and news, and the content structures in these documents have 
different style. Therefore, we have future plans to investigate 
the efficiency of MILO in other benchmarks such as WebKB 
and Ohsumed, and trying to find an appropriate way to 
further enhance the MILO’s performances. 
REFERENCES 
[1] T. M. Cover, and J. A. Thomas, Elements of Information Theory: 
Wiley-Interscience, 1991. 
[2] J. R. Quinlan, C4.5: Programs for Machine Learning (Morgan 
Kaufmann Series in Machine Learning): Morgan Kaufmann, 1993. 
[3] D. D. Lewis, “Naive (Bayes) at Forty: The Independence Assumption 
in Information Retrieval ” in Proceedings of ECML-98, 10th 
European Conference on Machine Learning, 1998. 
[4] P. Rullo, V. L. Policicchio, C. Cumbo et al., “Olex: effective rule 
learning for text categorization,” IEEE Transactions on Knowledge 
and Data Engineering, vol. 21, no. 8, pp. 1118-1132, 2009. 
[5] M. L. Antonie, and O. R. Zaiane, “Text document categorization by 
term association,” in Proceedings of 2002 IEEE International 
Conference on Data Mining, Maebashi city, Japan, 2002, pp. 19-26. 
[6] J.-J. Shen, and C.-C. Wu, “Meaningful Inner Link Objects for 
automatic text categorization,” in Proceedings of the 15th 
International Conference on Intelligent Information Hiding and 
Multimedia Signal Processing, Kyoto, Japan, 2009, pp. 266-269. 
[7] C. Fox, “A stop list for general text,” SIGIR Forum, vol. 24, no. 1-2, 
pp. 19-21, 1989. 
[8] F. Sebastiani, “Machine learning in automated text categorization,” 
ACM Computing Surveys, vol. 34, no. 1, pp. 1-47, 2002. 
[9] Y.-C. Chang, S.-M. Chen, and C.-J. Liau, “Multilabel text 
categorization based on a new linear classifier learning method and a 
category-sensitive refinement method,” Expert Systems with 
Applications, vol. 34, no. 3, pp. 1948-1953, 2008. 
[10] P. Rullo, C. Cumbo, and V. L. Policicchio, “Learning rules with 
negation for text categorization,” in Proceedings of the 2007 ACM 
symposium on Applied computing, Seoul, Korea, 2007. 
[11] A. Pietramala, V. L. Policicchio, P. Rullo et al., “A Genetic 
Algorithm for Text Classification Rule Induction,” in Proceedings of 
the European conference on Machine Learning and Knowledge 
Discovery in Databases - Part II, Antwerp, Belgium, 2008. 
[12] J. Wang, and G. Karypis, “On mining instance-centric classification 
rules,” IEEE Transactions on Knowledge and Data Engineering vol. 
18, no. 11, pp. 1497-1511, 2006. 
666
