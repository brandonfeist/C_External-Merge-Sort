Constrained Frequent Itemset Mining
from Uncertain Data Streams
Carson Kai-Sang Leung1, Boyu Hao, Fan Jiang
Department of Computer Science, The University of Manitoba
Winnipeg, MB, Canada
1kleung@cs.umanitoba.ca
Abstract—Frequent itemset mining is a common data min-
ing task for many real-life applications. The mined frequent
itemsets can be served as building blocks for various patterns
including association rules and frequent sequences. Many existing
algorithms mine for frequent itemsets from traditional static
transaction databases, in which the contents of each transaction
(namely, items) are definitely known and precise. However, there
are many situations in which ones are uncertain about the
contents of transactions. This calls for the mining of uncertain
data. Moreover, there are also situations in which users are
interested in only some portions of the mined frequent itemsets
(i.e., itemsets satisfying user-specified constraints, which express
the user interest). This leads to constrained mining. Furthermore,
due to advances in technology, a flood of data can be produced
in many situations. This calls for the mining of data streams. To
deal with all these situations, we propose tree-based algorithms
to efficiently mine streams of uncertain data for frequent itemsets
that satisfy user-specified constraints.
I. INTRODUCTION
Frequent itemset mining [3], [18], [23] aims to search for
implicit, previously unknown, and potentially useful sets of
items (aka itemsets) that are frequently co-occurring. The
mined frequent itemsets (FIs) can be used in as building
blocks of various patterns (e.g., association rules, correlation,
sequences, episodes, maximal patterns, closed patterns) for
many real-life applications. This explains why the problem
of mining FIs has been the subject of numerous studies
since its introduction [2]. In the early days, many of the
algorithms proposed were Apriori-based. They depend on a
generate-and-test paradigm: They find FIs from the transaction
database (DB) by first generating candidates and then checking
their support (i.e., their occurrences) against the DB. Han
et al. [14] improved efficiency of the mining process by
proposing the FP-growth algorithm, which uses a restricted
test-only approach (i.e., does not generate candidates, and
only tests for support). This algorithm constructs an extended
prefix-tree, called Frequent Pattern tree (FP-tree), to capture
the contents of the DB.
The above algorithms mine traditional static DBs (e.g., DBs
of market basket transactions, Web logs) containing precise
data. When mining these precise data, users definitely know
whether an item (or an event) is present in, or is absent
from, a transaction in the static DBs. However, there are
situations (e.g., medical diagnosis, environmental surveillance,
quantitative economics/survey research) in which users are
uncertain about the presence or absence of some items or
events [6], [15], [16], [27]. To elaborate, let us consider
a situation where a physician highly suspects (but cannot
guarantee) that a patient suffers from the H1N1 flu. The
uncertainty of such suspicion can be expressed in terms of
existential probability. In this uncertain DB of patient records,
each transaction ti represents a patient' s visit to a physician' s
office. Each item within ti represents a potential disease, and
is associated with an existential probability expressing the
likelihood of a patient having that disease in ti. For instance,
in ti, the patient has a 90% likelihood of having the H1N1
flu, and a 30% likelihood of having a seasonal flu regardless
of catching the H1N1 or not. With this notion, each item in
a transaction ti in traditional DBs containing precise data can
be viewed as an item with a 100% likelihood of being present
in ti. As mining of uncertain data [1], [5], [29] is in demand,
some algorithms (e.g., U-Apriori [7], UF-growth [22]) have
recently been designed to find all FIs from uncertain data.
Note that there are real-life situations in which users may
have some particular phenomena in mind on which to focus
the mining. For example, a store manager may want to find
groups of customers having an average age older than 65 or
sets of popular merchandise items having a total price exceeds
$500. Without user focus, the user often needs to wait for a
long period of time for numerous FIs, out of which only a tiny
fraction may be interesting to users. This leads to constrained
frequent itemset mining [4], [13], which aims to find only
the FIs that satisfy the user-specified constraints. DCF [17],
CAP [24] and FIC [25] are some algorithms that mine static
DBs of precise data for constrained FIs.
Constrained frequent itemset mining is not confined to pre-
cise data; it can also be applied to uncertain data. To elaborate,
having some particular phenomena in mind on which to focus
the mining while users are uncertain about the presence or
absence of items in a DB is not unusual. For instance, when
analyzing laboratory test data, there are some known factors
(e.g., human reaction time, measuring errors) contributing to
the uncertainty of the data. Analysts may be interested in only
the data belonging to patients observed to have abnormal blood
cell counts (rather than all the patient data). Hence, algorithms
(e.g., U-FPS [19]) for mining constrained FIs from uncertain
data are in need.
The automation of measurements and data collection has
produced tremendously huge amounts of data in many real-
life application areas. The recent development and increasing
978-1-4244-6523-1/10/$26.00 © 2010 IEEE ICDE Workshops 2010120
TABLE I
OUR PROPOSED ALGORITHMS VERSUS THEIR MOST RELEVANT ALGORITHMS
Our proposed
CAP [24], U-Apriori [7], U-FPS FP-streaming UF-streaming UF-streaming+,
DCF [17], UF-growth [22] [19] [12] [20] UF-streaming?,
FIC [25] CUF-streaming
Constrained mining ? ? ?
Mining uncertain data ? ? ? ?
Stream mining
? ? ?
use of a large number of sensors has added to this situation.
Consequently, these advances in technology have led to a
flood of data. We are now drowning in streams of (precise or
uncertain) data but starving for knowledge [26], [28]. In order
to make sense of these dynamic streaming data, algorithms
for mining data streams [8], [9], [11], [21] are in demand.
FP-streaming [12] and UF-streaming [20] are some examples
of algorithms that mine FIs from data streams.
Among the above algorithms (see Table I), the CAP, DCF
and FIC algorithms all find constrained FIs, but they mine
precise data. Both U-Apriori and UF-growth mine uncertain
data, but they find all unconstrained FIs. The U-FPS algorithm
mines uncertain data for FIs satisfying constraints, but it does
not handle data streams. The FP-streaming algorithm was
designed for mining streams of precise data; the UF-streaming
algorithm was designed for mining streams of uncertain data.
However, both find all FIs instead of only those satisfying the
constraints. Hence, a natural question to ask is: Is it possible
to mine streams of uncertain data for only those FIs that
satisfy user-specified constraints? In response to this question,
we propose three tree-based algorithms for mining streams of
uncertain data for FIs that satisfy user-specified constraints.
Here, our key contribution of this paper is the non-trivial in-
tegration of (i) mining uncertain data, (ii) constrained mining,
and (iii) stream mining. The resulting tree-based algorithms
avoid the candidate generate-and-test paradigm when mining
constrained FIs from uncertain data streams.
On the surface, the three proposed algorithms may appear to
handle a very specialized situation as they were designed for
mining constrained FIs from uncertain data streams. However,
it is important to realize that these algorithms are not confined
to this situation. They can be serve as alternatives to algorithms
for mining constrained or unconstrained FIs, from precise or
uncertain data, in the traditional DB or data streams (e.g.,
mining unconstrained FIs from traditional DBs containing
uncertain data). Thus, in terms of applicability, the proposed
algorithms can be used in a suitably broad range of real-life
applications.
This paper is organized as follows. The next section pro-
vides background and related work. In Sections III–V, we
describe our algorithms for mining constrained FIs from
streams of uncertain data. Section VI shows experimental
results. Finally, conclusions are presented in Section VII.
II. BACKGROUND AND RELATED WORK
In this section, we provide background on topics related
to mining data streams of uncertain data for FIs that satisfy
user-specified constraints.
A. Uncertain Data
When compared with precise data, each uncertain data trans-
action contains items and their existential probabilities. The
existential probability P (x, ti) of an item x in a transaction ti
indicates the likelihood of x being present in ti. Using the
“possible world” interpretation of uncertain data [7], [22],
there are two possible worlds for an item x and a transaction ti:
(i) the possible world W1 where x ? ti and (ii) the possible
world W2 where x ? ti. Although it is uncertain which of
these two worlds be the true world, the probability of W1 be
the true world is P (x, ti) and that of W2 is 1? P (x, ti).
In general, there are many items in each of the n transactions
in a TDB. Hence, the expected support of an itemset X in
the TDB can be computed by summing the support of X in
possible world Wj (while taking in account the probability of
Wj to be the true world) over all possible worlds:
expSup(X) =
?
j
[sup(X) in Wj × prob(Wj)] , (1)
where sup(X) in Wj , denoting the support of X in possible
world Wj , can be computed by counting the number of
transactions contain X in Wj . The probability of Wj to
be the true world, denoted by prob(Wj), can be computed
by
?n
i=1
(?
x?ti in WjP (x, ti)×
?
y ?ti in Wj[1? P (y, ti)]
)
.
When items within X are independent, Equation (1) can be
simplified [10] to become the following:
expSup(X) =
n?
i=1
(?
x?X
P (x, ti)
)
. (2)
With this setting, an itemset X is considered frequent if its
expected support equals or exceeds the user-specified support
threshold minsup.
B. Constraints
To express their interest in finding FIs that satisfy cer-
tain criteria, users can specify SQL-style constraints [17],
[24] such as aggregate constraints C1 ? min(X.WBC ) ?
10000/?L, C2 ? max(X.RBC ) ? 6.1 × 106/?L,
C3 ? avg(X.Temperature) ? ?5?C, and C4 ?
sum(X.Rainfall ) ? 200mm. Here, for medical laboratory
test results, constraint C1 says that the minimum white blood
cell count of all patients in X is at least 10,000 cells per
microlitre of blood and C2 says that the maximum red blood
cell count of all patients in X is at least 6.1 million cells
per microlitre of blood. For another domain (meteorological
records collected by wireless sensors), C3 says that the average
hourly temperature in X is at least ?5?C, and C4 says that
the total rainfall of all records in X is at least 200mm.
121
In order to efficiently find FIs that satisfy these constraints,
we better explore properties (e.g., anti-monotonicity, mono-
tonicity [18]) that are possessed by these constraints. Note that
C1 is anti-monotone because whenever X violates C1 (i.e., the
minimum WBC count < 10000/?L), all supersets of X also
violate C1 (as adding more patients to X will not increase the
minimum WBC count). C2 is monotone because whenever X
satisfies C2 (i.e., the maximum RBC count < 6.1× 106/?L),
all supersets of X also satisfy C2 (as adding more patients to
X will not lower the maximum RBC count).
C. Data Streams
When compared with traditional static data, streams are
continuous and unbounded. Moreover, data in the streams are
not necessarily uniformly distributed. Their distributions are
usually changing with time. Hence, we no longer have the
luxury of performing multiple data scans. Once the streams
flow through, we lose them. Moreover, a currently infrequent
itemset may become frequent in the future, and vice versa. We
have to be careful not to prune infrequent itemsets too early;
otherwise, we may not be able to get complete information
such as the support values of some itemsets (as it is impossible
to recall the pruned itemsets).
To mine FIs from data streams, Giannella et al. [12] de-
signed the FP-streaming algorithm. For an incoming batch of
transactions in a data stream, FP-streaming calls the FP-growth
algorithm [14] with a threshold that is lower than the usual
minimum support threshold minsup to find “frequent” itemsets.
Let us call this lower threshold preMinsup. Then, an itemset is
“frequent” if its actual support is no less than preMinsup. Note
that, although we are interested in truly frequent itemsets (i.e.,
itemsets with support ? minsup > preMinsup), FP-streaming
uses preMinsup in attempt to avoid pruning an itemset too
early. An itemset X having preMinsup ? sup(X) < minsup
is currently infrequent but may become frequent later; so, X
is not pruned by FP-streaming. Once the “frequent” itemsets
are found, the FP-streaming algorithm stores and maintains
these itemsets in another tree structure called FP-stream
before handling the next batch of streaming transactions. It is
important to note that, while the FP-streaming algorithm finds
“frequent” itemsets from dynamic streams of precise data, it
neither handles uncertain data nor does it explore constraints.
III. UF-STREAMING+ : MINING CONSTRAINED FREQUENT
ITEMSETS FROM UNCERTAIN DATA STREAMS
Inspired by FP-streaming, we propose in this section an
algorithm—called UF-streaming+—for mining constrained
FIs from uncertain data streams. When the uncertain data
stream flows in, our proposed algorithm applies the UF-growth
mining technique to the first batch of transactions in the
stream. Recall that UF-growth captures the contents of (a
static DB of) uncertain data in a tree structure called UF-tree,
and tries to find itemsets having expected support ? user-
specified threshold. Here, UF-streaming+ captures the con-
tents of transactions in the first batch of dynamic streams of
uncertain data in the UF-tree. A key difference between this
UF-tree and the usual FP-tree is that each tree node in the
latter consists of two components—namely, an item and its
(actual) support—whereas each node in the UF-tree contains
an extra component—i.e., its occurrence count in addition to
the item itself and its (expected) support. For precise data, the
actual support of an item is the same as its occurrence count:
If an item x appears in a transaction ti, x occurs once in ti
with the actual support of x equals 1. For uncertain data, if
x appears in ti with existential probability P (x, ti), x occurs
once in ti with the expected support of x equals P (x, ti).
Once the UF-tree is constructed, our proposed
UF-streaming+ algorithm mines “frequent” itemsets from
this tree using preMinsup (instead of the usual minsup).
An itemset X is “frequent” if expSup(X) ? preMinsup.
Recall that data in the streams are not necessarily uniformly
distributed, an itemset X that is infrequent in the current
batch may be frequent in subsequent batches in the current
sliding window (which may make X a frequent itemset in
the current window). As data streams are continuous and
unbounded, we can no longer go back to the current batch
and reconsider X once we moved to subsequent batches.
Consequently, if expSup(X) is currently slightly lower than
minsup, we better keep X. Otherwise, we may miss X (a
possibly frequent itemset).
UF-streaming+ mines “frequent” itemsets from the UF-tree
in a depth-first divide-and-conquer manner. The algorithm first
finds “frequent” domain items x1, x2, x3, x4, · · · . It then forms
projected DBs for x1 and its extensions (e.g., {x1}-, {x1, x2}-,
{x1, x2, x3}-projected DBs) to find “frequent” itemsets con-
taining x1. Afterwards, the algorithm forms projected DBs for
x2 and its extensions to find “frequent” itemsets containing x2.
This mining process is repeated for all other “frequent” domain
items (e.g., x3, x4, · · · ).
As a result, UF-streaming+ found all the “frequent” item-
sets. It then stores and maintains them in another tree structure
called UF-stream. In this structure, each path represents a
“frequent” itemset. Common items in itemsets share the tree
path in a similar fashion as in the FP-tree or the UF-tree.
However, each node in this UF-stream structure contains (i) the
item and (ii) a window table (containing a list of w expected
support values, one for each batch of streaming transactions).
As users are often interested in recent data than older data,
the UF-stream structure focuses on capturing only the w most
recent batches of transactions in the stream. So, when a new
batch of transactions flows in, the window slides and the
expected support of each node in the UF-stream structure also
shifts.
The above mining procedure is repeated for each subsequent
batch of transactions in the uncertain data streams. In other
words, for each batch, our proposed UF-streaming+ algo-
rithm (i) finds “frequent” itemsets and (ii) stores the mined
“frequent” itemsets in the UF-stream structure by sliding the
window and shifting the w expected support of each node in
the UF-stream structure so as to ensure that it always captures
the contents of the w most recent batches of transactions
in uncertain data streams. As a post-processing step, the
122
mined “frequent” itemsets stored in the UF-stream structure
are finally checked against the user-specified constraints. Only
those satisfying the constraints are returned to users. To gain
a better understanding of the UF-streaming+ algorithm, let us
consider Example 1.
Example 1: Consider the following uncertain data stream:
Batch Transactions Contents
t1 {a:0.9, b:0.7, c:0.7, e:0.6}
first t2 {a:0.9, c:0.8, e:0.7}
t3 {b:0.9, d:0.9, e:0.1}
t4 {b:1.0, c:0.3, d:1.0}
second t5 {a:0.9, c:1.0}
t6 {b:0.4, c:0.5, d:1.0}
t7 {a:0.8, c:0.9}
third t8 {b:1.0, c:0.1, d:1.0}
t9 {a:0.9, c:0.9, d:0.2}
with auxiliary information:
Items a b c d e
WBC (×103/µL) 11.5 11.0 10.5 9.5 9.0
Here, each transaction contains items and their corresponding
existential probabilities (e.g., the existential probability of
item a in transaction t1 is 0.9).
Let the user-specified support threshold minsup be set
to 1.2, and let the user-specified constraint be C1 ?
min(X.WBC) ? 10000/?L. Our proposed UF-streaming+
algorithm applies the UF-growth mining technique to the first
batch of transactions in the uncertain data stream using a
preMinsup lower than minsup (say, preMinsup = 0.9). The
algorithm constructs a UF-tree by scanning and inserting each
transaction into the UF-tree. It first inserts the contents of t1
into the tree, and results in a tree branch ?(a:0.9):1, (b:0.7):1,
(c:0.7):1, (e:0.6):1?. It then inserts the contents of t2 into
the UF-tree. Since the expected support of a in t2 is the
same as the expected support of a in an existing branch
(i.e., the branch for t1), the tree node (a:0.9) can be shared.
So, the occurrence count for the node (a:0.9) is incremented
to 2, and the remainder of t2 is added as a child of the
node (a:0.9):2. As a result, we get the tree branch ?(a:0.9):2,
(c:0.8):1, (e:0.7):1?. Afterwards, the contents of t3 are inserted
as a new branch ?(b:0.9):1, (d:0.9):1, (e:0.1):1? because the
node (b:0.9):1 cannot be shared with the node (a:0.9):2.
Consequently, at the end of the tree construction process, we
get the UF-tree, shown in Fig. 1(a), capturing the important
contents of the first batch of uncertain data.
Once the UF-tree is constructed for the first batch, the
algorithm first finds the “frequent” domain items a, b, c, d and
e (with their corresponding accumulated expected support of
1.8, 1.6, 1.5, 0.9 and 1.4 ? preMinsup). It then recursively
mines “frequent” itemsets from this tree with preMinsup as fol-
lows. It starts with item e (with expSup({e})=1.4): It extracts
from two tree paths—namely, (i) ?(a:0.9), (b:0.7), (c:0.7)? that
occurs once with (e:0.6) and (ii) ?(a:0.9), (c:0.8)? that occurs
once with (e:0.7)—and forms the {e}-projected database.
Then, expSup({a, e}) = (1 × 0.9 × 0.6) + (1 × 0.9 × 0.7)
= 1.17 ? preMinsup, and expSup({c, e}) = (1 × 0.7 × 0.6)
+ (1 × 0.8 × 0.7) = 0.98 ? preMinsup. (Note that item-
sets {d, e} & {a, c, e} are infrequent as expSup({d, e}) =
(e:0.1):1
(d:0.9):1
(b:0.9):1(a:0.9):2
(e:0.7):1
(c:0.8):1
(e:0.6):1
(c:0.7):1
(b:0.7):1
(a) The UF-tree for transactions in the first batch
d[0,2.0]c[1.35,0.9]
a[1.8,0.9]
e[1.17,0]
b[1.6,1.4] c[1.5,1.8] d[0.9,2.0] e[1.4,0]
e[0.98,0]
(b) UF-stream for “frequent” itemsets found in the 1st & 2nd batches
d[2.0,1.0]c[0.9,1.53]
a[0.9,1.7]
e[0,0]
b[1.4,1.0] c[1.8,1.9] d[2.0,1.2] e[0,0]
e[0,0]
(c) UF-stream for “frequent” itemsets found in the 2nd & 3rd batches
Fig. 1. The UF-tree and UF-stream structures for Example 1
1 × 0.9 × 0.1 = 0.09 < preMinsup and expSup({a, c, e})
= (1 × 0.9 × 0.7×0.6) + (1 × 0.9 × 0.8×0.7) ? 0.88 <
preMinsup.)
Next, the algorithm extracts appropriate paths to form
the {d}-projected database, and finds no “frequent” itemsets.
Afterwards, the mining process ends with item c. As a result,
UF-growth found “frequent” itemsets {a}, {a, c}, {a, e}, {b},
{c}, {c, e}, {d} and {e} (with their corresponding expected
support of 1.8, 1.35, 1.17, 1.6, 1.5, 0.98, 0.9 and 1.4 ?
preMinsup). (Among them, only {a}, {a, c}, {b}, {c} and {e}
are truly frequent as their expected support values ? minsup.)
Let the window size w=2 batches. Our proposed
UF-streaming+ algorithm then stores the eight mined “fre-
quent” itemsets in a UF-stream structure, which consists of
eight nodes representing the eight itemsets. Afterwards, the
second batch of uncertain data stream flows in. Our proposed
UF-streaming+ algorithm applies the same mining procedure
to the batch. Specifically, it first constructs a new UF-tree, from
which “frequent” itemsets {a}, {a, c}, {b}, {b, d}, {c} and {d}
(with their corresponding expected support values of 0.9, 0.9,
1.4, 1.4, 1.8 and 2.0) can be found. It then updates the existing
UF-stream structure by storing these itemsets in it. The result-
ing UF-stream structure, as shown in Fig. 1(b), consists of nine
nodes (due to the addition of the node d[0,1.4] on the path
?b[1.6,1.4], d[0,1.4]? representing the new “frequent” itemset
{b, d}with expSup=0 in the first batch and expSup=1.4 in the
second batch). The node b[1.6,1.4] represents the “frequent”
itemset {b} with expected support values of 1.6 and 1.4 in the
first and second batches respectively.
When the third batch flows in, the algorithm finds “frequent”
itemsets {a}, {a, c}, {b}, {b, d}, {c} and {d} (with their cor-
responding expected support values of 1.7, 1.53, 1.0, 1.0, 1.9
and 1.2). It then slides the window (of size w=2 batches) and
shifts the expected support of each node in the UF-stream
123
structure to make room for the third batch. The resulting
UF-stream structure, as shown in Fig. 1(c), captures the
expected support values for “frequent” itemsets found in the
second and third batches. (Note that nodes with zero expected
support, such as e[0,0], can be pruned; this results in six nodes
in the structure.)
Finally, as a post-processing step, the algorithm checks the
“frequent” itemsets stored in the FP-stream structure against
the aggregate constraint C1 and find only {a}:2.6, {a, c}:2.43,
{b}:2.4 and {c}:3.7 satisfying C1.
Analytically, UF-streaming+ captures the contents of each
batch of streaming transactions containing uncertain data—one
batch at a time—in the UF-tree, from which O(p) “frequent”
itemsets are found (where p 
 2m for m domain items). Note
that the size of the UF-tree is bounded above by the number
of item:expSup in a batch. All O(p) “frequent” (valid as well
as invalid) itemsets from the w most recent batches are stored
in the UF-stream structure (where w is the window size).
IV. UF-STREAMING? : CHECKING CONSTRAINTS EARLY
WHEN MINING CONSTRAINED FREQUENT ITEMSETS FROM
UNCERTAIN DATA STREAMS
Although UF-streaming+ finds constrained FIs from uncer-
tain data streams, it checks constraints in a post-processing
step. As a result, it wastes lots of space as it stores both valid
as well as invalid itemsets in the UF-stream structure. Here, we
propose another algorithm—called UF-streaming?—which
performs constraint checking as an intermediate step (instead
of a post-processing step). Specifically, the algorithm first uses
the same UF-growth mining technique to find all “frequent”
itemsets, and it then checks the mined itemsets against user-
specified constraints before storing the constrained itemsets in
the UF-stream structure. By doing so, the UF-stream structure
stores only valid itemsets. See Example 2.
Example 2: Revisit the uncertain data stream in Exam-
ple 1. With our proposed UF-streaming? algorithm, only valid
itemsets are stored in the UF-stream structures. See Fig. 2
(cf. Fig. 1(b) & (c)). Some memory space is saved as each
UF-stream structure consists of only four (instead of eight or
nine) nodes.
Analytically, UF-streaming? captures the contents of each
batch of transactions in the UF-tree, from which O(p) “fre-
quent” itemsets are found. All these O(p) itemsets are
c[1.35,0.9]
a[1.8,0.9] b[1.6,1.4] c[1.5,1.8]
(a) UF-stream for “frequent” itemsets found in the 1st & 2nd batches
c[0.9,1.53]
a[0.9,1.7] b[1.4,1.0] c[1.8,1.9]
(b) UF-stream for “frequent” itemsets found in the 2nd & 3rd batches
Fig. 2. The UF-stream structures for Example 2
then checked against the user-specified constraints, and only
O(?p) valid “frequent” itemsets (where constraint selectivity
? ? [0,1]) from the w most recent batches are stored in the
UF-stream structure.
V. CUF-STREAMING: EXPLORING CONSTRAINTS WHEN
MINING CONSTRAINED FREQUENT ITEMSETS FROM
UNCERTAIN DATA STREAMS
Along this direction, we propose the third algorithm—called
CUF-streaming—which pushes the user-specified constraints
inside the mining process and explores the properties of
these constraints. Specifically, when a batch of streaming
transactions containing uncertain data flows in, the algorithm
inserts items in each transaction into the UF-tree, in which
items are arranged according to some order R depending on
the type of constraints. Here, the new transaction is merged
with a child (or descendant) node of the root of the UF-tree
only if the same item:expSup exists in both the transaction
and the child (or descendant) nodes. The occurrence count
of a node is at least the sum of occurrence counts of all its
children nodes.
Once the UF-tree is constructed, CUF-streaming recursively
mines constrained “frequent” itemsets from the tree in a depth-
first divide-and-conquer manner: The algorithm first finds
some selected “frequent” domain items x1, x2, x3, x4, · · · . It
then forms projected DBs for x1 and its extensions to find
constrained “frequent” itemsets containing x1. Afterwards, the
algorithm forms projected DBs for x2 and its “extensions” to
find constrained “frequent” itemsets containing x2. This min-
ing process is repeated for the remaining selected “frequent”
domain items. Unlike UF-streaming+ or UF-streaming?, our
proposed CUF-streaming algorithm does not need to form
projected databases for all itemsets (domain items or their
extensions) as CUF-streaming explores the property of con-
straints. Thus, only some itemsets are selected to check
against the constraints, and only some itemsets are selected to
form projected DBs during the mining process. The selection
depends on the type of constraints.
TYPE I: ANTI-MONOTONE CONSTRAINT. Let attr denote
an attribute of an itemset X and const denote a constant. Our
proposed CUF-streaming algorithm arranges domain items in
a monotonic increasing or decreasing order R of attr values
such that invalid items come before/below valid items in the
UF-tree. For instance, if C is of the form min(X.attr) ?
const (e.g., C1), domain items are arranged in non-descending
order R+ of attr values (i.e., ?i, xi.attr ? xi+1.attr) from
leaves to the root; if C is of the form max(X.attr) ? const,
items are arranged in non-ascending order R? of attr values
(i.e., ?i, xi.attr ? xi+1.attr). By doing so, the algorithm
checks C against each domain item one at a time and stops as
soon as it finds the first valid item xv because all remaining
items xv+j (?j ? 1) are guaranteed to be valid (due to R).
The algorithm only needs to form a projected DB for each
xv+j (?j ? 0) because any itemsets that can be found in
{xk}-projected DB (for 1 ? k < v) are invalid due to the anti-
monotonicity of C (i.e., if xk violates C , then all supersets
124
of xk also violate C). For each xv+j , the algorithm forms
projected DBs for itemsets xv+j ? Y (for Y ? {xi | i >
v+ j}). “Frequent” itemsets found in these projected DBs are
guaranteed to be valid (due to the anti-monotonicity of C).
Note that, instead of checking all O(p) “frequent” item-
sets against C (as in UF-streaming+ and UF-streaming?),
CUF-streaming only needs to check O(m) domain items
(where m 
 p 
 2m) against C .
TYPE II: MONOTONE CONSTRAINT. CUF-streaming ar-
ranges domain items in a monotonic decreasing or increas-
ing order R? of attr values such that valid items come
before/below invalid items in the UF-tree. For instance, if C of
the form max(X.attr) ? const (e.g., C2), items are arranged
in non-ascending order R? of attr values from leaves to the
root; if C is of the form min(X.attr) ? const, domain items
are arranged in non-descending order R+ of attr values. By
doing so, the algorithm checks C against each domain item
until it finds the first invalid one xh because all remaining
items xh+j (?j ? 1) are guaranteed to be invalid (due to
R?). For each xv (for 1 ? v < h), the algorithm only needs
to form projected DBs for itemsets xv ? Y (for Y ? {xi |
i > v}). “Frequent” itemsets found in these projected DBs are
guaranteed to be valid due to the monotonicity of C (i.e., if
xv satisfies C , then all supersets of xv also satisfy C).
Again, CUF-streaming only needs to check O(m) domain
items against C .
TYPE III: CONVERTIBLE ANTI-MONOTONE CON-
STRAINT. Like Type II, CUF-streaming arranges domain
items in R? such that valid items come before/below invalid
items in the UF-tree. For instance, if C is of the form
avg(X.attr) ? const (e.g., C3) or sum(X?.attr) ? const
(where X? is an itemset with non-positive attr value),
domain items are arranged in non-ascending order R? of
attr values from leaves to the root; if C is of the form
avg(X.attr) ? const or sum(X+ .attr) ? const (where X+
is an itemset with non-negative attr value), items are arranged
in non-descending order R+ of attr values. By doing so, the
algorithm checks C against each domain item until it finds
the first invalid one xh as all remaining items xh+j (?j ? 1)
are guaranteed to be invalid (due to R?). Again, projected
DBs are formed only for the valid items xv (for 1 ? v < h).
However, “frequent” itemsets found in these projected DBs
need to be checked against C as not all of them are valid.
By exploring the convertible anti-monotonicity of C (if X
violates C , then all “extensions” of X also violate C), the
algorithm only needs to check C against each item in the
projected DB until it finds the first invalid one.
TYPE IV: CONVERTIBLE MONOTONE CONSTRAINT.
Again, CUF-streaming arranges domain items in R? such that
valid items come before/below invalid items in the UF-tree.
For instance, if C is of the form sum(X+ .attr) ? const
(e.g., C4), domain items are arranged in non-ascending order
R? of attr values from leaves to the root; if C is of
the form sum(X? .attr) ? const, items are arranged in
non-descending order R+ of attr values. By doing so, the
algorithm checks each domain item against C until it finds the
first invalid item xh. All remaining items xh+j (?j ? 1) are
guaranteed to be invalid (due to R?). Unlike the procedures for
other three types of constraints, the algorithm forms projected
DBs for “extensions” of valid items as well as invalid items.
While further constraint checking is unnecessary for “exten-
sions” of the valid items (due to convertible monotonicity of
C: All “extensions” of a valid X are guaranteed to be valid),
further constraint checking is needed for “extensions” of the
invalid items (because some of these “extensions” may be
valid).
With (i) m domain items and (ii) C of Type III or IV (having
selectivity ? ? [0,1]), CUF-streaming checks O(?p) items
(where m 
 ?p ? p 
 2m) against C .
The mined “frequent” itemsets that satisfy one of the above
four types of constraints are then stored in the UF-stream struc-
ture. Afterwards, CUF-streaming handles subsequent batches
of streaming transactions of uncertain data in a similar fashion.
Like UF-streaming?, CUF-streaming also stores only valid
itemsets in the UF-stream structure. Unlike UF-streaming?,
CUF-streaming mines constrained “frequent” itemsets more
effectively as it pushes the constraint inside the mining process
and explores properties of the constraint.
Example 3: Revisit the uncertain data stream in Example 1.
C1 is a Type I constraint. So, when the first batch of trans-
actions from uncertain data stream flows in, our proposed
CUF-streaming algorithm arranges domain items in ascending
order R+ of WBC counts (i.e., e, d, c, b, a) from leaves to the
root. The algorithm then checks C1 against each domain item
in the UF-tree (i.e., e, d) until it finds the first valid item c. It
forms projected DBs for valid items (i.e., c:1.5, b:1.6, a:1.8)
as well as their “extensions”. No more constraint checking
is needed as any “frequent” itemsets found in the projected
DBs of valid items and their “extensions” are guaranteed to
be valid (due to anti-monotonicity of C1). From the {c}-
projected DB, {a, c}:1.35. CUF-streaming then stores all four
constrained “frequent” itemsets {a}, {a, c}, {b} & {c} in the
UF-stream structure. Same approach is then applied to the
second batch, and results in the same UF-stream structure as
shown in Fig. 2(a). Afterwards, CUF-streaming applies the
same approach to the third batch; this results in the same
UF-stream structure as shown in Fig. 2(b).
VI. EXPERIMENTAL RESULTS
We used different datasets for experimental evaluation. For
space limitation, we reported here the experimental results on a
dataset generated by the program developed at IBM Almaden
Research Center [3]. This dataset contains 10M records with
an average transaction length of 10 items, and a domain
of 1,000 items. We assigned an existential probability from
the range (0,1] to each item in each transaction. We set the
window size to be w=5 batches and each batch to contain
1M transactions. In addition to this dataset, we also conducted
the following experiments using some other datasets, including
UCI real-life datasets as well as FIMI datasets. The observa-
tions or trends were consistent.
125
050
100
150
200
250
10 20 30 40 50 60 70 80 90
R
un
tim
e 
(in
 se
co
nd
s)
Selectivity (i.e., percentage of items selected)
CUF-streaming (w=5 batches, each with 1M transactions): Exploration of constraints C1-C4
Type IV constraint C4
Type II constraint C2
Type III constraint C3
Type I constraint C1
0
50
100
150
200
250
300
350
400
450
10 20 30 40 50 60 70 80 90
R
un
tim
e 
(in
 se
co
nd
s)
Selectivity (i.e., percentage of items selected)
CUF-streaming (w=50 batches, each with 1M transactions): Exploration of constraints C1-C4
Type IV constraint C4
Type II constraint C2
Type III constraint C3
Type I constraint C1
50
55
60
65
70
75
80
85
90
0 0.001 0.002 0.003 0.004 0.005
R
un
tim
e 
(in
 se
co
nd
s)
preMinsup (in percentage)
Runtime vs. existential probability & preMinsup
Items take on many different existential probability values
Items take on an average number of existential probability values
Items take on a few unique existential probability values
(a) Runtime vs. selectivity (w=5) (b) Runtime vs. selectivity (w=50) (c) Runtime vs. preMinsup
Fig. 3. Experimental results: runtimes
All experiments were run in a time-sharing environment in
an 800 MHz machine. The reported figures are based on the
average of multiple runs. Runtime includes CPU and I/Os;
it includes the time for both tree construction and frequent
itemset mining steps. We evaluated different aspects of the
proposed algorithms, which were implemented in C.
First, we compared the performance of the three proposed
algorithms using four different constraints (one from each
type of the above constraints). Experimental results showed
that the runtimes for both UF-streaming+ and UF-streaming?
were constant regardless of the constraint selectivity because
these two algorithms did not explore property nor did they
push the constraints inside the mining process. Specifically,
UF-streaming+ performed constraint checking as a post-
processing step, whereas UF-streaming? performed constraint
checking as an intermediate step prior to storing the “fre-
quent” itemsets into the UF-stream structure. In contrast,
CUF-streaming was more interesting as it runtimes depended
on the type of constraints as well as the constraint selectivity.
Specifically, the algorithm explored the anti-monotonicity of
C1, the monotonicity of C2, the convertible anti-monotonicity
of C3, and the convertible monotonicity of C4. As it ex-
plored properties of these four constraints and pushed the
constraints inside the mining process, CUF-streaming required
shorter runtimes than the other two algorithms. As shown in
Fig. 3(a), the runtimes for handling all four types of constraints
increased when the selectivity increased. Among them, C4
(a Type IV constraint) incurred the highest runtime because
CUF-streaming “extended” (i.e., formed projected DBs for)
both valid and invalid items. C2 (a Type II constraint) and
C3 (a Type III constraint) incurred the next two highest
runtimes For C2, the algorithm “extended” only valid items.
All “extensions” of valid items were valid. Due to the item or-
dering, the algorithm stopped checking constraints whenever it
detected the first invalid items. However, for C3, the algorithm
applied constraint checking on projected DBs for valid items
as well as their “extensions” because not all “extensions” of
valid items were valid. C1 (a Type I constraint) incurred the
lowest runtime among the four types of constraints because
CUF-streaming formed fewer “extensions” (as they consisted
of only valid items). Again, due to the item ordering, the
algorithm stopped checking constraints whenever it detected
the first valid items.
Next, we repeated the above experiment with a different
the window size: w=50 fixed-sized batches with each batch
containing 0.1M transactions (instead of using w=5 fixed-sized
batches with each batch containing 1M transactions). With this
setting, each batch was smaller (0.1M vs. 1M transactions).
Thus, each batch required lower runtime (e.g., for constructing
and mining UF-trees). However, the number of batches was
higher (50 vs. 5 batches) than the previous setting. This
explains why the runtimes (see Fig. 3(b)) took on a broader
range than the previous experimental results. For example,
when the selectivity of C2 was low (say, 10%), only a few
small UF-trees were constructed and mined (as the algorithm
only “extended” valid items) and a shorter runtime ? 110 sec.
(cf. 160 sec. in Fig. 3(a)) was required. As another example,
for C4, many bigger UF-trees were constructed and mined (as
the algorithm formed projected DBs for both valid as well as
invalid domain items), which took ? 400 sec. (cf. 230 sec. in
Fig. 3(a)).
As all three algorithms are approximate algorithms, we
evaluated the effect of preMinsup on the mining results. For
example, using w=5 batches, when preMinsup = 0.8 × minsup,
90% of the mined constrained “frequent” itemsets were truly
frequent. When preMinsup = 0.9 × minsup, 95% of the
mined constrained “frequent” itemsets were truly frequent.
However, lowering preMinsup or having more batches in the
sliding window had the benefits of increasing the chance of
not pruning relevant expected support information for truly
frequent itemsets. Moreover, as shown in Fig. 3(c), when
preMinsup increased, fewer itemsets had expected support ?
preMinsup, and thus shorter runtimes were required. The figure
also showed the effect of the distribution of item existential
probability. When items took on a few unique existential
probability values, the UF-tree became smaller. Thus, times
for both UF-tree construction and mining became shorter.
In addition, we also measured the number of nodes in each
UF-tree. The experimental results showed that the total number
of nodes in a UF-tree was no more than the total number
of items (with their existential probability) in all transactions
in the current batch of uncertain data stream. Furthermore,
we measured the number of nodes in the UF-stream structure
as well. As UF-streaming+ performed constraint checking at
a post-processing step, the size of UF-stream was observed
to be independent of the constraint selectivity. In contrast, as
126
UF-streaming? and CUF-streaming both pushed the constraint
early, the corresponding size of UF-stream was proportional
to the selectivity of constraints.
Finally, we evaluated the functionality and applicability
of our proposed algorithms. We again used four different
constraints, and we also set the constraint selectivity be
100% (i.e., all items are selected). Then, we compared our
three proposed algorithms with UF-streaming [6] (which was
designed to mine unconstrained “frequent” itemsets from
uncertain data streams). In terms of efficiency, the experi-
mental results showed that UF-streaming was slightly faster
because it did not perform any constraint checking whereas
our three proposed algorithms performed the extra constraint
checking step. Among them, CUF-streaming only performed
constraint checking on some “frequent” itemsets, and the other
two performed constraint checking on all “frequent” itemsets.
However, in terms of the mining results, we observed that
all four algorithms returned the same collection of “frequent”
itemsets. This illustrated that our proposed algorithms could be
used for mining unconstrained frequent itemsets from uncer-
tain data streams. Moreover, it is important to note that, while
the UF-streaming is confined to finding “frequent” itemsets
satisfying constraints with 100% selectivity, our algorithms are
capable of finding “frequent” itemsets that satisfy constraints
having lower selectivity.
Along this direction, we set w=1 batch containing the
entire dataset. Then, we compared our algorithms with
UF-growth [22] by assigning to each item in every transaction
in a dataset an existential probability of 1 (i.e., all items are
definitely present in the dataset) and preMinsup = minsup.
Again, we observed that all four algorithms returned the same
collection of frequent itemsets. This illustrated that our pro-
posed algorithms could also be used for mining unconstrained
frequent itemsets from static uncertain datasets.
VII. CONCLUSIONS
Frequent itemsets generally serve as building blocks for
various patterns in many real-life applications. Most of the
existing algorithms find unconstrained frequent itemsets from
traditional static transaction databases consisting of precise
data. However, there are situations in which ones are uncertain
about the contents of transactions. There are also situations
in which users are only interested in some subsets of all
the mined frequent itemsets. Furthermore, a flood of data
can be easily produced in many situations. To deal with all
these situations, we proposed three tree-based algorithms—
namely, UF-streaming+, UF-streaming? and CUF-streaming—
which integrate (i) mining of uncertain data, (ii) constrained
mining, and (iii) mining of data streams. These algorithms
effectively mine constrained frequent itemsets from uncertain
data streams.
ACKNOWLEDGMENT
This project is partially sponsored by Natural Sciences and
Engineering Research Council of Canada (NSERC) and the
University of Manitoba in the form of research grants.
REFERENCES
[1] C.C. Aggarwal et al., “Frequent pattern mining with uncertain data,” in
Proc. KDD 2009, pp. 29–37.
[2] R. Agrawal et al., “Mining association rules between sets of items in
large databases,” in Proc. ACM SIGMOD 1993, pp. 207–216.
[3] R. Agrawal and R. Srikant, “Fast algorithms for mining association
rules,” in Proc. VLDB 1994, pp. 487–499.
[4] R.J. Bayardo Jr., R. Agrawal, and D. Gunopulos, “Constraint-based
rule mining in large, dense databases,” Data Mining and Knowledge
Discovery, 4(2–3), pp. 217–240, July 2000.
[5] T. Bernecker et al., “Probabilistic frequent itemset mining in uncertain
databases,” in Proc. KDD 2009, pp. 119–127.
[6] R. Cheng et al., “Probabilistic verifiers: evaluating constrained nearest-
neighbor queries over uncertain data,” in Proc. IEEE ICDE 2008,
pp. 973–982.
[7] C.-K. Chui, B. Kao, and E. Hung, “Mining frequent itemsets from
uncertain data,” in Proc. PAKDD 2007, pp. 47–58.
[8] G. Cormode and M. Hadjieleftheriou, “Finding frequent items in data
streams,” in Proc. VLDB 2008, pp. 1530–1541.
[9] G. Cormode et al., “Finding hierarchical heavy hitters in streaming data,”
ACM TKDD, 1(4), article 2, Jan. 2008.
[10] X. Dai et al., “Probabilistic spatial queries on existentially uncertain
data,” in Proc. SSTD 2005, pp. 400–417.
[11] M.M. Gaber, A.B. Zaslavsky, and S. Krishnaswamy, “Mining data
streams: a review,” SIGMOD Record, 34(2), pp. 18–26, June 2005.
[12] C. Giannella et al., “Mining frequent patterns in data streams at multiple
time granularities,” in Data Mining: Next Generation Challenges and
Future Directions, ch. 6, AAAI/MIT Press, 2004.
[13] G. Grahne, L.V.S. Lakshmanan, and X. Wang, “Efficient mining of
constrained correlated sets,” in Proc. IEEE ICDE 2000, pp. 512–521.
[14] J. Han, J. Pei, and Y. Yin, “Mining frequent patterns without candidate
generation,” in Proc. ACM SIGMOD 2000, pp. 1–12.
[15] J. Huang et al., “MayBMS: a probabilistic database management sys-
tem,” in Proc. ACM SIGMOD 2009, pp. 1071–1074.
[16] C. Jin et al., “Sliding-window top-k queries on uncertain streams,” in
Proc. VLDB 2008, pp. 301–312.
[17] L.V.S. Lakshmanan, C.K.-S. Leung, and R.T. Ng, “Efficient dynamic
mining of constrained frequent sets,” ACM TODS, 28(4), pp. 337–389,
Dec. 2003.
[18] C.K.-S. Leung, “Frequent itemset mining with constraints,” in Encyclo-
pedia of Database Systems, pp. 1179–1183, Springer, 2009.
[19] C.K.-S. Leung and D.A. Brajczuk, “Efficient algorithms for mining
constrained frequent patterns from uncertain data,” in Proc. U '09, pp. 9–
18.
[20] C.K.-S. Leung and B. Hao, “Mining of frequent itemsets from streams
of uncertain data,” in Proc. IEEE ICDE 2009, pp. 1663–1670.
[21] C.K.-S. Leung and Q.I. Khan, “DSTree: a tree structure for the mining
of frequent sets from data streams,” in Proc. IEEE ICDM 2006, pp. 928–
933.
[22] C.K.-S. Leung, M.A.F. Mateo, and D.A. Brajczuk, “A tree-based
approach for frequent pattern mining from uncertain data,” in Proc.
PAKDD 2008, pp. 653–661.
[23] C.K.-S. Leung, R.T. Ng, and H. Mannila, “OSSM: a segmentation
approach to optimize frequency counting,” in Proc. IEEE ICDE 2002,
pp. 583–592.
[24] R.T. Ng et al., “Exploratory mining and pruning optimizations of
constrained associations rules,” in Proc. ACM SIGMOD 1998, pp. 13–
24.
[25] J. Pei, J. Han, and L.V.S. Lakshmanan, “Mining frequent itemsets with
convertible constraints,” in Proc. IEEE ICDE 2001, pp. 433–442.
[26] C. Re´ et al., “Event queries on correlated probabilistic streams,” in Proc.
ACM SIGMOD 2008, pp. 715–728.
[27] A.D. Sarma, M. Theobald, and J. Widom, “Exploiting lineage for
confidence computation in uncertain and probabilistic databases,” in
Proc. IEEE ICDE 2008, pp. 1023–1032.
[28] K. Yi et al., “Small synopses for group-by query verification on
outsourced data streams,” ACM TODS, 34(3), article 15, Aug. 2009.
[29] Q. Zhang, F. Li, and K. Yi, “Finding frequent items in probabilistic
data,” in Proc. ACM SIGMOD 2008, pp. 819–832.
127
