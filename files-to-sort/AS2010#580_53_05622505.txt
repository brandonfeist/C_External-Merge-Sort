2010 International Conference on Computer Application and System Modeling (ICCASM 2010) 
Iceberg model based on meteorological data mining 
Minjin 
Beijing union university 
Beijing china 
e-mail: xx0504335631@126.com 
Abstract- The mining efficient is not only about the 
designation of the mining algorithm, but also relating to the 
character of the data we want to explore. Different data, 
different focus. This paper is talking about a mining method on 
the meteorological data. Through building a iceberg model, 
and then dig the model, getting the frequent item set out 
further, it is important to get clear about the association rules 
of the weather's data. 
Keywords- Data mining Iceberg model Frequent itemset 
Weather's data association rules BUe 
I. INTRODUCTION 
Data mining[l] is about the process how to dig the 
knowledge from the database, data cube or other huge 
information data, and then support the decision. It has been 
used in business decision, science, medical research, market 
analysis and other areas widely. Through data mining, we 
can get the interesting relation among the itemset, and then 
through digging the weather's data, we can get the relation 
among the meteorological elements. Meanwhile we also can 
find the meteorological characteristics of the region and time 
we want to know. 
II. ICEBERG MODEL 
American psychologist, Mike, made the famous model 
about the quality of iceberg in 1973. This model is created to 
analyze human behavior and performance, and the model 
built here is to extract the information to meet the users' 
requirements based on huge data, and this data is equal to the 
tip of the iceberg which is floating in the sea. And we know 
the data in the tip rely on the data base completely. Historical 
meteorological data is always complicated to the analyst, and 
they just focus on the information the huge data can provide. 
Their information is just the result from the technology of 
data mining, but the efficient of mining is largely depends on 
the size of mining task, so it is necessary to reduce the size of 
the task before the extraction. 
• Definition 1: Frequent itemset 
A transaction database TDB is a set of transactions, 
where each transaction, denoted as a tuple<tid,X>,contains a 
set of items and is associated with a unique transaction 
identity tid , let I={il,i2, ..... .in} be the complete set of 
distinct items appearing in TDB. An itemset Y is a 
non-empty subset of I and is called an k-itemset if it contains 
k items, like {milk, tomato, fish} is 3-itemset,the number of 
transaction in TDB containing itemset Y is called the support 
of itemset Y,denoted as support(Y),given a minimum 
Meiyuan 
Beijing union university 
Beijing china 
e-mail: yuanmei@buu.edu.cn 
support threshold,min_sup, an itemset(Y) is frequent if 
sup(Y»=min _sup. 
Through apriori algorithm , it is clear what is frequent 
itemset and how to get it. After getting the k-itemset, get the 
candidate itemset and then scanning the TDB to find the 
k+ l-itemset those sup(k+ l-itemset»=min-sup. But this 
algorithm's fatal disadvantage is the candidate itemset is 
always large, so to get the k+1-itemset those 
support >=min _sup from the candidate itemset, the 
sup _ min(itemset)<sup _min would exclude, eg: to the 
meteorological data, when the annual precipitation meet 
the sup_min, but other meteorological elements don't , so the 
annual precipitation would get excluded. so it is important to 
dig the data on the users' concerned itemset, and then get the 
interesting information further, but if letting the default 
condition, so the operation is on the all frequent itemset(the 
amount of the itemset is the largest and it meet the 
threshold ). 
Apriori algorithm is always used to generate the frequent 
itemset, it is used to increase the efficient of the iceberg 
query. but this algorithm here is not suitable at all, the 
number of the candidate items is huge , so this paper take 
BUC algorithm [2]to generate the frequent itemset, using 
top-down to generate the K-itemset. BUC algorithm is 
optimized aggregate algorithm. It first calculates one group 
by, like time and then two group by like time , station and 
then three .... this top-down calculated way is the most 
important optimized factor. 
This way can fix the pruning in the iceberg model by 
dividing the data into parts and using recursive 
partitioning[3] . 
• The designation of iceberg model 
The skill of this model is iceberg queries, and this 
function mostly use aggregate functions, like count, sum and 
so on, then using the aggregate function's value compare to 
threshold, traditional iceberg query always use sorting and 
hash, and those way generally having huge 110 operations. 
For example : for a table T, having attributes 
tl,t2,t3 ..... tn, threshold C, the designation of the iceberg 
query like below[4]: 
Select T.tl, T.t2 ...... T.tn, count (*) 
From T 
Cube by T.tl, T.t2 ...... T.tn 
Having count (*»=C 
This way decrease the consuming on the calculate the 
aggregation function's value. 
For the Meteorological data's own characteristics, 
instantiating of the iceberg queries have its special area's 
requirement, the threshold is a expression of percentage, and 
978-1-4244-7237-6/10/$26.00 ©2010 IEEE V15-44 
2010 International Conference on Computer Application and System Modeling (ICCASM 2010) 
then diminishes the scale of data through choosing 
appropriate threshold, according to the needs of the 
professional users, building the k-itemset by using the 
iceberg queries. 
The most decisive factor is the how to choose the 
threshold, if the threshold is small, building this model 
comes meaningless, or the useful data would loss a lot. So 
how to decide the value of the threshold is always got by 
testing and testing , and it needs the professional users. The 
threshold here using the support S, S=count ({iI, i2, 
i3 .. .in}E D)/count(D), for a transaction database TDB here I 
is one attribute in TDB, it is a item, n is the number of the 
item in the itemset, D is the subitemset of the TDB. 
The building of the iceberg model 
• Definition: 
Frequent area: it is the itemset in the transaction 
database which it meets the requirement of the users, so the 
support of the itemset is larger than the min support. And the 
itemset is decided by the one area of the attribute eg: the 
itemset about the precipitation in the Meteorological data is 
between 20mm and 25mm occupying in the all data set is 
larger than the min support. 
The model here is dynamic for the users' different focus, 
if the user cares more about the precipitation, then building 
the model on the precipitation. 
eg: xl,x2 are the user's interesting attributes, and the 
threshold is S 
Select yl, y2, y3 count (*) as countl 
From T 
Where T.xl =area[xl] and T.x2=area[x2] 
Cube yl, y2, y3 count (*) 
Having (countll(count2 in ( 
Select count (*) as count2 
From T) 
))>=S 
The result of the frequent itemset is used for data mining, 
and then finds the associate rules. That is the destination of 
building the model. The above is showing what the result is. 
III. ASSOCIATE RULES 
Data association is one kind of important knowledge 
which can be found in the Data base, there exist some kind 
of regulation in the two or more variable's values. And this 
term is proposed by R.Agrawal and others, the typical 
example is "90% of buying behavior of buying bread would 
lead to buy the milk in one supermarket", data mining is to 
find the regulation and then make the right decision to gain 
better economic benefits for the supermarket's manager. But 
the regulation support the decision is just some critical 
association rules, not another set of rules. 
• Definition of the associate rules 
It includes two factors, one is the association between the 
items, eg: the gender is female and then position is secretary, 
the other is the association in the item, like the famous the 
sales of the beer is positive correlate to the sale of the diaper. 
Associate rules mining is to find the relation in the huge 
dataset, the two threshold: min support and min confidence 
are often used to evaluate if a rule is a strong or not in 
association rules mining. Interestingness is another important 
threshold besides mInImum support and mInImum 
confidence for association rules. A transaction database TDB 
is a set of transactions: I (iI, i2, i3 .... .in) is a set owing in 
different items. Every transaction has its own unique ID, if 
X, Y is the subset of I, and X!l Y = <t, associate rules is the 
form liking X ? Y succeed in TDB, meanwhile, meet the 
minimum support Sup and the minimum confidence Cof, X 
is the antecedent of the rules, and Y is the succedent of the 
rules[5]. 
Sup(X?Y) = [TE D I XUY]/[D] 
Cof(X ? Y) =Sup(X U Y)/sup(X) 
(1) 
(2) 
The rules meeting the two thresholds are called the strong 
association rules. 
The relation between the frequent itemset and associate 
rules. 
The associate rules get from the frequent itemset, so the 
associate rules' availability depends on the frequent itemset's 
precision, and another factor is the threshold is suitable or 
not. 
• The extraction of the associate rules 
This test is designed to improving the efficiency of the 
extract the regulations, to finding the key rules, and building 
model, which means that it has eliminate the outliers, and 
this experiment is to get the result like the rules about the 
diurnal water surface evaporation and the cloud cover on the 
certain height. 
The method to get the associate rules is using support S 
and confidence C, this two thresholds are used to weigh the 
rules' role. If a rule's probability is I, it does not means it is 
important, but for the probability is not the highest, but the 
rule is the most important. 
• The description of the algorithm about the 
meteorological data 
Using Depth-first strategy, so whenever getting the new 
itemset, it narrows the scope of the data search, that is the 
BUC algorithm's main idea. But this algorithm's main job is 
joining the item from left to right, and those operations are 
less gradually from left to right, so early detection of the item 
does not meet the threshold set pruning to reduce 
UntIecessary connections. 
For example: 
If existing the count of item about Extreme maximum 
temperature>20 is 20 and Precipitation from 08 to 08 
hours> 100mm is 30, the threshold is 25, and then it means 
the combination about Extreme maximum temperature>20, 
Precipitation from 08 to 08 hours> 1 OOmm would get pruned, 
so it is not necessary to scan the combination about 
Precipitation from 08 to 08, Extreme maximum 
temperature>20 in the DB. 
V15-45 
2010 International Conference on Computer Application and System Modeling (ICCASM 2010) 
The process of pseudo-code algorithm: 
Input the itemset A, B and the threshold S 
Scanf the whole DB calculate the count of A, count 
(Ai), the count of B, count (Bi) and count (*) 
Sli=count (Ai) /count (*), S2i=count (Bi)/count (*) 
For(i=I; i<n; i++) 
{if (Sli>S) save Sli; 
Else pruning Sli; 
if (Sli>S) save S2i; 
Else pruning S2i; 
Then 
Join Sli and S2i; 
Then insert into the tree of BUC 
} 
Repeat the operations above until the result of n times 
consisting with the result of n-l times. 
IV. THE RESULT OF MINING THE METEOROLOGICAL DATA 
The data is historical meteorological data, the data 
records' number is 2487059, the measure attributes' number 
is 15, including average site pressure, average air 
temperature, extreme minimum temperature, extreme 
maximum temperature, average water pressure, average 
relative humidity, average total cloud amount, average low 
cloud cover, average wind speed, average Ocm ground 
temperature, average Ocm ground, precipitation from 20 to 
20,sunshine hours, large-scale evaporation, small-scale 
evaporation, precipitation from 8 to 8. 
The threshold is 20,the results below is just part of the 
results, this is the resource of building the iceberg model, the 
first row is the support ,the second row is the number of the 
items, the third row is the frequent itemset. 
Figure I. The first result of the Experiment 
The minimum probability is 0.6 and the importance is 
0.03 the part mining result is below: 
The first row is probability, the second is importance, and 
the third row is the association rules. 
Figure 2. The second result of the Experiment 
The most important rule is average low cloud cover < 
11.6823767984 ->average wind speed < 17.5794318048, the 
importance is 0.82462129537459, and the probability is 
0.883 
Next is large-scale evaporation < 21.7302543424, 
average cloud cover< 17.178841904 ->average wind speed < 
17.5794318048,the importance is 0.155741340754574, the 
probability is 1.000. 
V. FURTHER DISCUSSION OF THE ALGORITHM 
Through building this model, reducing the amount of 
aggregating time largely, the associate rules get to make 
clear of the frequent itemset's regulation, especially the 
meteorological data. BUC is a mature algorithm, it 
improves the iceberg model's availability, this experiment's 
fault is loss the exceptional data, and this data would play a 
more important role in the future. And deviation is a way to 
deal with this problem, how this concept could be used in the 
meteorological data is the question we should explore. 
VI. CONCLUSION 
Through building the iceberg model, it is available to 
reduce the size of objects excavated; it is based on the BUC 
algorithm. And this BUC tree is built by deleting the itemset 
ahead, thereby reducing the number of connections largely, 
and this algorithm support the iceberg query, it is used to 
prune, the data which is processing in this way are easy to 
get the association rules. And this is the final destination of 
mining the data. 
REFERENCES 
[I] en liu a algorithm about iceberg queries for data mining 2004 
[2] hongmin liu and xie han the realization and the research of high 
efficient calculating method on data cube. 2006 
[3] jizhou luo, jianzhong Ii and kai zhao the iceberg cube algorithm on 
huge compressive data cube. 2006 (Journal of Software). 
[4] Yu Yin Iceberg-cube Computation with PC cluster The University of 
British Columbia April 200 I. 
[5] liangxi qin and zhongzhi shi mining the association rules for the 
network flowing based on the iceberg queries. 2005 (computer 
engineering) 
[6] xiaohong chen and yang qin the high efficient associate rules research 
on the data mining 2005 (computer engineering & science) 
V15-46 
2010 International Conference on Computer Application and System Modeling (ICCASM 2010) 
[7] yongyin wu,fang yuan and longgang xiang the bottom-up calculation 
on the incomplete Data Cube 2002 (computer engineering) 
[8] jiawei han Micheline Kamber Data Mining Concept and Techniques 
Machinery industry Press 
[9] ZhaoHui Tang Jamie Macleman the principles and applications of 
data mining-2005 server database 2007 Tsinghua University Press 
[10] deli zhu data mining and complete solution for BI 2007 electronic 
industry Press 
V15-47 
