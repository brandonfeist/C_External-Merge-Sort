Mining Multi-Class datasets using Genetic Relation Algorithm for
Rule Reduction
Eloy Gonzales, Non-Member, IEEE, Shingo Mabu, Member, IEEE, Karla Taboada, Member, IEEE,
Kaoru Shimada, Member, IEEE, Kotaro Hirasawa Member, IEEE
Abstract—This paper describes the use of a new evolutionary
method named Genetic Relation Algorithm (GRA) for reducing
the number of class association rules extracted by other methods
such as Apriori, Genetic Network Programming(GNP) , etc. The
purpose is to generate a small number of class association rules
in order to delete irrelevant and redundant rules. A reduced
rule set has advantages as it provides only useful rules and
makes its analysis more ef?cient. Our approach is based on
evaluating the distances between rules for evolving GRA and
also evaluating the distances between the data in the test set and
the rules for classi?cation. Two matching criteria are presented:
complete match and partial match. The classi?cation accuracy
obtained by our method is better compared to other reported
results in multi-class datasets showing an impressive reduction
rate.
I. INTRODUCTION
Data Mining is a technique employed to extract non-trivial
information from large datasets. Its interest is growing in
the research community. Association rule mining is one of
the interesting research topics in data mining and several
methods and techniques have been developed during the
last decade. However, most of the methods for extracting
association rules generate so many rules. Therefore, the
problem of selecting the most interesting and useful rules is
a research field that has not been completely solved because
of the algorithmic complexity to prune a large number of
rules.
This paper proposes the use of a new evolutionary algo-
rithm named Genetic Relation Algorithm (GRA) in order
to find and select only the most relevant and important
association rules among a big set of them avoiding the
redundant and uninteresting rules. In this paper, the large rule
set is generated using the conventional GNP-based mining
method [1] [2]. Our approach is based on the evaluation
of the distances between rules in every generation during
the evolutionary process. Two methods for evaluating the
distance are presented: complete match and partial match.
The results are evaluated by comparing the classification
accuracy of the reduced set of rules, the complete set of
rules and other conventional methods. Furthermore, it is
found from simulations that the proposed method not only
reduces the number of rules from a large set of rules, but
also improves the performance in terms of the classification
accuracy.
Authors are with the Graduate School of Information, Production and
Systems, Waseda University, Hibikino 2-7, Wakamatsu-ku, Kitakyushu,
Fukuoka 808-0135, Japan. (phone: +81 93 692-5261; fax: +81 93
692-5261; email: {egonzale, karla taboada}@asagi.waseda.jp, {k.shimada,
mabu}@aoni.waseda.jp, hirasawa@waseda.jp ).
II. RELATED WORK
There have been several attempts for rule reduction such as
removing redundant rules using the concepts of closed item
sets [3] [4] and representative rules [5], summarization of
association rules [6] and a modified Apriori-like[7] method
and visualization techniques. However, all of them show at
most the comparative classification accuracy as the complete
rule set.
Zaki et al. [4] present a framework based on FCI (Frequent
Close Itemset) that reduces number of redundant rules. The
authors used FCI to form a set of rules and inferred all
other association rules from that set. Since FCI is used for
choosing a set of rules based on the confidence values, the
number of rules grows as the number of FCI increases.
Therefore, it becomes difficult for end-users to infer other
rules when there is a large number of FCI. Additionally,
since this approach uses FCI, it may prune some important
rules without considering the confidence and support values
of those rules.
Liu et al [6] proposed an algorithm known as multilevel
organization and summarization of discovered rules. In this
algorithm, rules are summarized in a hierarchical order, so
that end users can browse all the rules at different levels in
details. However, this algorithm only summarizes the rules
and redundant rules may still exist in the final model.
Similar to all of the above mentioned redundant rule
reduction algorithms, the main objective of this paper is
to eliminate redundant and uninteresting association rules.
However, our proposed method have two distinct features
that distinguish it from all other existing algorithms.
• It uses an evolutionary algorithm named Genetic Re-
lation Algorithm (GRA) which encodes the association
rules in its nodes and genetic operations are carried out
as the conventional evolutionary methods.
• It is not based on any bias assumptions. In addition, our
method evaluates each rule with a set of rules based on
their distances in order to find redundant association
rules. Hence it eliminates redundant association rules
without losing any important knowledge from the re-
sultant rule set.
III. GENETIC RELATION ALGORITHM
In this section, the outline of Genetic Relation Algorithm
(GRA) is described. GRA is one of the evolutionary methods
for determining the best relations between events. The basic
structure of GRA is shown in Fig. 1. GRA is composed
3249978-1-4244-2959-2/09/$25.00 c© 2009 IEEE
of nodes and their directed or indirected branches. Nodes
represent events and branches represent the relations between
nodes. The genotype expression of GRA node is shown in
Fig. 2.
Fig. 1: Structure of Genetic Relation Algorithm (In case of
directed branches).
Fig. 2: Genotype expression of Genetic Relation Algorithm.
Fig. 2 describes the gene of node i, then, the set of these
genes represents the genotype of GRA individuals. IDi is
an identification number, for example, IDi = 1 means node
i has the directed branches to other nodes, while IDi = 2
means node i has the indirected branches to the nodes. Fi
denote the function of the node i. Ci1, Ci2, . . . , Cik denote
the nodes which are connected from node i, firstly, secondly,
. . . , and Si1, Si2, . . . , Sik denote the strength from node i
to node Ci1, Ci2, . . . , Cik or the strength between node i
and node Ci1, Ci2, . . . , Cik depending on the arguments of
node i. All individuals in a population have the same number
of nodes. The following genetic operators are used in GRA.
Crossover operator affects two parent individuals. All the
connections or contents of the selected nodes in two parents
are swapped each other by crossover rate of Pc. Mutation
operator affects one individual. The connections or contents
of each node are changed randomly by mutation rate of Pm.
GRA can be evolved in order to optimize the fitness function
defined by the strength Sij , in other words, the events and
their relations can be evolved by using the strength Sij . The
point of GRA is that all the connections between nodes do
not have to be defined, but the connection itself could be
evolved.
There are two kinds of GRA: GRA with directed branches
and GRA with indirected branches.
A. GRA with directed branches
In GRA with directed branches, the event is represented by
the node, while the relation between two events is represented
by directed branches with their strength. Fig. 3 is an example
of GRA with directed branches. Node i has strength Sij to
node j and node j has strength Sji to node i. The strength
between all nodes does not have to be defined.
Fig. 3: Genetic Relation Algorithm with directed branches.
B. GRA with indirected branches
In the GRA with indirected branches, the event is also rep-
resented by the node, while the relation between two events is
represented by indirected branches with their strength. Fig. 4
is an example of GRA with indirected branches. The relation
between node i and node j has a strength of Sij = Sji in
GRA with indirected branches.
Fig. 4: Genetic Relation Algorithm with indirected branches.
IV. GENETIC RELATION ALGORITHM AND ASSOCIATION
RULES
A. Association rules and their properties
In this section, the definition and properties of association
rules are briefly reviewed. The association rule mining is
3250 2009 IEEE Congress on Evolutionary Computation (CEC 2009)
stated as follows: [8] Let I = {A1, A2, . . . , Al} be a set of
attributes, also called items, over the binary domain {0, 1}.
Let G be a set of transactions, where each transaction T is
a set of attributes such that T ? I . Associated with each
transaction is a unique identifier whose set is called TID.
A transaction T contains X , a set of some attributes in I ,
if X ? T . An association rule is an implication of the form
of X ? Y , where X ? I , Y ? I , and X ? Y = ?. X
is called antecedent and Y is called consequent of the rule.
In general, a set of attributes is called an itemset or pattern.
More concisely, an itemset is a non-empty subset of I .
Each itemset has an associated measure of statistical
significance called support. If the fraction of transactions
containing X in G equals t, then support(X) = t. The rule
X ? Y has a measure of its strength called confidence
defined as the ratio of support(X ? Y )/support(X). This
measure indicates the relative frequency of the rule, that is,
the frequency with which the consequent is fulfilled when
the antecedent is also fulfilled .
A class association rule is defined to be an implication
with a class being its consequent
B. GRA for association rules with non-fixed consequent
For association rules with non-fixed consequent, GRA with
directed branches are used. Fig. 5 shows an example of GRA
for association rules with non-fixed consequent. In Fig. 5, the
antecedent and the consequent of each rule are represented
as nodes in GRA. The confidence of each rule is used as
the strength, for example. The genotype of the GRA in the
example of Fig. 5 is described in Table I
Fig. 5: Genetic Relation Algorithm for association rules with
non-fixed consequent.
TABLE I: Genotype of GRA for association rules with non-
fixed consequent
NodeID NodeFunction C1 C2 S1 S2
1 A 2 3 0.9 0.3
2 BC 3 - 0.8 -
3 K 2 4 0.7 0.4
4 M 1 - 0.6 -
C. GRA for association rules with fixed consequent
Association rules with fixed consequent are mainly used
for classification. GRA with indirected branches are used
for association rules with fixed consequent. Fig. 6 shows an
example of GRA for association rules with fixed consequent.
In Fig. 6, the antecedent of each rule is represented as the
node in GRA. The distance between the nodes is used as the
strength. The genotype of the GRA in the example of Fig. 6
is described in Table II
TABLE II: Genotype of GRA for association rules with fixed
consequent
NodeID NodeFunction C1 C2 C3 S1 S2 S3
1 AB 2 3 4 1.5 1.8 2.5
2 ABC 1 3 4 1.5 2.6 3.2
3 B 1 2 4 1.8 2.6 2.7
4 SK 1 2 3 2.5 3.2 2.7
Fig. 6: Genetic Relation Algorithm for association rules with
fixed consequent.
D. Distance between rules
The distance between the rules in the same class is
defined in this sub-section in order to realize GRA with fixed
consequent supposing we are dealing with the classification
problems.
Two types of distances are presented: complete match and
partial match.
1) Distance using complete match: The distance between
rule i and rule j denoted by D(i, j) is defined as follows:
D(i, j) = support(i) + support(j)? 2[support(i, j)], (1)
where,
support(i) is the support of rule i,
support(j) is the support of rule j, and
support(i, j) is the support of rule i and rule j.
The distance expresses the ratio of the number of records
with the same class in the database, where the matching
between rule i and the data and the matching between rule
j and the data do not coincide.
2009 IEEE Congress on Evolutionary Computation (CEC 2009) 3251
As an example, consider Table III and the following rule
i and rule j.
TABLE III: Example of record data
TID A B C D E F Class
1 1 1 0 1 1 0 c
2 1 1 1 1 1 1 c
3 1 0 0 0 1 1 c
4 0 0 1 1 0 1 c
5 1 1 0 0 1 1 c
rule i: ABE ? c
rule j: CDF ? c
The distance D(i, j) between rule i and rule j is calculated
as follows. The support of rule i is 3/5, that is, tuple 1, 2 and
5 match rule i. The support of rule j is 2/5, that is, tuple 2
and 4 match rule j. The support of both rules is 1/5, that is,
tuple 2 matches rule i and rule j. Therefore the distance is
calculated as:
D(i, j) = (3/5) + (2/5)? 2(1/5) = 3/5
2) Distance using partial match: The distance between
rule i and rule j using partial match is realized considering
the partial match between rules and data and it is defined as
follows:
D(i, j) =
?
d?D|D(d, i)?D(d, j)|
|D| , (2)
(0 ? D(i, j) ? 1.0)
where, D(d, i) = 1? rdiri , rdi is the number of attributes in
the antecedent of rule i which match data d, ri is the number
of attributes in the antecedent of rules i and D is the set of
data.
The definition of the partial match includes the definition
of the complete match as a special case.
E. Fitness of GRA
The fitness function of each GRA individual is defined as
follows:
Fitness =
1
|R|
?
i?R
1
|R(i)|
?
j?R(i)
D(i, j), (3)
where,
D(i, j) is the distance between rule i and rule j described
in Section IV-D. R is the set of suffixes of rules in GRA and
R(i) is the set of suffixes of rules whose distance is defined
between node i in GRA.
The fitness function evaluates the GRA individuals so that
the distances between rules are maximized. It is our interest
to find class association rules far away each other as much
as possible, because that means the rules are not similar.
Therefore, the fitness function eliminates the rules near each
other, as they are redundant or irrelevant.
F. Genetic Operators
In order to find really important class association rules,
the function of the nodes in GRA should be changed. It
is possible to realize the above effectively by GRA genetic
operations, because mutation and crossover will change
the connections or contents of the nodes. Three kinds of
genetic operators are used: crossover, mutation-1 (change the
connection of nodes) and mutation-2 (change the function of
nodes).
In each generation, all of the individuals in the population
are ranked by their fitnesses and the best individual is
preserved for the next generation. Then, tournament selection
of individuals is carried out for reproduction for the next gen-
eration. Finally, the individuals are generated by crossover
and mutation. These operators are executed for the gene of
the nodes of GRA.
This procedure is demonstrated using 240 individuals at
each generation. 80 individuals are selected from the popula-
tion by tournament selection and their genes are exchanged
by crossover. Then, 80 individuals are also selected from
the population and their genes are changed by mutation-1.
Finally, 79 individuals are also selected from the population
and their genes are changed by mutation-2. Therefore, the
239 new individuals generated by crossover, mutation-1 and
mutation-2 and one elite individual form the next population.
• Selection. Elite selection is used in order to preserve
the best individual in the current generation for the next
generation.
• Crossover: Uniform crossover is used. Two parent in-
dividuals are selected by tournament selection twice
for reproduction. Nodes are selected as the crossover
nodes with the probability of Pc. Two parents exchange
the gene of the corresponding crossover nodes. The
new individuals generated are the new ones in the next
population.
• Mutation-1: Mutation-1 operator affects one individual.
One parent individual is selected by tournament selec-
tion for reproduction. The connection of the nodes is
changed randomly by mutation rate of Pm1. The parent
individual in mutation-1 reproduces a new offspring and
it becomes the new one for the next generation.
• Mutation-2: Mutation-2 operator also affects one indi-
vidual. One parent individual is selected by tournament
selection for reproduction. This operator changes the
functions of the nodes by mutation rate Pm2. New
offspring is reproduced from the parent individual by
mutation-2 and becomes the new one for the next
generation.
G. Flowchart of GRA
Fig.7 shows the flowchart of GRA. The initialization
of GRA is carried out using the class association rules
generated by GNP and stored in the pool. That is, for the
first GRA population, each individual is generated assigning
the antecedent of a certain class association rule selected
3252 2009 IEEE Congress on Evolutionary Computation (CEC 2009)
randomly to one of the nodes of GRA randomly. It is ensured
that all nodes within one individual are different.
The evaluation of the individuals is carried out according
to their fitness values. Then, reproduction of the individuals
is carried out using the selection and genetic operations in
order to generate a population for the next generation. This
process is repeated until the last generation.
Fig. 7: Flowchart of Genetic Relation Algorithm.
H. Classification using the rules obtained by GRA
In our proposed method, i.e, the classification by selecting
a small number of really important class association rules
using GRA, is evaluated in terms of classification accuracy.
Two classification methodologies are described, using
complete match and partial match.
1) Classifier using complete match: This classification
methodology is proposed in [1][2], and described as follows:
1) RZ : Calculate the set of suffixes of rules in class Z,
(Z = 0, 1, 2 . . . K).
2) scoreZ(d): Compute the number of rules in class Z in
the classifier whose antecedent matches data d comple-
tely.
3) mZ(d) = 1? scoreZ(d)|RZ | : Predict in such a way that
data d belong to the class having the lowest mZ(d).
2) Classifier using partial match: The classification is
done by comparing the average distances from the test data
to the rules of each class as shown in Fig. 8. This is an
extension of the complete match method.
The classifier is described as follows:
1) RZ : Calculate the set of suffixes of rules in class Z,
(Z = 0, 1, 2 . . . K).
2) mZ(d): Compute the average distance between data d
and the rules in class Z.
mZ(d) =
1
|RZ |
?
i?RZ
DZ(d, i), (4)
where,
DZ(d, i) is the distance between data d and rule i in
class Z, which is calculated by the following:
DZ(d, i) = 1? rdi(Z)
ri(Z)
, (5)
where,
rdi(Z) is the number of attributes in the antecedent
of rule i in class Z which match data d
ri(Z) is the number of attributes in the antecedent
of rule i in class Z.
3) Predict in such a way that data d belongs to the class
having the lowest mZ(d). That is, data d belongs to
the closest class.
Fig. 8: Classification of data d
V. EXPERIMENTAL RESULTS
Two datasets from UCI ML Repository [9] were taken
to conduct the experiments. Lymphography and Vehicle
datasets. Lymphography is a medical dataset that contains the
information about primary tumor. It contains 18 attributes,
148 records and 4 classes. Vehicle dataset is an image
dataset that contains the information about the features of the
silhouette of different types of cars. The purpose is to classify
a given silhouette as one of four types of vehicles. This
database contains 846 records, 18 attributes and 4 classes.
The original datasets are discretized using a supervised
discretization method and the WEKA software [10]. Accord-
ing to this procedure:
1. Replace missing values with the most frequent values
(or means) for nominal (or numeric) attributes, respec-
tively
2. Discretize continuous attributes into nominal ones using
2009 IEEE Congress on Evolutionary Computation (CEC 2009) 3253
the Fayyad and Irani’s entropy based MDL method [11],
and finally
3. Convert the nominal values into binary values.
Once the dataset is discretized, 10-fold cross validation is
performed and the results are given as an average.
The training set and test set are generated randomly from
the dataset. Using the training set, the conventional GNP
based mining method [1][2] is applied to obtain a pool of
a large number of class association rules for each class Z.
Table IV shows the parameters for the conventional GNP
mining method.
TABLE IV: Parameters for Genetic Network Programming.
Parameter V alue
Number of GNP individuals 120
Number of generations 100
Number of Judgment nodes 95
Number of Processing nodes 10
?2 min 6.63
supmin 0.01
Then, GRA is applied to reduce the number of association
rules for each class using different reduction rates (ex. 1%,
5%, 10%, etc.). For example, the reduction rate of 5% means
that only 5% of the total number of rules generated by GNP
are selected by GRA. Table V shows the parameters for the
evolution of GRA.
TABLE V: Parameters for Genetic Relation Algorithm.
Parameter V alue
Number of GRA individuals 240
Number of generations 100
Number of nodes in each individual variable
Crossover probability 0.1
Mutation-1 probability 0.01
Mutation-2 probability 0.01
Finally, the classification accuracy is evaluated and com-
pared with other methods using the test set and.
All algorithms were coded in Java language. Experiments
were performed on a 600MHz Pentium PC with 512M main
memory, running Microsoft Windows XP.
In this paper, the complete match and partial match are
compared and evaluated in terms of the prediction accuracy.
Fig. 9 and Fig. 10 show the accuracy of the complete
match compared with the accuracy of the partial match with
different reduction rates for the Lymphography and Vehicle
datasets, respectively.
For instance, the reduction rate of 10% means that only
10% of the total number of rules generated by GNP are
selected by GRA.
It is shown from Fig. 9 and Fig. 10 that when the reduction
rate is small, GRA is able to get comparable accuracy to the
large set of rules, that is, 100% of the rules, especially in
the partial match, furthermore, it is shown that the accuracy
does not change drastically compared to the accuracy of the
large set of rules.
Fig. 9: Accuracy of dataset Lymphography
Fig. 10: Accuracy of dataset Vehicle
Table VI shows the accuracy comparison among the large
set of rules (100 %), the reduced set of rules by GRA
and several conventional methods. GRA in Table VI means
the average prediction accuracy calculated using the rules
reduced to 1%, 5%, 10% and 20% from all the rules obtained
by GNP when complete match and partial match are used.
The results for C4.5.[12] Ripper[13], CBA[14], CMAR[15]
and CPAR are taken from the paper of [16]. The first
column of Table VI shows the names of the datasets, the
second and third columns show the number of attributes
before and after the discretization process, respectively. The
fourth column shows the number of records in the datasets.
The rest of the columns shows the accuracy comparisons
among several methods. The accuracy of GNP in seventh
and eighth columns of Table VI is obtained considering the
complete large set of association rules stored in the pool and
using the classifier of the complete match and partial match,
respectively. The other methods use a relatively small number
of rules. The results show that the proposed method using
GRA outperforms the conventional ones and is comparable
with the GNP based mining method in terms of accuracy.
3254 2009 IEEE Congress on Evolutionary Computation (CEC 2009)
TABLE VI: Comparison of classification accuracy with other methods
Dataset Attr.
before
discret
Attr.
after
discret
No.
Recs
GRA GNP C4.5 CBA CMAR Ripper CPAR
CM PM CM PM
Lymph 19 55 148 85.60% 84.57% 85.76% 83.68% 73.50% 77.80% 83.10% 79.00% 82.30%
Vehicle 19 109 846 70.02% 75.14% 72.41% 75.65% 72.60% 68.70% 68.80% 62.70% 69.50%
GRA: Use the reduced set of rules
GNP: Use the large set of rules
PM: Partial Match
CM: Complete Match
VI. CONCLUSIONS AND FUTURE WORK
This paper proposes a novel approach to reduce a large
number of association rules using an evolutionary computa-
tion method named GRA. The main features of our proposed
method are as follows:
• GRA could be usefully applied to reduce a large set of
class association rules and to improve the classification
accuracy.
• Our method could be integrated to other conventional
association rule mining methods, since the input could
be any large set of rules.
• The classification accuracy is improved especially when
the partial match is used in the classifier.
For future work, we plan to extend this method to eliminate
redundant rules in general association rules, that is, the
association rules with non-fixed consequent (multiple items),
using the GRA with directed branches.
REFERENCES
[1] K. Shimada, K. Hirasawa and J. Hu, “Class Association Rule Mining
with Chi-Squared Test Using Genetic Network Programming”, In Proc.
of the IEEE Conference on Systems, Man and Cybernetics, pp. 5338-
5344, Taipei, 2006/10.
[2] K. Shimada, K. Hirasawa and T. Furuzuki, ”Genetic Network Program-
ming with Acquisition Mechanisms of Association Rules”, Journal of
Advanced Computational Intelligence and Intelligent Informatics, Vol.
10 No. 1, pp. 102-111, 2006.
[3] N. Pasquier, Y. Bastide, R. Taouil and L. Lakhal, “Discovering Frequent
Closed Itemsets for Association Rules”, In Proc. of the 7th Intl. Conf.
on Database Theory, LNCS 1540, pp. 398-416, 1999.
[4] M. J. Zaki, “Generating Non-redundant Association Rules”, In Proc. of
the KDD 2000, pp. 34-43, ACM press, 2000.
[5] M. Kryszkiewicz, “Representative Association Rules and Minimum
Condition Maximum Consequence Association Rules”, In J.M. Zytkow
and M. Quafalou (eds.), “Principles of Data Mining and Knowledge
Discovery”, PKDD ’98, LNCS 1510, pp. 361-369, Springer, 1998.
[6] B. Liu, M. Hu and W. Hsu, “Multi-Level Organization and Summariza-
tion of the Discovered Rules”. In the proc. KDD, pp. 208-217, 2000.
[7] J. Li, H. Shen and R. Topor, “Mining the Optimal Class Association
Rule Set”, Knowledge Based Systems. Vol. 15, No. 7, pp. 364-375,
Elsevier, 2002.
[8] C. Zhang and S. Zhang, Association Rule Mining: models and algo-
rithms, Springer, 2002.
[9] C. Blake and C. Merz, UCI Repository of machine learning databases,
http://www.ics.uci.edu/mlearn/MLRepository.html.
[10] Waikato Environment for Knowledge Analysis, Open source project
for machine learning, Released under GPL (General Public Licence)
http://www.cs.waikato.ac.nz/ ml/weka/.
[11] U. M. Fayyad and K. B. Irani, “Multi-interval discretization of
continuous valued attributes for classification learning” In Proc. of the
13th International Joint Conference on Artificial Intelligence, pp. 1022-
1027, Los Altos, CA: Morgan Kaufmann, 1993.
[12] J. R. Quinlan, “C4.5: Programs for Machine Learning”, Morgan
Kaufmann, 1993.
[13] W. Cohen, “Fast effective rule induction”, In Proc. of the ICML’95,
pp. 115-123, Tahoe City, CA, July 1995.
[14] B. Liu, W. Hsu and Y. Ma, “Integrating Classification and Association
Rule Mining”, In Proc. of the KDD’98, pp. 80-86, New York, NY, Aug.
1998.
[15] W. Li, J. Han and J. Pei, “CMAR: Accurate and efficient classification
based on multiple class-association rules”, In Proc. of the ICDM’01, pp.
369-376, San Jose, CA, Nov. 2001.
[16] X. Yin and J. Han, “CPAR:Classification based on Predictive Associ-
ation Rules”, In Proc. of the 2003 SIAM International Conf. on Data
Mining (SDM’03), 2003.
2009 IEEE Congress on Evolutionary Computation (CEC 2009) 3255
