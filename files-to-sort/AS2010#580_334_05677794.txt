A Growing Evolutionary Algorithm for Data Mining 
 
 
Zhan-min Wang 
School of Computer Science and 
Engineering 
Xi’an University of Technology  
Xi’an?China  
E-mail: wangzhanmin_xaut@163.com 
Hong-liang Wang 
Department of computer 
Shijiazhuang vocational technology 
institute  
ShiJiaZhuang?China  
E-mail: wanghongliang_sjz@163.com 
Du-wu Cui 
School of Computer Science and 
Engineering 
Xi’an University of Technology  
Xi’an?China  
E-mail: cuiduwu_xaut@163.com
 
 
Abstract—an unsuitable representation will make the task of 
mining class association rules very hard for a traditional genetic 
algorithm (GA). But for a given dataset, it is difficult to decide 
which one is the best representation used in the mining progress. 
In this paper, we analyses the effects of different representations 
for a traditional GA and proposed a growing evolutionary 
algorithm which was robust for mining class association rules in 
different datasets. Experiments showed that the proposed 
algorithm is effective in dealing with problems of deception, 
epistasis and multimodality in the mining task. 
Keywords-association rule; genetic algorithm; representation; 
I.  INTRODUCTION 
Genetic algorithms (GAs) are robust adaptive systems 
that have been applied successfully to hard optimization 
problems. There are two famous approaches commonly used 
by genetic algorithms as classifier systems, the Michigan 
approach and the Pittsburgh approach. The Michigan approach 
maintains a population in which each individual is a rule and 
the Pittsburgh approach maintains a population in which each 
individual is a rule set and is encoded as a variable-length 
string [1,2,3,4].  
Both of the two approaches run under the standard 
framework of GA and are based on the assumption that the 
fitness function value can be used to estimate the distance to 
the nearest global maximum. However, a lot of factors existed 
in the process of association rule mining largely affect the 
result of optimization, for example: epistasis, multimodality 
and deception. In this paper, we propose a growing 
evolutionary algorithm for data mining (GEA-DM) which 
decrease the affects coming from deception, epistasis and 
multimodality. 
II. REALTED WORK 
A. GA Difficulty 
The search for factors affecting the ability of the GA to 
solve optimization problems has been a major focus within the 
theoretical GA community. One approach to the question has 
been the study of deceptive problems proposed by Goldber 
(1987), based on the work of Bethke [5]. Das [6] claimed that 
deception is the only thing that is important in making a 
problem hard for a GA. However, Grefenstette [7] believed 
that deception is neither necessary nor sufficient for a problem 
to be hard for a GA. Now, it is clear that all of the facts, 
epistasis, multimodality, noise, deception and spurious 
correlations, can cause difficulty for a GA. 
A model of fitness landscapes developed by Jones [8] 
suggests that there are strong connections between GA search 
and heuristic search. A general principle of heuristic functions 
is that they should correlate well with the distance to the goal 
of the search. If the fitness function of GA is viewed as a 
heuristic function, the value of fitness function is the estimate 
distance to the nearest goal of the search. FDC is a method of 
quantifying the relationship between fitness and distance. 
Given a set F={f1, f2, …,  fn} of n individual fitness values 
and a corresponding set D={d1, d2, …, dn} of the n distances 
to nearest global maximum, we compute the correlation 
coefficient r, as 
r=cFD/sFsD,               where 
cFD= ?
=
??
n
i
ddiffi
n 1
))(1  
is the covariance of F and D, and sF, sD, f  and d  are the 
standard deviations and means of F and D respectively. 
B. Assciation Rule 
Let ?={i1,i2,…,im} be a set of literals, called items. Let D 
be a set of transaction, where each transaction T is a set of 
items such that T ? ?.  Formally, an association rule R is an 
implication X??Y, where X and Y are sets of items or 
itemsets in a given dataset. The confidence of the rule Conf(R) 
is the percentage of transactions that contain Y amongst the 
transactions containing X. The support of the rule Supp(R) is 
the percentage of transactions containing X and Y with respect 
to the number of all transactions. Given a set of transactions D, 
the problem of mining association rules is to generate all 
association rules that have support and confidence greater than 
the user-specified minimum support ( called minSuppport ) and 
minimum confidence (called minConfidence) respectively [9].  
Supp(R)>=minSupport                                                   (1) 
Conf(R)>=minConfidence                                             (2) 
This work was supported by the National Natural Science Foundation of 
China (Grant nos. 60873035) 
978-1-4244-7941-2/10/$26.00 ©2010 IEEE
If Supp(X)>=minSupport, we say that X is a frequent 
itemset. Generally, association classification approach consists 
of three major processes, namely rule discovery, rule selection 
and classification. In rule selection, we always select the 
subset which gives the best accuracy to form a classifier and 
we believe that rules with higher confidence would probably 
give higher prediction accuracy [10].  So, finding the best 
confident frequent itemset is considered as a worthwhile thing. 
III. GA DIFFICULTY IN ASSOCIATION RULE  
MINING 
A. The Different Representations of Rules in Rule Space 
In this section, we describe a dataset D which consist of 
four condition attributes A={a1,a2,a3,a4}, B={b1,b2,b3,b4}, 
C={c1,c2,c3,c4}, D={d1,d2,d3,d4} and a class attribute 
E={e1,e2}. Each attribute contains four values and we 
represent them with five variables X, Y, Z, M and N as 
follows: 
x=1?a1, x=2?a2, x=3?a3, x=4?a4 y=1?b1, y=2?b2, 
y=3?b3, y=4?b4 z=1?c1, z=2?c2, z=3?c3, z=4?c4 m=1
?d1, m=2?d2, m=3?d3, m=4?d4, n=1?e1, n=2?e2 
As can be seen in Fig.1-A, we describe a record of the 
dataset D with a circle. The yellow circles are records in class 
e1 and the green ones are records in class e2. 
However, if we change the representation of the record, we 
can obtain another distribution of the records in record space, 
which can be seen in Fig.1-B. 
x=1?a1, x=2?a4, x=3?a3, x=4?a2 y=1?b1, y=2?b3, 
y=3?b4, y=4?b2 z=1?c4, z=2?c2, z=3?c3, z=4?c1, 
m=1?d2, m=2?d4, m=3?d1, m=4?d3, n=1?e1, n=2?e2. 
a1 a2 a3 a4
b1
b2
b3
b4
c1
c2
c3
c4
d4 d3 d2 d1
a1 a2a3a4
b1
b2
b3
b4
c1
c2
c3
c4
d4d3 d2d1
A B  
Figure 1.  Records in Record Space 
B. The Effectiveness of Different Representation  
The association rule space S consists of all the possible 
frequent itemsets. Each possible frequent itemset is a point in 
this space. If a point in the rule space whose support is greater 
than support threshold, we call the point a frequent itemset. 
The objective of the association rule mining in space S is to 
extract all the frequent itemsets whose confidence is great than 
confidence threshold. 
Comparing with Fig.1, we showed the association rules 
whose support and confidence values are great than the support 
threshold 5% and confidence threshold 50% in Fig.3-A. The 
blue circles are the found rules. 
 
Figure 2.  Rules in Rule Space 
Comparing with Fig.2, after changing the representation, 
we showed the association rules whose support and confidence 
values are great than the two thresholds in Fig.2-B. The blue 
circles are the found rules. 
 
Figure 3.  Fitness Values of  The Founded Association Rules 
In traditional GA, the confidence value of a rule is always 
seen as a value of a fitness function. If an appropriate 
expression manner was chosen and the problem is an easy one 
to solve, the fitness function should be used to estimate the 
distance to the nearest global maximum. But if some factors 
exist in the process which affect the mining greatly or an 
unsuitable expression was selected, the fitness function will not 
drive the optimal process effectively. We can see this from 
Fig.3, A is in an appropriate expression manner and B is in an 
unsuitable expression manner. 
In fact, for a giving dataset D, it is very difficult to decide 
to which representations is suitable and at the same time, 
deception, epistasis and multimodality always existed in the 
optimal progress. These will make the traditional GA hard for 
mining association rules. 
IV.  GROWING EVOLUTIONARY ALGORITHM 
A. Growing Evolutionary Algorithm 
Mining the association rule with greatest confidence value 
can be considered as an optimization process in the rule space. 
The association rules can be considered as individuals in a 
population. The different representations will have a dramatic 
impaction on how easily the problem will be solved.  
Different with traditional EA, we represent individuals 
with different number of items which can be seen as different 
age. Every time a generation was produced, the ages of the 
individuals in it were set to 1 and began to grow. As the 
increasing of the age, the support values of them become more 
and more small. The confidence values were used for selecting 
good individuals within the population in which individuals 
are at the same age. Since the selection was implemented 
within a limited area (at the same age), it prevented the 
misleading of the fitness function. 
Step 0. Count the support of the all individuals with only 
one item and choose the one whose support value is greater 
than support thresholds to form the items set I, I={item1, 
item2, …, itemn}.  
Step 1. Randomly initialize a rule population PN, PN 
=p1+p2++pN. Each rule pi (i=1,2,…,N) in it has a class label 
and one item coming from set I. Calculate the support and 
confidence value of the rules. 
Step 2.  Select rules in population Pr,kN and form the 
population Pn,kM, Pn,kM= Ts (Pr,kN). The selection operation 
Ts is defined by the following. (1) Suppress the rules in the 
population Pr,kN whose support value are less than the support 
threshold “minSupport” and form the population P*r,kN. (2) 
Sort the individuals in the population P*r,kN according to their 
fitness and choose the first M highest confidence rules to form 
the population Pn,kM . 
Step 3.  Proliferate Ni clones for each rule pn,i(i=1,2,…,M) 
in the population Pn,kM and form the population CkQ, 
CkQ=Tc(Pn,kM)={c1k, c2k, …, cQk}. The number Ni is decided by 
the following equation, Q is the size of population CkQ. 
1
( ) / ( )
M
i ii
i
N Q conf R conf R
=
= × ?  
Step 4.  Bring up each clone in population CkQ and delete 
the same one, then suppress the rules whose support value are 
less than support threshold and form the population C*kQ, 
C*kQ=Ts(Tm(CkQ)) ={c*1k, c*2k, …, c*Qk}. The bring-up 
operation Tm is defined by the following: (1) Randomly 
choose an itemi from set I. If there is an itemk in the rule 
whose attribute is the same with itemi, randomly choose 
another one until no itemk in the rule whose attribute is the 
same with itemi. If no such an item can be found, the rule gets 
its maximal lifetime (the number of items of the a is equal to 
the number of attributes of a record) and is deleted. (2) If not 
be deleted, an itemi is added to it to form a new one. 
Step 5. If the population C*kQ is not empty, go setp 2. 
Otherwise, compares the best individual with the individual in 
memory pool, preserves the bigger one. If termination 
condition is met, the rule in memory pool is the best rule we 
searched. 
B.  The Effectiveness of Different Representation 
The drawback of the traditional GA used in mining 
association rule is the inefficiency in dealing with deception, 
epistasis and multimodality. However, with the proposed 
algorithm, the effect of the fitness function was restricted in a 
certain area in which all the individuals were at the same age. 
With different representation, the areas of different age are 
fixed. So efficiency of the proposed algorithm is independent 
of representation. We can see that from Fig.4 in which the 
rules with 1, 2, 3 and 4 items were described by red, pea green, 
pink and green circle. The rules whose confidence and support 
values are great than the two thresholds were shown by circle 
with a blue edge. The founded rules were shown in Fig.4-A, 
when we represent the individuals in the manner 1. In Fig.4-b, 
another representation manner was shown. If we look on the 
number of the circles between two rules as distance, we will 
find the distance between two rules are fixed with the 
changing of the representation. 
V. EXPERIMENTS 
In this section, we examine how such an algorithm can be 
used to search association rules. We have conducted an 
experiment on a 3.0 GHz Pentium PC with 512MB of memory 
running with Microsoft Windows XP to measure the 
performance of the proposed approach. The datasets used in the 
experiment are obtained from the UCI Maching Learing 
Repository [11]. 
We compared the proposed GEA-DM algorithm with 
GA algorithm in Table.1, which illustrates the statistic 
results of computation time, the threshold of confidence 
and support, the number of instances and attributes of the 
dataset. When the confidence threshold of dataset Balance 
Scale was set to 100%, we could almost not find a 
validated solution. So, we set it to 80%. One important 
thing we must emphasize is that the complexity of a dataset 
is decided not only by the number of instances and 
attributes, but also by the configuration of instances and the 
number of the items and classes in it. So, we should not 
simply compare computation times between different 
datasets. From table.1, we can see that the average 
performance of the proposed algorithm GEA-DM is better 
than the traditional GA in mining association rules. 
TABLE I.  COMPARISION IN DIFFERENT DATASET 
Dataset Attribute
s 
Instances Supp(R) Conf(R) GA(s) GEA-
DM(s)
Balance Scale 4 625 >3.8% >80% 12 8 
Hayes-Roth 5 132 >9.8% 100% 2.2 2 
Chess 6 28056 >0.039% 100% 979 768 
Car 
Evaluation 
6 1728 >33.3% 100% 16 9 
Nursery 8 12960 >33.3% 100% 166 132 
Contraceptive 
Method 
Choice 
9 1473 >1.4% 100% 91 69 
tic-tac-toe 
Endgame 
9 958 >9.5% 100% 59 42 
Poker Hand 11 25010 >0..56% 100% 7296 5402 
Adult 14 32561 >2.4% 100% 2434 1844 
Congressional 
Voting 
16 435 >50.57% 100% 174 132 
Mushroom 22 8124 >21.66% 100% 350 280 
 VI. DISCUSSION AND CONLUSION 
Inspired by the natural evolutionary process, we proposed 
an evolutionary algorithm, GEA-DM, for dealing with 
problems of mining association rules. In GEA-DM, each 
individual has a flexible life span during which individuals can 
grows and reproduces. The performance evaluation results 
have shown that the proposed GEA-DM approach has 
achieved good performance in comparison with conventional 
G algorithms. 
Being different from GA, the performance of the proposed 
algorithm is stable with different representations of individuals. 
This is very useful for dealing problems of deception and 
epistasis. Next, we will study on how to pass a message from 
parent individual to son individuals. 
REFERENCES 
 
[1] J. H. Holland, “Adaptation in natural and artificial systems”, Ann Arbor: 
University of Michigan Press,1975 
[2] J. H. Holland, “Adaptation progress in theoretical biology”, New 
York:Academic,vol.4,pp.263–293,1976 
[3] D. E. Goldberg, “ Genetic Algorithms in Search”, Optimization & 
Machine Learning. 1st ed. New York: Addison-Wesley, 1989 
[4] Albert Orriols-Puig, Jorge Casillas et al., “Fuzzy-UCS: a Michigan-style 
learning fuzzy-classifier system for supervised learning”. IEEE 
transactions on evolutionary computation. 13(2),2009. 
[5] A. D. Bethke, “Genetic algorithms as funtion optimizers”, Ph.D. thesis, 
University of Michigan, Ann Arbor, MI. 
[6] R. Das, L. D. Whitley, “The only challenging problems are deceptive: 
global search by solving order-1 hyperplanes.”, Proceedings of the 
Fourth International Conference on Genetic Algorithms. San Mateo, CA: 
Morgan Kaufmann, 1991. 
[7] J. J. Grefenstette, “Deception considered harmful”, Pages 75-91 of: 
Whitley, L. D., Foundations of genetic algorithms, vol.2. San Mateo, CA: 
Morgan Kaufmann.1993 
[8] T. C. Jones, “Evolutionary Algorithms, fitness landscapes and search.”, 
Ph.D. thesis, University of New Mexico, Albuquerque, NM.1995. 
[9] R. Agrawal, T. Imielinski and A. Swami. “Mining association rules 
between sets of items in large databases”. In Proc. of the ACM 
SIGMOD Conference on Management of Data, Washington, D.C, May 
1993. 
[10] J. Han, J. Pei and Y. Yin, “Mining frequent patterns without candidate 
generation,” in Proc.2000 ACM SIGMOD Intl. Conf. Manage Data, 
2000, pp. 1–12. 
[11] D. J. Newman, S. Hettich, C. Blake, and C. Merz, UCI Repository of 
Machine Learning Databases. Berleley, CA: Dept. Information Comput. 
Sci., University of California,1998. 
 
 
A 
 
B 
Figure 4.  The Founded Association Rules in Different Representation Manner 
