Distributed Classification using Class-Association 
Rules Mining Algorithm
Djamila Mokeddem 
Dept. of Computer Sciences, LSSD Laboratory  
University of Sciences and Technologies Mohamed Boudiaf  
Oran, Algeria 
D-Mokeddem@univ-usto.dz 
Hafida Belbachir 
Dept. of Computer Sciences, LSSD Laboratory  
University of Sciences and Technologies Mohamed Boudiaf  
Oran, Algeria 
H_belbach@yahoo.com 
Abstract— Associative classification algorithms have been 
successfully used to construct classification systems. The major 
strength of such techniques is that they are able to use the most 
accurate rules among an exhaustive list of class-association rules. 
This explains their good performance in general, but to the 
detriment of an expensive computing cost, inherited from 
association rules discovery algorithms. We address this issue by 
proposing a distributed methodology based on  FP-growth 
algorithm. In a shared nothing architecture, subsets of 
classification rules are generated in parallel from several data 
partitions. An inter-processor communication is established in 
order to make global decisions. This exchange is made only in the 
first level of recursion,  allowing each machine to subsequently 
process all its assigned tasks independently. The final classifier is 
built by a majority vote. This approach is illustrated by a 
detailed example, and an analysis of communication cost.
Keywords- Distributed data mining; Association rule mining; 
Class-association rules; FP-growth algorithm 
I. INTRODUCTION 
Many of data mining algorithms are suitable for distributed 
computing for two reasons: scaling up this algorithms, or 
mining inherently distributed data. This paper focuses on 
classification algorithms based on association rules, a.k.a 
“Associative Classification  algorithms” (AC)  which is a 
promising  approach in data mining that utilizes the association 
rule mining techniques to construct classification systems.  
Several studies [1] [2] [3] have provided evidence that such 
algorithms are able to extract classifiers competitive with those 
produced by many of traditional classifiers like decision trees, 
rule induction and probabilistic approaches. The advantage of 
this approach lies in  (1) its simplicity compared to other 
mathematical classification methods, and its competitive 
results in term of accuracy (2) the  exhaustive quest for all rules 
allows to find many interesting and useful rules not being 
discovered  by the state-of-the-art classification systems like 
C4.5 [4] (3) It produces an understand model based on user-
specified constraints. A typical associative classification 
system is constructed in two stages: (1) generating the 
complete set of class association rules (CARs) that satisfy the 
user-specified minimum support (called minSup) and 
minimum confidence (called minConf); (2) selecting the most 
accurate rules to build the classifier (pruning phase). The major 
strength of such systems is that they are able to use the most 
accurate rules for classification because their rule generators 
aim to find all rules. However, when datasets contain a large 
number of rows and/or columns, both rule generation and rule 
selection in such systems are time consuming. To address this 
issue, we  propose a distributed algorithm for  associative 
classification based on FP-growth algorithm [5]. The remainder 
of this paper is organized as follows: the next section gives an 
outline on AC  algorithms, the section 3 provides a review of 
related works. In section 4, a detailed framework of the 
proposed distributed algorithm is described, following by a 
conclusion and discussion  on the tracks which always deserve 
to be explored.
II. ASSOCIATIVE CLASSIFICATION ALGORITHMS
Classification is one task of data mining which allows 
predicting if a data instance is member of a predefined class. In 
a more general objective, the task of association rule discovery 
focus on finding rules of the form A ? B relating disjoint set of 
database attributes, which is interpreted to mean “if the set of 
attribute-values, or “itemset” A is found together in a database 
record, then it is likely that the itemset B will be present also”.   
The discovering of “rules” in these two tasks is a strong 
common point which has inspired several researchers  to 
explore the possibility of using association rule discovery 
methods in  classification. The basic idea is to view 
classification rules as a special case of association rules, in 
which only the class attribute is considered in the rule’s 
consequent.   In order to build classifier, an AC algorithm uses 
a training data set D to produce class association rules (CARs) 
in the form of  X ? y,  where X ? I, the set of “items”, and 
y ? Y , the set of class labels. An item is described as an 
attribute name Xi and a value xi, denoted < X, x >. The rule 
R: X ? y has support s, denoted as supp(R), if  s% of the cases 
in D contain X and are labeled with class y. A rule R: X ? y
holds in D with confidence c, denoted as conf(R), if c% of 
cases in D that contain X are labeled with class y. The main 
goal is to construct a set of CARs that satisfies the minimum 
support minSup and minimum confidence minConf constraints, 
and  that is able to predict the classes of previously unseen data 
(the test data set), as accurately as possible. Other measures can 
be used like the conviction which take into account  the class 
frequency, it is defined as: conv(X ? y) = ((1 ? supp(y))) ?
((1 ? conf(X ? y) )), where  supp(y) is the frequency of the 
class label  y in D.
334978-1-4244-8611-3/10/$26.00 ©2010 IEEE
A. Generating rules for classification 
To find the complete set of classification rules passing 
certain support and confidence thresholds, AC systems mine 
the training data set using a variant association rule discovery 
method. Several AC approaches, e.g. CBA [1], have been 
adopted from Apriori algorithm [6] which is based on an 
iterative process:  generate candidates, and generate frequent 
itemsets. This method is computationally expensive because it 
needs  repetitive scans of the data base.  Others, like CMAR 
[2], are based on FP-growth algorithm [5],  one of the most 
efficient serial algorithms to mine frequent item sets. The 
effectiveness of FP-growth algorithm comes from its FP-tree 
structure which   compresses a large  database into a compact 
form, and its divide-and-conquer methodology.  Furthermore, it 
avoids candidate generation, and needs only two scans of the 
entire data base. 
B. Pruning Rules 
Pruning techniques rely on the elimination of rules that are 
either redundant or misleading from taking any role in the 
prediction process of test data objects. The removal of such 
rules can make the classification process more effective and 
accurate. A survey of pruning methods is documented by  
Thabtha in [3].
III. RELATED WORK
The design of distributed associative classification systems  
does not differ too much from the design of distributed 
association rule mining methods, at least in the first step 
(generation of CARs). For this reason, a set of recent works on 
distributed FP-growth algorithm will be presented. These 
approaches proposed  for association rules discovery in 
distributed computing environment, may be adapted to 
generate the class-association rules in a distributed 
classification context. 
A. Class-Association Rule mining based on serial FP-growth 
In AC algorithms using  FP-growth, classification rules are 
generated by adapting this algorithm to take into account the 
class label. The key idea used in CMAR system [2] for 
example is to store the class label in the FP-tree structure. We 
illustrate this by the following example (fig. 1), where 
minSupp = 2 (66%). First, the training data set is scanned to 
find the list of frequent items F-list sorted by descending order 
of supports. A second scan is done to construct the FP-tree,
where  each frequent item in each record is inserted in the tree 
according to the order of  F-list. This operation allows to 
represent a shared prefix of several records only once, like the 
prefix “a1a2” shared by the first and second records. Support 
counts are summed in each node, and class label is attached to 
the last node in the path. In the phase of frequent CARs 
mining, frequent items are recursively mined as follows: 
construct  conditional pattern base (CPB) for each item in F-
list, construct conditional FP-tree (CFP-tree) from CPB. When 
the conditional FP-tree contains a single path, the recursion is 
stopped and frequent patterns are enumerated using a 
combining method. This process is more detailed in [5]. Once a 
frequent item is found, rules containing this item can be 
generated immediately, by merging class values in the lower 
branches of the considered item. For example, to find rules 
having  b3,  CPB is built by enumerating prefixes of b3 in the 
initial FP-tree:{a1a2(y1:1); a1(y1:1)}.  
Figure 1. An example of bulding FP-tree taking into account the class labels. 
Then CFP-tree is obtained considering only frequent items 
in  CPB. In this case, CFP-tree contains only one node  a1:2, 
and the CARs generated are: b3 y1,  and  b3a1 y1, 
obtained by combining the node a1 of CFP-tree, and the
considered item b3. The two rules have support 2 and 
confidence 100%. The remaining rules can be mined similarly. 
B. Distributed  association rule mining based on fp-growth 
algorithm 
In very large databases and with low value of minimum 
support, the used FP-tree structure may not fit into the main 
memory. Several algorithms have been proposed in the 
literature to address the problem of frequent itemset mining in 
parallel and distributed environment. Many of them are based 
on apriori algorithm, e.g. CD and DD strategies [7], but they 
still suffer from sequential a priori limitations. Inspired by its 
intrinsic divide-and-conquer nature, and its performance gain 
over a priori-based methods, FP-growth has been the subject of 
several studies in parallel and distributed frequent itemsets 
mining. The main goal is to reduce the time spent in 
computation, with a minimum of interaction between data sites. 
A compromise between memory constraint and inter-processor 
communication cost is difficult to obtain in distributed frequent 
itemset mining. The ideal state is a model that allows a “total” 
independent processing between data sites. This may be 
possible generally if a certain form of data replication is 
assumed like in [8]. Other works are based on an information 
exchange in different forms: local conditional pattern bases [9] 
or sub-trees [10]. Clearly there are tradeoffs between these two 
different approaches. Algorithm designers must compromise 
between approaches that copy local data globally and 
approaches which retrieve information from remote machines 
as needed. 
C. Distributed associative classification
To our knowledge, and up to the writing of these lines, 
there are been  few published studies that have focused on 
distributed associative classification. Thakur [11] proposed a 
parallel model for CBA system [1]. The parallel CAR 
generation phase is an adaptation of CD approach [7] for 
mining CARs in associative classification.  The training data 
set is partitioned among P processors, and in each iteration, 
each site calculates local counts of the same set of candidates 
and broadcasts these to all other processors. After a 
synchronization step, CARs are generated from frequent 
itemsets. This strategy inherits two major limitations of CD 
approach : a   tight synchronization at the end of each step, and 
the duplication of the entire set of candidates at each site.
335
IV. THE PROPOSED MODEL
The goal of this work is to propose a distributed model for 
the associative classification technique. We start by presenting
a simple sequential algorithm based on FP-growth approach. 
A. The sequential associative classification algorithm 
In the sequential version (Algorithm 1), the list of CARs is 
generated by FP-growth algorithm like in CMAR. To evaluate  
the rank of a rule, a total order is defined as follows: Given two 
rules r1 and r2, r1 is said having higher rank than r2, denoted as 
r1 r2, if and only if (1) conv(r1)>conv(r2); (2) conv(r1) = 
conv(r2) but conf(r1)>conf(r2); (3) conv(r1)=conv(r2), 
conf(r1)=conf(r2) but supp(r1)>supp(r2); or (4) The 
convictions, confidences and supports of  r1 and r2 are the 
same, but r1 has fewer attribute values in its left hand side than 
r2 does. The use of conviction measure might give more 
information on the rule rank,  because the class frequency is 
taken into account (cf. 2). 
Algorithm 1 Sequential AC
Input: training data set D, minSup, minConf
Output: the list of CARs 
1. Scan D and build F-list  
2. Scan D and build FP-tree structure, storing also  the class 
labels of each record; 
3. For each item in F-list, construct CPB and CFP-tree 
4. Mine recursively each CFP-tree, by extracting CARs.  
5. Sort the CARs. 
Seeing that this algorithm will be parallelized, we propose 
to build the classifier by all the rules without any pruning 
operation. The classification  of a new record will be made as 
follows:  go through the list of all sorted rules until the first rule 
that covers the example is found (a rule r covers an instance d
if d satisfies all conditions of the rule body of r);  classify the 
example according to the class at the right -hand side of the 
rule. If no rule covers this example, mark the example as 
unclassified. 
B. Associative Classification in a distributed environment 
In this scheme (Algorithm 2),  processors work in parallel 
on a shared-nothing architecture. The processors begin with 
counting local support for each item. This counts are then 
exchanged across the group in order to each processor 
calculates their sum. After discarding globally infrequent items, 
each processor  constructs the F-list structure sorted by 
descending order of frequent item supports (lines 2-4). In the 
next phase, local  CFP-trees are built by scanning local data 
partitions and considering only local items belonging to F-list.
The class label is attached to the last node in the path (line 5). 
Thereafter, the local CFP-trees are used in parallel to generate 
local conditional pattern bases CPBs in each processor, for 
each item in  F-list (line 6). 
This partial information will be communicated between the 
processors and merged to generate the initial global CPB for 
each item. A trivial method to assign tasks to different 
processors is to perform a bloc-cyclic item distribution, so that 
the item i will be assigned to the processor number ( ?
1)% . Thereby, each processor will send its local CPBs to
corresponding processors avoiding to send “all” to “all” (line 7-
9).
Algorithm 2 Distributed AC 
Input: P data partitions a training data set D; minSup, minConf  
Output: A classifier Clmaj representing a majority vote between P 
classifiers,  
1. For  = 0 to  ? 1 do in parallel, in processors 	. . 
 
2. Scan local data partition Dj and count  local support for 
each item , (); 
3. Broadcast () for each item ; 
4. Build - sorted in support descending order, by a global 
reduction () = ? () , let  the size of  -.  
5. Scan local data partition and build local - with local 
items, according to the order of -;  
6. For each item   in  -  which belongs to the local 
partition, build local conditional pattern base (); 
7. The    items of  F-list  are equitably assigned to each 
processor according a block-cyclic data distribution; 
8. Send ()  to the corresponding processors; 
9. Merge received and local () for assigned items  
10.Delete local - and local () for not assigned items i; 
11.Apply Sequential AC algorithm serially and independently 
on assigned items; 
12.Sort locally the subset of rules CARj; 
13. End do in parallel 
This data distribution strategy  could “contribute” to 
balance the load between the  processors, because generally 
the amount of work needed to process an item of F-list
increases for the items with low supports, thus those having 
longer prefixes.  After this communication, each machine 
independently mines recursively its assigned items, without 
any  need of synchronization. At this level, several data 
structures can be deleted from distributed main memories: 
local CFP-trees and local CPBs for the items assigned to other 
processors (line 10). At the end, if each site products a subset 
of CARs, the union of all subsets must be exactly the total rule 
set obtained in the serial version of the algorithm. To classify 
a new record d, a simple and intuitive technique consists in 
performing a majority vote between the P components of the 
final composite model. This strategy was used successfully in 
ensemble learning methods, with e.g. decision trees as base 
classifier [12]. The instance to classify is presented to each 
classifier Clj, (j= 1..P), like in the sequential version, thus it 
will be classified according to the prediction of the majority, 
denoted by  Cl(d) = argmax?!? I!Cl = c"#$ ", where c 
is a class label, Clj is the classifier obtained in the processor j, 
and I(A) is an indicator function that returns 1 if A is true and 
0 otherwise. The example in (fig.2, a) illustrates the parallel 
construction of  F-list structure in three processors. After 
communicating local counts, processors compute the sum and  
discard non global frequent items. Instead of building a global 
FP-tree, which may not fit in main memory, local FP-trees are 
constructed in each processor (fig.2, b), with the same method 
336
presented in figure 1, but considering only global frequent 
items, i.e. belonging to F-list. 
Figure 2. An example of generating CARs in a shared-nothing architecture. 
Thereafter, local CPBs are built in each processor, for each 
item in F-list structure, and communicated to corresponding 
processors using a bloc-cyclic distribution strategy. So, the 
items {b3, b2} will be assigned to processor P0; {a1, c3} to 
P1; and  {a2, b1} to P2. The process of local CARs mining 
will be executed independently in the three processors 
resulting on the classifier Cl1Cl2Cl3. For example (fig.2, c), 
in the first recursion level, processor P0 generates the CAR 
“b3 y1” for the assigned item b3. On the other hand, 
processor P2 repeats  the recursion on the built CFP-tree for 
the item a2,  until obtaining one branch. 
C. Communication cost analysis 
In the proposed algorithm, the information exchange 
allows to build the initial global F-list, and the initial CPBs.  
The  first communication phase is not very expensive because 
each processor broadcasts only support counts for all items (q 
items) to all the processors (P processors). So in this stage 
there are P!P-1"q  messages exchanged.  All this information 
exchanged is integer valued and its volume is very small. On 
the other hand, the major communication cost comes from the 
exchange of the initial CPBs across all processors. This can be 
optimized by avoiding an all-to-all strategy. Each processor 
has information on what it must send to each  other processor, 
such as the CPB for the item i  (i= 1..k) is sent to the processor 
number (i-1)%P . In this phase, each processor send 
(k/P) CPBs  to corresponding processors, so the total number 
of  messages is k. The larger of these messages depends on the 
maximum size of CPB. In the worst case, the item is present in 
all transactions of the data partition, with the largest prefix of 
size (m ? 1) (m is the number of attributes). So the maximum 
message size in this communication phase is O &(m-1)(n/P)', 
where n is the total number of records in the training data set 
D. 
V. CONCLUSION
Many distributed algorithms have been proposed for 
classification algorithms like decision trees, but so far, there 
are few  works   in associative classification. In this paper, we 
have presented a distributed model which allows the  class-
association rules discovery in  a shared-nothing architecture.  
Our solution embraces one of the fastest known sequential 
algorithms (FP-growth), and extends it to generate 
classification rules  in a parallel setting. The divide-and-
conquer nature of this latter facilitates the parallelization 
process, however, since the data set is distributed, global 
decision making becomes a difficult task. To avoid the 
replication of data in the sites, we have chosen to 
communicate the needed information. This exchange is made 
only in the first level of recursion,  allowing each machine to 
subsequently process all its assigned tasks independently. At 
the end, a global classifier is built by all discovered rules,  and 
applying a majority vote strategy. In order to evaluate this 
choices, it is imperative to carry out an experimental 
evaluation which permits us in the future to analyze several 
important costs: accuracy, scalability, speedup, memory 
usage, communication, synchronization, and also the load 
balancing. 
REFERENCES
[1] B. Liu, W. Hsu, Y. Ma, “Integrating classification and association rule 
mining,” Proceedings of the International Conference on Knowledge 
Discovery and Data Mining. New York, NY: AAAI Press, pp. 80–86,
1998. 
[2] W. Li, J. Han, J. Pei, “CMAR: Accurate and efficient classification 
based on multiple-class association rule,” Proceedings of the 
International Conference on Data Mining , San Jose, CA, pp. 369–376,
2001. 
[3] F. Thabtah, “Pruning techniques in associative classification: Survey and 
comparison,” Journal of Digital Information Management 4, pp. 202–
205, 2006. 
[4] J.R. Quinlan, “C4.5 Programs for Machine Learning,” Morgan 
Kaufmann Publishers, Inc., 1993. 
[5] J. Han, J. Pei, Y. Yin, “Mining frequent patterns without candidate 
generation,” Proceedings of the ACM SIGMOD International 
Conference on Management of Data. Dallas, TX: ACM Press, pp. 1–12,
2000. 
[6] R. Agrawal, R. Srikant “Fast algorithms for mining association rule,”
Proceedings of the 20th International Conference on Very Large Data 
Bases, Morgan Kaufmann, Santiago, Chile, pp. 487–499, 1994. 
[7] K.M. Yu, J. Zhou, W.C. Hsiao, “Load balancing approach parallel 
algorithm for frequent pattern mining,”  PaCT , pp. 623-631, 2007. 
[8] R. Agrawal, J. Sharfer, “Parallel Mining of Association Rules,” IEEE 
Transaction on Knowledge and Data Engineering, 8(6), pp. 962–969, 
1996. 
[9] G. Buehrer, S. Parthasarathy, S. Tatikonda, T. Kurc, J. Saltz, “Toward 
terabyte pattern mining: An architecture-conscious solution,”
Proceedings of the 12th ACM SIGPLAN Symposium on Principles and 
Practice of Parallel Programming, pp. 2–12, 2007. 
[10] H.D.K. Moonesinghe, C. Moon-Jung, T. Pang-Ning, ”Fast Parallel 
Mining of Frequent Itemsets,” Technical Report MSU-CSE-06-29. Dept. 
of Computer Science and Engineering, Michigan State University, 2006. 
[11] G. Thakur, C.J. Ramesh, “A Framework For Fast Classification 
Algorithms,” International Journal Information Theories & Applications 
V..15, pp. 363-369, 2008. 
[12] N. Chawla, S. Eschrich, L.O. Hall, “Creating Ensembles of Classifiers,”
IEEE International Conference on Data Mining,  pp. 580-581, 2001. 
337
