                                 978-1-4244-5934-6/10/$26.00 ©2010 IEEE                                 1474
2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2010)                       
 
 
 Application of Cluster in Quantitative Association 
Rules Mining 
Lanfang Lou   Jianhai Hou   Qingxian Pan  Lihong Wang  
School of Computer Science and Technology, Yantai University, Yantai , China 
E-mail: loulanfang@126.com 
Abstract— On mining quantitative association rules and the 
segmentation of numerical attributes variables of ordered data, 
Fisher cluster method can be used to determine the segmentation 
range and the segmentation number of this variable value. The 
method takes the data interval and data density into account. 
Therefore it is of great significance to data pre-processing.    
Keywords-quantitative association rules; cluster; numerical 
attribute 
I. INTRODUCTION 
The mining of association rules is an important research 
topic in the data mining field, which is initiated by Agrawal in 
1993. It is often used to mine the tacit rules and associations 
between data, which can help the decision-maker to make 
correct and sensible decision. 
Quantitative association rules refer to categorical 
attributes and numeric attributes, for example: scores and 
prices of goods belong to numeric attributes, sex and 
birthplace belong to categorical attributes. The discretization 
of numeric attributes is the key to quantitative association 
rules mining, and is determinative to quality of quantitative 
association rules mining. In this essay, cluster method is used 
to realize data partitioning. When ordered data are 
partitioned, the relative distances between data into account is 
very important. 
II.  QUESTION DESCRIPTION  
A. Quantitative association rules 
Suppose I={i1,i2,…,im} is a data itemsets, and P is a 
positive integers set. Iv represents set of I×P, and?x,v?Iv 
means that the value of attribute X is V. IR stands for set of 
{ >< ulx ,, I×P×P| l?u. If x is variable of numeric attributes, 
ul =  . If x is variable of categorical attribute. To the triad 
of >< ulx ,, I, we can use either the value among interval [l,u] 
to indicate numeric attribute x or the value l be to indicate 
categorical attribute x. This triad is known as an item. To 
every RIX ? , set of {x| >< ulx ,, X} is abbreviated as 
Attributes(X). 
Suppose D is a records set, every record R is a set of 
attributes values, namely VIR ? . Suppose every attribute only 
appears once in a record, namely attribute is single valued, not 
multi-valued. If RqxXulx >?<?>?<? ,(,,  makes )uql ?? , 
record R is considered to include RIX ? . 
Quantitative association rules can be abbreviated as the 
implications of YX ? , RR IYIX ?? , and Attributes(X) 
?Attributes(Y) = ?. If there are S records which contain YX ?  
in the records set D, then the support of rule YX ?  is S. If in 
the records set D, c×100% which contains X record also has Y, 
then the confidence level of rule YX ?  is C. If a record D is 
given, the task of quantitative association rules mining is to 
find all quantitative association rules, and the supports and 
confidence levels of these rules are higher than or equal to the 
least supports and the least confidence level that the users 
defined. 
B. Correlative works 
    In document [2], the question of quantitative association 
rules mining is initiated, and partial completeness is used to 
decide whether or not a numeric attribute is divided, and how 
to divide, and how much intervals which is appropriate. This 
can assure the intervals neither too big nor too small. At the 
same time, the least supports that the user defined is used to 
decide the extension of combination consecutive 
values/intervals. If the supports of intervals combination are 
more than the least supports the user defined, then the intervals 
combination stopped. Although partial completeness method is 
designed to aim at numerical data, this method only considers 
the general characteristic of numerical attribute of data, and 
neglects the measure of relative distance, the density of 
intervals, and distance of intervals between data. What’s more, 
in the case of data deflection, the partition effect is not perfect. 
In document [2], ordered data?1, 2, 3?is equal to order data
?1, 20,300?in semantic. In document [3], data mining can be 
                                                                                                                                          1475
 
 
defined as numerical rule like yx qxqx =?= , the predecessor 
and successor is a signal pair of <attribute, value>, and the 
predecessor of rule can be extended to uxl ??  which is 
similar to the method of dividing numeric attribute variable 
into interval. In document [4], when data are divided, only the 
order of ordered data is considered and the interval of data 
value is neglected. The classification method involved in 
document [6] is only applicable to the case when the variables 
are equaled equitable and independent. But in some cases, 
given variables or samples have a certain order, and this order 
can not be broken up. It can only be divided into many sections 
according to the order. 
C. Reflection method 
If the value variation range of categorical attributes 
variables and numeric attributes variables is small, the mining 
of quantitative association rules can be mapped to the mining 
of boolean association rules.  
If the original numeric attributes correspond to multiple 
boolean attributes, then the quantities of these boolean 
attributes depends on the value domain of numeric attributes 
and their division methods. To the domain value of boolean 
of?“attribute A”, “value val”?, if “attribute A” in original 
record has “value VAL”, then its corresponding boolean 
domain value is “1”, otherwise the value is “0”. If the value 
domain of a numeric attribute variable is very great, then it’s 
necessary to divided the value into many intervals, and then 
reflect every <attribute, interval> to the boolean attribute.   
For example: 
part of attributes in table?are mapped  to table ?. 
TABLE I.   ANALYSIS TABLE OF CLASS ADVISER TESTING 
NO Positional 
title 
Professiona
teacher 
Have teaching 
duties or not  
Valid 
tickets
grade 
01 
Associate 
professor 
yes yes 98% A 
02 Lecturer no yes 100% A 
03 professor no no 98% B 
04 Lecturer yes no 20% A 
05 Lecturer no no 40% B 
06 lecturer yes no 90% B 
07 assistant no no 69% C 
… … … … …  
When values of numeric attributes are parted, there are 
two questions: ?1?the least supports question. If the value of 
a numeric attributes variable is divided into many intervals, 
supports of every interval may be small. So some rules 
corresponding to these attributes cannot be produced.?2?the 
least confidence level question. When values are divided into 
some intervals, some information is lost. Only when certain 
data item of forerunner is a single value, some rules may have 
the least confidence level. With the increase of the interval 
quantities, more information may be lost.  
TABLE II.  MAPPING TABLE OF A PART OF ATTRIBUTES VARIABLES OF 
ANALYSIS TABLE OF CLASS ADVISER TESTING 
III.  QUANTITATIVE ASSOCIATION RULE BASED ON INTERVAL 
DISTANCE 
The section uses cluster method to divide data. When 
ordered data are divided, it’s of great importance to pay 
attention to the relative distances among data. Ordered data are 
a set of data with linear sequence. Here, data division method 
in document [2] is called even depth segmentation, and data 
segmentation method that takes the relative distance among 
data and interval density into account is called interval 
segmentation  
In table ?, even depth segmentation of valid tickets is 
compared with interval distance segmentation. In fact, intervals 
containing nearest data values are more significant than 
interval containing further data values.  
 
 
 
 
N
O 
Valid 
tickets 
20%..75
% 
Valid 
tickets 
76%..
100% 
profess
ional 
teache
r   
Non- 
professi
onal 
teacher 
Have 
teaching 
duties or 
not 
Not  
Have 
teaching 
duties  
01 0 1 1 0 1 0 
02 0 1 0 1 1 0 
03 0 1 0 1 0 1 
04 1 0 1 0 0 1 
05 1 0 ? ? 0 1 
06 0 1 1 0 0 1 
07 1 0 0 1 0 1 
… … … … …   
                                                                                                                                          1476
 
 
TABLE III.  CONTRAST TABLE OF EVEN DEPTH SEGMENTATION AND 
INTERVAL SEGMENTATION OF VALID TICKETS  
Even depth segmentation Interval segmentation 
Segment 
tag 
Interval 
 
Segment 
tag 
interval 
1 [20?40%] 1 [20?40%] 
2 [69?90%] 2 [69?69%] 
3 [90?98%] 3 [90?90%] 
4 [98?100%] 4 [98?100%] 
IV. CLUSTER METHOD OF ORDERED SAMPLES: FISHER METHOD 
In the following, we will introduce simply fisher cluster 
method, and the detail is in document [5]. Suppose that there 
are N variables, and they are classified into K catalogs. If these 
variables are equal, then we can prove that all the possible 
classifications areas follow:  
R?n?k?=
ni
k
ikk
i
i
k c
?
=
? ?
0
)1(
!
1
           (1) 
When these n variables are ordered, the unique method 
classified into K catalogs is as follows:  
1
1),(
?
?
=?
k
nCKnR                     (2) 
Suppose that the ordered variable (samples)are in turn  
X1,X2 ,…,Xn, and every variable Xi(i=1, …,n)is S dimension 
column vector. Because catalog quantities of ordered variable 
will linearly increase with catalog number K, we can get the 
first best solution in all classification methods.  
Definition 1  Suppose that a certain categorization of 
variables  X1,X2,…,X  is { Xi,Xi+1,…,Xj },j?I,and its average 
value vector is : 
?
=
+?
=
j
il
lij xij
x
1
1
              ?3? 
And the diameter of class { Xi ,Xi+1, …,Xj },j?I is: 
D(I,j)=
))(( ijl
j
il
ijl xxxx ???
= . 
It describes the differences between the variables in the 
variable range{ Xi,Xi+1,…,Xj },j?i. The fewer the D(I,j) is, the 
smaller the variables differences are. On the contrary, the more 
D (I,j) is, the bigger the differences among variables are, 
namely variables are more dispersion. 
When S=1, the diameter of {Xi,Xi+1,…, Xj }can be 
expressed as: 
D (I,j)=
|| iji
j
il
xx ??
=                   ?4? 
And
?
=
+?
=
j
il
lij xij
x
1
1
. 
 
Definition 2  ? In the non-confusion case in future, variable 
iX  is abbreviated  as its suffix I.?Suppose that n variables is 
classified into K catalogs, some classification is :  
P(n,k):{i1=1,i1+1,…,i2-1}{i2,i2+1,i3-1}…{ik,ik+1,…,n}, i1=1?i2?
…? ki n? ?Then error function definition of this classification 
is: e[P(n?k)]=
)1,( 1
1
?+
=
? jk
j
j iiD
                 (5) 
Consider the quadratic sum of total dispersion: 
   S (total )=
)()'(
1
XXXX l
n
l
l ???
=  
    =
),,()',,( 1111
1
1
XXXXXXXX
jjjjjjjj
ij
j
iiiiIiiii
K
j
i
il
i ?+??+? ????
=
?
=
??+
 
=
)1,( 1
1
?+
=
? jk
j
j iiD
+
)((
1
1 j
k
j
j ii ??
=
+ XX
jj ii
?
?1, )’ (
XX
jj ii
?
?1, ) 
=e[P(n?K)]+eA[P(n?K)]                  (6) 
And 
?
=
=
n
l
lXn
X
1
1
 is called general mean value; 
eA[P(n,K)]is called quadratic sum among catalogs which 
reflects the difference among catalogs. When n,K is fixed , and 
S is a constant, e[P(n,K)] and eA[P(n,K)] vary with different 
classification. Obviously, when e[P(n,K)]is smaller, then 
eA[P(n,K)] is bigger, and the classification is more logical. So 
the best classification is a classification which makes e[P(n,K)] 
smallest. 
Theorem 1.1 the recursion formula of 
[ ]),(min
-1 1
Knpe
nii k ?<<= is:      
[ ]),(min
-1 1
Knpe
nii k?<<= =
[ ] )},()1,1(min{min
-1 1
njDKjpe
niinjk k
+??
?<<=??       ?7? 
Demonstration:  when k=2, obviously there is: 
 
[ ])2,(min
-1 1
npe
nii k ?<<= = nji ?<<= -1 1
min
{D(1,j-1)+D(j,n)} 
              = nj?<2
min
{D(1,j-1)+D(j,n)} 
             =
[ ])},()1,1(min{min
112 1
njDjPe
jinj
+?
??=?<  
                                                                                                                                          1477
 
 
   To K, there is also: 
[ ]),(min
-1 1
Knpe
nii k ?<<= =
[ ] )},()1,1(min
-1 1
niDKipe kknii k
+??
?<<=             
=
[ ] )},()1,1(min
1-1, 11
niDKipe kkiiinik kkk
+??
??<<=?=
?               
=
[ ] )},()1,1(min{min
1-1 11
niDKipe kkiiinjk kk
+??
??<<=??
?  
justified. 
According to formula?7?, confirm jk, which makes the 
following formula correct. 
[ ] )},()1,1(min
1-1 11
njDKipe kkjii kk
+??
??<<=
?  
=
[ ] )},()1,1(min{min
1-1 11
njDkjpe
jiinjK k
+??
??<<=??
?  
=
)],([min
-1 1
knPe
nii k ?<<=  
Then NO. K catalog is gotten, and GK={jk,jk+1,…,n},then  
confirm jk-1, which makes the following formula correct. 
[ ] )}1,()2,1(min 111-1 121 ?+?? ????<<= ?? kkkjii jjDKjpekk  
 =
[ ] )}1,()2,1(min{min
1-11 21
?+??
??<<=???
?
kjiijjK
jjDkjpe
kk  
=
)]1,1([min
1-1 11
??
??<<=
?
kjPe kjii kk  
 
Thirdly NO. K-1 catalog is gotten, and GK-1={jk-1,jk-1+1, 
…,jk-1}.with the similar method, NO.k-2 catalog can be gotten. 
GK-1={jk-2,jk-2+1, …,jk-2-1},…,NO.1 catalog G1= {j1=1,j1+1, 
…,j2-1}. Obviously there is the following formula 8:  
[ ]),(min)1,(
-111 1
knPejjD
niil
k
l
l
k ?<<=
+
=
=??
             ?8? 
We can draw a conclusion from the above, in order to get 
the best solution, we only need to calculate D(I,j) }1{ nji ???  
and make appropriate judgement.  
V. APPLICATION   
Table IV describes the classification of valid tickets in 
table?. If the number of classification k can be not determined 
ahead of time, we can draw the curve of min e[P(n,k)]and k. 
To calculate conveniently, valid tickets are magnified 10 times. 
The main calculating process is as follows: 
 
 
TABLE IV.  CLASSIFICATION  OF  VALID  TICKETS 
When K=1, x =(2+ 4+6.9+9+9.8+10)/6=6.95   
MIN E[P(6,1)]=
)1,( 1
1
1
?+
=
? l
L
l JJD
=15.9 
When K=2, x 1==4.3   x 2=9.6 
MIN E[P(6,2)]=6.4 
With the same method, the following can be figured out: 
MIN E[P(6,3)]=4.1, MIN E[P(6,4)]=1.2 
MIN E[P(6,5)]=0.2, MIN E[P(6,6)]=0 
0
0. 5
1
1. 52
2. 5
3
3. 5
4
4. 5
5
5. 5
6
6. 5
7
7. 5
8
1 2 3 4 5 6
 
Figure 1.  Curve of Mine[P(n,K)] and K 
According to figure?, we know the curve turns at the 
corner k=4?this means 4 is the best number of classification. 
Although when K=6, the value of MIN E[P(6,6)]is the smallest, 
data  items may not  have enough supports, thereby some 
rules associated with these attributes can not be found, which 
loses the significance of quantitative association rules.  
VI. CONCLUSION  
When mining Quantitative association rules, if values of 
numerical attributes variables is smaller, we can reflect the 
value directly in consecutive integers; if values of numerical 
attributes variables are evenly distributed, we can choose 
interval number to do even interval segment. To numerical 
attributes variables of ordered number, Fisher cluster method 
Number of classification Classification cases 
1 (2  4  6.9  9  9.8  10) 
2 (2  4  6.9)  (9  9.8  10) 
3 (2)  (4  6.9)  (9  9.8  10) 
4 (2)  (4)  (6.9)  (9  9.8  10) 
5 (2)  (4)  (6.9)  (9) (9.8  10) 
6 (2)  (4)  (6.9)  (9)  (9.8)  (10) 
                                                                                                                                          1478
 
 
can be used to determine segmentation range and segmentation 
quantities of the variable value, and reflect these intervals to 
consecutive integers; if supports of numerical attributes 
variables is smaller than the least supports the user defined, 
adjacent two intervals can be merged into an interval according 
to partial completeness method in document [2]. In a word, we 
can use corresponding algorithm to mine frequent itemsets, 
and get association rules from frequent itemsets in data mining.  
REFERENCES 
[1] Agrawal R?Srikant R. Fast algorithms for mining association rules in 
large databases. In: Proc of the 20th Int'l Conf on Very Large Databases. 
Santiago? Chile? 478-499? 1994 
[2]R.Srikant and R.Agrawal.Mining Quantitative Association Rules in Large 
Relational Tables.In ACM SIGMOD Conf. Management of Data?June 1996. 
[3]G.Piatetsky-Shapiro. Discovery?analysis?and presentation of strong 
rules. In G.Piatetsky-Shapiro and W.J.Frawley? editors? Knowledge 
Discovery in Databases?pages 229-248.AAAI/MIT Press?Menlo Park?
CA?1991.  
[4]G.Piatetsky-Shapiro. Discovery?analysis?and presentation of strong 
rules. In G.Piatetsky-Shapiro and W.J.Frawley? editors? Knowledge 
Discovery in Databases?pages 229-248.AAAI/MIT Press?Menlo Park?
CA?1991.  
[5]YaoTing Zhang.KaiTai Fang. Multi-dimensional Statistical Analysis 
Introduction. Scientific Publishing house.1997. 
[6]Tian Zhang.BIRCH:An Efficient Data Clustering Method for Very Large 
Databases.In Proc. 1997 ACM-SIGMOD Int. Conf. Mnagement of  data ?
p103-114?May 1997. 
