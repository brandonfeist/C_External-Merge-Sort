Identification and Tracking Using Laser and Vision of 
People Maneuvering in Crowded Environments 
Masafumi Hashimoto 
Faculty of Engineering and Science 
Doshisha University 
Kyotanabe, Kyoto 610-0394 Japan 
mhashimo@mail.doshisha.ac.jp 
Tomoki Konda 
Graduate School of Engineering 
Doshisha University 
Kyotanabe, Kyoto 610-0394 Japan 
Abstract-This paper presents a people tracker that uses a 
two-layered laser range sensor (LRS) and a fisheye camera. The 
LRS detects waists and knees of people, and the camera captures 
color images of torsos of people detected by the LRS. From the 
data of people's position obtained by the LRS and the color 
histogram obtained by the camera, heuristic-rule-based and 
global-nearest-neighbor-based data association can identify 
multiple people in crowded environments. The identified people 
are tracked via a model-based tracker; an interacting multiple 
model estimator is applied to track people maneuvers, such as 
walking, running, sudden starting/stopping, and sudden turning. 
Simulation and experimental results show that our people 
identification and tracking method provides better performance 
than conventional methods. 
K?words-Laser range sensor, Vision, People tracking, 
Interacting multiple model estimator, Color histogram, Data 
association 
I. INTRODUCTION 
Tracking (i.e., estimating motion) of multiple interacting 
people is an important issue in surveillance, security, service 
robotics, intelligent tr ansport system and other fields. There 
has been much interest in the use of stereo vision and laser 
range sensors (LRS) for these purposes. In this paper, we 
focus on laser-based tracking. 
Compared with vision-based tracking, laser-based tracking 
is insensitive to lighting conditions and processing the data 
collected is quicker. Thus, many laser-based tracking systems 
have been developed recently [1-10]. In sparsely populated 
environments, laser-based tracking can provide good accuracy; 
however, the tracking accuracy becomes much worse in 
cluttered and crowded env ironments, where many interactions 
and occlusions of multiple people occur. To cope with this 
problem, multiple people tracking has been achieved using 
Bayesian filters such as the Kalman filter and Particle filter 
[5-10]. Also, data association (one-to-one matching of tracked 
people and laser measurements) has been implemented for 
reliable tracking, e.g., in the joint probabilistic data association 
filter (JPDAF) and the multiple hypothesis tracker (MHT) [18, 
978-1-4244-6588-0/101$25.00 ©2010 IEEE 
Zhitao Bai 
Graduate School of Engineering 
Doshisha University 
Kyotanabe, Kyoto 610-0394 Japan 
Kazuhiko Takahashi 
Faculty of Engineering and Science 
Doshisha University 
Kyotanabe, Kyoto 610-0394 Japan 
19, 22]. However, conventional laser-based people tracking 
has inherent and obvious limitations. 
Most laser-based tracking systems use a laser that scans in 
a single plane and model tracking as a leg tracking problem, 
where people are represented by the states of two legs, either 
in a single augmented state or as a high-level track to which 
two low-level leg tracks are associated. This approach makes 
it difficult to detect several people accurately in crowded 
environments, leading to frequent track lost. A typical solution 
to this problem is to simultaneously detect different body parts 
of each person, such as legs and torsos, using multilayered 
LRS [15-17]. 
Moreover, laser-based tracking systems cannot provide 
rich information such as the color of detected objects. 
Therefore, it is difficult to obtain a set of features that uniquely 
distinguish one person from another, leading to 
misidentification in cluttered environments. To cope with this 
problem, several systems using a laser scanner and a camera 
have been proposed recently to track people [11-14]; a color 
image taken by the camera is used for face-color and torso­
color recognition to match non-confirmed feet with faces and 
torsos. 
Most of the conventional laser-based tracking systems 
using Bayesian filters are built under the assumption that 
people walk at nearly const ant speed. Therefore, when people 
move randomly and flexibly (maneuver) while performing 
actions such as walking, runn ing, sudden starting and stopping, 
and sudden turning, tracking performance deteriorates 
significantly. People typically move and act under the 
constraints of an environment, making human behavior 
strongly place dependent. With maneuvering people, a single 
model (constant velocity model) is insufficient to represent the 
motion of people. A multiple-model-based approach in which 
different models run in parallel and describe different aspects 
of the behavior of people is an effective technique to deal with 
maneuvering people; in aerospace, one typical multiple­
model-based approach to tracking maneuvering targets is the 
Interact ing Multiple Model ( IMM) estimator [20, 21]. 
3145 
This paper presents a method us ing a two-layered LRS and 
color vision for tracking people maneuvering in crowded 
env ironments. The two-layered LRS simultaneously detecting 
different body parts of people allows robust detection of 
people in crowded environments, while the use of color vision 
allows reliable identification of people. The IMM-based 
tracking also enables tracking maneuvering people. This paper 
is organized as follows: Section I I  overviews our experimental 
system; Sections I I I  and IV present our tracking method; in 
Section V, we describe a simulation and an experiment to 
evaluate the tracking method, followed by our conclusions. 
I I. SYSTEM OVERVIEW 
Figure 1 shows the overview of our experimental system, 
which consists of two LRSs (LRSH and LRSd and a fish eye 
camera. We constructed a two-layered LRS by setting the two 
LRSs at different heights; each LRS takes distance samples by 
scanning a single laser beam in a horizontal plane. LRSH 
observes around the waists of people, and LRSL observes 
around their knees. The fish eye camera was used because its 
view angle (185°) is similar to that of the LRSs (180°). 
Figure 2 shows the sequence of people detection and 
tracking. The distance samples (laser measurements) related to 
people are extracted from the laser images obtained by LRSH 
and LRSL by using the background subtraction method. The 
distance samples coming from the same object are similar, 
while those coming from different objects are significantly 
different. Thus, the distance samples are clustered by checking 
the gap between two adjacent samples. The laser 
measurements are grouped based on sample clustering to 
obtain a representative point for each group. This clustering 
analysis is performed individually for each LRS. 
A window is set around each person's torso based on the 
laser image taken by LRSH, as shown in Fig. 3. A color image 
is captured within the window with the fish eye camera to 
calculate a color (hue) histogram. People are detected and 
identified based on the distance samples obtained by the LRS 
and the color histogram obtained by the camera. The identified 
people are tracked by the method presented in the next section. 
Figure 1. Experimental system. 
Waist detection Knee detection 
................................................. ...................................... 
Fisheye Camera 
Data association 
Figure 2. Sequence of people detection and tracking. 
Figure 3. Torso area captured by the fish eye camera. 
I I I. PEOPLE TRACKING 
A. People Tracking using Interacting Multiple Model (IMM) 
Estimator 
We classify motion patterns of a person into the following 
three patterns: 
a) Constant velocity model (Model ): The person moves 
nearly straight at almost constant speed. 
b) Sudden acceleration/deceleration model (Model 2): The 
person moves nearly straight, but accelerates and decelerates 
suddenly. 
c) Sudden tum model (Model 3): The person turns quickly. 
The equation of motion for each of the three models is 
given by Eqs. (A. 1) and (A. 2) in Appendix. We represent the 
equation of motion for the n-th model, where n = 1, 2, 3, by 
the following vector form. 
(1) 
where xn = (xn ,yn ,On, vn ,onl . (xn ,yn) is the person's 
position in a global coordinate frame, and 0 n is the direction 
of motion of the person. vn and On are the linear and turning 
velocities, respectively, of the person. L1vn = (L1vn ,L10n)T is 
3146 
a disturbance, and it is assumed to be a zero-mean white noise 
sequence with variance Qn = diag(q; ,q;) . 
We set q:, and q? to be small; qJ = q? and q; > q? ; and 
q;, > q:, and q? > q? . In our experiments shown in Section V, 
we set their variances at 
QI = diag(O. l m 2 /S4 ,0.1 rad2 /S4) ,  
Q 2 = diag(l 00 m 2 /s 4,0.1 rad 2 /s 4) , and 
Q3 = diag(90 m 2 /S4 ,800 rad2 /S4) .  
The LRS measurements give the waist (leg) positions of 
the person in the global coordinate frame as z = (z x' Z y) T • 
The measurement model related to LRSH and LRSL together is 
z, = Hx; + Liz, 
where H = (1 ° ° ° OJ . ° 1 ° ° ° 
(2) 
Liz is measurement noise, and it is assumed to be a zero-mean 
white noise sequence with the variance R. In our experiments 
shown in Section V, we set the variance to be 
R = diag(O.O l m2 ,0.01 m2) .  
On the basis of Eqs. (1) and (2), we track each 
maneuvering person via the IMM estimator [20, 21]. To track 
multiple people, as shown in Fig. 4, a validation region is set 
around the predicted position of each tracked person. The 
measurement inside the validation region is considered to 
come from the tracked person, and it is applied to update the 
track within the IMM estimator. As shown in Fig. 4 (a) and (c), 
in crowded environments, multiple measurements exist inside 
a validation region; multiple tracked people also compete for 
measurements. To achieve reliable data association (matching 
of tracked people and measurements), we apply the global 
nearest neighboring (GNN) algorithm [22, 23], in which the 
distance measure can be minimized. We defme the distance 
measure based on the laser measurements (distance samples 
related to people's waists and knees) and camera 
measurements (color histograms related to people's torsos). 
Our methods for setting the validation region and distance 
measure are detailed in Section IV. 
As with person #1 in Fig. 4 (b), the track of the person 
matched with his or her waist measurement taken by LRSH is 
updated using that waist measurement. The track of person #2 
who is not matched with any waist measurement is updated 
using the knee measurements taken by LRSL• As with person 
#1 in Fig. 4(b) and (d), the person matched with both waist 
and knee measurements is updated using only the waist 
measurement. 
B. Track management 
Moving people always appear in and disappear from the 
sensing area of the LRSs. They also interact and get occluded. 
In order to cope with such conditions, we implement track 
management based on the following rules: 
a) Track initiation: As shown in FigA (a) and (d), 
measurements that are not matched with any tracked people 
are considered to come from new people or false alarms. The 
false alarms disappear soon. For this reason, we tentatively 
initiate tracking of the measurements with the IMM estimator. 
When the laser measurements are obtained for more than 0.5 
seconds, they are considered to come from new people and 
tracking is continued. If the measurements disappear within 
0.5 seconds, they are considered to be false alarms, and the 
tentative tracking is terminated. 
b) Track termination: When the tracked people leave the 
sensing area of the LRSs or when they get occluded, no 
measurements exist within their validation regions. If the 
occlusion is temporary, the measurements appear again. We 
thus predict the positions of the tracked people with the IMM 
estimator. If the measurements appear again within 1.5 
seconds, we proceed with tracking the objects. Otherwise, we 
terminate the tracking. 
IV. DATA ASSOCIATION 
A. Setting Validation Region 
In this subsection, we describe the method for setting the 
validation region. Based on the n-th motion model (where n = 
1, 2, 3) of the person, the IMM estimator predicts the position 
of the tracked person by 
Waist JtI?as:I'remel1t '\ 
i.) \ , ' 
. :, ,/ . 
Person #1 \:L.. Person #2 : 
(a) Tracked people and 
waist measurements 
(c) Tracked people and 
knee measurements 
(b) Data association with 
waist measurement 
(d) Data association with 
knee measurements 
(3) 
Figure 4. Data association of tracked people and laser measurements. The 
closed circle and open diamond symbols signifY the tracked person and the 
measurement, respectively. The dashed line in (a) and (c) shows the validation 
region. The red lines in (b) and (d) indicate the result of data association. 
3147 
3 Tmn It t"'-I h -n ? mn -m d mn ,.. w ere Xt = ?c xt , an c = 3 
m=1 ?Tmn-m ? f-lt-I 
m=1 
rn is the probability that the m-th model changes to the n-th 
model, where m = 1, 2,3, and n = 1, 2,3. xtnl and itt"'-I are the 
state estimate and posterior probability of the m-th model, 
respectively. 
As shown in Fig. 5, three circle regions with a constant 
radius r are set around the predicted positions, Al , p/ , and 
N , of the tracked person, and their union is set as the 
validation region for data association. In the simulation and 
experiment discussed in Section V, we set the radius at r = 1.0 
m for LRSH and r = 0.5 m for LRSL• 
B. Calculating Distance Measure 
In this subsection, we describe our method for calculating 
the distance measure (cost) applied to the GNN-based data 
association. 
We consider that, in a validation region, 1 people exist and 
J measurements are received, where 1 does not necessarily 
equal J. The j-th measurements taken by the LRSH and the 
camera are denoted by Zit and V it , respectively, where 
j = 1,2" " , J ;  Zit is the waist position of a tracked person 
detected by LRSH, and V it is a color histogram calculated 
from camera image related to the tracked person. The color 
histogram is represented within the HSV color space. We then 
defme the distance measure dij from the i-th tracked person to 
the j-th measurement, where i = 1,2" ,1 and j = 1,2" " , J 
[7] as: 
(4) 
where N (i, zjt) is the Mahalanobis distance when the laser 
measurement zjt is matched with the i-th tracked person. E ( i, 
Vjt) is the normalized color distance when the j-th color 
histogram V it is matched with the i-th tracked person. 
Figure 5. Validation region; the red marks signity the predicted positions of 
the tracked person. Orange dotted circles show the regions with constant 
radius set around the predicted positions. The black dashed line shows the 
validation region for purpose of data association. 
The IMM estimator predicts the n-th-model-based position 
of the i-th tracked person. We therefore obtain the following 
distance: 
(5) 
where n = 1, 2, 3. p; is the n-th-model-based predicted 
position of the i-th tracked person. Sr is the covariance of the 
prediction error (zi - pn· 
We define the Mahalanobis distance in Eq. (4) by N(i, zjt) = 
minimum (AI, A2, A3). 
We determine a reference color histogram related to the i­
th person by Vi = (Ul,112,'UY",U360)T and a color histogram 
calculated from the j-th VISIOn measurement by 
Vi = (UI,U2,U3, .. ,U360)T. The normalized color distance E(i, 
Vjt) in Eq. (4) is derived from the chi-squared similarity 
measure and defined as 
(6) 
The reference color histogram related to a person is 
generated and registered into a computer when the person is 
first visible in the sensing area. It is also discarded when the 
person disappears from the sensing area. To improve the 
accuracy of color recognition, we update the reference color 
histogram as follows: 
- -Vii =aViI_1 + (1-a)Vit (7) 
where we set a to be 0.4 in our experiments described in 
Section V. 
When the tracked person is invisible to LRSH, we cannot 
calculate the normalized color distance E(i, Vjt) . In such cases, 
instead of Eq. (4), we determine the distance measure as 
dij = N (i,Zjt) using the knee measurements taken by LRSL• 
V. SIMULATION AND EXPERIMENTAL RESULTS 
A. Simulation Results a/Tracking a Single Person 
To evaluate the performance of IMM-based tracking, we 
simulated tracking of a maneuvering person. In the simulation, 
the person is assumed to be always detected correctly. Figure 6 
(a), (b) and (c) shows the true footprint, linear velocity and 
turning velocity, respectively, of the person. We set the 
transition probability matrix to be 
[ 0.9 
T = 0.05 
0.05 
0.05 0.05: 
0.9 0.05 . 
0.05 0.9 
3148 
For comparison, we also conducted Kalman-filter-based 
and Particle-filter-based tracking; we used only the sudden 
turning model (model 3) and set the covariance 
Q3 = diag(300 m 2 /s 4,300 rad 2 /s 4) for the Kalman filter and 
Q3 = diag(250 m 2 /s 4,250 rad 2 /s 4) for the Particle filter. 
The covariance values were chosen by trial and error such that 
they provide the best tracking performance. In the Particle 
filter, the number of particles was 1000, and resampling was 
based on typical random sampling. 
We present the results of 100 Monte Carlo simulations. To 
evaluate the tracking performance, we calculated the following 
normalized ?osition error (NPE) [24] and normalized velocity 
error (NVE) : 
NPE= 
100 
? )(Xit -Xit)2 +(Yit -Yit)2] i=1 100 ? )(Xit -ZXit)2 +(Yit -zyiti] i=1 
100 L (vit -Vit) 
NVE = -:,'",,' =;- 1 ___ _ 100 
L (vit -Zvit) i=1 
1 I 2 2 where Zvit =- ... ';<ZXit -Zxit-I ) + (Zyit -ZyiH) 
T 
(8) 
(9) 
(Xi ,  Yi) is the true position of the person in the i-th simulation, 
where i = 1-100, and Vi is the true linear velocity of the person. 
(Zxi , Zyi) and Zvi are the position and linear velocity, 
respectively, of the person calculated from the sensor 
measurements. (Xi' Y i) and Vi are the filter estimates. T is 
the sampling period. 
From the defmition of NPE and NVE, values below unity 
signify good estimates; the smaller the values, the better the 
tracking performance. Figure 7 (a) and (b) shows the results of 
NPE and NVE, respectively. The sampling period (scan) was 
0.1 seconds. It is clear from these results that IMM-based 
tracking provides better performance than Kalman-filter-based 
and Particle-filter-based tracking. 
B. Experimental Results of Tracking Five People 
We conducted experiments on tracking five people in a 
narrow area (4 m x 2.2 m) shown in Fig. 8. They moved 
randomly and did maneuvers such as sudden stops, sudden 
starts, and sudden turns. They wore colored shirts: white 
(person #1), black (person #2), pink (person #3), blue (person 
#4), and purple (person #5). 
We tracked the people for 26.2 seconds (262 scans). Figure 
9 shows their estimated footprints. To evaluate the 
performance of our method, we counted the following two 
errors in our experiment: misidentification and track lost. For 
I Because we cannot calculate the tuning velocity directly from sensor 
measurements, we do not consider the normalized turning velocity error. 
comparison, we calculated the same two errors occurring when 
other methods were used. The results are shown in Table 1. It 
is clear from this table that our method provides better 
performance than the other methods. 
Scan 
7?-.-.-?-.-??-r? 
6 
2 
?Ll????72??3???5?? 6 
X[m] 
(a) Footprint 
(b) Linear velocity (c) Turning velocity 
Figure 6. Simulation condition; footprint, linear velocity, and turning velocity 
of a moving person. The LRS and camera are set at the origin of the xy 
coordinate frame in (a). 
14,------,-------,------.------,-----, 
1.2 
LJ.l 
0.. I Z 
4 
LJ.l3 
> Z2 
-IMM 
-KF 
----PF 
20 40 60 80 Scan 
(a) Normalized position error (NPE) 
o?-?-?-?-??-?-?-?-?-? 
o 20 40 60 80 Scan 
(b) Normalized velocity error (NVE) 
Figure 7. Simulation result: thick solid, thin solid, and dashed lines are the 
results by IMM-, Kalman-filter-, and Particle-filter-based tracking methods, 
respectively. 
3149 
Figure 8. Experimental condition; view by the fisheye camera. 
u, 
O'?' 'I. ?, -.?, ?? o--?U'?'??I.'?' 
X1nj 
(a) Person # I 
0.' 
??' ?.I?' ? .?'?4 ?' ?0--?O '?'??I.'?' 
X1"j 
(b) Person #2 
2' 
I2 ,. I 2 
I., 
0.' 
o?, ?.,?, ?.? , ??., o--?o.,?,??,.,?, 
X[ml 
(c) Person #3 
I2 
,. 
IJ 
O? 
0':-, ?.I.-:- , ?.,:--??.5""""O ?O?.'?' ?'''''.5 --', 
X1m] 
(e) Person #5 
,. 
I., 
0.5 
0.':-, ?., .;-, ?.;-, --';;?.' O""""O::-:.,--:-, --:,7., ....,:, X[mJ 
(d) Person #4 
Figure 9. Experimental results; footprints of people estimated using the IMM 
estimator. The LRS and camera are set at the origin of the xy coordinate frame. 
Table I. Experimental result; the number of misidentifications and track lost. 
2-layered LRSHand 2-layered LRS and camera LRS camera 
IMM Misidentification 0 3 3 Track lost 0 5 8 
Kalman Misidentification 2 0 19 
filter Track lost 5 4 
VI. CONCLUSIONS 
This paper presented a people tracking method that uses a 
two-layered LRS and color vision via a camera. The LRS 
allowed detecting people in crowded environments, while 
color vision allowed identifying individual people. IMM-based 
tracking enabled tracking maneuvering people. Results of 
simulation and experiment validated our tracking method. 
In this paper, a single tracker was set in an environment. 
Our future research will involve the use of static mUltiple 
trackers and a moving tracker. 
ApPENDIX 
The three models of a moving person are given as follows: 
• Constant velocity model (model 1) and sudden 
acceleration/deceleration model (model 2) 
+ 
1 I 2 '  2. XI_I -4(VI-IT+2dVI-IT M8t-JT sm81_1 
I I 2 '  2 YI_I +4(VI-IT+2dVI-IT M81_IT cos81_1 
81_1 
o 
1 2 (VI_IT + -dVt-JT )cos8t-J 2 
(VI_IT + ? dVI_IT2 )sin 81_1 
2 
1 . 2 
2d8t-JT 
dVt-J T 
dBI_IT 
• Sudden tum model (Model 3) 
1 2 (VI_IT + 2dVI-IT ) cos 81_1 
(VI_IT + ? dVI_IT2) sin 81_1 
+ 1 · 2 
2d81-IT 
dVI_IT 
dBI_IT 
(A. 1) 
(A.2) 
where (x, y) is the person's position in a global coordinate 
frame, and () is the direction of movement of the person. v 
and e are the linear and turning velocities, respectively, of the 
3150 
person. Liv and Lie are disturbances. r is the sampling 
period. 
REFERENCES 
[I] People Detection and Tracking, Proc. of the IEEE ICRA 2009 Workshop, 
2009. 
[2] E.Prassler, lScholz and AElfes, "Tracking Multiple Moving Objects for 
Real-Time Robot Navigation," Autonomous Robots, Vo1.8, pp.l05-116, 
2000. 
[3] C.Wang, C.Thorpe, and S.Thrun, "Online Simultaneous Localization and 
Mapping with Detection and Tracking of Moving Objects: Theory and 
Results from a Ground Vehicle in Crowded Urban Areas," Proc. of Int. 
Conf. on Robotics and Automation (ICRA 2003), CD-ROM, 2003. 
[4] K.Nakamura, H.Zhao, R.Shibasaki, K.Sakamoto, T.Ooga, and 
N.Suzukawa, 'Tracking Pedestrians by using Multiple Laser Range 
Scanners," Proc. of ISPRS Congress, Vol. 35, Part B4, pp.1260-1265, 
2004. 
[5] M. Hashimoto, S. Ogata, F. Oba, and T. Murayama, "A Laser Based 
Multi-Target Tracking for Mobile Robot," Intelligent Autonomous 
Systems 9, pp.135-144, 2006. 
[6] H. Nishimura, M. Hashimoto, Y. Matsui, and K. Takahashi, "People 
Tracking with Multiple Laser Range Sensors," Proc. of Int. Conf. on 
Mechatronics and Information Technology (ICMIT2007), Proc. of SPIE 
Vol. 6794 679410-1-6, 2007. 
[7] A.Fod, AHoward, and MJ Mataric, "A Laser-Based People Tracker," 
Proc. of 2002 IEEE Int. Conf. on Robotics & Automation (ICRA 2002), 
pp.3024-3029,2002. 
[8] D.Schulz, W.Burgard, D.Fox, and AB.Cremers.' :'P.eople Trackin.g ,:"ith Mobile Robot using Sample-based Joint ProbabIlistIc Data ASSOCiatIon 
Filters," Int. l of Robotics Research, pp.99-115, 2003. 
[9] O.Frank, lNieto, J.Guivant, and S.Scheding, "Multiple Target Tracking 
Using Sequential Monte Carlo Methods and Statistical Data Association," 
Proc. of 2003 IEEE Int. Conf. on Robotics & Automation (ICRA 2003), 
pp.2718-2723,2003. 
[IO] M.Lindstrom and l-O.Eklundh, "Detecting and Tracking Moving 
Objects from a Mobile Platform Using a Laser Range Scanner," Proc. of 
2001 IEEEIRSJ Int. Conf. on Intelligent Robots and Systems (IROS2001), 
CD-ROM, 2001. 
[II] N.Bellotto, and H. Hu, "Multisensor-Based Human Detection and 
Tracking for Mobile Service Robot," IEEE Trans. on Systems, Man, and 
Cybernetics, Vol. 39, No. I, pp.l67-181, 2009. 
[12] R. Kurazume, H. Yamada, K. Murakami, Y. Iwashita, and T. Hasegawa, 
"Target Tracking Using SIR and MCMC Particle Filters by Multiple 
Cameras and Laser Range Finders," Proc. of 2008 IEEEIRSJ Int. Conf. on 
Intelligent Robots and Systems (lROS2008), pp. 3838-3844,2008. 
[13] 1. Cui, H. Xha, H. Zhao, and R. Shibasaki, "Multi-Modal Tracking of 
People Using Laser Scanners and Video Camera," Image and Vision 
Computing, Vo1.26, No.2, pp.240-252, 2008. 
[14] X. Song, l Cui, H. Zhao, and H. Zha, "Bayesian Fusion of Laser and 
Vision for Multiple People Detection and Tracking," Proc. of SICE 
Annual Conf., 2008. 
[I5] K.Takagi, S.Ando, and M.Hashimoto, "Pedestrian Detection Usin? ?n­
vehicle Multilayer LIDAR," Proc. of 13th World Congress & ExhibitIOn 
on Intelligent Transport Systems and Services, 2006. 
[16] S. Gidel, C. Blanc, T. Chateau, P. Checchin, L. Trassoudaine, :'A ?ethod 
based on Multilayer Laserscanner to Detect and Track Pedestnans m 
Urban Environment," Proc. of IEEE Intelligent Vehicle Symp., pp. 157-
162,2009. 
[17] A Carballo, A Ohya, amd S. Yuta, "Fusion of Double Layered Multiple 
Laser Range Finders for People Detection from a Mobile Robot," Proc. of 
IEEE Int. Conf. on Multisensor Fusion and Integration for Intelligent 
Systems (MFI 2008), pp. 677-682, 2008. 
[18] Y.Bar-Shalom and I.E.Fortmann, "Tracking and Data Association," 
Academic Press,Inc., 1988. 
[19] K.R.Pattipati, R.L.Popp and I.Kirubarajan, "Survey of Assignment . Techniques for Multitarget Tracking,: Multitarget-Multisensor Trackmg: 
Applications and Advances volume III (Edited by Y.Bar-Shalom and 
W.D. Blair)," Artech House, Inc., pp.77-159, 2000. 
[20] H.A.P.Blom, and Y.Bar-Shalom, "The Interacting Multiple Model 
Algorithm for Systems with Markovian Switching Coefficient," IEEE 
Trans. on Automatic Control, Vol.33, No.8, pp.780-783, 1988. 
[21] E. Mazor, A Averbuch, Y. Bar-Shalom, and J. Dayan, "Interacting 
Multiple Model Methods in Target Tracking: A Survey," IEEE Trans. on 
Aerospace and Electronic Systems, Vol.34, No.1, pp.103-123, 1998. 
[22] P. Konstantinova, A Udvarev, and T. Semerdjiev, "A Study of a Target 
Tracking Algorithm Using Global Nearest Neighbor Approach," Proc. of 
Int. Conf. on Systems and Technologies, 2003. 
[23] H.W. Kuhn, "The Hungarian Method for the Assignment Problem," 
Naval Research Logistics Quarterly, Vol. 2, No.l-2, pp.83-98, 1955. 
[24] S. McGinnity, and G. W. Irwin, "Maneuvering Target Tracking Using a 
Multiple-Model Bootstrap Filter," Sequential Monte Carlo Methods in 
Practice (A Doucet, N. D. Freitas, and N. Gordon, Eds), New York: 
Springer-Verlag, 2001, pp. 479-497. 
3151 
