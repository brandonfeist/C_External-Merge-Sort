2010 2nd International Asia Conference on Informatics in Control, Automation and Robotics 
978-1-4244-5194-4/10/$26.00 ©2010 IEEE                                                                  CAR 2010 
Research on Algorithm of Association Rules in 
Distributed Database System
Lijuan Zhou
Information Engineering College
Capital Normal University
Beijing, China
Zlj87@tom.com
Shuang Li
Information Engineering College
Capital Normal University
Beijing, China
lishuang924@163.com
Mingsheng Xu
Information Engineering College
Capital Normal University
Beijing, China
xmsh1987@gmail.com
Abstract—This dissertation proposes a new algorithm of
distributed mining association rules using the improved
Apriori algorithm, based on analyses and introduction of the 
basic concepts and algorithms of mining association rules and
mining association rules in distributed databases. Using
improved Apriori algorithm to directly produce all of local
frequent itemset in each crunode, rather than iteratively 
selecting candidate itemset. Then gather all of local
multifarious itemset to broadcast to the general node,
producing the global frequent itemset of association rules. In 
the process, the data is no longer saved with the affair ID as 
the key word. We take the item ID as the new key word. The 
performance of the improved Apriori algorithm has been 
improved through cutting down the store space. While the 
general node gathers all of local frequent itemset to select the 
global frequent itemset, it needs only a broadcast probably, 
needing three broadcasts worst. This raised the efficiency of
the new algorithm of Association Rules in Distributed 
Database System.
Index Terms—association rules, distributed database, data 
mining 
I. INTRODUCTION
With the widely application of the Internet/Intranet and 
the sophisticated of distributed technologies, database and 
data warehouse has been applied in the distributed 
environment of amounts of data. As the data are located in 
different places, an efficient distributed algorithm is needed 
to mining the large number of distributed data. The 
proprietary mining algorithm is corresponding to the 
characteristics of association rules mining in distributed 
database system. At present, the researchers of data mining 
have done some research on this. There are many distributed 
mining algorithm of association rules and the improved ones 
which has mined the valuable association rules from the 
distributed database system. But the traffic of generating 
candidate set is too heavy directly affecting the efficiency of 
the algorithm especially faced enormous databases or sparse 
data. In this paper, based on the analysis the theory of 
association rules and the apriori algorithm we proposed an 
improved apriori algorithm to create local frequent itemset
rapidly, and then generate the global frequent itemsets only 
through a communication.
II. ASSOCIATION RULES AND IMPROVED 
ALGORITHM
A. Association Rules Concept
Association Rule provides a kind of very simple but 
worthy description form for the rule mode in data mining. 
Association Rule is one of the most popular methods 
descript partial mode of data mining. It simply presents the 
rate of some particular affairs in database taking place 
together, particularly is applicable to a sparse data sets.
x Define 1Item: It is a field of the transaction database. 
We usually use small letter i
[1]
m
x Define 2 Transaction: It is corresponding to a record 
of the database. Transaction usually is marked as
small letter t
to mark item.
i. ti={i1,i2,…,ip}. Each transaction has 
an only identifier called TID. The whole set of 
transaction ti constitutes a database D. D= 
{t1,t2,…,tn
x Define 3 Itemset: It is the sets of whole item in a 
transaction database. And it is marked as capital I. 
Any sub sets x of I is itemset in D. I={i
}.
1,i2,…,im
x Define 4 the Dimension of itemset: The count of 
items in an itemset is called the dimension of the 
itemset. The itemset with dimension K is marked as 
K-Itemset.
}.
x Define 5 Association Rule: The association rule is a 
contain model looking like X o Y. There into, 
X ? I,Y ? I,and X?< z ? . Its meaning is the 
emergence of X causing the emergence of Y.X?Y 
separately is the premise and the conclusion of the 
association rule.
x Define 6 Support: The support of association rule 
XoY in transaction database is a ratio. The ratio is 
between the count of itemset which contains X and 
Y, and the count of all of itemset. That marks 
support(XoY). That is the percent of the itemset 
containing X and Y at the same time in the 
transaction database. If item t=XUY, then the 
support of item t support (t) =support(XoY).The 
more high the support of association rule is, the 
more high the frequency of association rule’s and 
itemset’ emergence is. The minimum support 
appointed by the customer according to needing is 
marked as MinSup.

2010 2nd International Asia Conference on Informatics in Control, Automation and Robotics 
978-1-4244-5194-4/10/$26.00 ©2010 IEEE                                                                  CAR 2010 
Research on Algorithm of Association Rules in 
Distributed Database System
Lijuan Zhou
Information Engineering College
Capital Normal University
Beijing, China
Zlj87@tom.com
Shuang Li
Information Engineering College
Capital Normal University
Beijing, China
lishuang924@163.com
Mingsheng Xu
Information Engineering College
Capital Normal University
Beijing, China
xmsh1987@gmail.com
Abstract—This dissertation proposes a new algorithm of
distributed mining association rules using the improved
Apriori algorithm, based on analyses and introduction of the 
basic concepts and algorithms of mining association rules and
mining association rules in distributed databases. Using
improved Apriori algorithm to directly produce all of local
frequent itemset in each crunode, rather than iteratively 
selecting candidate itemset. Then gather all of local
multifarious itemset to broadcast to the general node,
producing the global frequent itemset of association rules. In 
the process, the data is no longer saved with the affair ID as 
the key word. We take the item ID as the new key word. The 
performance of the improved Apriori algorithm has been 
improved through cutting down the store space. While the 
general node gathers all of local frequent itemset to select the 
global frequent itemset, it needs only a broadcast probably, 
needing three broadcasts worst. This raised the efficiency of
the new algorithm of Association Rules in Distributed 
Database System.
Index Terms—association rules, distributed database, data 
mining 
I. INTRODUCTION
With the widely application of the Internet/Intranet and 
the sophisticated of distributed technologies, database and 
data warehouse has been applied in the distributed 
environment of amounts of data. As the data are located in 
different places, an efficient distributed algorithm is needed 
to mining the large number of distributed data. The 
proprietary mining algorithm is corresponding to the 
characteristics of association rules mining in distributed 
database system. At present, the researchers of data mining 
have done some research on this. There are many distributed 
mining algorithm of association rules and the improved ones 
which has mined the valuable association rules from the 
distributed database system. But the traffic of generating 
candidate set is too heavy directly affecting the efficiency of 
the algorithm especially faced enormous databases or sparse 
data. In this paper, based on the analysis the theory of 
association rules and the apriori algorithm we proposed an 
improved apriori algorithm to create local frequent itemset
rapidly, and then generate the global frequent itemsets only 
through a communication.
II. ASSOCIATION RULES AND IMPROVED 
ALGORITHM
A. Association Rules Concept
Association Rule provides a kind of very simple but 
worthy description form for the rule mode in data mining. 
Association Rule is one of the most popular methods 
descript partial mode of data mining. It simply presents the 
rate of some particular affairs in database taking place 
together, particularly is applicable to a sparse data sets.
x Define 1Item: It is a field of the transaction database. 
We usually use small letter i
[1]
m
x Define 2 Transaction: It is corresponding to a record 
of the database. Transaction usually is marked as
small letter t
to mark item.
i. ti={i1,i2,…,ip}. Each transaction has 
an only identifier called TID. The whole set of 
transaction ti constitutes a database D. D= 
{t1,t2,…,tn
x Define 3 Itemset: It is the sets of whole item in a 
transaction database. And it is marked as capital I. 
Any sub sets x of I is itemset in D. I={i
}.
1,i2,…,im
x Define 4 the Dimension of itemset: The count of 
items in an itemset is called the dimension of the 
itemset. The itemset with dimension K is marked as 
K-Itemset.
}.
x Define 5 Association Rule: The association rule is a 
contain model looking like X o Y. There into, 
X ? I,Y ? I,and X?< z ? . Its meaning is the 
emergence of X causing the emergence of Y.X?Y 
separately is the premise and the conclusion of the 
association rule.
x Define 6 Support: The support of association rule 
XoY in transaction database is a ratio. The ratio is 
between the count of itemset which contains X and 
Y, and the count of all of itemset. That marks 
support(XoY). That is the percent of the itemset 
containing X and Y at the same time in the 
transaction database. If item t=XUY, then the 
support of item t support (t) =support(XoY).The 
more high the support of association rule is, the 
more high the frequency of association rule’s and 
itemset’ emergence is. The minimum support 
appointed by the customer according to needing is 
marked as MinSup.

|{ | , } |( ) ( )
| |
ti X Y ti ti Dsupport X Y p X Y
D
? ? ?o   ?
x Define 7 Confidence: It is the ratio between the 
count of transaction containing X and Y and the 
count of transaction containing X. That is marked as 
confidence(XoY). Confidence is the percent of the 
transaction sets containing X and Y at the same time 
in the transaction database. This is a conditional 
probability. The higher the confidence of association 
rule is, the more dependable the rule is. The 
minimum confidence appointed by the customer 
usually is marked as
MinConf.
|{ | , } | ( )( ) ( | )
|{ | , } | ( )
ti X Y ti ti D support X Yconfidence X Y p Y X
tj X tj tj D support X
? ? ? ?o    
? ?
x Define 8 Frequent Itemset: That means the itemset, 
whose support is not lower than the minimum 
support (MinSup).
x Define 9 Strong rule and Weak rule: If 
support(XoY)?0LQ6XS DQG FRQILGHQFH;oY) 
?0LQ&RQIWKHQPDUNWKHDVVRFLDWLRQUXOH;oY as 
a strong rule, otherwise mark it as a weak rule.
B. Apriori Algorithm and Related Algorithm Analyse
In 1994 Agrawal etc. put forward famous Apriori 
algorithm according to the property of association rule: the 
sub sets of the frequent itemset is also frequent itemset, the 
supersets of non-frequent itemset is also non-frequent 
itemset. The algorithm each time makes use of k-frequent 
itemset carrying on conjunction to get k+1 candidate itemset. 
Then get k+1 frequent itemset through cutting. So keep on, 
until there is not frequent itemset. The Apriori algorithm has 
to firstly produce frequent itemset with small count. Next 
through iteration produces frequent itemset with great count. 
So when Apriori algorithm produces each length frequent 
itemset, it has to scan the database for a time. In addition, it 
must produce candidate itemset at first. Thus the time of 
scanning the database is up to the count of the largest 
frequent itemset.
C. Basic Thought and Process of Improved Algorithm
Literature [3] introduces two pieces of inference as 
follows:
Inference 1: If the count of item in K-frequent itemset 
LK is smaller than or equal to K, then the itemset is the sets 
of the largest frequent itemset.
Inference 2: The count of item in the largest frequent 
itemset LK is smaller than or equal to the largest count K of 
items, which satisfies support in all of transactions.
The New_Apriori algorithm makes use of 
above-mentioned inference, at first making sure the count K 
of item of the largest frequent itemset. It directly gets the 
largest frequent itemset LK
As follows introduce the New_Apriori algorithm with an 
example:
among these transactions, whose 
count of item is larger than or equal to K. Then consider 
these transactions one by one in order, whose count of item 
is k-1 and k-2.Dine the frequent itemset, which does not 
belong to the largest frequent itemset with the largest count 
of item.
Table 1 is an original affair database. Each affair T 
represents a student's record; the I1-I7
Scan the database to take down C[n], C[1]=1, C[2]=5, 
C[3]=8, C[4]=4, C[5]=2;and the Flag1mark of each
transaction. At the same time, change data storage structure, 
taking item as the key word:
means different 
attributes respectively. Minimum support min_support =4.
TABLE I. ORIGINAL AFFAIR DATABASE
Affair Item Flag1
T I1 1,I 27
T2 I        2,I3,I 34
T I3 4,I5,I 37
T I4 2,I4,I5 3
T I5 5,I6,I 37
T I6 2,I 24
T I7 3,I5,I 37
T I8 2,I4,I5,I 46
T I9 5,I 27
T I10 2,I4,I 35
T I11 4,I 27
T I12 1,I2,I3,I4,I 55
T I13 2,I4,I5,I6,I 57
T I14 2,I3,I 34
T I15 4,I5,I 37
T I16 3,I 24
T I17 1,I3,I5,I7 4
T I18 2,I4,I5,I 46
T I19 15
T I20 2,I4,I5,I 46
Because C [5] <min_support, C [4]>min_support, the
largest count k of item of the largest frequent assumed before 
is 4. Now from the tansformed data storage select the item 
with Flag2?PLQBVXSSRUW7KHQLQLWVWUDQVDFWLRQVHWVZHORRN
into the transaction whether its Flag1i is larger than k. If 
such transaction’s count satisfies the minimum support, take 
down temporarily. Then get the lately small database.
TABLE II. DATA STORAGE TAKING ITEM AS KEY WORD
Item Affair Flag2
I T1 1,T12,T 317
I2 T      2,T4,T6,T8,T10,T12,T13,T14,T18,T 1020
I T3 2,T7,T12,T14,T16,T 617
I T4 2
,T3,T4,T6,T8,T10,T11,T12,T13,T14,T15,T16,T1
8,T20
14
I T5 3
,T4,T5,T7,T8,T9,T10,T12,T13,T15,T17,T18,T19
,T 1420
I T6 5,T8,T13,T18,T 520
I T7 1,T3,T5,T7,T9,T11,T13,T15,T 917

|{ | , } |( ) ( )
| |
ti X Y ti ti Dsupport X Y p X Y
D
? ? ?o   ?
x Define 7 Confidence: It is the ratio between the 
count of transaction containing X and Y and the 
count of transaction containing X. That is marked as 
confidence(XoY). Confidence is the percent of the 
transaction sets containing X and Y at the same time 
in the transaction database. This is a conditional 
probability. The higher the confidence of association 
rule is, the more dependable the rule is. The 
minimum confidence appointed by the customer 
usually is marked as
MinConf.
|{ | , } | ( )( ) ( | )
|{ | , } | ( )
ti X Y ti ti D support X Yconfidence X Y p Y X
tj X tj tj D support X
? ? ? ?o    
? ?
x Define 8 Frequent Itemset: That means the itemset, 
whose support is not lower than the minimum 
support (MinSup).
x Define 9 Strong rule and Weak rule: If 
support(XoY)?0LQ6XS DQG FRQILGHQFH;oY) 
?0LQ&RQIWKHQPDUNWKHDVVRFLDWLRQUXOH;oY as 
a strong rule, otherwise mark it as a weak rule.
B. Apriori Algorithm and Related Algorithm Analyse
In 1994 Agrawal etc. put forward famous Apriori 
algorithm according to the property of association rule: the 
sub sets of the frequent itemset is also frequent itemset, the 
supersets of non-frequent itemset is also non-frequent 
itemset. The algorithm each time makes use of k-frequent 
itemset carrying on conjunction to get k+1 candidate itemset. 
Then get k+1 frequent itemset through cutting. So keep on, 
until there is not frequent itemset. The Apriori algorithm has 
to firstly produce frequent itemset with small count. Next 
through iteration produces frequent itemset with great count. 
So when Apriori algorithm produces each length frequent 
itemset, it has to scan the database for a time. In addition, it 
must produce candidate itemset at first. Thus the time of 
scanning the database is up to the count of the largest 
frequent itemset.
C. Basic Thought and Process of Improved Algorithm
Literature [3] introduces two pieces of inference as 
follows:
Inference 1: If the count of item in K-frequent itemset 
LK is smaller than or equal to K, then the itemset is the sets 
of the largest frequent itemset.
Inference 2: The count of item in the largest frequent 
itemset LK is smaller than or equal to the largest count K of 
items, which satisfies support in all of transactions.
The New_Apriori algorithm makes use of 
above-mentioned inference, at first making sure the count K 
of item of the largest frequent itemset. It directly gets the 
largest frequent itemset LK
As follows introduce the New_Apriori algorithm with an 
example:
among these transactions, whose 
count of item is larger than or equal to K. Then consider 
these transactions one by one in order, whose count of item 
is k-1 and k-2.Dine the frequent itemset, which does not 
belong to the largest frequent itemset with the largest count 
of item.
Table 1 is an original affair database. Each affair T 
represents a student's record; the I1-I7
Scan the database to take down C[n], C[1]=1, C[2]=5, 
C[3]=8, C[4]=4, C[5]=2;and the Flag1mark of each
transaction. At the same time, change data storage structure, 
taking item as the key word:
means different 
attributes respectively. Minimum support min_support =4.
TABLE I. ORIGINAL AFFAIR DATABASE
Affair Item Flag1
T I1 1,I 27
T2 I        2,I3,I 34
T I3 4,I5,I 37
T I4 2,I4,I5 3
T I5 5,I6,I 37
T I6 2,I 24
T I7 3,I5,I 37
T I8 2,I4,I5,I 46
T I9 5,I 27
T I10 2,I4,I 35
T I11 4,I 27
T I12 1,I2,I3,I4,I 55
T I13 2,I4,I5,I6,I 57
T I14 2,I3,I 34
T I15 4,I5,I 37
T I16 3,I 24
T I17 1,I3,I5,I7 4
T I18 2,I4,I5,I 46
T I19 15
T I20 2,I4,I5,I 46
Because C [5] <min_support, C [4]>min_support, the
largest count k of item of the largest frequent assumed before 
is 4. Now from the tansformed data storage select the item 
with Flag2?PLQBVXSSRUW7KHQLQLWVWUDQVDFWLRQVHWVZHORRN
into the transaction whether its Flag1i is larger than k. If 
such transaction’s count satisfies the minimum support, take 
down temporarily. Then get the lately small database.
TABLE II. DATA STORAGE TAKING ITEM AS KEY WORD
Item Affair Flag2
I T1 1,T12,T 317
I2 T      2,T4,T6,T8,T10,T12,T13,T14,T18,T 1020
I T3 2,T7,T12,T14,T16,T 617
I T4 2
,T3,T4,T6,T8,T10,T11,T12,T13,T14,T15,T16,T1
8,T20
14
I T5 3
,T4,T5,T7,T8,T9,T10,T12,T13,T15,T17,T18,T19
,T 1420
I T6 5,T8,T13,T18,T 520
I T7 1,T3,T5,T7,T9,T11,T13,T15,T 917

Now try to get U2 ?84 ?85 ?86. If the count of item 
in the intersection is larger than or equal to min_support, 
then {I2, I4, I5, I6} is the largest frequent itemset according 
to the inference in literature[3]. We get the intersection {T8, 
T13, T18, T20}, and the count of item is 4. This satisfies the 
minimum support min_support. Thus it can be seen, {I2, I4, 
I5, I6} is the largest frequent itemset. The record is< I2, I4, I5, 
I6, 4>. In the record, the number 4 expresses the amount of 
elements of the intersection. In other words, there are 4 
records including I2, I4, I5, I6
TABLE III. THE LATELY SMALL DATABASE
at the same time.
Item Affair
I2 T      8,T12,T13,T18,T20       (U2)
I T4 8,T12,T13,T18,T20       (U4)
I T5 8,T12,T13,T17,T18,T20    (U5)
I T6 8,T13,T18,T20       (U6)
Owing to there are just I2, I4, I5, I6four items in this 
experiment, it is equal to the largest count k of item of the 
largest frequent assumed. So in the process of solving, when 
we try to get U2?84=Ua, if the count of Ua’s item is smaller 
than 4, |Ua
According to the specialty of Apriori algorithm: All of 
the sun sets of frequent itemset are frequent itemset, we can 
from the largest frequent itemset{ I
|<4, there is no need continue to get the 
intersection. Because the count doesn’t satisfy the minimum 
support, at this time the count of considering item is smaller 
than k. Then there isn’t the largest frequent itemset. If we 
don’t find 4-frequent itemset, then the largest count k of item 
of the largest frequent assumed decrease 1.Get the largest 
frequent itemset with the method above-mentioned.
2, I4, I5, I6} get part of 
2-frequent itemset{I2, I4},{I2, I5},{I2, 
I6},{I4,I5},{I4,I6},{I5,I6} and part of 3-frequent itemset { I2, 
I4, I5}etc. We can get the intersection in the same: record < 
I2, I4, 5>, < I2, I5, 5>, < I2, I6, 4>, < I4,I5
And then continue to try to get the largest frequent 
itemset to database. We just need to take the k as 2 or 3. In 
this experiment , at last we find all of the largest frequent 
itemset{ I
, 5>etc.
2, I4, I5, I6,{I3,I4},{I4,I7},{I5,I7}. And {I2, I4, I5, 
I6
III. BASIC THOUGHT AND PROCESS OF 
DISTRIBUTION DATA MINING
} is the largest frequent itemset with the largest count of 
item.
Distributed data mining algorithms can be divided into 
two categories, data distribution algorithm (DD 
algorithm).and the count distributed algorithm (CD 
algorithm).
The basic idea of DD algorithm is to designate the 
candidate set evenly distributed among the nodes. This can 
reduces the number of candidate set to be scanned when the 
nodes increases. The problem is that the traffic between the 
nodes can reduce the efficiency of the algorithm.
CD algorithm is a typical parallel algorithm based on 
apriori algorithm. Each sub-database scans the database 
independently to calculate the support of each candidate set, 
and then set the sum of the supports of the entire candidate 
set as the global support. If the global support is greater than 
the mini-support, we regard the itemset as the global 
frequent itemset. CD algorithm begins the next scan through 
the next synchronization point after the end of each scan. 
With the increase of the nodes, the traffic of the algorithm 
will be increasing quickly and the efficiency of the algorithm 
will be significantly reduced.
In order to reduce the traffic between the various 
partitions, D.W.Chenung proposed a fast association rule 
mining algorithm based on distributed system FDM. The 
algorithm first run apriori algorithm to find the local frequent 
itemset in each partition, and then changed the sum degree of 
the support between the global frequent itemset. The data 
size needed to be handled was reduced by generating a 
smaller number of candidate set.
In this paper, the global frequent sets can be acquired 
between 1time and 3 times based on the improved apriori 
algorithm.
IV. DISTRIBUTED ASSOCIATION MINING BASED 
ON THE IMPROVED APRIORI ALGORITHM
In this paper, we assume that the system contains three 
nodes S1, S2 and S3. The database is divided into DB1, DB2
and DB3. The data in the global database contain 60 
transactions and each local database has 20 transactions. 
Also we assume that the mini-support degree s=20%.DB1 
acquire all the local frequent itemset < I2, I4, 5>, < I2, I5, 5>, 
< I2, I6, 4>, < I4, I5, 5> in the DB1 through the improved 
apriori algorithm. Similarly DB2 and DB3
TABLE IV. ALL OF LOCAL FREQUENT ITEMSET
respectively 
obtain their own local frequent itemset through the improved 
apriori algorithm. Then, all of local frequent itemset are 
broadcast to the node S and summarized at the node S.
X.sup1 X.sup2 X.sup3
I2,I4         I5 3,I4      I5 3,I4      6
I2,I6 I      4        2,I4      I4 3,I5      4
… I2,I6      10
I2,I4,I5,I6 4
I3,I4          4
I4,I7          4
I5,I7          7
There are 60 records in the database. The mini-support 
degree is 20%.So if the support of the itemset satisfy 
60*20%=12, the itemset is a global frequent set. The 
calculation of the support of the itemset is divided into four 
cases:
As {I3, I4}. { I3, I4
As {I
} is local frequent itemset in each 
sub-database so it must accord with the mini-support 
requirement. It must be global frequent itemset.
2, I6}. { I2, I6
As {I
} is not the local frequent itemset in 
all the sub-database, but the support of it satisfies the 
requirement of the mini-support. It is a global frequent 
itemset.
2, I4}. { I2, I4} has two local supports sup1, sup2, 
and sup1+sup2=9<12, then it doesn’t accord with the 
mini-support requirement. But 12-9<4*(3-2),and 12 
represents the global mini-support, 9 represents the current 
mini-support, 4 represents the local mini-support, 3 
represents the total count of nodes, and 2 represents the 
count of local support. { I2, I4} may be a global frequent 

Now try to get U2 ?84 ?85 ?86. If the count of item 
in the intersection is larger than or equal to min_support, 
then {I2, I4, I5, I6} is the largest frequent itemset according 
to the inference in literature[3]. We get the intersection {T8, 
T13, T18, T20}, and the count of item is 4. This satisfies the 
minimum support min_support. Thus it can be seen, {I2, I4, 
I5, I6} is the largest frequent itemset. The record is< I2, I4, I5, 
I6, 4>. In the record, the number 4 expresses the amount of 
elements of the intersection. In other words, there are 4 
records including I2, I4, I5, I6
TABLE III. THE LATELY SMALL DATABASE
at the same time.
Item Affair
I2 T      8,T12,T13,T18,T20       (U2)
I T4 8,T12,T13,T18,T20       (U4)
I T5 8,T12,T13,T17,T18,T20    (U5)
I T6 8,T13,T18,T20       (U6)
Owing to there are just I2, I4, I5, I6four items in this 
experiment, it is equal to the largest count k of item of the 
largest frequent assumed. So in the process of solving, when 
we try to get U2?84=Ua, if the count of Ua’s item is smaller 
than 4, |Ua
According to the specialty of Apriori algorithm: All of 
the sun sets of frequent itemset are frequent itemset, we can 
from the largest frequent itemset{ I
|<4, there is no need continue to get the 
intersection. Because the count doesn’t satisfy the minimum 
support, at this time the count of considering item is smaller 
than k. Then there isn’t the largest frequent itemset. If we 
don’t find 4-frequent itemset, then the largest count k of item 
of the largest frequent assumed decrease 1.Get the largest 
frequent itemset with the method above-mentioned.
2, I4, I5, I6} get part of 
2-frequent itemset{I2, I4},{I2, I5},{I2, 
I6},{I4,I5},{I4,I6},{I5,I6} and part of 3-frequent itemset { I2, 
I4, I5}etc. We can get the intersection in the same: record < 
I2, I4, 5>, < I2, I5, 5>, < I2, I6, 4>, < I4,I5
And then continue to try to get the largest frequent 
itemset to database. We just need to take the k as 2 or 3. In 
this experiment , at last we find all of the largest frequent 
itemset{ I
, 5>etc.
2, I4, I5, I6,{I3,I4},{I4,I7},{I5,I7}. And {I2, I4, I5, 
I6
III. BASIC THOUGHT AND PROCESS OF 
DISTRIBUTION DATA MINING
} is the largest frequent itemset with the largest count of 
item.
Distributed data mining algorithms can be divided into 
two categories, data distribution algorithm (DD 
algorithm).and the count distributed algorithm (CD 
algorithm).
The basic idea of DD algorithm is to designate the 
candidate set evenly distributed among the nodes. This can 
reduces the number of candidate set to be scanned when the 
nodes increases. The problem is that the traffic between the 
nodes can reduce the efficiency of the algorithm.
CD algorithm is a typical parallel algorithm based on 
apriori algorithm. Each sub-database scans the database 
independently to calculate the support of each candidate set, 
and then set the sum of the supports of the entire candidate 
set as the global support. If the global support is greater than 
the mini-support, we regard the itemset as the global 
frequent itemset. CD algorithm begins the next scan through 
the next synchronization point after the end of each scan. 
With the increase of the nodes, the traffic of the algorithm 
will be increasing quickly and the efficiency of the algorithm 
will be significantly reduced.
In order to reduce the traffic between the various 
partitions, D.W.Chenung proposed a fast association rule 
mining algorithm based on distributed system FDM. The 
algorithm first run apriori algorithm to find the local frequent 
itemset in each partition, and then changed the sum degree of 
the support between the global frequent itemset. The data 
size needed to be handled was reduced by generating a 
smaller number of candidate set.
In this paper, the global frequent sets can be acquired 
between 1time and 3 times based on the improved apriori 
algorithm.
IV. DISTRIBUTED ASSOCIATION MINING BASED 
ON THE IMPROVED APRIORI ALGORITHM
In this paper, we assume that the system contains three 
nodes S1, S2 and S3. The database is divided into DB1, DB2
and DB3. The data in the global database contain 60 
transactions and each local database has 20 transactions. 
Also we assume that the mini-support degree s=20%.DB1 
acquire all the local frequent itemset < I2, I4, 5>, < I2, I5, 5>, 
< I2, I6, 4>, < I4, I5, 5> in the DB1 through the improved 
apriori algorithm. Similarly DB2 and DB3
TABLE IV. ALL OF LOCAL FREQUENT ITEMSET
respectively 
obtain their own local frequent itemset through the improved 
apriori algorithm. Then, all of local frequent itemset are 
broadcast to the node S and summarized at the node S.
X.sup1 X.sup2 X.sup3
I2,I4         I5 3,I4      I5 3,I4      6
I2,I6 I      4        2,I4      I4 3,I5      4
… I2,I6      10
I2,I4,I5,I6 4
I3,I4          4
I4,I7          4
I5,I7          7
There are 60 records in the database. The mini-support 
degree is 20%.So if the support of the itemset satisfy 
60*20%=12, the itemset is a global frequent set. The 
calculation of the support of the itemset is divided into four 
cases:
As {I3, I4}. { I3, I4
As {I
} is local frequent itemset in each 
sub-database so it must accord with the mini-support 
requirement. It must be global frequent itemset.
2, I6}. { I2, I6
As {I
} is not the local frequent itemset in 
all the sub-database, but the support of it satisfies the 
requirement of the mini-support. It is a global frequent 
itemset.
2, I4}. { I2, I4} has two local supports sup1, sup2, 
and sup1+sup2=9<12, then it doesn’t accord with the 
mini-support requirement. But 12-9<4*(3-2),and 12 
represents the global mini-support, 9 represents the current 
mini-support, 4 represents the local mini-support, 3 
represents the total count of nodes, and 2 represents the 
count of local support. { I2, I4} may be a global frequent 

itemset. So the node S broadcasts { I2, I4} to S1, S2, S3. If 
the sum of all of local support accords with the global 
mini-support, { I2, I4} is a global frequent itemset. 
As {I3, I5}. { I3, I5} has one local support sup3=4. This 
doesn’t satisfy the requirement of the mini-support. At the 
same time, 12-4 =4*(3-1), { I3, I5
V. CONCLUSIONS
} can not be a global 
frequent itemset.
At last, we have mined all of the global frequent itemset. 
That is to say to mine the association rules in distributed 
database.
The traditional algorithms create the local candidate sets 
firstly, and then determine whether the local frequent 
itemsets is the global frequent itemsets by the traffic between 
the nodes. The most different between the proposed 
algorithm and the traditional algorithms is that it firstly 
generates all the local frequent itemsets at each node and 
then communicates to the top point. At the top point, there 
are four cases to deal with all the local frequent itemsets. For 
the fastest case, the determination could be made by the 
completion of round-trip communications. At the same time, 
all the operations of this algorithm are completed not by 
traditional data storage but by a new data storage in which 
the item is considered as the keyword. The method can save 
storage space, especially for sparse data. So the support can 
be calculated by the intersection of the transaction sets, 
which is much easier than by accessing to the database. 
Finally, the association rules in the distributed database are 
mined.
ACKNOWLEDGMENT
This work was supported by Beijing Educational 
Committee science and technology development plan 
project(KM200810028016), Capital Normal University 
project and Beijing Information project.
REFERENCES
[1] R Agrawal, TImielinski, A Swamj. Mining associationrules between 
sets of items in large databases[C]. In: Proceedings of zhe ACM SIG 
MOD International Conference on Management of data. Washington 
DC,1993
[2] Li Xinzheng. Improved apriori algorithm for efficiency. 
Control & Automation,2006,v3(3).
[3] Zhu Ye,Ye Gaoying. Improvement of Apriori Algorithm in 
Association Rule Mining. Modern Electronic 
Technique,2008,v31(18).
[4] Li Xiaohong, Shang Jin. An Improved New Apriori Algorithm. 
Computer Science.2007,v34(4).
[5] Tan Ran, Lu Zhengqiu, Yan Xinping. Similarity-based distributed 
data mining model research.Application Research of 
Computers.Mar.2008, v25(3).
[6] Wang Yue, Cao Changxiu. The Method Research of Mining 
Association Rules in Distributed Environments. College of 
Automation Chongqing University. Mar.2003.
[7] Wei Suyun, Ji Genlin. Research on Algorithms for Distributed Ming 
of Association Rules. School of Mathematics and Computer Science 
Nanjing Normal Univercity. April.2006.

itemset. So the node S broadcasts { I2, I4} to S1, S2, S3. If 
the sum of all of local support accords with the global 
mini-support, { I2, I4} is a global frequent itemset. 
As {I3, I5}. { I3, I5} has one local support sup3=4. This 
doesn’t satisfy the requirement of the mini-support. At the 
same time, 12-4 =4*(3-1), { I3, I5
V. CONCLUSIONS
} can not be a global 
frequent itemset.
At last, we have mined all of the global frequent itemset. 
That is to say to mine the association rules in distributed 
database.
The traditional algorithms create the local candidate sets 
firstly, and then determine whether the local frequent 
itemsets is the global frequent itemsets by the traffic between 
the nodes. The most different between the proposed 
algorithm and the traditional algorithms is that it firstly 
generates all the local frequent itemsets at each node and 
then communicates to the top point. At the top point, there 
are four cases to deal with all the local frequent itemsets. For 
the fastest case, the determination could be made by the 
completion of round-trip communications. At the same time, 
all the operations of this algorithm are completed not by 
traditional data storage but by a new data storage in which 
the item is considered as the keyword. The method can save 
storage space, especially for sparse data. So the support can 
be calculated by the intersection of the transaction sets, 
which is much easier than by accessing to the database. 
Finally, the association rules in the distributed database are 
mined.
ACKNOWLEDGMENT
This work was supported by Beijing Educational 
Committee science and technology development plan 
project(KM200810028016), Capital Normal University 
project and Beijing Information project.
REFERENCES
[1] R Agrawal, TImielinski, A Swamj. Mining associationrules between 
sets of items in large databases[C]. In: Proceedings of zhe ACM SIG 
MOD International Conference on Management of data. Washington 
DC,1993
[2] Li Xinzheng. Improved apriori algorithm for efficiency. 
Control & Automation,2006,v3(3).
[3] Zhu Ye,Ye Gaoying. Improvement of Apriori Algorithm in 
Association Rule Mining. Modern Electronic 
Technique,2008,v31(18).
[4] Li Xiaohong, Shang Jin. An Improved New Apriori Algorithm. 
Computer Science.2007,v34(4).
[5] Tan Ran, Lu Zhengqiu, Yan Xinping. Similarity-based distributed 
data mining model research.Application Research of 
Computers.Mar.2008, v25(3).
[6] Wang Yue, Cao Changxiu. The Method Research of Mining 
Association Rules in Distributed Environments. College of 
Automation Chongqing University. Mar.2003.
[7] Wei Suyun, Ji Genlin. Research on Algorithms for Distributed Ming 
of Association Rules. School of Mathematics and Computer Science 
Nanjing Normal Univercity. April.2006.

