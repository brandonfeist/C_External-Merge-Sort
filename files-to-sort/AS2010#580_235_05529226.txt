201O 2nd International Conforence on Education Technology and Computer (ICETC) 
Intelli-Log : A Real-time Log Analyzer 
Nathaphon Kiatwonghong \ Songrit Maneewongvatana 2 
Computer Engineering 
King Mongkut's University of Technology Thonburi 
Bangkok, Thailand 
1 s0450002@st.kmutt.ac.th 
2 songrit@cpe.kmutt.ac.th 
Abstract- In this paper, we present a technique to analyze and 
correlate the different types of computer log files. Log files are 
generated from servers and network devices to record 
operations that occur in the computers and networks. As log 
files are too enormous to manualize, we develop a tool to 
maximize accuracy as well as efficiency while high speed 
processing is the goal. Firstly, we must improve the accuracy 
by using learning algorithms to classify the normal operations 
from the abnormal ones such algorithms include tf-idf, 
association rules, k-means clustering, and decision tree. 
Secondly, we may adapt for less accuracy in order to gain 
speed for both with and/or without parallel processing 
techniques. We also construct an adaptive learning algorithm 
to update the model. Then we flush out out-of-date model while 
the logs are being captured and processed. The result can 
achieve the goal as they can reach about 30-404Yo in real-time 
processing with nearly zero false positive results. 
Keywords-component; Data Mining; Data Stream; Learning 
Algorithm; Computer Log Analysis 
I. INTRODUCTION 
Presently, computer network is the global network. Other 
than generally uses, there are numerous computer attackers 
trying to scan ports and finding weak spots to attack the 
computers. Therefore, most systems will have event 
recorders called Log which records the events regularly 
occur or sequency. The example of purposes of log such as 
many programmers can fmd out bugs, and many 
administrators can audit the invalid operations etc. Logs can 
be kept in either human readable format or machine language. 
To achieve highest utilization, we must convert them into 
one common format, analyze them and then come to the 
conclusion. 
Most commercial intrusion analyzer products are 
currently just signature-based detection. The advantages of 
this method are high accuracy and high speed. However, it 
does not work well to handle attack patterns that do not 
match patterns or zero day attacks. 
We adapt the leaming algorithms and join to the 
signature-based technique which we gain the satisfactory 
result. We needed the result that had small number of false 
positives and known zero days attacks. 
A. Background 
II. THEORETICAL ISSUE 
Recently, many analyzers are still based on the use of 
signature-based detection that uses local database to keep 
track the anomaly patterns and reduce false alarm like 
SNORT [1]. Some analyzers use behavior-based technique to 
detect the new attack but it leads to high false alarm rate [2]. 
Those who applied leaming algorithm technique to achieve 
higher accuracy, however most of them focus on network 
traffic detector [3, 4, and 5]. On the other hand, leaming 
algorithm technique requires too much time and has many 
parameters. 
B. Related Works 
Paul D. et al. [2], introduced new leaming algorithm for 
network intrusion detection. Their new algorithm builds rare 
class prediction models for identifying known intrusion and 
their variations. The algorithm also created anomaly 
detection schemes for detecting novel attack whose nature is 
unknown. They use 2 algorithms to work together. First, the 
PN rule which is a two stage leaming algorithm which based 
on computing the rules. The first stage tried to discover P­
rules that cover most of the intrusion example, then the 
second stage discover N-rules which eliminate false alarms 
generated in the fIrst P step. Next, CREDOS which is a novel 
algorithm that use ripple down rule. It uses over fItting 
technique with the training data then prunes them to improve 
generalization capability. The results showed that their 
algorithm can discover more new intrusions than standard 
leaming algorithm but there are too many false positives. As 
a result, the owners suggest using of this system in 
conjunction with rule-based intrusion detection like SNORT 
for better result in real environment. 
Pingchuan M. [8], used many leaming algorithms to 
incorporate with log analyzer. The algorithms are divided 
into 2 parts. The unsupervised learning algorithms like 
Bayesian clustering and k-means clustering. The supervised 
learning algorithms, Naive Bayes classifIer, and decision 
tree. His method started by ftltering the data then converted 
to dataset. Next, his analyzer sent data to unsupervised 
algorithms first and take result plug-in to the supervise 
algorithms. This combination can produce the good results, 
978-1-4244-6370-11$26.00 to 2010 IEEE VI-383 
2010 2nd International Conference on Education Technology and Computer (ICETC) 
but it consumes too much time and can not guarantee for the 
best result. 
C. Theoretical Aspects 
To complete the main algorithm, we use many learning 
algorithms in this paper. We start with Syslog protocol which 
use collect and centralize log data. Next, we discuss about 
association rule, tf-idf, k-means clustering, and decision tree. 
The Syslog protocol [13], is a standard for forwarding log 
messages in a computer network. The term Syslog is often 
used for both the actual Syslog protocol as well as the 
applications that sending Syslog messages. Syslog is a 
client/server protocol; the Syslog sender sends a small (less 
than IKB) textual message to the Syslog receiver. The 
receiver is commonly called Syslog daemon or Syslog server. 
As Syslog can integrate log data from many different types 
of systems into a central repository. 
The association rule learning [14] is a popular and well 
researched method for discovering interesting relations 
during variables in large databases. Piatetsky-Shapiro 
describes analyzing and presenting strong rules discovered in 
databases using different measures of interestingness. Based 
on the concept of strong rules, Agrawal et al. introduced 
association rules for discovering regularities while products 
in large scale transaction data recorded by point-of-sale (POS) 
systems in supermarkets, web usage mining, intrusion 
detection and bioinformatics. 
The tf-idf weight [15] or term frequency-inverse 
document frequency is a weight often used in information 
retrieval and text mining. This weight a statistical measure 
used to evaluate how important a word is to a document in a 
collection or corpus. The importance increases 
proportionally to the number of times a word appears in the 
document. However, it is offset by the frequency of the word 
in the corpus. Variations of the tf-idf weighting scheme are 
often used by search engines as a central tool in scoring and 
ranking a document's relevance given a user query. The term 
count in the given document simply the number of times a 
given term appears in that document. This count is usually 
normalized to prevent a bias towards longer documents. That 
is because they may have a higher term count regardless of 
the actual importance of that term in the document. We need 
to give a measure of the importance of the term ti within the 
particular document 4. Thus we have the term frequency 
defmed in Equation (1). 
containing the term, and then taking the logarithm of that 
quotient as defined in Equation (2). 
Where 
• 
• 
idJ: = log IDI , I{d : t, E d} 1 
IDI is the total number of documents in the corpus 
(2) 
I{d: I, E d} 1 is number of documents where the term ti 
appears (that is ni,j :t:. 0). If the term is not in the 
corpus, this will lead to a division-by-zero. It is 
common to use 1+ I{d:t E d} 1 then (tf-idif) =tl' xidij, , '.1 tI'.1 , 
A high weight in tf-idf is reached by a high term 
frequency (in the given document) and a low document 
frequency of the term in the whole collection of documents; 
the weights hence tend to filter out common terms. The tf-idf 
value for a term will always be greater than or equal to zero. 
In statistics and machine learning, k-means clustering [16] 
is a method of cluster analysis which aims to partition n 
observations into k clusters in which each observation 
belongs to the cluster with the nearest mean. It is similar to 
the expectation-maximization algorithm for mixtures of 
Gaussians in that they both attempt to fmd the centers of 
natural clusters in the data. 
Given a set of observations (x},x2, ... ,xn), where each 
observation is a d-dimensional real vector, then k-means 
clustering aims to partition the n observations into k sets 
(k<n) s={S},S2"",Sk} so as to minimize the within-cluster 
sum of squares (WCSS) as you see in Equation (3): 
(3) 
where i is the mean of Si' 
Given an initial set of k means ml
(i), . . .  ,mk(l) which may 
be specified randomly or by some heuristic, the algorithm 
proceeds by alternating between two steps: 
Assignment step: Assign each observation to the cluster 
with the closest mean (i.e. partition the observations 
according to the Voronoi diagram generated by the means). 
II" n?. 
'.1'.1 = 
, .1 "" n . 
(1) Update step: Calculate the new means to be the centroid 
L-k k.1 
Where ni,j is the number of occurrences of the 
considered term (ti) in document 4, and the denominator is 
the sum of number of occurrences of all terms in document 4. 
The inverse document frequency is a measure of the 
general importance of the term. We obtained it by dividing 
the number of all documents by the number of documents 
of the observations in the cluster. 
The algorithm is deemed to have converged when the 
assignments no longer change. 
Decision tree learning [16] in data mining and machine 
learning uses a decision tree as a predictive model which 
maps observations about an item to conclusions the item's 
target value. More descriptive names for such tree models 
are classification trees or regression trees. In these structures, 
Vl-384 
2010 2nd International Conforence on Education Technology and Computer (ICETC) 
leaves represent classifications and branches represent 
conjunctions of features lead to those classifications. 
Decision tree learning is a common method used in data 
mining. The goal is create a model that predicts the value of 
a target variable based on several input variables. 
III. METHODOLOGY 
In computer network, there will be some detect anomaly 
or suspicious actions which we propose the analyzing a 
correlating log files. The universal format which we suggest 
was to centralize logs data, converted log to common log 
format. After processing through learning algorithms 
technique combination like association rules, tf-idf, k-means 
clustering and decision tree; we will be able to classify the 
abnormal from the normal operations. Then we did translate 
to easy to understand format for administrators like graph 
that is potential solution to the problem. 
We started correlate logs after collecting log files, and 
then we setup a table to separate each source, user cache 
table, result cache table and common table which consist of 
common field like date/time, source IP Address, source name, 
destination IP Address, destination name, source port, 
destination port, etc. We listed the important filed in Table 1 
below. We found the first fmal mapped-data in this common 
table by matching logs from each source with cache user 
table which contained IP-port sequence or MAC Address 
within short time. We also put the latest database in cache 
table for speedy calculation. We got the ready to process 
data after different sources of input have been processed. 
Field Name 
Conn_id 
Updated 
Usemame 
Src_ip 
Src_mac 
DesUp 
Service 
14:33:34 __ apaome: tie _u U.)li) 10.?2.7.1i9 - - (01 ApE/2010:1t:a 
pdf/cu7 lliD.pdf HTTP/t.O· _ 
14:22:2.
-
__ apaclu!: ... l.a!." as:. * 202.146 .214 - - (Ol/Apr/2010:U: 
i.o=l-.lK.E:n.g._W.3c:.UO{E:n.91roa-=n,aU20T.cllno y).? 8TTP/l.0· 
.t.u.c,.ac:.,h/c:d/th/<:>uicU..-_php· _ 
Figure I. The correlation process 
TABLE!. COMMON TABLE FIELD 
description 
Logs from same connection was group to same conn_id 
Date/time from service logs 
Usemame of user that authenticate from server 
User IP address 
User MAC address 
Service server IP address 
Service name 
Msg Message from logs 
Score_based _id Score of selected match class 
Score Score generated from customized tf-idf algorithm 
Result The result which can be good, bad or unknown 
reason The reason of result 
A. Class Rank Algorithm 
The process of learning algorithm, which we called Class 
Rank algorithm, was started from grouped the logs from 
same connection into single unit called detection unit. Then, 
we used signature-based technique with rule classifier to fmd 
out known good class (or white class) and bad class (or black 
class). Both classes had set as based class for calculation. 
The left over was assigned as unknown class which had to 
re-calculate score one-by-one using tf-idf algorithm. Next, 
we did individually compare the qualification whether it was 
close to the black class. For black class, the score will be 
range from -1 to o. We also had to calculate score for the 
qualification of the same unknown class. Then we compared 
whether it was more close to the white class or not. For this 
class range, score will be between 0 up to 1. K-means and 
decision tree were done the class-assigned decision job. If 
the score was uncertain, k-means and decision tree might 
decide to put it back to the unknown class. The process was 
repeatedly done till the rest of unknown class could be all 
classified. Please refer to Figure 2. 
Black 4 
- I 
I ? White 
Group A ? 
I Nov 10 00:00 .... 
I Nov 10 01:10 .... 
I 
I 
?? 3 
r::: .:i L Good base + l TF-IDF "'.erolt 7/+/- r;. 1. 
JI\ '\,.' 
I Nov 10 02:00 ... . I 
Gmup of event D.o,ion tm 
? 
(Auociation rule) l Bad base -
Bas? on rule A 
Figure 2. Analysis algorithm 
The cache result was regularly flushed out to keep the 
resource free and reduced time to process. The system can be 
summarize as shown in Figure 3. 
< Convert to common format > 
cleaning I 
Remove outliner I I Preprocessing module 
r log files correlation I 
I Processing module ? Move Known Activity to base I 
Analyze module 
L -tf-idf classifier 
-k-mean dustering 
-decision tree 
rl Cleanup data I I Post processing module Y Convert to selected format I 
( • Report module ) 
VI-385 
20i 0 2nd international Conference on Education Technology and Computer (ICETC) 
Figure 3. The flow chart of system 
B. Experimental Benchmark 
We started the test of algorithm by collected the log files 
from servers of King Mongkut's University of Technology 
Thonburi and compared with human verify dataset. 
Supposedly, we performed the test 10 times 
independently. The first 5 tests were constrained by time 
limited, while the last 5 did not have such limitation. The test 
could give the accurate percentage as well as false positive 
rate. 
For efficiency, we measured the system load by tested the 
program and could fmd consumption percentage by using 4 
main factors including CPU load, Memory usage, Disk usage 
and Network usage of host machine. 
We balanced between 2 factors: accuracy and efficiency 
to make reasonable cost system. The learning algorithm 
combination algorithms were use to increase accuracy. Then, 
we tuned the parameters of algorithms to get high efficiency 
at the lowest overhead possible. We also suggest distributed 
architecture form of this system to increase performance. 
The additional usability could increase by construct an 
adaptive learning algorithm to update the model on the fly 
while the logs are being captured. The old data was flushed 
out every interval of time. The new data flow which is high 
flow rate of data, was processed depend on windows time to 
make model fresh. 
IV. RESULTS 
We prepared the prototype with PHP programming 
language along with MySQL database. We selected PHP 
because it was easy to write with many support functions. 
For database, we selected MySQL because it is one of best 
free database with high performance. We needed to examine 
the algorithm. Then we decided to select some type of log to 
process, which reduce implementation time. The select logs 
to process were only web server log, DHCP log and radius 
log. However, the algorithm could support many type of 
computer logs with filtering support options. We also had 
written the code with parallel technique option. Then, it 
could achieve high performance by increase number of 
processing threads and could run with many servers. We had 
use only one high performance database server to protect the 
consistency. 
After we ran the prototype of class rank algorithm, we 
could easily see that it could get satisfaction. It could receive 
up to 70-80% correct results if we gave enough time to 
process. Besides, we must think about the computation time. 
Therefore, we needed to ignore some target results to reduce 
time consumption. The result dropped to around 30-40% 
target results, with nearly real-time situation. The advantage 
of an algorithm is that it has nearly zero false positive (about 
<5%). Then, we could rely on most result they provided. 
However, results had depended on some initial data because 
some parts in process algorithm use k-means cluster 
technique. 
90 
80 - I -
70 -+- Real-time 
60 Detection 
? 50- ___ Unlimited time 
£ ??- Detection ..... ..... 
20 -
10 -
0 RunlD 
1 2 3 4 5 
Figure 4. Detection Results 
The detection results depend on time usage as you see 
from Figure 4. The result can reach about 70-80% as 
describe above if we leave system run in no limited time 
mode. We should think efficiency in real-time environment. 
Then, we adjusted to filter some results out to reduce time 
consumption. Finally, we got target results around 30-40%, 
with parallel mode using acceptable time which is delay 
about 5 minutes. We think of this result can acceptable in 
real-time environment. Although, percent results seem to be 
low but we can install this system as plus-in to normally 
signature-based program in real environment. Therefore, 
some results were captured by the primary detection software 
and our system is the next filter to capture the missing targets. 
2.5 
... ? 2 ? 
.......... ? 
? 1.5 
? -+-Real-time ? 
? Detection If 1 ___ Unlimited time 
0.5 Detection 
0 RunlD 
1 2 3 4 5 
Figure 5. False Positive Results 
The false positive rate shown in Figure 5 of this 
algorithm was surprising low. This is the strong point of this 
algorithm. It could take around only 1-2% of all found 
targets. We could take some parameters to make the 
detection results more than above value and time 
consumption was reduce. Nevertheless, it took too much 
false positive. Therefore, we adjusted the parameters to keep 
the false positive low while receive the highest detection 
possible. Finally, it showed the results as you can see in 
Figure 4. 
VI-386 
20i 0 2nd international Conference on Education Technology and Computer (ICETC) 
90 
80 ... - I 
70 -- I 
60 -+- Real-time Parallel 
? 50 ___ Unlimited time Parallel 
? 40 
0.. 30 -+- Real-time Single 
20 ?Unlimited time Single 
10 ? 0 lI( ? )I( ? I RunlD 
1 2 3 4 5 
Figure 6. System Load 
The system load was very high as Figure 6. This load 
came from parallel technique. We ran multiple threads of 
process algorithm to accelerate the speed but we had limit 
resource to test prototype. Therefore, the time consumption 
was too much than we plan. We use only around 5-8% 
system load if we ran only one thread. 
V. DISCUSSION 
Computer networks are dangerous environment today. 
Many computer systems integrate operation recorders called 
Logs or Event Logs. Administrators need a solution for 
centralizing logs from every computer. In addition, central 
log system should generate alerts or take some actions the 
abnormal operations occurring in real-time or near real-time 
to make the whole system more secure. 
A. Achievements 
This paper discuses how to collect logs data from devices 
in the network into central server. Convert them into one 
common format Then, analyze and interpret data to get the 
important information from many garbage data. Finally, 
convert again into the graphical formats for easy understand. 
Therefore, administrators can understand easily. The system 
use the adaptive learning algorithm to process the data 
stream and data model can change along the time. Therefore, 
our system flush out the old model when it thinks the model 
is too old. 
This system is target to detect the abnormal activities in 
computers logs using combination between signatures-based 
and learning algorithms based technique. The learning 
algorithms used in this paper were associate rule, tf-idf, k­
means clustering, and decision tree. We started to collect the 
logs to central, removed garbage data and make correlation 
between them into one common table. After that we applied 
customized learning algorithms to receive the target results. 
We received good response about the results after 
implementation_ It showed the algorithm could achieve high 
performance, good accuracy and high efficiency with parallel 
processing. We got low false positive results but it stilled 
misses some targets when we needed to use it in real-time 
processing because we filtered out some target results to 
reduce time consumption. The overall result was still 
acceptable when it work in real-time environment We 
performed the test for 10 times and logically compared the 
results with three benchmarks which is detection result, false 
positive result, and system load. 
B. Summary 
We ran system on real data environment which divided 
into training data and test data_ We got expected results. The 
algorithm could process in limited time processing at an 
acceptable accuracy. On the other hand, we could increase 
the accuracy to more than 70% with unlimited time 
processing. The false positive rate was under 5% in both 
cases. 
Finally, the main target of our system which is making an 
automatic system could detect problem automatically 
without human interaction. In conclusion, our proposed 
prototype system could give you the incredible low work 
load for administrators about manual logs examination. 
REFERENCES 
[I] M. Roesch, "Snort: Lightweight Intrusion Detection for Networks", in 
Proc. of 13th USEN1X Conference on System Administration. 
USENIX Association, Nov. 1999, pp. 229-238. 
[2] Paul D., Levent E., Vipin K., Aleksandar L., Jaideep S., and Pang T., 
2002, Data Mining for Network Intrusion Detection, Computer 
Science Department University of Minnesota, Minneapolis, USA. 
[3] GyAorgy 1. S., Hui x., Eric E., Vipin K.,2006, Scan Detection: A 
Data Mining Approach, Computer Science University of Minnesota. 
[4] Wenke L., Salvatore 1. S. , Philip K. C., Eleazar E. , Wei F., Matthew 
M., Shlomo H. , and Junxin Z., 2001, Real Time Data Mining-based 
Intrusion Detection, Computer Science Department, Columbia 
University, New York. 
[5] Tao P., Wanli Z., 2006, "Data Mining for Network Intrusion 
Detection System in Real Time", IJCSNS International Journal of 
Computer Science and Network Security, Vol.6 NO.2B. 
[6] Gerhard M., Georg C., 2008, Real-time Analysis of Flow Data for 
Network Attack Detection, Computer Networks and Internet Wilhelm 
Schickard Institute for Computer Science University of Tuebingen, 
Germany. 
[7] Spiros P., Anthony B., Christos F., 2003, "AWSOM: Adaptive, 
Hands-Off Stream Mining", the 29th International Conference on 
Very Large Databases (VLDB'03), Berlin, Germany, September 9-12. 
[8] Pingchuan M., 2003, Log Analysis-Based Intrusion Detection via 
Unsupervised Learning, Master of Science, School of Informatics, 
University of Edinburgh. 
[9] S Terry B., 2004, Data Mining Methods for Network Intrusion 
Detection, University of California, Davis. 
[10] Ying 1., Jing D., Chang T., 2006, Spatial-Temporal Data Mining in 
Traffic Incident Detection, Department of Computer Science, 
Virginia Polytechnic Institute and State University. 
[II] Witcha C., Prof. Dr. Abdul H. A., Associate Prof Dr.Mohd N., 2005, 
"Unsupervised Anomaly Detection with Unlabeled Data Using 
Clustering", Proceedings of the Postgraduate Annual Research 
Seminar. 
[12] Matthew G. S., Eleazar E., Erez Z., Salvatore 1. S., 2001, Data 
Mining Methods for Detection of New Malicious Executables, 
Department of Computer Science, Columbia University. 
[13] Syslog protocol [Online] Available from: 
http://en.wikipedia.org/wiki/ Syslog [29 November 2009]. 
[l4] Michael H., 2006, A Model-Based Frequency Constraint for Mining 
Associations from Transaction Data, Economics and Business 
Administration Vienna University. 
VI-387 
2010 2nd International Conforence on Education Technology and Computer (ICETC) 
[15] Juan R., 2003, Using TF-IDF to Determine Word Relevance in 
Document Queries, Department of Computer Science, Rutgers 
University. 
[16] Gereon F., Christian S., 2005, A fast k-mean implementaion using 
coresets, Department of Computer Science, Paderborn University, 
Germany. 
[17] Sung-Hyuk C., 2008, A Genetic Algorithm for Constrncting Compact 
Binary Decision Trees, Department of Computer Science, Pace 
University, New York, USA. 
VI-388 
