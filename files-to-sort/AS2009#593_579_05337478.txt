Meaningful Inner Link Objects for Automatic Text Categorization 
 
 
Jau-Ji Shen*, Jia-Chiuan Wu 
Department of Management Information Systems, National Chung Hsing University, 
250, Kuo Kuang Rd., Taichung 402, Taiwan. 
* jjshen@dragon.nchu.edu.tw 
 
Abstract 
 
This paper presents a novel approach for automatic 
text categorization. The mainstream of the research on 
rule-based classifier regards document as a container 
of term, and generates rules by using the term 
distribution in documents. General speaking, there 
must be existed some kind of semantic relevance 
between term and paragraph in a document. We call it 
Meaningful Inner Link Objects-MILO which must be 
varied with different semantics of a document itself. 
While this paper concentrates on using these MILOs 
that associate with semantic relevance for text 
categorization, hence we focus on two problems: (1) 
finding the best MILOs which associate with semantic 
relevance; and (2) using these specific MILOs to build 
a classifier for text categorization. From the 
experiment results, our proposed classification 
approach base on MILO has a better accuracy while 
other state of the art technique without considering the 
relevance between term and paragraph. 
 
1. Introduction 
 
Text categorization is the task of classifying 
unlabelled documents into one or several predefined 
categories. It’s gradually becoming a subject and 
receives much concern from researchers in recent years. 
Because the continuing popularity of Internet 
technology, user can readily access enormous digital 
documents. Hence to efficient organize and retrieve 
these digital documents have become a very important 
subject for discussion. 
Currently, Text categorization is a discipline at the 
crossroads of information retrieval and machine 
learning that is mentioned in [1]. The process of text 
categorization can be simply divided into 
preprocessing and classifier construction. As shown on 
Fig.1, there are techniques in preprocessing, such as 
tf·rf, information gain, chi-square, etc. And these 
techniques are arranged in [2]. These techniques assign 
appropriate weights to the terms. And the terms in 
higher weights will be selected as indices of a 
document, and other terms in lower weights will be 
filtered out. The classifier construction such as Bayes 
classifier [3] is a probabilistic classifier by applying 
bayes theory for classification that bases on the 
hypothesis of independence between terms’ 
relationship. K-nearest neighbor classifier [4] is an 
example-based classifier which categories an 
unlabelled document according to the distance 
information with K nearest examples in the classifier. 
In addition, decision tree classifier [5], association rule 
classifier [6], etc, all of them achieve very significant 
performance in text categorization. 
 These techniques mentioned above do not consider 
the paragraph factor, hence to investigate the 
relationship about paragraphs further that needs to 
normalize irregularity paragraphs at first, and then 
builds a classifier which considers the paragraph 
factor. Next section will illustrate the way to build a 
classifier which considers the paragraph factor. 
The remainder of this paper is organized as follows: 
Section 2 introduces our new concept and illustrates 
the way to build a classifier. Section 3 provides 
experiment results and Section 4 draws a conclusion. 
 
 
Figure 1. Concept diagram of text classifier 
construction 
 
2. Building a MILO classifier 
 
2.1. Basic concepts 
 
This section will introduce a new concept that is 
presented in this paper. Firstly, a document is written 
2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing
978-0-7695-3762-7/09 $26.00 © 2009 IEEE
DOI 10.1109/IIH-MSP.2009.21
266
base on the thought of author. And a thought is fully 
presented in a document which should be precisely 
structured and be logically. For example, the head part 
of a document will introduce the document’s topic, the 
body part will mainly present the details about this 
topic, and the tail part will draw conclusions. The 
semantics of a document will generate certain term 
structures which are formed across paragraphs for 
some specific meaning. On the other words, the 
linkage of terms across different paragraphs in a 
certain order denotes the author’s specific meaning. 
The term’s linkage across different paragraphs is called 
as Meaningful Inner Link Objects (MILO). Fig.2 
illustrates the concept of MILO. 
 
 
Figure 2. Concept of MILO 
 
As shown on the dotted lines of Fig.2, MILO is a 
directional linking relationship which is composed by 
terms across paragraphs. Such as [A1?B2?A3] is a 
MILO with length 3 which is composed from a term of 
P1 to a term of P2 and then ending at a term of P3 
respectively. Similarly, [A1?A3] is a MILO with 
length 2 which is composed from a term of P1 to a term 
of P3. [A1] is a length 1 MILO in P1. We use the 
collection of extracted MILOs from all documents in 
each category to build a classifier. 
 
2.2. The process of building MILO classifier 
 
This section will illustrate the way to use MILO for 
text categorization. Fig.3 shows the classifier building 
process. First of all, the selected dataset is divided into 
training set and testing set in a certain proportion. The 
classifier is trained by training set and evaluated by 
testing set. 
 
Figure 3. Construction forMILO classifier 
 
2.2.1. Preprocessing of training set. According to 
patterns of MILO which is illustrated in Fig.2, they are 
denoted by the term’s directional linkage across the 
paragraphs in each document, hence each document 
will be normalized into three paragraphs first. Scott 
Piao [7] provides this paragraph normalization tool on 
the website of The National Centre for Text Mining 
(NaCTeM). After normalizations, three paragraphs are 
stored into three databases respectively. For noisy 
reduction, a stop word list [8] is applied to remove 
general stop words. In order to define each term’s 
relevance degree in different paragraphs, the chi-square 
function is used to calculate the weight of each term’s 
relevance degree from the three databases respectively. 
Eq.1 is the chi-square formula defined base on the term 
occurrences as follow, 
)dc)(ba)(db)(ca(
)c*bd*a(*N)C,t( i
++++
?
=
2
2? ,          (1) 
a: occurrences of term ti in category C, 
b: occurrences of other terms in category C, 
c: occurrences of term ti in other categories, 
d: occurrences of other terms in other categories, 
N:a+b+c+d, total occurrences of all terms in all 
categories. 
Finally, each normalized paragraph of every 
document in each category is denoted by top T chi-
square value terms per category in each database. On 
the other words, all selected terms are t1, t2," , tT 
which are used to denote a category. 
 
2.2.2. MILO mining. The selected T terms t1, t2," , tT 
with relative importance in each document’s 
normalized paragraphs will be used to find the MILOs 
as shown Fig.2. Since MILOs are concealed in the 
document, we present three algorithms for MILO 
mining. For any document d in training set, let’s define 
the following notations first, 
C : the category of document d, 
P1: the first normalized paragraph of d denoted by 
(o11, o12," , o1?), where o1i, i=1,2," , ?, is the 
number of occurrences of term ti? {t1, t2," , 
267
tT}. On the other words, P1 contains ? different 
terms which belong to {t1, t2," , tT}, 
P2, P3: the second and third normalized paragraph 
of d denoted by (o21, o22," , o2?) and (o31, 
o32," , o3?) respectively with the similar 
meaning as P1. 
CM(l): a set of length l MILOs in category C, 
OCM(l): a set of number of occurrence with respect 
to each MILO in CM(l). 
Algorithm 1 Extract MILO with length 1 from a  
document d in C 
Input P1, P2 and P3 of d 
Output CM(1) and OCM(1) 
Methods:     
(1) for each o1i in vector P1, 1 ? i ? ? 
(2) OCM(1)?CM(1) ? o1i 
(3) CM(1)?CM(1) ? [t1i] 
(4) for each o2j in vector P2, 1 ? j ? ? 
(5) OCM(1)?OCM(1) ? o2j 
(6) CM(1)?CM(1) ? [t2j] 
(7) for each o3k in vector P3, 1 ? k ? ? 
(8) OCM(1)?OCM(1) ? o3k 
(9) CM(1)?CM(1) ? [t3k] 
Because the MILO extracted by Algorithm 1 has 
length 1, hence occurrence of each extracted MILO is 
equal to the occurrence of this MILO’s term. 
Algorithm 2 Extract MILO with length 2 from a  
document d in C 
Input P1, P2 and P3 of d 
Output CM(2) and OCM(2) 
Methods: 
(1) for each o1i in vector P1, 1 ? i ? ? 
(2)  for each o2j in vector P2, 1 ? j ? ? 
(3) OCM(2)?OCM(2) ? min{o1i , o2j} 
(4) CM(2)?CM(2) ? [t1i?t2j] 
(5) for each o2j in vector P2, 1 ? j ? ? 
(6)  for each o3k in vector P3, 1 ? k ? ? 
(7) OCM(2)?OCM(2) ? min{o2j , o3k} 
(8) CM(2)?CM(2) ? [t2j?t3k] 
(9) for each o1i in vector P1, 1 ? i ? ? 
(10) for each o3k in vector P3, 1 ? k ? ? 
(11)  OCM(2)?OCM(2) ? min{o1i , o3k} 
(12)  CM(2)?CM(2) ? [t1i?t3k] 
Algorithm 2 extracts MILOs with length 2, such that 
the linkage of MILOs are composed as P1 to P2, P1 to 
P3, and P2 to P3 respectively as shown in Fig.2. 
Algorithm 3 Extract MILO with length 3 from a  
document d in C 
Input P1, P2 and P3 of d 
Output CM(3) and OCM(3) 
Methods: 
(1) for each o1i in vector P1, 1 ? i ? ? 
(2)  for each o2j in vector P2, 1 ? j ? ? 
(3)    for each o3k in vector P3, 1 ? k ? ? 
(4)  OCM(3)?OCM(3) ? min{o1i , o2j, o3k} 
(5)  CM(3)?CM(3) ? [t1i?t2j?t3k] 
Algorithm 3 extracts all the MILOs across three 
normalized paragraphs in an ordered sequence from P1 
to P2 and to P3 finally.  
The previous designed algorithms will extract all 
MILOs with lengths 1, 2 and 3 from each document of 
a category C. By three extracted MILOs, a classifier of 
training set is built. On the other words, for a category 
C, the collection of MILOs CM(1), CM(2) and CM(3) 
are now built as a classifier. 
 
2.3. Prediction with MILOs 
 
When a MILO classifier predicts an unlabelled 
document’s category, unlabelled document should be 
normalized into 3 paragraphs at first, then all MILOs 
of this document are extracted by Algoritms 1, 2 and 3. 
In order to define the MILO’s weighting in 
categorization, the following Algorithm Confidence is 
designed. 
Algorithm Confidence 
Input D: a set of MILOs from an unlabelled 
document, UCM: CM(1) ? CM(2) ? CM(3), and 
OCM: OCM(1) ? OCM (2) ? OCM (3) 
Output the confidence value of D with respect to 
certain category C 
Methods: 
(1) SC ? ? /* SC : a set of C’s MILOs which match 
patterns in D */ 
(2) weight ?0 /* weight of the length of MILO */ 
(3) ConfvalueC? 0 /* confidence summation */ 
(4) for each MILO ??UCM  
(5) for each MILO ? in D 
(6)     if(? = ?)  
(7)     SC ?SC ? ? 
(8) for each MILO ??SC 
(9)    if(?.length = 1)  
(10)     weight ? w1 
(11)    if(?.length = 2)  
(12)      weight ? w2 
(13)    if(?.length = 3)  
(14)      weight ? w3 
(15)  ConfvalueC ?ConfvalueC + ?. confC * weight 
(16)  /* confC is defined as Eq.(2) */ 
According to any given category’s MILO 
collection, steps (4)-(7) perform pattern’s matching 
between the unlabelled document and the given 
category C. If any MILO is matched, that means not 
only terms matching but also the direction of MILO is 
matched. The matched MILOs will be stored into SC. 
Steps (8)-(15) add confidences of all matched MILOs 
in SC. The following Eq.2 is the formula to calculate 
confidence of a MILO, 
  categories all in  MILO  of occurences total
Ccategory   in  MILO of occurences totalconfC ?
?
=
.       (2) 
For the weights w1, w2, and w3, if a matched MILO 
has the longer length, the more important it is (i.e., 
w3>w2>w1). Algorithm Confidence calculates the 
268
confidence value to estimate the similarity between an 
unlabelled document and a given category C.  
The following steps show how all algorithms are 
operated as a classifier to classify a given unlabelled 
document. 
1. For each category C in training set, Algorithm 1, 2 
and 3 are applied on each document of C to extract 
all MILOs of C. 
2. The MILO’s classifier is composed by all MILO’s 
set of all categories in training set. 
3. For any given unlabelled document D, Algorithm 
1, 2 and 3 are applied first to find all MILOs of D, 
and then Algorithm Confidence is performed to 
calculate the confidence value of D to each 
category in training set. 
4. Suppose the confidence value of D to a category X 
have the largest value then we let D belong to X. 
 
3. Experiments results 
 
In this paper, The benchmark in this experiment is 
20 Newsgroups [9]. The version of this benchmark in 
this experiment is “bydate” version which is already 
divided into 60 percent training set and 40 percent 
testing set. And according to [1], the evaluation 
measures, micro F1 and macro F1 are used for 
performance evaluation. The experiments are 
conducted on five subsets of 20 Newsgroups as shown 
on Table 1 respectively. Table 2 displays the best 
experiment results which selects 700 terms(i.e. T=700) 
in highest chi-square value from all categories in each 
database. We compare our results with the research of 
Pu et al [10] in 2007. The experiment results of our 
method have better classification accuracy than the 
best results of Pu et al’s method.  
 
4. Conclusion 
 
Our new approach also provides the following two 
benefits: (1) the MILOs are human readable and allows 
manual maintenance if necessary. (2) The MILOs in 
the classifier could be incrementally updated. When 
the new documents are presented for retraining, 
Algorithm 1, 2 and 3 can extract MILOs from these 
new documents and then importing these new MILOs 
into the classifier directly without reconsidering the 
past documents. Hence, the cost of retraining is lower 
than SVM, association rules," , etc. With these 
advantages of the MILO-based approach for text 
classification, there are still areas for further 
improvement. We plan to increase the efficiency of 
algorithms for reducing the training time and testing 
time, or to find a way which measures the importance 
of the MILO to build the classifier with higher quality. 
 
Table 1. The partition of 20 newsgroup according to 
subject matter 
comp.graphics 
comp.os.ms-windows.misc
comp.sys.ibm.pc.hardware
comp.sys.mac.hardware 
comp.windows.x 
rec.autos 
rec.motorcycles 
rec.sport.baseball 
rec.sport.hockey 
sci.crypt 
sci.electronics 
sci.med 
sci.space 
misc.forsale talk.politics.misc 
talk.politics.guns 
talk.politics.mideast 
talk.religion.misc 
alt.atheism 
soc.religion.christian 
 
Table 2. The results of MILO categorization(T=700, 
w1=0.33, w2=0.66, w3=1) 
Data sets Measures Purpose 
method  
Pu et al’s  
method 
5 comp ma-F1 0.685  0.671  
mi-F1 0.690  0.670  
4 rec 
  
ma-F1 0.911  0.905  
mi-F1 0.911  0.904  
4 sci 
  
ma-F1 0.856  0.848  
mi-F1 0.856  0.843  
3 politics 
  
ma-F1 0.813  0.798  
mi-F1 0.824  0.802  
3 religion 
  
ma-F1 0.715  0.752  
mi-F1 0.732  0.773  
Average ma-F1 0.796  0.795  
mi-F1 0.803  0.798  
 
References 
 
[1] F. Sebastiani, “Machine Learning in Automated Text 
Categorization,” ACM Comput. Surv., vol. 34, no. 1, pp. 1-
47, 2002. 
[2] L. Man, T. Chew Lim, S. Jian et al., “Supervised and 
Traditional Term Weighting Methods for Automatic Text 
Categorization,” IEEE Trans. Pattern Anal. Mach. Intell, vol. 
31, no. 4, pp. 721-735, 2009. 
[3] D. D. Lewis, “Naive (Bayes) at Forty: The Independence 
Assumption in Information Retrieval,” Proc. European Conf. 
Machine Learning, pp. 4-15, 1998. 
[4] Y. Yang, and X. Liu, “A Re-examination of Text 
Categorization Methods,” Proc Int’l Conf. Machine 
Learning, pp. 412-420, 1997. 
[5] J. R. Quinlan, C4.5, 1993. 
[6] M.-L. Antonie and O. R. Zaiane, “Text Document 
Categorization by Term Association,” Proc IEEE int’l Conf. 
Data Mining, 2002. 
[7] S. Piao, Sentence and Paragraph Breaker, In 
<http://text0.mib.man.ac.uk:8080/scottpiao/sent_detector>. 
[8] C. Fox, A stop list for General Text, ACM SIGIR Forum, 
vol. 24, no. 1-2, pp. 19-21, 1989. 
[9] K. Lang, The 20 Newsgroups Data Set, In 
<http://people.csail.mit.edu/jrennie/20Newsgroups/>. 
[10] W. Pu, N. Liu, S. Yan et al., “Local Word Bag Model 
for Text Categorization,” Proc IEEE int’l Conf. Data Mining, 
2007. 
269
