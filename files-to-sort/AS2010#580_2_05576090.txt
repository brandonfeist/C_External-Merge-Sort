  
Decision Rule Acquisition Algorithm based on Association-Characteristic 
Information Granular Computing 
 
 JianFeng Xu 1,Lan Liu1, GuangZuo Zheng 2 , Yao Zhang 2 
 1 NanChang University, Jiangxi, 330031, P.R.China 
2 Galway-Mayo Institute of Technology, Ireland 
E-mail: jianfeng_x@ncu.edu.cn,liulanxf@sohu.com  
 cryingzgz@gmail.com, you1gerenaiwo@gmail.com 
 
Abstract 
     This is a article of research which bases on the 
classical granular computing and the Association Rules, 
focus on the association rules and decision-making rules 
of the information system. Firstly, defines a association-
rule characteristic Information Granule, which can be 
treated as a sub-definition of the Information Granule, 
and a association-rule characteristic Information 
Granularity matirx is defined as well. Secondly, we 
present a new approach of computing on support degree 
and confidence degree, which are commonly used in 
problems of association-rule and decision-rule in 
information systems. Finally we build a whole 
knowledge-obtaining algorithm for the association rules 
and decision-making, and test its availability by 
numerous experiments.   
1. Introduction 
 
 Granular Computing is a fast-developing subject, 
and it is a fusion of the rouge set, fuzzy mathematic and 
Artificial Intelligence.[1,2,3,4] Base on the philosophical 
thinking, it builds the granular structure of the real-
world problem, and finds the solution of that problem by 
setting the computing model on the granular structure. It 
is a combination of Philosophical Thinking, 
Methodology and Computing Model. Studying the 
granular computing methods and building the granular 
computing models, under the guide of the philosophical 
thinking, is the main research direction of Granular 
Computing in the future.     
In this article, we present the definitions of the 
association-rule Information Granularity matrix, the 
association-rule Information Granul, the association-rule 
Information Granularity matrix, and the Algorithm that 
based on the theories above.   
 
2. Primary Definitions  
 
    All printed material, including text, illustrations, and 
charts, must be kept within a print area of 6-1/2 inches 
(16.51 cm) wide by 8-7/8 inches (22.51 cm) high. Do 
not write or print anything outside the print area. All text 
must be in a two-column format. Columns are to be 3-
1/16 inches (7.85 cm) wide, with a 3/8 inch (0.81 cm) 
space between them. Text must be fully justified. 
    A format sheet with the margins and placement 
guides is available as both Word and PDF files as 
<format.doc> and <format.pdf>. It contains lines and 
boxes showing the margins and print areas. If you hold 
it and your printed page up to the light, you can easily 
check your margins to see if your print area fits within 
the space allowed. 
Definition 1: Information system S =?U?AT?,the 
U is the dataset of training samples , AT is attribute set. 
One of the records of the training sample in S is event T, 
UT ? ? 
     Definition 2: granulating the information system: 
information system S?set B?AT, according to 
whether B has a value, actual range VB={ 1, 0}. 
    Then B can decompose into binary granularity set 
according to the quotient set U/IND{B}[5]. And the 
combination of all the Information Granul is 
GM?granularity matrix?. 
   For example: in different events, the property B, which 
is in the information system S, is set to 
1? ? ? ? ? ? ? ? ? ? ?0 0 1 0 0 1 0 0 1 1 0, then B can be 
decomposed into a binary granulation: 
b1=100100100110?  
For example:three atributes a,b and c, which are in 
the information system S, are set value of 
101000011001? ? ?100101001101 110101000101 then 
these atributes can be converted into 3 binary 
granulation:  
a={101000011001} 
b={100101001101}  
c={110101000101} 
And they can be convert into the matrix GM?  
2010 IEEE International Conference on Granular Computing
978-0-7695-4161-7/10 $26.00 © 2010 IEEE
DOI 10.1109/GrC.2010.38
812
  
   ??
???
?
=
011101010001
011001010011
011010000110
GM  
   Definition 3: Granular degree of binary information 
granule. Assuming the amount of digit 1 in a binary 
information granule of attribute P is the granular degree 
of this information granule, |P|, then, the support number 
of an itemset of this attribute P is equal to the granular 
degree |P|. For example, the granular degree |a| of binary 
information granule a: 101000011001, is 5. 
   Definition 4:  Ik (K itemset) and its granulating. In the 
information system S, the set which contains the 
associating K attributes is called K itemset. Its 
granulating is the conjunctive of those k binary granule 
of attributes, And the outcome is a binary Information 
Granule whose granular degree is much smaller. 
   For example? information system S?C1, C2 …Ck, are 
the binary information granules of those K atributes, 
Then: C1 ?  C2 ?  … ? Ck, are K itemset? its granular 
degree is |C1 ?  C2 ?  … ? Ck, |. 
    Definition 5? support degree? The support degree of 
the k itemset, is the probability of the k itemset in U, 
equal to P(Ik)=|Ik|/|U|?The granular degree of the 
itemset is its support number.  
   Definition 6: K Item association-rule. The support 
degree of Ik  which is bigger than an assigned value?  
    Definition 7? Confidence degree. Assume C is the 
condition attribute of the association-rule, D is the 
decision-making attribute of the association-rule, then 
confidence degree of C?D is P(D|C )?|D ?  C |/|C |. 
Definition 8: decision-making rules. The association-
rule which satisfies the minimum-support-degree and 
minimum-confidence-degree at the same time, the rule 
C?D is called decision-making rule. 
 
3. The Basic Theory of Association-rule 
Characteristic Information Granule 
 
   Definition 9: the granulating of association-
characteristic of Ik:.is the binary bunch whose length is 
|AT|, which is associated with the attributes subscript.  
The binary bunch value will be set to '1' ,whose position 
relative with the attribute appears in Ik t, otherwise it will 
be set to '0'.  
   All of the association-characteristic information 
granule in Ik could be combined into a matrix RMk.? We 
will use association-characteristic information granule 
Pki to represent associated IKi.. Pki is a line of RMk? For 
example, in the information system S, AT={a,b,c}. 2 
item-set? a,b? ?, b,c? ,{a,c} means 
P21?? ? ?110 P22=? ? ?011 P23=? ?101 ;its 2 itemset's 
association-rules martix. 
 ??
???
?
=
101
011
110
2RM  
   Theorem 1: the amount of atributes in the information 
system S is N?? |AT|? N? , its 1 itemset association-
rule matrix is, N steps matrix.For example: in the 
information system S ,AT={a,b,c}. 1 itemset 
association-rule matrix is: 
??
???
?
=
001
010
100
1RM  
   Definition 10: the granulating of the K itemset's 
association rules.:same with definition 9. 
    Theorem 2: the granule degree of Pk iS 
Pki°GM=||Pki ? GMj||  
If |Pki ? GMj|?Pki ? then||Pki ? GMj|| ?1, otherwise 
||Pki ? GMj|| ?0  
Prove: 
  ? if |Pki ? GMj|?|Pki| means the jth event satisfy the 
association rules Pki, ? the Information Granule which 
Pki stands for should be set to 1, otherwise should be 0. ? if |Pki ? GMj|?|Pki | stand for the number jth event 
do not satisfy the itemset association Pki , ? so Pki so 
the Information Granule which Pki stands for should be 
set to 1. 
 ? the granule degree of Pk:: 
|Pk|?Pki°GM=?
=
?
U
j
jki GMP
1
 
  Theorem 3: Assume RMki,RMkj as two item 
association-characteristic information granule,and 
|RMki ? RMkj|?K+1,?RMki ? RMkj. are k+1 item 
association-characteristic information granule. 
Definion 4 K items association-rules information 
granule Pk,  is a combined with condition attribute set C 
and decision-making attribute set D, 
DkCkk PPP ?? ?= , the support degree of decision-
making rules C?D ?|Pk|/|Pk-D|. 
 
4. Decision Rule Acquisition Algorithm 
based on Association-Characteristic 
 
   Assume that there are n attributes and m events in the 
information system S. 
Step 1: Granulating the information system S, create the 
GM of the information system. 
Step 2: Build RM1 based on Theorem 1. 
813
  
Step 3:According to Definition 5 and Theorem 2, 
calculate the support degree of the GM1. 
Step 4:Delete the association-characteristic information 
granule form the matrix whose support degree is below 
the specified minimum-support-degree. 
Step 5: FOR(i?1,i<?n? i++?  
{ 
According to Theorem 3, can obtain the i+1 association-
characteristic information granule matrix (RMi+1), form 
RMi.    
Delete the association-characteristic information granule 
whose support degree is below specified 
minimum?support?degree. 
If the matrix is empty, then break   
} 
step6: According to Theorem 5 computing the decision-
making rules. 
 
5. Simulation Experiment 
   Information system S as the Table 1. 
Table1. list of the experiment information of the system 
U AT 
T1 a,b,d 
T2 a,b,c,e 
T3 a,c 
T4 a,b,c 
T5 a,c,d 
T6 a,b,d,e 
T7 a,b 
T8 a,c,f 
T9 a 
T10 b,c 
T11 c,g 
 
STEP1: according to definition 7, granulating the 
existence of each attribute in database,  obtain the binary 
granule of all the attribute sets. The result is shown 
below?  
a? (11111111100), 
b? (11010110010), 
c? (00111001011), 
d? (11001100000), 
e? (01000100000), 
f? (00000000100), 
g? (00000000001). 
 Combine information granule of the attributes above to 
the following information granule matrix: 
??
??
?
?
?
??
??
?
?
?
=
10000000000
00000000010
00100010000
01100110000
10011100101
01101011001
01111111110
g
f
e
d
c
b
a
GM  
 
 
STEP2: build RM1 based on Theorem 1 as follows. 
??
??
?
?
?
??
??
?
?
?
=
??
??
??
?
?
??
??
??
?
?
=
0000001
0000010
0000100
0001000
0010000
0100000
1000000
7,1
6,1
5,1
4,1
3,1
2,1
1,1
1
P
P
P
P
P
P
P
RM  
STEP3: according to Definition 5 and Theorem 2,  
calculate the support degree of the GM1. 
P1?1°GM?[1000000]°GM=?
=
?
11
1
6,1
j
jGMP =9 
P1?2°GM?[0100000]°GM=?
=
?
11
1
6,1
j
jGMP =6 
P1?3°GM?[0010000]°GM=?
=
?
11
1
6,1
j
jGMP =7 
P1?4°GM?[0001000]°GM=?
=
?
11
1
6,1
j
jGMP =3 
P1?5°GM?[0000100]°GM=?
=
?
11
1
6,1
j
jGMP =2 
P1?6°GM?[0000010]°GM=?
=
?
11
1
6,1
j
jGMP =1 
P1?7°GM?[0000001]°GM=?
=
?
11
1
6,1
j
jGMP =1 
STEP4: If the association-characteristic granule whose 
support number is below 2(e.g. P1?6 and P1?7), it will be 
deleted from matrix. And then obtains a new matrix RX1' 
as the follows: 
??
?
?
?
??
?
?
?
=
0000100
0001000
0010000
0100000
1000000
'
1RX  
STEP5: According to Theorem 3, obtain Two item 
association-characteristic information granule matrix 
RM2 and two item association-characteristic information 
granule matrix RM2' of association-rule(as shown in 
Figure 1 and Figure 2). Then obtain the RM3, RM3' ,RM4 
and RM4' as the same way(as shown in Figure 3,4,5,6).It 
814
  
is shown that the RM4' is empty, which means, RM3' is 
the target. 
??
??
??
?
?
?
??
??
??
?
?
?
=
0001100
0010100
0011000
0100100
0101000
0110000
1000100
1001000
1010000
1100000
2RX
 
Figure 1:RM2  
 
??
??
??
?
?
??
??
??
?
?
=
0001100
0100100
0101000
0110000
1000100
1001000
1010000
1100000
'
2RX  
Figure 2: RM2'  
 
??
??
??
??
?
?
??
??
??
??
?
?
=
0101100
0110100
0111000
1001100
1101000
1001100
1010100
1011000
1100100
1101000
1110000
3RX  
Figure 3: RM3 
 
??
???
?
=
1011000
1101000
1110000'
3RX  
Figure 4: RM3'  
 
 
[ ]11110004 =RX  
Figure 5: RM4 
 
[ ]?='4RX  
Figure 6: RM4' 
Since |RX4'|?0, RX3 is the target , and from RX3 we can 
get 3 association rules: {a,b,e:2}?{a,b,d:2}?{a,b,c:2}  
STEP6: assume the value of minimum-confidence is 
30??according to Theorem 5, calculates the decision-
making rules. Obtains the decision-making rule as the 
following process: 
{a,b,e:2}? p(be|a)=2/9  , p(ae|b)=2/6  , p(ab|e)=2/2  , 
p(a|be)= 2 /2  p(b|ae)= 2/2, p(e|ab)= 2/5. 
{a,b,d:2}? p(bd|a)= 2/9 , p(ab|d)=2/3  , p(ad|b)=2/6  , 
p(a|bd)= 2/2   p(d|ab)= 2/5 p(b|ad)=2/3. 
{a,b,c:2}? p(bc|a)= 2/9 , p(ab|c)=2/7  , p(ac|b)=2/6  , 
p(a|bc)= 2/3   p(b|ac)=2/5  p(c|ab)=2/5. 
Then delete the one whose support degree is below 30% 
and get the final result: 
(ae ? ? ? ??b) , (ab?e) , (a?be) (b?ae) (e?ab) (ab?d)  
(ad ??b)  
(a ? ? ? ? ??bd) (d?ab) (b?ad) (ac?b) (a?bc) (b?ac)
? ?(c?ab)  
6.  Algorithm Analysis?  
   To show the advancements of this algorithm, 
experiment is taken under the environment of Intel core 
2.0 GHz, 3 GB memory, Windows7 operation system, 
and the programming language is C++. The screenshot 
of the experiment is Figure 7.  
 
 
Figure 7 The screenshot of the experiment 
 
   The system use dataset which is collected from a 
financial company. There are 5000 records and 90 
attributes in database. 
    Compares the algorithm in our artical with Apriori 
algorithm, figure 8 offers distinguish of execution time 
between the two algorithms when their support degrees 
are different. Fgure 9 shows that, the less support degree 
is, the shorter execution time of our algorithm will be, it 
figures out that our algorithm will be much faster when 
the support degree is small, and it will be more effective 
when the records in database are huge. 
0
2
4
6
8
10
20% 30% 40% 50%
Classical 
Approach
Article 
Approach
 
Figure 8, comparison of time cost while the support 
degrees are different  
 
815
  
0
10
20
30
40
50
60
70
20% 45% 70% 100%
Classical 
Approach
Article 
Approach
 
Figure 9, comparison of time cost while the records 
are different  
 
The algorithm that we presented in the article does 
not need to store all the candidate itemsets, it just needs 
to save boolean type “0” and “1” as bit mode. As a 
result, this will save more memory, and with better 
spatial features. 
  
7.  Conclusion 
    Under the background of granule computing, our 
research team present a new algorithm to solve the 
problems of association-rule and decision-making rule 
in data mining. The results of the experiments prove that 
our algorithm is feasible and efficient. There are still a 
lot of work to do to improve the efficiency of our 
algorithm, such as the reduction of the granule matrix in 
step 5. Overall, the work of research show the 
promising future of granule computing. 
 
 
 
8. References 
[1] Duoqian miao Guoyin,wang .Granulation computing: the 
past?now and  futrue[M].peiking: ,2007:pp154~160 
[2] qing liu. RoughRough,the set inference[M].peking:sicience 
publishing house ,2001:pp100~116 
[3] Zadeh, Lotfi A. Granular computing and rough set 
theory[C] International Conference on Rough Sets and 
Intelligent Systems Paradigms, RSEISP 2007?Warsaw, 
Poland?pp.1~4 
[4] Xie K M? ?Chen ZH Xie G,et al.BGRC for superheated 
steam temperature system modeling in power plant[C]The 
2006IEEE International Confedence on Granular 
Computing,,Atlanta,USA,2006: pp708~711 
[5] Qing Liu, S.L.Jiang.Reasoning about Information Granules 
Based on Rough Logic[C]. RSCTC 2002,LNAI 2475, 
pp.139~143 
[6] Yiyu Yao.Granular Computing and Congnitive 
Information[C]. ICCI2006,Vol.I,pp17 ~18 
 
 
 
 
816
