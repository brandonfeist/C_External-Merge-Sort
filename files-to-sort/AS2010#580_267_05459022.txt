Numeric Missing Value’s Hot Deck Imputation 
Based on Cloud Model and Association Rules 
 
Wang Zhao-hong 
Department of Computer Science and Technology 
Weifang University 
Weifang, China 
wangzhhwfxy@163.com 
 
 
Abstract—Filling missing value is main task of data-processing, at 
present Hot Deck Imputation is preferred. Defining the similar 
standard of Hot Deck Imputation objectively becomes an 
important prerequisite. The Cloud model combines ambiguity 
and randomness organically to fit the real world data objectively. 
first get the cloud models which present the raw no missing value,  
then to discrete the numeric value and do the association rules 
mining in the discrete value to get the knowledge base, filling the 
missing value with the value which generated by the cloud model 
from the knowledge base. The method considered the original 
data’s distribution as a whole and to improve its precision with 
association rules from the raw data for each record, it simulates 
the humans’ behavior; this method has smaller absolute mean 
difference than other methods. 
Keywords- Missing value; Cloud model; Association rules; Hot 
Deck Imputation  
I.  INTRODUCTION 
Data in the database that contains noise, the missing data, 
and ambiguity will affect the validity of data mining, 
knowledge discovery model should have a certain degree of 
fault tolerance, it is urgent to find a mechanism to handle 
missing value efficiently and accurately. Dealing with 
incomplete data sets methods can be divided into the following 
three categories: remove the tuples, data filling, and do the data 
mining with the missing data. 
A. Delete the tuples 
Delete the tuples with missing attribute values to get the 
information completeness by reducing the historical data 
amount, discarded a lot of hidden information in these objects. 
When missing data is a larger proportion of the data, 
particularly there is non-random distribution of missing data; 
this approach may lead to deviation, which leads to wrong 
conclusions [1]. 
B. Data Filling  
This method is used to fill the missing values with a certain 
value. Usually based on statistical theory, according to the 
decision table the remaining values’ distribution to fill a 
missing value, there are several commonly used filling method 
[2], [3] in data mining. 
1) filling manually 
The user fill the missing data, this method is time-
consuming. It is not feasible with large-scale data or a huge 
number of missing values. 
2) Treating Missing Attribute values as Special values 
 Deal with the missing value as a special value, [4] 
indicate that the method may lead to serious data deviation. 
Generally these kinds of methods are not recommended. 
3) Mean/Mode Complete 
 If the missing value is numeric, to fill the missing value 
with the average of all the other non missing value; if the 
missing value is not numeric, to fill the missing value with the 
most likely value in all the other tuples without missing value. 
Reference [1] thinks in real applications, generally people 
have very little prior knowledge with the owned data, so this 
approach is obviously not satisfactory. 
4)  Hot deck imputation 
To fill the missing values with its most similar values in 
all the data set. The concept of this method is very simple, and 
to estimate the missing value using relationship between the 
data of a tuple. At present [5], [6], [7] think that the hot deck 
method is superior to other method. 
There are several other filling methods such as EM 
(Expectation maximization), and MI (Multiple Imputation) 
and so on, these method are relatively complex, and the EM 
method convergence rate is slow, time complexity is large. 
From the use of the frequency and extent of research, 
now, generally [8] think that filling method is the most 
commonly used method to deal with missing values. Its main 
disadvantage is difficult to define similar standards, there are 
more subjective factors. 
Cloud model can transform qualitative and quantitative 
each other, considering the original data distribution in the 
database, using the cloud model to discrete non-missing values 
of database, then use the association rules to generate 
knowledge base with non-missing value tuples, when fill the 
missing value, first search  knowledge base to find associated 
knowledge, then use association rules to find the 
corresponding numerical cloud model, to fill missing value 
with the cloud generator, the model itself has the universal 
nature of social phenomena and nature, the filled data has the 
same distribution with the original data, the association rules 
This work was supported by the Scientific and technological 
development projects of Shandong Province (Grant Nos.
2008GG30001030), Natural Science Fund Project of Weifang 
University (Grant Nos. 2008Z08) 
2010 Second International Workshop on Education Technology and Computer Science
978-0-7695-3987-4/10 $26.00 © 2010 IEEE
DOI 10.1109/ETCS.2010.299
238
 limit makes filled value of a record relevant to the other 
attribute information of itself, so the similarity of Hot deck 
imputation are defined objectively. 
II. CLOUD MODEL 
Cloud model is a qualitative and quantitative conversion 
model; it combines ambiguity and randomness organically. 
Set U is a mathematical domain U={x}, T is the language 
value associated with the U. ?T (x) is a stable tendency 
random number which expressed the elements x subordination 
of T concept, subordination’s distribution in the domain is 
known as the cloud [9]. 
Fig 1 presents a cloud model; cloud mathematical 
expected curve is its subordination curves from the view of 
fuzzy set theory. However, "thickness" of the curve is uneven, 
waist is the most scattered, the top and bottom of the curve are 
convergent, cloud's "thick" reflects the subordination degree 
randomness, near to or away from the concept center have 
smaller subordination randomness, while concept center have 
the largest subordination randomness, which is consistent with 
people's subjective feelings. 
 
Figure 1. The digital features of the normal cloud  
The digital features of the normal cloud characterized by 
three values with the expectation Ex, entropy En, excess 
entropy He) (see Fig 1), the expected value Ex: the center value 
of the concept domain, is the most representative qualitative 
value of the concept, it should be 100% belongs to the concept; 
entropy En: is a qualitative measure of the concept’s ambiguity, 
reflecting the accepted range values of the concept domain; 
hyper entropy He: can be described as entropy En of entropy, 
reflecting the degree of dispersion of the cloud droplets. The 
normal cloud is the most important cloud model, because 
various branches of the social and natural sciences have proved 
the normal distribution’s universality. The equation of normal 
cloud curve: 
 e 2En 2
Ex)(? 2
)(MEC A
 P    DE F  
Expectation curve is a normal curve, for a qualitative 
knowledge, the elements outside the Ex r 3En in its 
corresponding cloud model all can be ignored, because it has 
been proved that approximately 99.74% elements of model fall 
into the range of Ex r 3En by the mathematical characteristics 
of normal distribution. Extending the normal cloud model to 
get trapezoidal cloud model, trapezoidal cloud model can be 
expressed by six values, they are the expected number: Ex1 and 
Ex2, entropy: En1 and En2, hyper entropy: He1 and He2. 
Trapezoidal cloud curve equations are determined by the 
expectation and the Entropy are following:

 
 
 
A
A
A
2(x Ex1)
2 1 3 1 12En1
Ex1<=x<=Ex2
2(x Ex2)
2 2 2 3 22En2
MEC (x)
MEC (x) = 1
MEC (x)
e
e
Ex En x Ex
Ex x Ex En
    
   
 
 

Clearly, the left and the right half-cloud expectation curve 
is a normal curve.  
It can complete the qualitative and quantitative transform 
more accurately, if there is a range belongs to the concept 
totally, then it can be expressed by the upper edge of Trapezoid, 
if only one value belongs to the concept totally, then the upper 
edge of Trapezoid degenerate to a point, trapezoidal cloud 
model also degenerated into the normal cloud model. He1 and 
He2 can have different values, and thus the concept of the 
border on behalf of different fuzzy situation, when the He1 and 
He2 all degenerate to 0, trapezoidal cloud model expressed a 
concept with accurate border subordination, when one of the 
He1 or He2 degenerate to 0, which expressed a concept with 
one r accurate border subordination and one vague border 
subordination, so trapezoidal cloud model has a better 
generality. 
III. GENERATED NUMERICAL CHARACTERISTICS OF CLOUDS 
WITH EXISTING NON-EMPTY ORIGINAL DATA 
A. Cloud Transform
Any function can be decomposed into cloud-based 
superposition with allowed error range, which is Cloud 
Transform. The equation is:
)()(
1
xfxg j
m
j
jc¦
 
|

))()(0(
1
H ¦
 
xfxg jj
m
j
c

g(x): data distribution function 
fj(x): cloud-based expectations function  
cj :coefficients 
m: the number of superimposed cloud,  
?: user-defined maximum error 
From the concept of clouds: in the domain the element’s 
subordination to the concept has statistical and random 
properties. In addition, the high-frequency elements’ 
contributions to the concept are higher than the low-frequency 
elements. That is the reason to use probability density function 
of data distribution to get the concept set, so the concept 
division algorithms can be done. 
According to the definition of cloud transform, the 
quantitative attribute’s domain dividing into m concepts can 
evolve to a problem to get answers from the formula:  
239
¦  
 
m
j
ijjjjjjij HeHeEnEnExExfxg c
1
)2,1,2,1,2,1()( H
I.e. to get 
Ex1j, Ex2j, En1j, En2j, He1j, He2j and cj for each cloud concept, 
the quantitative attribute domain is divided into a number of 
concepts by using cloud model, the data in each concept 
aggregate, and the data between different concepts separated. 
B. The original non-empty data discrete algorithm 
Counting non-missing values of the original data to get the 
histogram of data distribution, local peak of the histogram is 
that the data aggregation part, taking it as a concept center is 
reasonable, the higher the peak, indicating more data 
convergence there, deal with it with priority. The discrete 
grouping algorithm is:
Algorithm 1: Discrete algorithm  
Input: the domain of property that need the concept 
division, the property values attributes i corresponds to them, 
the overall error threshold ?, and the peak height error 
threshold ?y, the length error ?x between trapezoidal top edge 
and the minimum value . 
Output: m concepts and the corresponding digital features 
of attribute i.  
{ (1) Count the each possible values x of attribute i and get the 
actual data distribution function g (x); 
(2) j=0; 
(3)Clouds=)?g’(x)=g(x); 
(4) while max(g’(x))>? 
(5)   Exj=Find_Ex(g’(x)); 
(6)    Ex1j=search1(g’(x),?y,?x); 
(7)    Ex2j=search2(g’(x),?y,?x); 
(8)    En1j=Find_En(cj,Ex1j,?); 
(9)    En2j=Find_En(cj,Ex2j,?); 
(10)   gj(x)=cj*Cloud(Ex1,En1j,Ex2j,En2j); 
(11)   g’(x)=g’(x)-gj(x); 
(12)   j=j+1; 
(13) endwhile 
(14) for j=0 to m-1 do 
(15) Clouds(Ex1j,=Ex2j,En1j,En2j,He1j,He2j)=Calculate_ 
He(gj(x), g’(x),Cloud(Ex1j,Ex2j,En1j,En2j)); 
(16) endfor} 
In Step 1, using statistical methods to get the actual data 
distribution function g (x); Step 2, 3 does variable 
initialization; Step 4 the division of the process is ended, if the 
error limit less than a given error, Step 5 Search for the peak 
value of cj of property in the data distribution function g(x), 
and its corresponding value x is defined as the cloud model 
center (expectation); Step 6, 7 search approximate horizon line 
near the peak (within the error limit threshold ?y), if the width 
is greater than the minimum width of the threshold value ?x, 
where were identified as uniformly distributed, the two 
endpoints of the line are recorded as the trapezoidal top edge 
endpoints Ex1j, Ex2j; otherwise get the trapezoidal top edge 
endpoints are equal to the peak point value Ex1j=Ex2j=Exj, 
trapezoidal cloud degenerated into the normal cloud. 
Trapezoidal cloud height coefficient is the function value of 
the Ex1j or Ex2j. The step 8, 9 calculate cloud model entropy 
En1j and En2j to fit g (x) for the half-liter cloud with Ex1j, 
half-falling cloud with Ex2j. to get En1j searching left area of 
the cloud model with Ex1j, to get En2j searching right area of 
the cloud model with Ex2j, the entropy value increase from 0 
step from the smaller value until the threshold ? is greater than 
the difference between the half-normal cloud value and 
distribution histogram value; the step 10 calculate distribution 
function of the corresponding Trapezoidal; step 11 use the 
original data minus the known distribution function of 
trapezoidal cloud model data distribution to get the new data 
distribution function g'(x); Repeat step 4 to 12 until the peak 
value is less than the error threshold. Step 14, 15, 16 
determine half hyper entropy of all cloud model with the 
residuals of distribution histogram. 
Through the above processing, the non-missing numeric 
values are divided into several cloud models, the non-empty 
numerical attribute can be discrete based on their 
subordination to the cloud model. The data after discretization 
can be easily carried out Boolean association rule mining. 
IV. ASSOCIATION RULE MINING 
Association rule mining refers mainly to get the 
knowledge such as "customers bought tea also purchased the 
coffee," which meet the minimum support and the minimum 
confidence. At present association rules can be divided into 
two types: Boolean association rules and quantitative 
association rules, and most of the research are focused on the 
Boolean association rules research. 
First, give the formal description of association rules 
mining: Suppose I = (i1, i2... im) is a collection of m different 
items, T = (t1, t2... tn) is a transaction database, tj is a group of 
items of I set, tj?I. Each transaction with a unique identifier 
TID linked. If X is a subset of I with X?tj, we say that a 
transaction contains X. An association rule is a "X?Y" 
implication, in which X?I, Y?I, and X?Y=?.  
Definition 1: If the ratio of transaction T contains X Y is ?
Sup, the association rules X?Y in T has a support degree Sup. 
                      
%100
n
y)support(xSup u 
                          
Support (X Y) stands for the number which support ?
X Y transactions, n stands for the total number of ?
transactions.  
Definition 2: If the ratio transaction T contains X also contains 
Y is Conf, then the confidence level of association rules X?Y 
in T is Conf. 

%100
support(x)
y)support(xConf u 

Association rules are called the strong association rules if 
they meet the minimum support and minimum confidence. 
Discrete grouping algorithm divided non-empty original 
quantitative attribute into multiple concepts, if one attribute 
value is mapped to a different concept, mapped to the concept 
whose subordination value is the greatest. A quantity data 
corresponding to the original meaning is to buy not to buy tea, 
through the concept discrete algorithm is now extended to buy 
very much more tea, buy a lot of tea buy a few tea. The 
question naturally converted to Boolean association rule 
240
mining, using the famous Apriori algorithm which generated 
frequent item sets and FP-Growth algorithm which does not 
produce the frequent item set to generate association rules 
database, which contains knowledge like <tea, 
more>?<coffee, little> knowledge. 
V. GENERATE THE MISSING VALUE WITH CLOUD MODEL 
AND ASSOCIATION RULES 
Use three digits values of cloud model as a basis to fill the 
cloud droplets, the cloud droplets and the original data which 
generated three digits values of the cloud model are in the same 
distribution, characteristics the same data group, with these 
data to fill the missing values. This method combined 
organically the fuzzy nature and the randomness, which has a 
better exactness matching the human society behavioral 
characteristics. The accuracy of each record is limited by 
association rules, such as <tea, more>?<coffee, very little>, 
that is, tea cloud model if it is "more", then the missing value to 
fill the coffee just use the "few". Generate missing value to fill 
the value is to make the greatest possible reduction of the raw 
data. 
Algorithm 2: The missing value reduction algorithm 
Input: association rule knowledge base, and the original 
database with a quantitative missing value 
Output: the database with no quantitative missing value  
{(1) Locate the records with missing value;  
(2) Find association rule "A?B" according to record’s non-
missing attribute A in the Knowledge Base  
(3) Get cloud model digital features (Ex, En, He) from B  
(4) Generate the normal random number En' with En as 
expectation, He as mean square error;  
(5) Generate the normal random number x with Ex as 
expectation, En' as mean square error to fill the missing value;  
(6) Repeat steps 1-5 until the required number of missing 
values produced.} 
VI. EXPERIMENTAL ANALYSIS 
Take two typical supermarket food rice and bean as 
example, the majority retail sales data scattered between 
0.5-5kg, Extract 6000 transaction records as experiment 
data, set empty value in 600 records use random method, 
respectively (1) filling manually; (2) Treating Missing 
Attribute values as Special values; (3) Mean/Mode 
Complete; (4) Hot deck imputation: is given in this article 
based on cloud model and association rules. Calculate 
Absolute mean difference between the fill value and the 
original value of various methods, the smaller absolute 
mean difference, the higher the method accuracy. The Data 
are showed in table 1. 
TABLE I. EXPERIMENTAL DATA ANALYSIS  
Method Method 1 Method 2 Method 3 Method 4
Absolute mean difference 0.463 1.391 0.562 0.486 
Seen from the experimental results, filling manually 
method’s overall error is not large, because the people know 
actual fact well, but it cost too much time. Method 2 and 
method 3 change the raw data distribution, they are 
meaningfulness. Methods 4 take into account both the 
distribution of the raw data, but also each record data 
correlation, so the method has high accuracy, to improve its 
accuracy further, reduce discrete particle size of the original 
non-empty data, and lower the minimum support for 
association rules and, but this will take more time, the 
minimum support degree and the minimum confidence degree 
shouldn’t too small, if so, the association rules does not make 
sense, the minimum support degree and the minimum 
confidence degree are given by the experts in the field. 
VII. CONCLUSION 
Filling an empty value based on cloud models and 
association rules first consider the distribution of the original 
data, use the cloud model to discrete the original non-missing 
data, the do the data association rules mining with the Boolean 
data and get knowledge base. Use the knowledge in 
knowledge base and the numerical cloud model features to fill 
the empty values, the missing value substitute have the same 
characteristics with the cloud model that generated the concept 
of the original data. The individual differences reflected the 
double features in ambiguity and randomness; it simulates the 
real-world general similarities and slight differences of the 
same group. For each record, use association rules in the 
knowledge base to enhance the substitute precision of each 
record further, the accuracy of the method is higher than 
others. 
REFERENCES 
[1] M.l. Brown and J.F. Kros, Industrial Management and Data Systems, vol. 
103, 2003,  pp.611-621.   
[2] X.L. Huang, A pseudo-nearest-neighbor approach for missing data 
recovery on Gaussian random data sets?Pattern Recognition Letters? 
vol. 23, 2002, pp.1613-1622? 
[3] J.W. Grzymala-Busseand M. Fu, A comparison of several approaches to 
missing attribute values in data mining. In: Proc of the 2nd Int’ Conf on 
Rough Sets and Current Trends in Computing. Berlin: Springer-Verlag, 
2000, pp. 378-385. 
[4] R. K. Pearson, The Problem of Disguised Missing Data ACM SIGKDD 
Explorations Newsletter, Vol. 8, Issue 1, June 2006? 
[5] P. Jonsson and W. Claes, An evaluation of k-nearest neighbour 
imputation using likeUt data, Proceedings-10th international symposium 
on software metrics, 2004. 
[6] J. Yu, General C-means clustering model. IEEE Tranctions on Pattern 
Analysis and Machine Intelligence, 2005, vol. 27l, pp. 1197-1211. 
[7] S.C. Zhang,  Z.X. Qin, S.L. Shengand C.L Ling, “missing is useful”: 
missing values in cost-sensitive decision trees. IEEE Transactions on 
Knowledge and Data Engineering, 2005, Vol.17 pp.1012-1016.? 
[8] J. Han, and M. Kamber, Data Mining Concepts and Techniques, 2nd ed., 
Morgan Kaufmann Publishers: 2006, pp.98-112? 
[9] Li De-yi and Liu Chang-yu, Discussion of the universal nature on the 
normal cloud model. China Engineering Science, 2004,vol. 6,  pp. 28-33. 
[10] Mehmed Kantardzic, Datamining Concepts, Models, Methods and 
Algorithms, Beijing: Tsinghua University Press, 2003? 
[11] J Han, J Pei, Y Yin, Mining Frequent Patterns without Candidate 
Generation A Frequent Pattren Tree Approach, data Mining and 
knowledge discovery, 2004, vol. 8,  pp.53-87? 
241
